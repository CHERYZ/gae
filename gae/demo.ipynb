{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:58: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:61: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\initializations.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:33: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:33: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:84: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:12: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:13: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:97: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:98: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.77950 train_acc= 0.01315 val_roc= 0.67342 val_ap= 0.71392 time= 0.47741\n",
      "Epoch: 0002 train_loss= 0.94929 train_acc= 0.00159 val_roc= 0.73665 val_ap= 0.76481 time= 0.30489\n",
      "Epoch: 0003 train_loss= 0.73620 train_acc= 0.00759 val_roc= 0.71591 val_ap= 0.73580 time= 0.31009\n",
      "Epoch: 0004 train_loss= 0.73241 train_acc= 0.01108 val_roc= 0.71188 val_ap= 0.73794 time= 0.28825\n",
      "Epoch: 0005 train_loss= 0.74392 train_acc= 0.00416 val_roc= 0.75580 val_ap= 0.77108 time= 0.30125\n",
      "Epoch: 0006 train_loss= 0.71695 train_acc= 0.01092 val_roc= 0.80283 val_ap= 0.79826 time= 0.28996\n",
      "Epoch: 0007 train_loss= 0.69735 train_acc= 0.04581 val_roc= 0.82645 val_ap= 0.80259 time= 0.31670\n",
      "Epoch: 0008 train_loss= 0.67990 train_acc= 0.14000 val_roc= 0.83198 val_ap= 0.80532 time= 0.31329\n",
      "Epoch: 0009 train_loss= 0.65770 train_acc= 0.23100 val_roc= 0.83211 val_ap= 0.81156 time= 0.29532\n",
      "Epoch: 0010 train_loss= 0.63344 train_acc= 0.29418 val_roc= 0.83642 val_ap= 0.82136 time= 0.32132\n",
      "Epoch: 0011 train_loss= 0.61226 train_acc= 0.33864 val_roc= 0.84257 val_ap= 0.82964 time= 0.30155\n",
      "Epoch: 0012 train_loss= 0.59554 train_acc= 0.37768 val_roc= 0.84850 val_ap= 0.83541 time= 0.31324\n",
      "Epoch: 0013 train_loss= 0.58160 train_acc= 0.41601 val_roc= 0.85204 val_ap= 0.83773 time= 0.28826\n",
      "Epoch: 0014 train_loss= 0.57035 train_acc= 0.44924 val_roc= 0.85560 val_ap= 0.84166 time= 0.29470\n",
      "Epoch: 0015 train_loss= 0.56119 train_acc= 0.47466 val_roc= 0.86072 val_ap= 0.84826 time= 0.30167\n",
      "Epoch: 0016 train_loss= 0.55198 train_acc= 0.49126 val_roc= 0.86857 val_ap= 0.85903 time= 0.30864\n",
      "Epoch: 0017 train_loss= 0.54235 train_acc= 0.50046 val_roc= 0.87717 val_ap= 0.87066 time= 0.31754\n",
      "Epoch: 0018 train_loss= 0.53378 train_acc= 0.50398 val_roc= 0.88535 val_ap= 0.88124 time= 0.31197\n",
      "Epoch: 0019 train_loss= 0.52758 train_acc= 0.50445 val_roc= 0.89153 val_ap= 0.88869 time= 0.32281\n",
      "Epoch: 0020 train_loss= 0.52303 train_acc= 0.50493 val_roc= 0.89372 val_ap= 0.89203 time= 0.30361\n",
      "Epoch: 0021 train_loss= 0.51907 train_acc= 0.50699 val_roc= 0.89521 val_ap= 0.89469 time= 0.29771\n",
      "Epoch: 0022 train_loss= 0.51556 train_acc= 0.51052 val_roc= 0.89569 val_ap= 0.89682 time= 0.31362\n",
      "Epoch: 0023 train_loss= 0.51261 train_acc= 0.51418 val_roc= 0.89657 val_ap= 0.89811 time= 0.30484\n",
      "Epoch: 0024 train_loss= 0.50996 train_acc= 0.51712 val_roc= 0.89864 val_ap= 0.90128 time= 0.31408\n",
      "Epoch: 0025 train_loss= 0.50732 train_acc= 0.51948 val_roc= 0.90139 val_ap= 0.90486 time= 0.30920\n",
      "Epoch: 0026 train_loss= 0.50465 train_acc= 0.52119 val_roc= 0.90449 val_ap= 0.90801 time= 0.30671\n",
      "Epoch: 0027 train_loss= 0.50200 train_acc= 0.52229 val_roc= 0.90663 val_ap= 0.91017 time= 0.31127\n",
      "Epoch: 0028 train_loss= 0.49946 train_acc= 0.52317 val_roc= 0.90862 val_ap= 0.91234 time= 0.30573\n",
      "Epoch: 0029 train_loss= 0.49701 train_acc= 0.52439 val_roc= 0.90993 val_ap= 0.91360 time= 0.30168\n",
      "Epoch: 0030 train_loss= 0.49465 train_acc= 0.52648 val_roc= 0.91080 val_ap= 0.91435 time= 0.30070\n",
      "Epoch: 0031 train_loss= 0.49239 train_acc= 0.52937 val_roc= 0.91100 val_ap= 0.91466 time= 0.31162\n",
      "Epoch: 0032 train_loss= 0.49022 train_acc= 0.53277 val_roc= 0.91130 val_ap= 0.91526 time= 0.30488\n",
      "Epoch: 0033 train_loss= 0.48807 train_acc= 0.53624 val_roc= 0.91237 val_ap= 0.91655 time= 0.30582\n",
      "Epoch: 0034 train_loss= 0.48586 train_acc= 0.53944 val_roc= 0.91333 val_ap= 0.91750 time= 0.30730\n",
      "Epoch: 0035 train_loss= 0.48365 train_acc= 0.54251 val_roc= 0.91526 val_ap= 0.91919 time= 0.31132\n",
      "Epoch: 0036 train_loss= 0.48167 train_acc= 0.54485 val_roc= 0.91665 val_ap= 0.92051 time= 0.27843\n",
      "Epoch: 0037 train_loss= 0.48016 train_acc= 0.54618 val_roc= 0.91774 val_ap= 0.92143 time= 0.30406\n",
      "Epoch: 0038 train_loss= 0.47899 train_acc= 0.54681 val_roc= 0.91837 val_ap= 0.92182 time= 0.30687\n",
      "Epoch: 0039 train_loss= 0.47783 train_acc= 0.54750 val_roc= 0.91872 val_ap= 0.92180 time= 0.31228\n",
      "Epoch: 0040 train_loss= 0.47660 train_acc= 0.54840 val_roc= 0.91866 val_ap= 0.92135 time= 0.32548\n",
      "Epoch: 0041 train_loss= 0.47552 train_acc= 0.54915 val_roc= 0.91933 val_ap= 0.92201 time= 0.31424\n",
      "Epoch: 0042 train_loss= 0.47459 train_acc= 0.54972 val_roc= 0.92031 val_ap= 0.92280 time= 0.30687\n",
      "Epoch: 0043 train_loss= 0.47352 train_acc= 0.54992 val_roc= 0.92192 val_ap= 0.92452 time= 0.32686\n",
      "Epoch: 0044 train_loss= 0.47223 train_acc= 0.55012 val_roc= 0.92288 val_ap= 0.92524 time= 0.31015\n",
      "Epoch: 0045 train_loss= 0.47093 train_acc= 0.55028 val_roc= 0.92362 val_ap= 0.92592 time= 0.30768\n",
      "Epoch: 0046 train_loss= 0.46972 train_acc= 0.55084 val_roc= 0.92419 val_ap= 0.92667 time= 0.30639\n",
      "Epoch: 0047 train_loss= 0.46851 train_acc= 0.55194 val_roc= 0.92429 val_ap= 0.92696 time= 0.31053\n",
      "Epoch: 0048 train_loss= 0.46732 train_acc= 0.55336 val_roc= 0.92468 val_ap= 0.92750 time= 0.31067\n",
      "Epoch: 0049 train_loss= 0.46624 train_acc= 0.55478 val_roc= 0.92520 val_ap= 0.92806 time= 0.30617\n",
      "Epoch: 0050 train_loss= 0.46522 train_acc= 0.55594 val_roc= 0.92615 val_ap= 0.92903 time= 0.30958\n",
      "Epoch: 0051 train_loss= 0.46419 train_acc= 0.55683 val_roc= 0.92728 val_ap= 0.93025 time= 0.32707\n",
      "Epoch: 0052 train_loss= 0.46318 train_acc= 0.55755 val_roc= 0.92818 val_ap= 0.93112 time= 0.31823\n",
      "Epoch: 0053 train_loss= 0.46221 train_acc= 0.55813 val_roc= 0.92907 val_ap= 0.93206 time= 0.29440\n",
      "Epoch: 0054 train_loss= 0.46127 train_acc= 0.55876 val_roc= 0.92980 val_ap= 0.93262 time= 0.31211\n",
      "Epoch: 0055 train_loss= 0.46029 train_acc= 0.55963 val_roc= 0.93020 val_ap= 0.93300 time= 0.31864\n",
      "Epoch: 0056 train_loss= 0.45927 train_acc= 0.56044 val_roc= 0.93053 val_ap= 0.93348 time= 0.31069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0057 train_loss= 0.45824 train_acc= 0.56136 val_roc= 0.93124 val_ap= 0.93432 time= 0.33385\n",
      "Epoch: 0058 train_loss= 0.45718 train_acc= 0.56223 val_roc= 0.93193 val_ap= 0.93559 time= 0.30570\n",
      "Epoch: 0059 train_loss= 0.45614 train_acc= 0.56305 val_roc= 0.93282 val_ap= 0.93675 time= 0.29570\n",
      "Epoch: 0060 train_loss= 0.45516 train_acc= 0.56364 val_roc= 0.93353 val_ap= 0.93770 time= 0.29817\n",
      "Epoch: 0061 train_loss= 0.45424 train_acc= 0.56410 val_roc= 0.93415 val_ap= 0.93845 time= 0.31050\n",
      "Epoch: 0062 train_loss= 0.45331 train_acc= 0.56451 val_roc= 0.93497 val_ap= 0.93944 time= 0.30696\n",
      "Epoch: 0063 train_loss= 0.45238 train_acc= 0.56502 val_roc= 0.93539 val_ap= 0.94001 time= 0.33679\n",
      "Epoch: 0064 train_loss= 0.45149 train_acc= 0.56573 val_roc= 0.93601 val_ap= 0.94075 time= 0.30776\n",
      "Epoch: 0065 train_loss= 0.45065 train_acc= 0.56644 val_roc= 0.93662 val_ap= 0.94155 time= 0.30876\n",
      "Epoch: 0066 train_loss= 0.44983 train_acc= 0.56713 val_roc= 0.93669 val_ap= 0.94173 time= 0.30957\n",
      "Epoch: 0067 train_loss= 0.44905 train_acc= 0.56780 val_roc= 0.93727 val_ap= 0.94250 time= 0.31096\n",
      "Epoch: 0068 train_loss= 0.44830 train_acc= 0.56850 val_roc= 0.93791 val_ap= 0.94320 time= 0.32923\n",
      "Epoch: 0069 train_loss= 0.44755 train_acc= 0.56921 val_roc= 0.93801 val_ap= 0.94360 time= 0.30566\n",
      "Epoch: 0070 train_loss= 0.44682 train_acc= 0.56997 val_roc= 0.93821 val_ap= 0.94414 time= 0.29901\n",
      "Epoch: 0071 train_loss= 0.44611 train_acc= 0.57063 val_roc= 0.93838 val_ap= 0.94469 time= 0.30925\n",
      "Epoch: 0072 train_loss= 0.44538 train_acc= 0.57123 val_roc= 0.93873 val_ap= 0.94536 time= 0.32322\n",
      "Epoch: 0073 train_loss= 0.44465 train_acc= 0.57173 val_roc= 0.93873 val_ap= 0.94567 time= 0.29669\n",
      "Epoch: 0074 train_loss= 0.44394 train_acc= 0.57218 val_roc= 0.93885 val_ap= 0.94610 time= 0.29916\n",
      "Epoch: 0075 train_loss= 0.44325 train_acc= 0.57250 val_roc= 0.93893 val_ap= 0.94649 time= 0.31471\n",
      "Epoch: 0076 train_loss= 0.44256 train_acc= 0.57284 val_roc= 0.93887 val_ap= 0.94672 time= 0.31451\n",
      "Epoch: 0077 train_loss= 0.44187 train_acc= 0.57315 val_roc= 0.93861 val_ap= 0.94678 time= 0.30812\n",
      "Epoch: 0078 train_loss= 0.44119 train_acc= 0.57344 val_roc= 0.93846 val_ap= 0.94666 time= 0.34537\n",
      "Epoch: 0079 train_loss= 0.44050 train_acc= 0.57378 val_roc= 0.93860 val_ap= 0.94702 time= 0.32664\n",
      "Epoch: 0080 train_loss= 0.43980 train_acc= 0.57415 val_roc= 0.93877 val_ap= 0.94727 time= 0.31927\n",
      "Epoch: 0081 train_loss= 0.43911 train_acc= 0.57461 val_roc= 0.93876 val_ap= 0.94743 time= 0.31163\n",
      "Epoch: 0082 train_loss= 0.43842 train_acc= 0.57510 val_roc= 0.93876 val_ap= 0.94756 time= 0.31959\n",
      "Epoch: 0083 train_loss= 0.43772 train_acc= 0.57562 val_roc= 0.93851 val_ap= 0.94752 time= 0.29871\n",
      "Epoch: 0084 train_loss= 0.43704 train_acc= 0.57630 val_roc= 0.93818 val_ap= 0.94738 time= 0.29801\n",
      "Epoch: 0085 train_loss= 0.43637 train_acc= 0.57704 val_roc= 0.93791 val_ap= 0.94735 time= 0.31438\n",
      "Epoch: 0086 train_loss= 0.43571 train_acc= 0.57792 val_roc= 0.93780 val_ap= 0.94751 time= 0.31290\n",
      "Epoch: 0087 train_loss= 0.43507 train_acc= 0.57878 val_roc= 0.93750 val_ap= 0.94743 time= 0.33503\n",
      "Epoch: 0088 train_loss= 0.43445 train_acc= 0.57957 val_roc= 0.93717 val_ap= 0.94721 time= 0.30267\n",
      "Epoch: 0089 train_loss= 0.43386 train_acc= 0.58038 val_roc= 0.93673 val_ap= 0.94702 time= 0.31312\n",
      "Epoch: 0090 train_loss= 0.43330 train_acc= 0.58122 val_roc= 0.93630 val_ap= 0.94683 time= 0.31137\n",
      "Epoch: 0091 train_loss= 0.43276 train_acc= 0.58207 val_roc= 0.93582 val_ap= 0.94666 time= 0.30572\n",
      "Epoch: 0092 train_loss= 0.43224 train_acc= 0.58290 val_roc= 0.93568 val_ap= 0.94675 time= 0.31452\n",
      "Epoch: 0093 train_loss= 0.43174 train_acc= 0.58376 val_roc= 0.93512 val_ap= 0.94646 time= 0.30963\n",
      "Epoch: 0094 train_loss= 0.43126 train_acc= 0.58461 val_roc= 0.93459 val_ap= 0.94603 time= 0.29968\n",
      "Epoch: 0095 train_loss= 0.43080 train_acc= 0.58527 val_roc= 0.93383 val_ap= 0.94554 time= 0.32324\n",
      "Epoch: 0096 train_loss= 0.43035 train_acc= 0.58605 val_roc= 0.93340 val_ap= 0.94532 time= 0.31398\n",
      "Epoch: 0097 train_loss= 0.42991 train_acc= 0.58667 val_roc= 0.93292 val_ap= 0.94510 time= 0.30623\n",
      "Epoch: 0098 train_loss= 0.42948 train_acc= 0.58728 val_roc= 0.93237 val_ap= 0.94483 time= 0.29139\n",
      "Epoch: 0099 train_loss= 0.42907 train_acc= 0.58786 val_roc= 0.93157 val_ap= 0.94445 time= 0.29504\n",
      "Epoch: 0100 train_loss= 0.42868 train_acc= 0.58836 val_roc= 0.93105 val_ap= 0.94412 time= 0.30758\n",
      "Epoch: 0101 train_loss= 0.42830 train_acc= 0.58871 val_roc= 0.93039 val_ap= 0.94366 time= 0.29094\n",
      "Epoch: 0102 train_loss= 0.42795 train_acc= 0.58902 val_roc= 0.92972 val_ap= 0.94333 time= 0.30489\n",
      "Epoch: 0103 train_loss= 0.42762 train_acc= 0.58933 val_roc= 0.92914 val_ap= 0.94293 time= 0.31752\n",
      "Epoch: 0104 train_loss= 0.42731 train_acc= 0.58957 val_roc= 0.92877 val_ap= 0.94283 time= 0.30378\n",
      "Epoch: 0105 train_loss= 0.42702 train_acc= 0.58988 val_roc= 0.92832 val_ap= 0.94266 time= 0.30486\n",
      "Epoch: 0106 train_loss= 0.42675 train_acc= 0.59020 val_roc= 0.92792 val_ap= 0.94234 time= 0.31883\n",
      "Epoch: 0107 train_loss= 0.42648 train_acc= 0.59055 val_roc= 0.92751 val_ap= 0.94216 time= 0.30923\n",
      "Epoch: 0108 train_loss= 0.42623 train_acc= 0.59085 val_roc= 0.92724 val_ap= 0.94207 time= 0.30966\n",
      "Epoch: 0109 train_loss= 0.42598 train_acc= 0.59111 val_roc= 0.92690 val_ap= 0.94180 time= 0.30566\n",
      "Epoch: 0110 train_loss= 0.42574 train_acc= 0.59134 val_roc= 0.92657 val_ap= 0.94156 time= 0.30280\n",
      "Epoch: 0111 train_loss= 0.42550 train_acc= 0.59158 val_roc= 0.92644 val_ap= 0.94155 time= 0.32614\n",
      "Epoch: 0112 train_loss= 0.42528 train_acc= 0.59186 val_roc= 0.92630 val_ap= 0.94161 time= 0.31760\n",
      "Epoch: 0113 train_loss= 0.42506 train_acc= 0.59220 val_roc= 0.92601 val_ap= 0.94157 time= 0.30533\n",
      "Epoch: 0114 train_loss= 0.42485 train_acc= 0.59238 val_roc= 0.92572 val_ap= 0.94154 time= 0.31273\n",
      "Epoch: 0115 train_loss= 0.42465 train_acc= 0.59256 val_roc= 0.92572 val_ap= 0.94169 time= 0.30203\n",
      "Epoch: 0116 train_loss= 0.42444 train_acc= 0.59279 val_roc= 0.92560 val_ap= 0.94163 time= 0.32289\n",
      "Epoch: 0117 train_loss= 0.42424 train_acc= 0.59301 val_roc= 0.92549 val_ap= 0.94158 time= 0.29271\n",
      "Epoch: 0118 train_loss= 0.42404 train_acc= 0.59312 val_roc= 0.92557 val_ap= 0.94168 time= 0.31683\n",
      "Epoch: 0119 train_loss= 0.42385 train_acc= 0.59332 val_roc= 0.92554 val_ap= 0.94178 time= 0.31206\n",
      "Epoch: 0120 train_loss= 0.42366 train_acc= 0.59347 val_roc= 0.92541 val_ap= 0.94177 time= 0.32080\n",
      "Epoch: 0121 train_loss= 0.42347 train_acc= 0.59358 val_roc= 0.92531 val_ap= 0.94178 time= 0.30762\n",
      "Epoch: 0122 train_loss= 0.42328 train_acc= 0.59367 val_roc= 0.92517 val_ap= 0.94170 time= 0.30670\n",
      "Epoch: 0123 train_loss= 0.42309 train_acc= 0.59381 val_roc= 0.92488 val_ap= 0.94150 time= 0.30674\n",
      "Epoch: 0124 train_loss= 0.42291 train_acc= 0.59386 val_roc= 0.92481 val_ap= 0.94158 time= 0.30164\n",
      "Epoch: 0125 train_loss= 0.42272 train_acc= 0.59386 val_roc= 0.92474 val_ap= 0.94156 time= 0.30119\n",
      "Epoch: 0126 train_loss= 0.42254 train_acc= 0.59391 val_roc= 0.92445 val_ap= 0.94146 time= 0.30070\n",
      "Epoch: 0127 train_loss= 0.42236 train_acc= 0.59396 val_roc= 0.92456 val_ap= 0.94162 time= 0.29723\n",
      "Epoch: 0128 train_loss= 0.42217 train_acc= 0.59398 val_roc= 0.92455 val_ap= 0.94166 time= 0.30466\n",
      "Epoch: 0129 train_loss= 0.42199 train_acc= 0.59395 val_roc= 0.92430 val_ap= 0.94159 time= 0.30964\n",
      "Epoch: 0130 train_loss= 0.42182 train_acc= 0.59392 val_roc= 0.92426 val_ap= 0.94167 time= 0.31860\n",
      "Epoch: 0131 train_loss= 0.42164 train_acc= 0.59391 val_roc= 0.92403 val_ap= 0.94157 time= 0.29769\n",
      "Epoch: 0132 train_loss= 0.42147 train_acc= 0.59383 val_roc= 0.92406 val_ap= 0.94168 time= 0.30124\n",
      "Epoch: 0133 train_loss= 0.42130 train_acc= 0.59376 val_roc= 0.92393 val_ap= 0.94170 time= 0.32995\n",
      "Epoch: 0134 train_loss= 0.42114 train_acc= 0.59372 val_roc= 0.92381 val_ap= 0.94181 time= 0.31255\n",
      "Epoch: 0135 train_loss= 0.42099 train_acc= 0.59365 val_roc= 0.92364 val_ap= 0.94177 time= 0.30190\n",
      "Epoch: 0136 train_loss= 0.42084 train_acc= 0.59360 val_roc= 0.92355 val_ap= 0.94178 time= 0.30856\n",
      "Epoch: 0137 train_loss= 0.42070 train_acc= 0.59349 val_roc= 0.92355 val_ap= 0.94187 time= 0.29695\n",
      "Epoch: 0138 train_loss= 0.42056 train_acc= 0.59343 val_roc= 0.92340 val_ap= 0.94185 time= 0.30623\n",
      "Epoch: 0139 train_loss= 0.42043 train_acc= 0.59328 val_roc= 0.92342 val_ap= 0.94195 time= 0.30433\n",
      "Epoch: 0140 train_loss= 0.42030 train_acc= 0.59318 val_roc= 0.92345 val_ap= 0.94206 time= 0.32212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0141 train_loss= 0.42019 train_acc= 0.59302 val_roc= 0.92356 val_ap= 0.94224 time= 0.30517\n",
      "Epoch: 0142 train_loss= 0.42008 train_acc= 0.59291 val_roc= 0.92378 val_ap= 0.94256 time= 0.29821\n",
      "Epoch: 0143 train_loss= 0.41997 train_acc= 0.59288 val_roc= 0.92371 val_ap= 0.94258 time= 0.30016\n",
      "Epoch: 0144 train_loss= 0.41987 train_acc= 0.59274 val_roc= 0.92380 val_ap= 0.94281 time= 0.29893\n",
      "Epoch: 0145 train_loss= 0.41978 train_acc= 0.59270 val_roc= 0.92377 val_ap= 0.94287 time= 0.29896\n",
      "Epoch: 0146 train_loss= 0.41969 train_acc= 0.59271 val_roc= 0.92385 val_ap= 0.94299 time= 0.30684\n",
      "Epoch: 0147 train_loss= 0.41960 train_acc= 0.59273 val_roc= 0.92387 val_ap= 0.94307 time= 0.30582\n",
      "Epoch: 0148 train_loss= 0.41951 train_acc= 0.59284 val_roc= 0.92394 val_ap= 0.94318 time= 0.30067\n",
      "Epoch: 0149 train_loss= 0.41942 train_acc= 0.59290 val_roc= 0.92395 val_ap= 0.94325 time= 0.32715\n",
      "Epoch: 0150 train_loss= 0.41934 train_acc= 0.59301 val_roc= 0.92398 val_ap= 0.94328 time= 0.33714\n",
      "Epoch: 0151 train_loss= 0.41925 train_acc= 0.59310 val_roc= 0.92407 val_ap= 0.94336 time= 0.31019\n",
      "Epoch: 0152 train_loss= 0.41916 train_acc= 0.59324 val_roc= 0.92413 val_ap= 0.94344 time= 0.30387\n",
      "Epoch: 0153 train_loss= 0.41908 train_acc= 0.59336 val_roc= 0.92414 val_ap= 0.94345 time= 0.29876\n",
      "Epoch: 0154 train_loss= 0.41899 train_acc= 0.59349 val_roc= 0.92417 val_ap= 0.94354 time= 0.31419\n",
      "Epoch: 0155 train_loss= 0.41890 train_acc= 0.59363 val_roc= 0.92417 val_ap= 0.94353 time= 0.29594\n",
      "Epoch: 0156 train_loss= 0.41881 train_acc= 0.59375 val_roc= 0.92416 val_ap= 0.94354 time= 0.30979\n",
      "Epoch: 0157 train_loss= 0.41872 train_acc= 0.59386 val_roc= 0.92434 val_ap= 0.94375 time= 0.32814\n",
      "Epoch: 0158 train_loss= 0.41863 train_acc= 0.59405 val_roc= 0.92442 val_ap= 0.94379 time= 0.34110\n",
      "Epoch: 0159 train_loss= 0.41854 train_acc= 0.59416 val_roc= 0.92446 val_ap= 0.94387 time= 0.29869\n",
      "Epoch: 0160 train_loss= 0.41845 train_acc= 0.59429 val_roc= 0.92453 val_ap= 0.94390 time= 0.30511\n",
      "Epoch: 0161 train_loss= 0.41835 train_acc= 0.59445 val_roc= 0.92459 val_ap= 0.94398 time= 0.31362\n",
      "Epoch: 0162 train_loss= 0.41826 train_acc= 0.59457 val_roc= 0.92474 val_ap= 0.94404 time= 0.33502\n",
      "Epoch: 0163 train_loss= 0.41816 train_acc= 0.59470 val_roc= 0.92478 val_ap= 0.94412 time= 0.31090\n",
      "Epoch: 0164 train_loss= 0.41806 train_acc= 0.59481 val_roc= 0.92488 val_ap= 0.94423 time= 0.32290\n",
      "Epoch: 0165 train_loss= 0.41796 train_acc= 0.59499 val_roc= 0.92507 val_ap= 0.94444 time= 0.30421\n",
      "Epoch: 0166 train_loss= 0.41785 train_acc= 0.59515 val_roc= 0.92536 val_ap= 0.94464 time= 0.32973\n",
      "Epoch: 0167 train_loss= 0.41775 train_acc= 0.59534 val_roc= 0.92533 val_ap= 0.94467 time= 0.30703\n",
      "Epoch: 0168 train_loss= 0.41764 train_acc= 0.59553 val_roc= 0.92544 val_ap= 0.94481 time= 0.31905\n",
      "Epoch: 0169 train_loss= 0.41753 train_acc= 0.59575 val_roc= 0.92575 val_ap= 0.94507 time= 0.31561\n",
      "Epoch: 0170 train_loss= 0.41741 train_acc= 0.59590 val_roc= 0.92573 val_ap= 0.94513 time= 0.31467\n",
      "Epoch: 0171 train_loss= 0.41730 train_acc= 0.59610 val_roc= 0.92562 val_ap= 0.94507 time= 0.31093\n",
      "Epoch: 0172 train_loss= 0.41718 train_acc= 0.59630 val_roc= 0.92559 val_ap= 0.94510 time= 0.31324\n",
      "Epoch: 0173 train_loss= 0.41707 train_acc= 0.59656 val_roc= 0.92562 val_ap= 0.94517 time= 0.30712\n",
      "Epoch: 0174 train_loss= 0.41695 train_acc= 0.59679 val_roc= 0.92570 val_ap= 0.94522 time= 0.33702\n",
      "Epoch: 0175 train_loss= 0.41683 train_acc= 0.59698 val_roc= 0.92583 val_ap= 0.94533 time= 0.32640\n",
      "Epoch: 0176 train_loss= 0.41672 train_acc= 0.59718 val_roc= 0.92596 val_ap= 0.94546 time= 0.30318\n",
      "Epoch: 0177 train_loss= 0.41660 train_acc= 0.59741 val_roc= 0.92614 val_ap= 0.94557 time= 0.31055\n",
      "Epoch: 0178 train_loss= 0.41649 train_acc= 0.59765 val_roc= 0.92633 val_ap= 0.94574 time= 0.30951\n",
      "Epoch: 0179 train_loss= 0.41637 train_acc= 0.59789 val_roc= 0.92663 val_ap= 0.94601 time= 0.29997\n",
      "Epoch: 0180 train_loss= 0.41627 train_acc= 0.59816 val_roc= 0.92682 val_ap= 0.94622 time= 0.30474\n",
      "Epoch: 0181 train_loss= 0.41616 train_acc= 0.59839 val_roc= 0.92682 val_ap= 0.94619 time= 0.31738\n",
      "Epoch: 0182 train_loss= 0.41606 train_acc= 0.59857 val_roc= 0.92690 val_ap= 0.94626 time= 0.30296\n",
      "Epoch: 0183 train_loss= 0.41597 train_acc= 0.59877 val_roc= 0.92698 val_ap= 0.94631 time= 0.33251\n",
      "Epoch: 0184 train_loss= 0.41588 train_acc= 0.59895 val_roc= 0.92706 val_ap= 0.94643 time= 0.31673\n",
      "Epoch: 0185 train_loss= 0.41579 train_acc= 0.59913 val_roc= 0.92703 val_ap= 0.94645 time= 0.31612\n",
      "Epoch: 0186 train_loss= 0.41571 train_acc= 0.59934 val_roc= 0.92689 val_ap= 0.94640 time= 0.30503\n",
      "Epoch: 0187 train_loss= 0.41563 train_acc= 0.59953 val_roc= 0.92689 val_ap= 0.94642 time= 0.30634\n",
      "Epoch: 0188 train_loss= 0.41556 train_acc= 0.59965 val_roc= 0.92690 val_ap= 0.94644 time= 0.31629\n",
      "Epoch: 0189 train_loss= 0.41549 train_acc= 0.59985 val_roc= 0.92683 val_ap= 0.94633 time= 0.31306\n",
      "Epoch: 0190 train_loss= 0.41543 train_acc= 0.59992 val_roc= 0.92705 val_ap= 0.94653 time= 0.31265\n",
      "Epoch: 0191 train_loss= 0.41537 train_acc= 0.60014 val_roc= 0.92687 val_ap= 0.94642 time= 0.31512\n",
      "Epoch: 0192 train_loss= 0.41531 train_acc= 0.60015 val_roc= 0.92724 val_ap= 0.94662 time= 0.30171\n",
      "Epoch: 0193 train_loss= 0.41527 train_acc= 0.60050 val_roc= 0.92647 val_ap= 0.94604 time= 0.30900\n",
      "Epoch: 0194 train_loss= 0.41526 train_acc= 0.60034 val_roc= 0.92763 val_ap= 0.94678 time= 0.30879\n",
      "Epoch: 0195 train_loss= 0.41534 train_acc= 0.60067 val_roc= 0.92620 val_ap= 0.94584 time= 0.31669\n",
      "Epoch: 0196 train_loss= 0.41539 train_acc= 0.60015 val_roc= 0.92796 val_ap= 0.94697 time= 0.31405\n",
      "Epoch: 0197 train_loss= 0.41531 train_acc= 0.60108 val_roc= 0.92695 val_ap= 0.94633 time= 0.33084\n",
      "Epoch: 0198 train_loss= 0.41496 train_acc= 0.60169 val_roc= 0.92651 val_ap= 0.94597 time= 0.32230\n",
      "Epoch: 0199 train_loss= 0.41500 train_acc= 0.60173 val_roc= 0.92786 val_ap= 0.94684 time= 0.33996\n",
      "Epoch: 0200 train_loss= 0.41508 train_acc= 0.60208 val_roc= 0.92685 val_ap= 0.94620 time= 0.31760\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.918161229111832\n",
      "Test AP score: 0.9313614685825307\n"
     ]
    }
   ],
   "source": [
    "%run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:58: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:61: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\initializations.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:101: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:79: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:33: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:33: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:84: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:12: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:13: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:97: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train.py:98: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 0.79735 train_acc= 0.05140 val_roc= 0.65908 val_ap= 0.65483 time= 0.48082\n",
      "Epoch: 0002 train_loss= 0.79669 train_acc= 0.05404 val_roc= 0.66433 val_ap= 0.67580 time= 0.28491\n",
      "Epoch: 0003 train_loss= 0.79501 train_acc= 0.05326 val_roc= 0.65635 val_ap= 0.68037 time= 0.27980\n",
      "Epoch: 0004 train_loss= 0.79168 train_acc= 0.04857 val_roc= 0.64763 val_ap= 0.67998 time= 0.27379\n",
      "Epoch: 0005 train_loss= 0.78593 train_acc= 0.04467 val_roc= 0.63781 val_ap= 0.67668 time= 0.28276\n",
      "Epoch: 0006 train_loss= 0.77700 train_acc= 0.04255 val_roc= 0.63197 val_ap= 0.67517 time= 0.28773\n",
      "Epoch: 0007 train_loss= 0.76442 train_acc= 0.04199 val_roc= 0.62862 val_ap= 0.67509 time= 0.28295\n",
      "Epoch: 0008 train_loss= 0.74855 train_acc= 0.04234 val_roc= 0.62444 val_ap= 0.67340 time= 0.28080\n",
      "Epoch: 0009 train_loss= 0.73138 train_acc= 0.04376 val_roc= 0.62132 val_ap= 0.67251 time= 0.26750\n",
      "Epoch: 0010 train_loss= 0.71737 train_acc= 0.04816 val_roc= 0.61872 val_ap= 0.67163 time= 0.29187\n",
      "Epoch: 0011 train_loss= 0.71252 train_acc= 0.05748 val_roc= 0.61942 val_ap= 0.67320 time= 0.27205\n",
      "Epoch: 0012 train_loss= 0.71524 train_acc= 0.07751 val_roc= 0.62164 val_ap= 0.67574 time= 0.28076\n",
      "Epoch: 0013 train_loss= 0.71359 train_acc= 0.10263 val_roc= 0.62655 val_ap= 0.67898 time= 0.27986\n",
      "Epoch: 0014 train_loss= 0.70455 train_acc= 0.13133 val_roc= 0.63196 val_ap= 0.68256 time= 0.28261\n",
      "Epoch: 0015 train_loss= 0.69223 train_acc= 0.16622 val_roc= 0.63780 val_ap= 0.68539 time= 0.29769\n",
      "Epoch: 0016 train_loss= 0.68062 train_acc= 0.20297 val_roc= 0.64624 val_ap= 0.69188 time= 0.29172\n",
      "Epoch: 0017 train_loss= 0.67121 train_acc= 0.24006 val_roc= 0.65699 val_ap= 0.70125 time= 0.28275\n",
      "Epoch: 0018 train_loss= 0.66331 train_acc= 0.27434 val_roc= 0.66530 val_ap= 0.70958 time= 0.28128\n",
      "Epoch: 0019 train_loss= 0.65543 train_acc= 0.30584 val_roc= 0.67180 val_ap= 0.71575 time= 0.28407\n",
      "Epoch: 0020 train_loss= 0.64641 train_acc= 0.33443 val_roc= 0.67618 val_ap= 0.71919 time= 0.28550\n",
      "Epoch: 0021 train_loss= 0.63580 train_acc= 0.35941 val_roc= 0.67884 val_ap= 0.71916 time= 0.27327\n",
      "Epoch: 0022 train_loss= 0.62384 train_acc= 0.38090 val_roc= 0.68010 val_ap= 0.72023 time= 0.28697\n",
      "Epoch: 0023 train_loss= 0.61121 train_acc= 0.39885 val_roc= 0.68213 val_ap= 0.72243 time= 0.27691\n",
      "Epoch: 0024 train_loss= 0.59888 train_acc= 0.41444 val_roc= 0.68443 val_ap= 0.72366 time= 0.28468\n",
      "Epoch: 0025 train_loss= 0.58783 train_acc= 0.42769 val_roc= 0.68683 val_ap= 0.72586 time= 0.29269\n",
      "Epoch: 0026 train_loss= 0.57866 train_acc= 0.43873 val_roc= 0.68899 val_ap= 0.72845 time= 0.28589\n",
      "Epoch: 0027 train_loss= 0.57132 train_acc= 0.44843 val_roc= 0.69269 val_ap= 0.73141 time= 0.28969\n",
      "Epoch: 0028 train_loss= 0.56513 train_acc= 0.45793 val_roc= 0.69738 val_ap= 0.73570 time= 0.28873\n",
      "Epoch: 0029 train_loss= 0.55915 train_acc= 0.46740 val_roc= 0.70221 val_ap= 0.73966 time= 0.28997\n",
      "Epoch: 0030 train_loss= 0.55269 train_acc= 0.47667 val_roc= 0.70787 val_ap= 0.74502 time= 0.26123\n",
      "Epoch: 0031 train_loss= 0.54552 train_acc= 0.48514 val_roc= 0.71331 val_ap= 0.75013 time= 0.29115\n",
      "Epoch: 0032 train_loss= 0.53786 train_acc= 0.49226 val_roc= 0.71977 val_ap= 0.75547 time= 0.28794\n",
      "Epoch: 0033 train_loss= 0.53010 train_acc= 0.49746 val_roc= 0.72550 val_ap= 0.76034 time= 0.27884\n",
      "Epoch: 0034 train_loss= 0.52269 train_acc= 0.50218 val_roc= 0.73143 val_ap= 0.76539 time= 0.28627\n",
      "Epoch: 0035 train_loss= 0.51598 train_acc= 0.50575 val_roc= 0.73745 val_ap= 0.77028 time= 0.27135\n",
      "Epoch: 0036 train_loss= 0.51021 train_acc= 0.50805 val_roc= 0.74344 val_ap= 0.77678 time= 0.27884\n",
      "Epoch: 0037 train_loss= 0.50543 train_acc= 0.50957 val_roc= 0.74849 val_ap= 0.78256 time= 0.28570\n",
      "Epoch: 0038 train_loss= 0.50159 train_acc= 0.51067 val_roc= 0.75394 val_ap= 0.78716 time= 0.29476\n",
      "Epoch: 0039 train_loss= 0.49854 train_acc= 0.51123 val_roc= 0.75869 val_ap= 0.79172 time= 0.27805\n",
      "Epoch: 0040 train_loss= 0.49604 train_acc= 0.51160 val_roc= 0.76329 val_ap= 0.79663 time= 0.27997\n",
      "Epoch: 0041 train_loss= 0.49386 train_acc= 0.51209 val_roc= 0.76706 val_ap= 0.80025 time= 0.28326\n",
      "Epoch: 0042 train_loss= 0.49183 train_acc= 0.51311 val_roc= 0.76942 val_ap= 0.80199 time= 0.28296\n",
      "Epoch: 0043 train_loss= 0.48981 train_acc= 0.51427 val_roc= 0.77215 val_ap= 0.80491 time= 0.28341\n",
      "Epoch: 0044 train_loss= 0.48778 train_acc= 0.51561 val_roc= 0.77478 val_ap= 0.80774 time= 0.28957\n",
      "Epoch: 0045 train_loss= 0.48573 train_acc= 0.51705 val_roc= 0.77763 val_ap= 0.81035 time= 0.28146\n",
      "Epoch: 0046 train_loss= 0.48370 train_acc= 0.51837 val_roc= 0.77950 val_ap= 0.81262 time= 0.28389\n",
      "Epoch: 0047 train_loss= 0.48173 train_acc= 0.51949 val_roc= 0.78156 val_ap= 0.81453 time= 0.28973\n",
      "Epoch: 0048 train_loss= 0.47983 train_acc= 0.52035 val_roc= 0.78370 val_ap= 0.81594 time= 0.28632\n",
      "Epoch: 0049 train_loss= 0.47802 train_acc= 0.52118 val_roc= 0.78553 val_ap= 0.81709 time= 0.27627\n",
      "Epoch: 0050 train_loss= 0.47630 train_acc= 0.52173 val_roc= 0.78678 val_ap= 0.81800 time= 0.28489\n",
      "Epoch: 0051 train_loss= 0.47470 train_acc= 0.52212 val_roc= 0.78824 val_ap= 0.81915 time= 0.28839\n",
      "Epoch: 0052 train_loss= 0.47322 train_acc= 0.52246 val_roc= 0.78978 val_ap= 0.82035 time= 0.28277\n",
      "Epoch: 0053 train_loss= 0.47190 train_acc= 0.52258 val_roc= 0.79145 val_ap= 0.82216 time= 0.29039\n",
      "Epoch: 0054 train_loss= 0.47073 train_acc= 0.52253 val_roc= 0.79304 val_ap= 0.82383 time= 0.28100\n",
      "Epoch: 0055 train_loss= 0.46968 train_acc= 0.52247 val_roc= 0.79378 val_ap= 0.82418 time= 0.30103\n",
      "Epoch: 0056 train_loss= 0.46873 train_acc= 0.52243 val_roc= 0.79508 val_ap= 0.82598 time= 0.28974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0057 train_loss= 0.46782 train_acc= 0.52242 val_roc= 0.79631 val_ap= 0.82725 time= 0.27792\n",
      "Epoch: 0058 train_loss= 0.46693 train_acc= 0.52236 val_roc= 0.79781 val_ap= 0.82913 time= 0.28568\n",
      "Epoch: 0059 train_loss= 0.46603 train_acc= 0.52230 val_roc= 0.79893 val_ap= 0.83043 time= 0.27081\n",
      "Epoch: 0060 train_loss= 0.46512 train_acc= 0.52212 val_roc= 0.80040 val_ap= 0.83246 time= 0.27977\n",
      "Epoch: 0061 train_loss= 0.46418 train_acc= 0.52198 val_roc= 0.80180 val_ap= 0.83448 time= 0.28300\n",
      "Epoch: 0062 train_loss= 0.46323 train_acc= 0.52190 val_roc= 0.80324 val_ap= 0.83633 time= 0.28682\n",
      "Epoch: 0063 train_loss= 0.46227 train_acc= 0.52175 val_roc= 0.80457 val_ap= 0.83885 time= 0.28193\n",
      "Epoch: 0064 train_loss= 0.46131 train_acc= 0.52165 val_roc= 0.80626 val_ap= 0.84130 time= 0.27735\n",
      "Epoch: 0065 train_loss= 0.46035 train_acc= 0.52164 val_roc= 0.80765 val_ap= 0.84313 time= 0.29174\n",
      "Epoch: 0066 train_loss= 0.45941 train_acc= 0.52166 val_roc= 0.80931 val_ap= 0.84579 time= 0.28474\n",
      "Epoch: 0067 train_loss= 0.45849 train_acc= 0.52167 val_roc= 0.81067 val_ap= 0.84703 time= 0.28034\n",
      "Epoch: 0068 train_loss= 0.45759 train_acc= 0.52180 val_roc= 0.81197 val_ap= 0.84874 time= 0.27916\n",
      "Epoch: 0069 train_loss= 0.45673 train_acc= 0.52202 val_roc= 0.81279 val_ap= 0.84978 time= 0.28174\n",
      "Epoch: 0070 train_loss= 0.45591 train_acc= 0.52212 val_roc= 0.81327 val_ap= 0.85062 time= 0.29170\n",
      "Epoch: 0071 train_loss= 0.45512 train_acc= 0.52220 val_roc= 0.81408 val_ap= 0.85164 time= 0.28256\n",
      "Epoch: 0072 train_loss= 0.45437 train_acc= 0.52236 val_roc= 0.81535 val_ap= 0.85290 time= 0.28441\n",
      "Epoch: 0073 train_loss= 0.45366 train_acc= 0.52259 val_roc= 0.81651 val_ap= 0.85397 time= 0.27778\n",
      "Epoch: 0074 train_loss= 0.45298 train_acc= 0.52282 val_roc= 0.81743 val_ap= 0.85489 time= 0.28438\n",
      "Epoch: 0075 train_loss= 0.45234 train_acc= 0.52300 val_roc= 0.81792 val_ap= 0.85528 time= 0.27873\n",
      "Epoch: 0076 train_loss= 0.45173 train_acc= 0.52326 val_roc= 0.81856 val_ap= 0.85553 time= 0.28213\n",
      "Epoch: 0077 train_loss= 0.45114 train_acc= 0.52349 val_roc= 0.81921 val_ap= 0.85586 time= 0.27968\n",
      "Epoch: 0078 train_loss= 0.45056 train_acc= 0.52375 val_roc= 0.81998 val_ap= 0.85644 time= 0.28230\n",
      "Epoch: 0079 train_loss= 0.44998 train_acc= 0.52398 val_roc= 0.82086 val_ap= 0.85707 time= 0.27895\n",
      "Epoch: 0080 train_loss= 0.44940 train_acc= 0.52431 val_roc= 0.82118 val_ap= 0.85738 time= 0.28327\n",
      "Epoch: 0081 train_loss= 0.44881 train_acc= 0.52470 val_roc= 0.82148 val_ap= 0.85721 time= 0.28348\n",
      "Epoch: 0082 train_loss= 0.44822 train_acc= 0.52517 val_roc= 0.82170 val_ap= 0.85726 time= 0.27222\n",
      "Epoch: 0083 train_loss= 0.44762 train_acc= 0.52560 val_roc= 0.82254 val_ap= 0.85796 time= 0.28107\n",
      "Epoch: 0084 train_loss= 0.44703 train_acc= 0.52611 val_roc= 0.82310 val_ap= 0.85814 time= 0.28389\n",
      "Epoch: 0085 train_loss= 0.44643 train_acc= 0.52653 val_roc= 0.82336 val_ap= 0.85832 time= 0.27393\n",
      "Epoch: 0086 train_loss= 0.44584 train_acc= 0.52699 val_roc= 0.82387 val_ap= 0.85870 time= 0.29820\n",
      "Epoch: 0087 train_loss= 0.44526 train_acc= 0.52743 val_roc= 0.82421 val_ap= 0.85904 time= 0.29092\n",
      "Epoch: 0088 train_loss= 0.44468 train_acc= 0.52791 val_roc= 0.82450 val_ap= 0.85940 time= 0.27962\n",
      "Epoch: 0089 train_loss= 0.44412 train_acc= 0.52833 val_roc= 0.82485 val_ap= 0.85994 time= 0.27431\n",
      "Epoch: 0090 train_loss= 0.44357 train_acc= 0.52888 val_roc= 0.82517 val_ap= 0.86055 time= 0.29617\n",
      "Epoch: 0091 train_loss= 0.44303 train_acc= 0.52934 val_roc= 0.82540 val_ap= 0.86089 time= 0.28774\n",
      "Epoch: 0092 train_loss= 0.44250 train_acc= 0.52985 val_roc= 0.82533 val_ap= 0.86100 time= 0.28377\n",
      "Epoch: 0093 train_loss= 0.44199 train_acc= 0.53039 val_roc= 0.82560 val_ap= 0.86134 time= 0.29284\n",
      "Epoch: 0094 train_loss= 0.44149 train_acc= 0.53097 val_roc= 0.82582 val_ap= 0.86175 time= 0.29130\n",
      "Epoch: 0095 train_loss= 0.44099 train_acc= 0.53147 val_roc= 0.82573 val_ap= 0.86177 time= 0.28271\n",
      "Epoch: 0096 train_loss= 0.44050 train_acc= 0.53193 val_roc= 0.82562 val_ap= 0.86176 time= 0.30219\n",
      "Epoch: 0097 train_loss= 0.44001 train_acc= 0.53240 val_roc= 0.82572 val_ap= 0.86213 time= 0.28797\n",
      "Epoch: 0098 train_loss= 0.43952 train_acc= 0.53301 val_roc= 0.82566 val_ap= 0.86231 time= 0.27820\n",
      "Epoch: 0099 train_loss= 0.43904 train_acc= 0.53353 val_roc= 0.82577 val_ap= 0.86273 time= 0.28426\n",
      "Epoch: 0100 train_loss= 0.43856 train_acc= 0.53405 val_roc= 0.82566 val_ap= 0.86291 time= 0.28674\n",
      "Epoch: 0101 train_loss= 0.43809 train_acc= 0.53455 val_roc= 0.82563 val_ap= 0.86313 time= 0.27777\n",
      "Epoch: 0102 train_loss= 0.43761 train_acc= 0.53506 val_roc= 0.82550 val_ap= 0.86355 time= 0.27337\n",
      "Epoch: 0103 train_loss= 0.43715 train_acc= 0.53554 val_roc= 0.82562 val_ap= 0.86392 time= 0.29598\n",
      "Epoch: 0104 train_loss= 0.43669 train_acc= 0.53589 val_roc= 0.82551 val_ap= 0.86399 time= 0.28773\n",
      "Epoch: 0105 train_loss= 0.43623 train_acc= 0.53626 val_roc= 0.82547 val_ap= 0.86419 time= 0.27843\n",
      "Epoch: 0106 train_loss= 0.43579 train_acc= 0.53657 val_roc= 0.82524 val_ap= 0.86429 time= 0.28163\n",
      "Epoch: 0107 train_loss= 0.43536 train_acc= 0.53695 val_roc= 0.82499 val_ap= 0.86446 time= 0.28718\n",
      "Epoch: 0108 train_loss= 0.43495 train_acc= 0.53715 val_roc= 0.82510 val_ap= 0.86497 time= 0.29422\n",
      "Epoch: 0109 train_loss= 0.43455 train_acc= 0.53733 val_roc= 0.82528 val_ap= 0.86535 time= 0.30569\n",
      "Epoch: 0110 train_loss= 0.43416 train_acc= 0.53748 val_roc= 0.82537 val_ap= 0.86559 time= 0.28810\n",
      "Epoch: 0111 train_loss= 0.43378 train_acc= 0.53763 val_roc= 0.82563 val_ap= 0.86603 time= 0.29644\n",
      "Epoch: 0112 train_loss= 0.43342 train_acc= 0.53775 val_roc= 0.82572 val_ap= 0.86643 time= 0.27930\n",
      "Epoch: 0113 train_loss= 0.43306 train_acc= 0.53789 val_roc= 0.82570 val_ap= 0.86679 time= 0.28719\n",
      "Epoch: 0114 train_loss= 0.43270 train_acc= 0.53799 val_roc= 0.82577 val_ap= 0.86708 time= 0.28823\n",
      "Epoch: 0115 train_loss= 0.43234 train_acc= 0.53807 val_roc= 0.82577 val_ap= 0.86736 time= 0.28916\n",
      "Epoch: 0116 train_loss= 0.43197 train_acc= 0.53813 val_roc= 0.82603 val_ap= 0.86791 time= 0.28678\n",
      "Epoch: 0117 train_loss= 0.43160 train_acc= 0.53818 val_roc= 0.82611 val_ap= 0.86831 time= 0.30103\n",
      "Epoch: 0118 train_loss= 0.43121 train_acc= 0.53823 val_roc= 0.82622 val_ap= 0.86844 time= 0.27829\n",
      "Epoch: 0119 train_loss= 0.43082 train_acc= 0.53838 val_roc= 0.82634 val_ap= 0.86869 time= 0.29022\n",
      "Epoch: 0120 train_loss= 0.43042 train_acc= 0.53850 val_roc= 0.82651 val_ap= 0.86885 time= 0.30310\n",
      "Epoch: 0121 train_loss= 0.43002 train_acc= 0.53866 val_roc= 0.82657 val_ap= 0.86914 time= 0.28304\n",
      "Epoch: 0122 train_loss= 0.42962 train_acc= 0.53891 val_roc= 0.82656 val_ap= 0.86923 time= 0.27780\n",
      "Epoch: 0123 train_loss= 0.42921 train_acc= 0.53916 val_roc= 0.82663 val_ap= 0.86920 time= 0.27184\n",
      "Epoch: 0124 train_loss= 0.42881 train_acc= 0.53937 val_roc= 0.82643 val_ap= 0.86929 time= 0.29521\n",
      "Epoch: 0125 train_loss= 0.42842 train_acc= 0.53962 val_roc= 0.82638 val_ap= 0.86931 time= 0.27990\n",
      "Epoch: 0126 train_loss= 0.42803 train_acc= 0.53989 val_roc= 0.82615 val_ap= 0.86908 time= 0.27612\n",
      "Epoch: 0127 train_loss= 0.42765 train_acc= 0.54009 val_roc= 0.82618 val_ap= 0.86904 time= 0.29970\n",
      "Epoch: 0128 train_loss= 0.42727 train_acc= 0.54033 val_roc= 0.82572 val_ap= 0.86875 time= 0.29054\n",
      "Epoch: 0129 train_loss= 0.42690 train_acc= 0.54050 val_roc= 0.82585 val_ap= 0.86897 time= 0.27752\n",
      "Epoch: 0130 train_loss= 0.42653 train_acc= 0.54075 val_roc= 0.82572 val_ap= 0.86889 time= 0.27679\n",
      "Epoch: 0131 train_loss= 0.42617 train_acc= 0.54093 val_roc= 0.82556 val_ap= 0.86869 time= 0.28176\n",
      "Epoch: 0132 train_loss= 0.42582 train_acc= 0.54106 val_roc= 0.82540 val_ap= 0.86865 time= 0.28275\n",
      "Epoch: 0133 train_loss= 0.42547 train_acc= 0.54124 val_roc= 0.82510 val_ap= 0.86861 time= 0.28527\n",
      "Epoch: 0134 train_loss= 0.42512 train_acc= 0.54142 val_roc= 0.82475 val_ap= 0.86850 time= 0.27931\n",
      "Epoch: 0135 train_loss= 0.42478 train_acc= 0.54159 val_roc= 0.82468 val_ap= 0.86861 time= 0.27825\n",
      "Epoch: 0136 train_loss= 0.42445 train_acc= 0.54182 val_roc= 0.82385 val_ap= 0.86827 time= 0.27240\n",
      "Epoch: 0137 train_loss= 0.42413 train_acc= 0.54204 val_roc= 0.82366 val_ap= 0.86837 time= 0.27708\n",
      "Epoch: 0138 train_loss= 0.42381 train_acc= 0.54219 val_roc= 0.82337 val_ap= 0.86850 time= 0.27998\n",
      "Epoch: 0139 train_loss= 0.42350 train_acc= 0.54243 val_roc= 0.82349 val_ap= 0.86875 time= 0.27837\n",
      "Epoch: 0140 train_loss= 0.42321 train_acc= 0.54256 val_roc= 0.82259 val_ap= 0.86824 time= 0.29211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0141 train_loss= 0.42292 train_acc= 0.54274 val_roc= 0.82222 val_ap= 0.86842 time= 0.27787\n",
      "Epoch: 0142 train_loss= 0.42264 train_acc= 0.54287 val_roc= 0.82235 val_ap= 0.86877 time= 0.28172\n",
      "Epoch: 0143 train_loss= 0.42237 train_acc= 0.54292 val_roc= 0.82249 val_ap= 0.86908 time= 0.27979\n",
      "Epoch: 0144 train_loss= 0.42210 train_acc= 0.54304 val_roc= 0.82245 val_ap= 0.86933 time= 0.29669\n",
      "Epoch: 0145 train_loss= 0.42185 train_acc= 0.54307 val_roc= 0.82236 val_ap= 0.86944 time= 0.28080\n",
      "Epoch: 0146 train_loss= 0.42159 train_acc= 0.54305 val_roc= 0.82267 val_ap= 0.86993 time= 0.27984\n",
      "Epoch: 0147 train_loss= 0.42135 train_acc= 0.54313 val_roc= 0.82235 val_ap= 0.86974 time= 0.28519\n",
      "Epoch: 0148 train_loss= 0.42110 train_acc= 0.54311 val_roc= 0.82228 val_ap= 0.86992 time= 0.27921\n",
      "Epoch: 0149 train_loss= 0.42086 train_acc= 0.54315 val_roc= 0.82219 val_ap= 0.87009 time= 0.27324\n",
      "Epoch: 0150 train_loss= 0.42062 train_acc= 0.54315 val_roc= 0.82220 val_ap= 0.87019 time= 0.27674\n",
      "Epoch: 0151 train_loss= 0.42038 train_acc= 0.54325 val_roc= 0.82210 val_ap= 0.87035 time= 0.28606\n",
      "Epoch: 0152 train_loss= 0.42014 train_acc= 0.54329 val_roc= 0.82216 val_ap= 0.87046 time= 0.28663\n",
      "Epoch: 0153 train_loss= 0.41991 train_acc= 0.54333 val_roc= 0.82196 val_ap= 0.87040 time= 0.29620\n",
      "Epoch: 0154 train_loss= 0.41967 train_acc= 0.54344 val_roc= 0.82199 val_ap= 0.87050 time= 0.28239\n",
      "Epoch: 0155 train_loss= 0.41944 train_acc= 0.54344 val_roc= 0.82223 val_ap= 0.87070 time= 0.27645\n",
      "Epoch: 0156 train_loss= 0.41920 train_acc= 0.54342 val_roc= 0.82220 val_ap= 0.87081 time= 0.29619\n",
      "Epoch: 0157 train_loss= 0.41897 train_acc= 0.54341 val_roc= 0.82206 val_ap= 0.87076 time= 0.28077\n",
      "Epoch: 0158 train_loss= 0.41874 train_acc= 0.54349 val_roc= 0.82209 val_ap= 0.87092 time= 0.28194\n",
      "Epoch: 0159 train_loss= 0.41851 train_acc= 0.54347 val_roc= 0.82216 val_ap= 0.87097 time= 0.28485\n",
      "Epoch: 0160 train_loss= 0.41828 train_acc= 0.54344 val_roc= 0.82217 val_ap= 0.87108 time= 0.28675\n",
      "Epoch: 0161 train_loss= 0.41805 train_acc= 0.54337 val_roc= 0.82225 val_ap= 0.87110 time= 0.28765\n",
      "Epoch: 0162 train_loss= 0.41782 train_acc= 0.54336 val_roc= 0.82228 val_ap= 0.87107 time= 0.28971\n",
      "Epoch: 0163 train_loss= 0.41759 train_acc= 0.54341 val_roc= 0.82235 val_ap= 0.87113 time= 0.27181\n",
      "Epoch: 0164 train_loss= 0.41735 train_acc= 0.54342 val_roc= 0.82262 val_ap= 0.87147 time= 0.28773\n",
      "Epoch: 0165 train_loss= 0.41712 train_acc= 0.54349 val_roc= 0.82283 val_ap= 0.87157 time= 0.28886\n",
      "Epoch: 0166 train_loss= 0.41689 train_acc= 0.54355 val_roc= 0.82280 val_ap= 0.87144 time= 0.27862\n",
      "Epoch: 0167 train_loss= 0.41666 train_acc= 0.54366 val_roc= 0.82283 val_ap= 0.87132 time= 0.28378\n",
      "Epoch: 0168 train_loss= 0.41642 train_acc= 0.54380 val_roc= 0.82284 val_ap= 0.87118 time= 0.28180\n",
      "Epoch: 0169 train_loss= 0.41620 train_acc= 0.54381 val_roc= 0.82303 val_ap= 0.87121 time= 0.28133\n",
      "Epoch: 0170 train_loss= 0.41597 train_acc= 0.54393 val_roc= 0.82306 val_ap= 0.87114 time= 0.28272\n",
      "Epoch: 0171 train_loss= 0.41574 train_acc= 0.54408 val_roc= 0.82329 val_ap= 0.87119 time= 0.29336\n",
      "Epoch: 0172 train_loss= 0.41552 train_acc= 0.54419 val_roc= 0.82335 val_ap= 0.87111 time= 0.30565\n",
      "Epoch: 0173 train_loss= 0.41530 train_acc= 0.54425 val_roc= 0.82361 val_ap= 0.87122 time= 0.28679\n",
      "Epoch: 0174 train_loss= 0.41508 train_acc= 0.54433 val_roc= 0.82350 val_ap= 0.87107 time= 0.28630\n",
      "Epoch: 0175 train_loss= 0.41486 train_acc= 0.54438 val_roc= 0.82379 val_ap= 0.87111 time= 0.28076\n",
      "Epoch: 0176 train_loss= 0.41465 train_acc= 0.54452 val_roc= 0.82391 val_ap= 0.87103 time= 0.28335\n",
      "Epoch: 0177 train_loss= 0.41444 train_acc= 0.54462 val_roc= 0.82423 val_ap= 0.87107 time= 0.27799\n",
      "Epoch: 0178 train_loss= 0.41423 train_acc= 0.54476 val_roc= 0.82460 val_ap= 0.87116 time= 0.27122\n",
      "Epoch: 0179 train_loss= 0.41403 train_acc= 0.54486 val_roc= 0.82457 val_ap= 0.87112 time= 0.28419\n",
      "Epoch: 0180 train_loss= 0.41383 train_acc= 0.54504 val_roc= 0.82482 val_ap= 0.87121 time= 0.28190\n",
      "Epoch: 0181 train_loss= 0.41363 train_acc= 0.54508 val_roc= 0.82499 val_ap= 0.87133 time= 0.27987\n",
      "Epoch: 0182 train_loss= 0.41344 train_acc= 0.54518 val_roc= 0.82521 val_ap= 0.87149 time= 0.27839\n",
      "Epoch: 0183 train_loss= 0.41325 train_acc= 0.54524 val_roc= 0.82547 val_ap= 0.87166 time= 0.26733\n",
      "Epoch: 0184 train_loss= 0.41306 train_acc= 0.54526 val_roc= 0.82588 val_ap= 0.87197 time= 0.28035\n",
      "Epoch: 0185 train_loss= 0.41287 train_acc= 0.54520 val_roc= 0.82614 val_ap= 0.87215 time= 0.27820\n",
      "Epoch: 0186 train_loss= 0.41269 train_acc= 0.54526 val_roc= 0.82630 val_ap= 0.87225 time= 0.28067\n",
      "Epoch: 0187 train_loss= 0.41251 train_acc= 0.54527 val_roc= 0.82630 val_ap= 0.87228 time= 0.28474\n",
      "Epoch: 0188 train_loss= 0.41234 train_acc= 0.54521 val_roc= 0.82648 val_ap= 0.87247 time= 0.28806\n",
      "Epoch: 0189 train_loss= 0.41217 train_acc= 0.54519 val_roc= 0.82656 val_ap= 0.87240 time= 0.27807\n",
      "Epoch: 0190 train_loss= 0.41200 train_acc= 0.54517 val_roc= 0.82669 val_ap= 0.87251 time= 0.27567\n",
      "Epoch: 0191 train_loss= 0.41184 train_acc= 0.54518 val_roc= 0.82673 val_ap= 0.87261 time= 0.28831\n",
      "Epoch: 0192 train_loss= 0.41168 train_acc= 0.54532 val_roc= 0.82683 val_ap= 0.87258 time= 0.27482\n",
      "Epoch: 0193 train_loss= 0.41152 train_acc= 0.54540 val_roc= 0.82683 val_ap= 0.87281 time= 0.28502\n",
      "Epoch: 0194 train_loss= 0.41137 train_acc= 0.54547 val_roc= 0.82700 val_ap= 0.87294 time= 0.28087\n",
      "Epoch: 0195 train_loss= 0.41122 train_acc= 0.54551 val_roc= 0.82741 val_ap= 0.87319 time= 0.27712\n",
      "Epoch: 0196 train_loss= 0.41107 train_acc= 0.54559 val_roc= 0.82764 val_ap= 0.87339 time= 0.28017\n",
      "Epoch: 0197 train_loss= 0.41093 train_acc= 0.54563 val_roc= 0.82777 val_ap= 0.87357 time= 0.28057\n",
      "Epoch: 0198 train_loss= 0.41078 train_acc= 0.54573 val_roc= 0.82823 val_ap= 0.87390 time= 0.28770\n",
      "Epoch: 0199 train_loss= 0.41064 train_acc= 0.54580 val_roc= 0.82806 val_ap= 0.87379 time= 0.28328\n",
      "Epoch: 0200 train_loss= 0.41050 train_acc= 0.54590 val_roc= 0.82799 val_ap= 0.87374 time= 0.27977\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.8687749568824286\n",
      "Test AP score: 0.9014739847874598\n"
     ]
    }
   ],
   "source": [
    "# Use none features.\n",
    "# 复现原论文中的 VGAE*\n",
    "%run train.py --features=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphConvolution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e62d7b9387e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# 测试修改 lost函数 与 隐变量 z\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 将z_log_std -> z_std，去掉log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m self.z_std = GraphConvolution(input_dim=FLAGS.hidden1,\n\u001b[0m\u001b[0;32m      5\u001b[0m                                     \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                     \u001b[0madj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GraphConvolution' is not defined"
     ]
    }
   ],
   "source": [
    "# GCM MODEL VAE\n",
    "# 测试修改 lost函数 与 隐变量 z\n",
    "# 将z_log_std -> z_std，去掉log\n",
    "self.z_std = GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                    output_dim=FLAGS.hidden2,\n",
    "                                    adj=self.adj,\n",
    "                                    act=lambda x: x,\n",
    "                                    dropout=self.dropout,\n",
    "                                    logging=self.logging)(self.hidden1)\n",
    "\n",
    "self.z = self.z_mean + tf.random_normal([self.n_samples, FLAGS.hidden2]) * self.z_std\n",
    "\n",
    "# Latent loss\n",
    "self.kl = (0.5 / num_nodes) * tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                1 + 2 * tf.log(model.z_std) - tf.square(model.z_mean) -\n",
    "                tf.square(model.z_std), 1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行\n",
    "!python3 train_debug.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
