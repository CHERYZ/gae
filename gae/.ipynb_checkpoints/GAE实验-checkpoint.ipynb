{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCM MODEL VAE\n",
    "# 测试修改 lost函数 与 隐变量 z\n",
    "# 将z_log_std -> z_std，去掉log\n",
    "self.z_std = GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                    output_dim=FLAGS.hidden2,\n",
    "                                    adj=self.adj,\n",
    "                                    act=lambda x: x,\n",
    "                                    dropout=self.dropout,\n",
    "                                    logging=self.logging)(self.hidden1)\n",
    "\n",
    "self.z = self.z_mean + tf.random_normal([self.n_samples, FLAGS.hidden2]) * self.z_std\n",
    "\n",
    "# Latent loss\n",
    "self.kl = (0.5 / num_nodes) * tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                1 + 2 * tf.log(model.z_std) - tf.square(model.z_mean) -\n",
    "                tf.square(model.z_std), 1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcefc342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:103: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:106: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\initializations.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:42: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:73: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 0001 log_lik= 1.7382555 train_kl= -0.00004 train_loss= 1.73829 train_acc= 0.49590 val_roc= 0.70847 val_ap= 0.74090 time= 2.08216\n",
      "Epoch: 0002 log_lik= 1.4739331 train_kl= -0.00015 train_loss= 1.47409 train_acc= 0.47183 val_roc= 0.69307 val_ap= 0.73578 time= 0.08613\n",
      "Epoch: 0003 log_lik= 1.269342 train_kl= -0.00052 train_loss= 1.26986 train_acc= 0.39381 val_roc= 0.69985 val_ap= 0.74496 time= 0.08096\n",
      "Epoch: 0004 log_lik= 1.121372 train_kl= -0.00092 train_loss= 1.12229 train_acc= 0.35924 val_roc= 0.70819 val_ap= 0.75268 time= 0.08906\n",
      "Epoch: 0005 log_lik= 0.98995245 train_kl= -0.00138 train_loss= 0.99133 train_acc= 0.33653 val_roc= 0.71479 val_ap= 0.75911 time= 0.08449\n",
      "Epoch: 0006 log_lik= 0.8815848 train_kl= -0.00195 train_loss= 0.88353 train_acc= 0.31160 val_roc= 0.72360 val_ap= 0.76675 time= 0.08372\n",
      "Epoch: 0007 log_lik= 0.80670536 train_kl= -0.00262 train_loss= 0.80933 train_acc= 0.28753 val_roc= 0.74503 val_ap= 0.78624 time= 0.08564\n",
      "Epoch: 0008 log_lik= 0.7509126 train_kl= -0.00339 train_loss= 0.75430 train_acc= 0.28486 val_roc= 0.77042 val_ap= 0.80664 time= 0.08416\n",
      "Epoch: 0009 log_lik= 0.71851546 train_kl= -0.00426 train_loss= 0.72278 train_acc= 0.29951 val_roc= 0.78265 val_ap= 0.81225 time= 0.08264\n",
      "Epoch: 0010 log_lik= 0.7055344 train_kl= -0.00517 train_loss= 0.71071 train_acc= 0.28869 val_roc= 0.78180 val_ap= 0.80997 time= 0.08164\n",
      "Epoch: 0011 log_lik= 0.69513905 train_kl= -0.00610 train_loss= 0.70124 train_acc= 0.23245 val_roc= 0.78278 val_ap= 0.81104 time= 0.08264\n",
      "Epoch: 0012 log_lik= 0.6908046 train_kl= -0.00699 train_loss= 0.69780 train_acc= 0.16316 val_roc= 0.80361 val_ap= 0.82396 time= 0.08113\n",
      "Epoch: 0013 log_lik= 0.68462396 train_kl= -0.00779 train_loss= 0.69241 train_acc= 0.15333 val_roc= 0.83069 val_ap= 0.83952 time= 0.08064\n",
      "Epoch: 0014 log_lik= 0.6721885 train_kl= -0.00850 train_loss= 0.68069 train_acc= 0.17370 val_roc= 0.84197 val_ap= 0.84056 time= 0.08035\n",
      "Epoch: 0015 log_lik= 0.6584917 train_kl= -0.00915 train_loss= 0.66764 train_acc= 0.21496 val_roc= 0.84228 val_ap= 0.83814 time= 0.07965\n",
      "Epoch: 0016 log_lik= 0.6435734 train_kl= -0.00974 train_loss= 0.65332 train_acc= 0.23244 val_roc= 0.84101 val_ap= 0.83780 time= 0.08840\n",
      "Epoch: 0017 log_lik= 0.6284616 train_kl= -0.01027 train_loss= 0.63873 train_acc= 0.25551 val_roc= 0.84360 val_ap= 0.83813 time= 0.08164\n",
      "Epoch: 0018 log_lik= 0.61224324 train_kl= -0.01073 train_loss= 0.62298 train_acc= 0.30164 val_roc= 0.84980 val_ap= 0.84007 time= 0.08332\n",
      "Epoch: 0019 log_lik= 0.5967285 train_kl= -0.01110 train_loss= 0.60783 train_acc= 0.34788 val_roc= 0.85236 val_ap= 0.84025 time= 0.08273\n",
      "Epoch: 0020 log_lik= 0.5797545 train_kl= -0.01138 train_loss= 0.59114 train_acc= 0.39378 val_roc= 0.85164 val_ap= 0.84131 time= 0.08429\n",
      "Epoch: 0021 log_lik= 0.5682987 train_kl= -0.01161 train_loss= 0.57990 train_acc= 0.42354 val_roc= 0.85454 val_ap= 0.84738 time= 0.08222\n",
      "Epoch: 0022 log_lik= 0.55819404 train_kl= -0.01183 train_loss= 0.57002 train_acc= 0.44051 val_roc= 0.85833 val_ap= 0.85181 time= 0.08363\n",
      "Epoch: 0023 log_lik= 0.5471987 train_kl= -0.01206 train_loss= 0.55926 train_acc= 0.45777 val_roc= 0.86117 val_ap= 0.85573 time= 0.08331\n",
      "Epoch: 0024 log_lik= 0.5408848 train_kl= -0.01225 train_loss= 0.55314 train_acc= 0.47197 val_roc= 0.86341 val_ap= 0.86042 time= 0.08164\n",
      "Epoch: 0025 log_lik= 0.5336684 train_kl= -0.01239 train_loss= 0.54606 train_acc= 0.48291 val_roc= 0.86676 val_ap= 0.86646 time= 0.08431\n",
      "Epoch: 0026 log_lik= 0.53007406 train_kl= -0.01251 train_loss= 0.54258 train_acc= 0.48240 val_roc= 0.87221 val_ap= 0.87416 time= 0.08264\n",
      "Epoch: 0027 log_lik= 0.5275679 train_kl= -0.01260 train_loss= 0.54017 train_acc= 0.48066 val_roc= 0.87688 val_ap= 0.88028 time= 0.08466\n",
      "Epoch: 0028 log_lik= 0.52164835 train_kl= -0.01265 train_loss= 0.53430 train_acc= 0.48584 val_roc= 0.87966 val_ap= 0.88493 time= 0.08761\n",
      "Epoch: 0029 log_lik= 0.5097837 train_kl= -0.01264 train_loss= 0.52242 train_acc= 0.49408 val_roc= 0.88226 val_ap= 0.88931 time= 0.08184\n",
      "Epoch: 0030 log_lik= 0.50107414 train_kl= -0.01259 train_loss= 0.51366 train_acc= 0.50131 val_roc= 0.88399 val_ap= 0.89127 time= 0.08102\n",
      "Epoch: 0031 log_lik= 0.49485552 train_kl= -0.01253 train_loss= 0.50738 train_acc= 0.50689 val_roc= 0.88599 val_ap= 0.89284 time= 0.08703\n",
      "Epoch: 0032 log_lik= 0.49061388 train_kl= -0.01246 train_loss= 0.50308 train_acc= 0.51092 val_roc= 0.88706 val_ap= 0.89236 time= 0.08396\n",
      "Epoch: 0033 log_lik= 0.4885915 train_kl= -0.01240 train_loss= 0.50099 train_acc= 0.51415 val_roc= 0.88885 val_ap= 0.89277 time= 0.08300\n",
      "Epoch: 0034 log_lik= 0.48893964 train_kl= -0.01233 train_loss= 0.50127 train_acc= 0.51211 val_roc= 0.88972 val_ap= 0.89248 time= 0.08302\n",
      "Epoch: 0035 log_lik= 0.48756698 train_kl= -0.01228 train_loss= 0.49985 train_acc= 0.51425 val_roc= 0.88956 val_ap= 0.89110 time= 0.08096\n",
      "Epoch: 0036 log_lik= 0.48602968 train_kl= -0.01223 train_loss= 0.49826 train_acc= 0.51488 val_roc= 0.88975 val_ap= 0.89040 time= 0.08100\n",
      "Epoch: 0037 log_lik= 0.48335782 train_kl= -0.01219 train_loss= 0.49555 train_acc= 0.52044 val_roc= 0.89040 val_ap= 0.88994 time= 0.09008\n",
      "Epoch: 0038 log_lik= 0.48113668 train_kl= -0.01214 train_loss= 0.49328 train_acc= 0.52336 val_roc= 0.89122 val_ap= 0.89034 time= 0.08099\n",
      "Epoch: 0039 log_lik= 0.47956344 train_kl= -0.01207 train_loss= 0.49164 train_acc= 0.52381 val_roc= 0.89193 val_ap= 0.89172 time= 0.08202\n",
      "Epoch: 0040 log_lik= 0.47775307 train_kl= -0.01200 train_loss= 0.48975 train_acc= 0.52399 val_roc= 0.89161 val_ap= 0.89208 time= 0.08312\n",
      "Epoch: 0041 log_lik= 0.4748422 train_kl= -0.01193 train_loss= 0.48677 train_acc= 0.52652 val_roc= 0.89141 val_ap= 0.89295 time= 0.08483\n",
      "Epoch: 0042 log_lik= 0.47298244 train_kl= -0.01186 train_loss= 0.48485 train_acc= 0.53021 val_roc= 0.89141 val_ap= 0.89395 time= 0.08103\n",
      "Epoch: 0043 log_lik= 0.47099614 train_kl= -0.01181 train_loss= 0.48281 train_acc= 0.53063 val_roc= 0.89147 val_ap= 0.89536 time= 0.08330\n",
      "Epoch: 0044 log_lik= 0.47044533 train_kl= -0.01177 train_loss= 0.48221 train_acc= 0.52828 val_roc= 0.89196 val_ap= 0.89710 time= 0.08472\n",
      "Epoch: 0045 log_lik= 0.46930185 train_kl= -0.01172 train_loss= 0.48102 train_acc= 0.52678 val_roc= 0.89127 val_ap= 0.89707 time= 0.08398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0046 log_lik= 0.4682382 train_kl= -0.01167 train_loss= 0.47991 train_acc= 0.52637 val_roc= 0.89050 val_ap= 0.89706 time= 0.08401\n",
      "Epoch: 0047 log_lik= 0.46622354 train_kl= -0.01164 train_loss= 0.47787 train_acc= 0.52943 val_roc= 0.89082 val_ap= 0.89813 time= 0.08095\n",
      "Epoch: 0048 log_lik= 0.46438277 train_kl= -0.01163 train_loss= 0.47601 train_acc= 0.53103 val_roc= 0.89111 val_ap= 0.89923 time= 0.08251\n",
      "Epoch: 0049 log_lik= 0.46341902 train_kl= -0.01161 train_loss= 0.47503 train_acc= 0.53072 val_roc= 0.89156 val_ap= 0.90008 time= 0.08297\n",
      "Epoch: 0050 log_lik= 0.4628276 train_kl= -0.01159 train_loss= 0.47442 train_acc= 0.53031 val_roc= 0.89203 val_ap= 0.90114 time= 0.08200\n",
      "Epoch: 0051 log_lik= 0.4621205 train_kl= -0.01157 train_loss= 0.47369 train_acc= 0.52898 val_roc= 0.89245 val_ap= 0.90201 time= 0.08299\n",
      "Epoch: 0052 log_lik= 0.46074495 train_kl= -0.01154 train_loss= 0.47228 train_acc= 0.52949 val_roc= 0.89277 val_ap= 0.90330 time= 0.08284\n",
      "Epoch: 0053 log_lik= 0.4593534 train_kl= -0.01151 train_loss= 0.47086 train_acc= 0.52914 val_roc= 0.89302 val_ap= 0.90390 time= 0.08205\n",
      "Epoch: 0054 log_lik= 0.45855746 train_kl= -0.01148 train_loss= 0.47003 train_acc= 0.53081 val_roc= 0.89325 val_ap= 0.90473 time= 0.08609\n",
      "Epoch: 0055 log_lik= 0.4576008 train_kl= -0.01145 train_loss= 0.46905 train_acc= 0.53148 val_roc= 0.89449 val_ap= 0.90639 time= 0.08999\n",
      "Epoch: 0056 log_lik= 0.4569026 train_kl= -0.01142 train_loss= 0.46832 train_acc= 0.53120 val_roc= 0.89520 val_ap= 0.90764 time= 0.08300\n",
      "Epoch: 0057 log_lik= 0.45673352 train_kl= -0.01141 train_loss= 0.46814 train_acc= 0.53213 val_roc= 0.89570 val_ap= 0.90844 time= 0.08010\n",
      "Epoch: 0058 log_lik= 0.4554434 train_kl= -0.01140 train_loss= 0.46684 train_acc= 0.53186 val_roc= 0.89581 val_ap= 0.90869 time= 0.08401\n",
      "Epoch: 0059 log_lik= 0.45475954 train_kl= -0.01139 train_loss= 0.46615 train_acc= 0.53370 val_roc= 0.89646 val_ap= 0.90944 time= 0.08295\n",
      "Epoch: 0060 log_lik= 0.454865 train_kl= -0.01139 train_loss= 0.46625 train_acc= 0.53435 val_roc= 0.89689 val_ap= 0.90981 time= 0.08273\n",
      "Epoch: 0061 log_lik= 0.45435616 train_kl= -0.01139 train_loss= 0.46575 train_acc= 0.53380 val_roc= 0.89757 val_ap= 0.91074 time= 0.08529\n",
      "Epoch: 0062 log_lik= 0.45400602 train_kl= -0.01140 train_loss= 0.46541 train_acc= 0.53577 val_roc= 0.89800 val_ap= 0.91145 time= 0.08598\n",
      "Epoch: 0063 log_lik= 0.45243037 train_kl= -0.01141 train_loss= 0.46384 train_acc= 0.53574 val_roc= 0.89806 val_ap= 0.91157 time= 0.08602\n",
      "Epoch: 0064 log_lik= 0.45228267 train_kl= -0.01141 train_loss= 0.46370 train_acc= 0.53603 val_roc= 0.89787 val_ap= 0.91131 time= 0.08107\n",
      "Epoch: 0065 log_lik= 0.45181054 train_kl= -0.01141 train_loss= 0.46322 train_acc= 0.53659 val_roc= 0.89795 val_ap= 0.91132 time= 0.08002\n",
      "Epoch: 0066 log_lik= 0.45126268 train_kl= -0.01141 train_loss= 0.46267 train_acc= 0.53588 val_roc= 0.89783 val_ap= 0.91101 time= 0.08496\n",
      "Epoch: 0067 log_lik= 0.4505753 train_kl= -0.01142 train_loss= 0.46199 train_acc= 0.53660 val_roc= 0.89770 val_ap= 0.91078 time= 0.08298\n",
      "Epoch: 0068 log_lik= 0.450394 train_kl= -0.01143 train_loss= 0.46183 train_acc= 0.53743 val_roc= 0.89784 val_ap= 0.91128 time= 0.08400\n",
      "Epoch: 0069 log_lik= 0.4499397 train_kl= -0.01144 train_loss= 0.46138 train_acc= 0.53633 val_roc= 0.89776 val_ap= 0.91146 time= 0.08508\n",
      "Epoch: 0070 log_lik= 0.44932857 train_kl= -0.01143 train_loss= 0.46076 train_acc= 0.53860 val_roc= 0.89766 val_ap= 0.91125 time= 0.08499\n",
      "Epoch: 0071 log_lik= 0.44846168 train_kl= -0.01144 train_loss= 0.45990 train_acc= 0.53844 val_roc= 0.89740 val_ap= 0.91088 time= 0.08350\n",
      "Epoch: 0072 log_lik= 0.44802427 train_kl= -0.01145 train_loss= 0.45947 train_acc= 0.53794 val_roc= 0.89611 val_ap= 0.90996 time= 0.08392\n",
      "Epoch: 0073 log_lik= 0.44798577 train_kl= -0.01145 train_loss= 0.45944 train_acc= 0.53779 val_roc= 0.89557 val_ap= 0.90991 time= 0.08550\n",
      "Epoch: 0074 log_lik= 0.44746205 train_kl= -0.01145 train_loss= 0.45892 train_acc= 0.53799 val_roc= 0.89497 val_ap= 0.90990 time= 0.08199\n",
      "Epoch: 0075 log_lik= 0.44730848 train_kl= -0.01144 train_loss= 0.45875 train_acc= 0.53731 val_roc= 0.89523 val_ap= 0.91022 time= 0.08193\n",
      "Epoch: 0076 log_lik= 0.4469282 train_kl= -0.01144 train_loss= 0.45837 train_acc= 0.53696 val_roc= 0.89534 val_ap= 0.91019 time= 0.08454\n",
      "Epoch: 0077 log_lik= 0.44619474 train_kl= -0.01145 train_loss= 0.45764 train_acc= 0.53725 val_roc= 0.89463 val_ap= 0.90971 time= 0.08287\n",
      "Epoch: 0078 log_lik= 0.44599283 train_kl= -0.01146 train_loss= 0.45745 train_acc= 0.53825 val_roc= 0.89400 val_ap= 0.90943 time= 0.08007\n",
      "Epoch: 0079 log_lik= 0.4455628 train_kl= -0.01146 train_loss= 0.45703 train_acc= 0.53820 val_roc= 0.89404 val_ap= 0.90971 time= 0.08599\n",
      "Epoch: 0080 log_lik= 0.4449832 train_kl= -0.01147 train_loss= 0.45645 train_acc= 0.53833 val_roc= 0.89416 val_ap= 0.90993 time= 0.08499\n",
      "Epoch: 0081 log_lik= 0.4448657 train_kl= -0.01148 train_loss= 0.45635 train_acc= 0.53772 val_roc= 0.89472 val_ap= 0.91042 time= 0.08499\n",
      "Epoch: 0082 log_lik= 0.44472802 train_kl= -0.01149 train_loss= 0.45622 train_acc= 0.53833 val_roc= 0.89394 val_ap= 0.90969 time= 0.08105\n",
      "Epoch: 0083 log_lik= 0.44404367 train_kl= -0.01151 train_loss= 0.45555 train_acc= 0.53821 val_roc= 0.89294 val_ap= 0.90918 time= 0.08895\n",
      "Epoch: 0084 log_lik= 0.4438917 train_kl= -0.01152 train_loss= 0.45541 train_acc= 0.53807 val_roc= 0.89286 val_ap= 0.90939 time= 0.08401\n",
      "Epoch: 0085 log_lik= 0.44309843 train_kl= -0.01152 train_loss= 0.45462 train_acc= 0.53792 val_roc= 0.89342 val_ap= 0.90960 time= 0.08300\n",
      "Epoch: 0086 log_lik= 0.44265485 train_kl= -0.01153 train_loss= 0.45418 train_acc= 0.53766 val_roc= 0.89398 val_ap= 0.90997 time= 0.08305\n",
      "Epoch: 0087 log_lik= 0.443065 train_kl= -0.01154 train_loss= 0.45460 train_acc= 0.53822 val_roc= 0.89348 val_ap= 0.90958 time= 0.08103\n",
      "Epoch: 0088 log_lik= 0.44234315 train_kl= -0.01155 train_loss= 0.45389 train_acc= 0.53891 val_roc= 0.89309 val_ap= 0.90920 time= 0.08093\n",
      "Epoch: 0089 log_lik= 0.44175342 train_kl= -0.01155 train_loss= 0.45330 train_acc= 0.53835 val_roc= 0.89315 val_ap= 0.90940 time= 0.08002\n",
      "Epoch: 0090 log_lik= 0.4417829 train_kl= -0.01156 train_loss= 0.45334 train_acc= 0.53847 val_roc= 0.89354 val_ap= 0.90970 time= 0.08100\n",
      "Epoch: 0091 log_lik= 0.44123855 train_kl= -0.01157 train_loss= 0.45281 train_acc= 0.53807 val_roc= 0.89393 val_ap= 0.91019 time= 0.08297\n",
      "Epoch: 0092 log_lik= 0.4415666 train_kl= -0.01158 train_loss= 0.45315 train_acc= 0.53830 val_roc= 0.89413 val_ap= 0.91061 time= 0.08303\n",
      "Epoch: 0093 log_lik= 0.44080135 train_kl= -0.01158 train_loss= 0.45238 train_acc= 0.53826 val_roc= 0.89342 val_ap= 0.90990 time= 0.08899\n",
      "Epoch: 0094 log_lik= 0.44044474 train_kl= -0.01158 train_loss= 0.45203 train_acc= 0.53796 val_roc= 0.89346 val_ap= 0.91036 time= 0.08309\n",
      "Epoch: 0095 log_lik= 0.44003242 train_kl= -0.01159 train_loss= 0.45163 train_acc= 0.53855 val_roc= 0.89437 val_ap= 0.91131 time= 0.08094\n",
      "Epoch: 0096 log_lik= 0.43960804 train_kl= -0.01161 train_loss= 0.45122 train_acc= 0.53739 val_roc= 0.89492 val_ap= 0.91159 time= 0.08392\n",
      "Epoch: 0097 log_lik= 0.43948776 train_kl= -0.01162 train_loss= 0.45111 train_acc= 0.53727 val_roc= 0.89527 val_ap= 0.91188 time= 0.08112\n",
      "Epoch: 0098 log_lik= 0.43928152 train_kl= -0.01162 train_loss= 0.45090 train_acc= 0.53752 val_roc= 0.89513 val_ap= 0.91209 time= 0.08495\n",
      "Epoch: 0099 log_lik= 0.43858045 train_kl= -0.01163 train_loss= 0.45021 train_acc= 0.53790 val_roc= 0.89474 val_ap= 0.91236 time= 0.08200\n",
      "Epoch: 0100 log_lik= 0.43805647 train_kl= -0.01165 train_loss= 0.44970 train_acc= 0.53858 val_roc= 0.89490 val_ap= 0.91267 time= 0.08511\n",
      "Epoch: 0101 log_lik= 0.43771115 train_kl= -0.01166 train_loss= 0.44937 train_acc= 0.53817 val_roc= 0.89572 val_ap= 0.91273 time= 0.08096\n",
      "Epoch: 0102 log_lik= 0.43731752 train_kl= -0.01166 train_loss= 0.44898 train_acc= 0.53809 val_roc= 0.89675 val_ap= 0.91398 time= 0.08295\n",
      "Epoch: 0103 log_lik= 0.43725386 train_kl= -0.01169 train_loss= 0.44894 train_acc= 0.53773 val_roc= 0.89617 val_ap= 0.91390 time= 0.08511\n",
      "Epoch: 0104 log_lik= 0.4364746 train_kl= -0.01170 train_loss= 0.44818 train_acc= 0.53845 val_roc= 0.89578 val_ap= 0.91377 time= 0.08298\n",
      "Epoch: 0105 log_lik= 0.43625844 train_kl= -0.01170 train_loss= 0.44796 train_acc= 0.53788 val_roc= 0.89631 val_ap= 0.91443 time= 0.08495\n",
      "Epoch: 0106 log_lik= 0.43578428 train_kl= -0.01170 train_loss= 0.44749 train_acc= 0.53861 val_roc= 0.89699 val_ap= 0.91478 time= 0.08002\n",
      "Epoch: 0107 log_lik= 0.43554822 train_kl= -0.01172 train_loss= 0.44726 train_acc= 0.53739 val_roc= 0.89738 val_ap= 0.91555 time= 0.08396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0108 log_lik= 0.4352376 train_kl= -0.01173 train_loss= 0.44697 train_acc= 0.53798 val_roc= 0.89747 val_ap= 0.91585 time= 0.08358\n",
      "Epoch: 0109 log_lik= 0.43466038 train_kl= -0.01176 train_loss= 0.44642 train_acc= 0.53845 val_roc= 0.89819 val_ap= 0.91589 time= 0.08185\n",
      "Epoch: 0110 log_lik= 0.4345824 train_kl= -0.01176 train_loss= 0.44635 train_acc= 0.53926 val_roc= 0.89806 val_ap= 0.91600 time= 0.08304\n",
      "Epoch: 0111 log_lik= 0.4342624 train_kl= -0.01176 train_loss= 0.44602 train_acc= 0.53827 val_roc= 0.89847 val_ap= 0.91681 time= 0.08200\n",
      "Epoch: 0112 log_lik= 0.43389 train_kl= -0.01177 train_loss= 0.44566 train_acc= 0.53857 val_roc= 0.89852 val_ap= 0.91714 time= 0.08407\n",
      "Epoch: 0113 log_lik= 0.43395585 train_kl= -0.01180 train_loss= 0.44575 train_acc= 0.54026 val_roc= 0.89845 val_ap= 0.91693 time= 0.08299\n",
      "Epoch: 0114 log_lik= 0.4330358 train_kl= -0.01181 train_loss= 0.44484 train_acc= 0.53995 val_roc= 0.89874 val_ap= 0.91720 time= 0.08502\n",
      "Epoch: 0115 log_lik= 0.4327936 train_kl= -0.01181 train_loss= 0.44461 train_acc= 0.53924 val_roc= 0.89889 val_ap= 0.91756 time= 0.08095\n",
      "Epoch: 0116 log_lik= 0.432395 train_kl= -0.01182 train_loss= 0.44422 train_acc= 0.53971 val_roc= 0.89907 val_ap= 0.91808 time= 0.08204\n",
      "Epoch: 0117 log_lik= 0.43298897 train_kl= -0.01183 train_loss= 0.44482 train_acc= 0.53987 val_roc= 0.89902 val_ap= 0.91784 time= 0.08199\n",
      "Epoch: 0118 log_lik= 0.43238792 train_kl= -0.01184 train_loss= 0.44423 train_acc= 0.54031 val_roc= 0.89906 val_ap= 0.91739 time= 0.09052\n",
      "Epoch: 0119 log_lik= 0.43192798 train_kl= -0.01185 train_loss= 0.44378 train_acc= 0.53934 val_roc= 0.89878 val_ap= 0.91759 time= 0.09696\n",
      "Epoch: 0120 log_lik= 0.43153644 train_kl= -0.01185 train_loss= 0.44339 train_acc= 0.54001 val_roc= 0.89858 val_ap= 0.91750 time= 0.08597\n",
      "Epoch: 0121 log_lik= 0.43139088 train_kl= -0.01186 train_loss= 0.44325 train_acc= 0.53966 val_roc= 0.90003 val_ap= 0.91845 time= 0.08117\n",
      "Epoch: 0122 log_lik= 0.43106022 train_kl= -0.01186 train_loss= 0.44292 train_acc= 0.54088 val_roc= 0.90053 val_ap= 0.91864 time= 0.07979\n",
      "Epoch: 0123 log_lik= 0.43108255 train_kl= -0.01186 train_loss= 0.44294 train_acc= 0.54007 val_roc= 0.89894 val_ap= 0.91773 time= 0.08912\n",
      "Epoch: 0124 log_lik= 0.4309004 train_kl= -0.01187 train_loss= 0.44277 train_acc= 0.53952 val_roc= 0.89923 val_ap= 0.91762 time= 0.08186\n",
      "Epoch: 0125 log_lik= 0.43054938 train_kl= -0.01189 train_loss= 0.44244 train_acc= 0.53933 val_roc= 0.90059 val_ap= 0.91840 time= 0.08399\n",
      "Epoch: 0126 log_lik= 0.4301165 train_kl= -0.01190 train_loss= 0.44202 train_acc= 0.54059 val_roc= 0.90110 val_ap= 0.91889 time= 0.08303\n",
      "Epoch: 0127 log_lik= 0.4304495 train_kl= -0.01190 train_loss= 0.44235 train_acc= 0.53893 val_roc= 0.90009 val_ap= 0.91888 time= 0.08302\n",
      "Epoch: 0128 log_lik= 0.4298608 train_kl= -0.01191 train_loss= 0.44177 train_acc= 0.54019 val_roc= 0.89903 val_ap= 0.91791 time= 0.09297\n",
      "Epoch: 0129 log_lik= 0.42950952 train_kl= -0.01192 train_loss= 0.44143 train_acc= 0.53983 val_roc= 0.89990 val_ap= 0.91790 time= 0.08203\n",
      "Epoch: 0130 log_lik= 0.4290309 train_kl= -0.01193 train_loss= 0.44096 train_acc= 0.53953 val_roc= 0.90065 val_ap= 0.91875 time= 0.08295\n",
      "Epoch: 0131 log_lik= 0.42908445 train_kl= -0.01194 train_loss= 0.44102 train_acc= 0.54023 val_roc= 0.90035 val_ap= 0.91908 time= 0.08249\n",
      "Epoch: 0132 log_lik= 0.4291697 train_kl= -0.01195 train_loss= 0.44112 train_acc= 0.54135 val_roc= 0.89958 val_ap= 0.91848 time= 0.08548\n",
      "Epoch: 0133 log_lik= 0.42849872 train_kl= -0.01195 train_loss= 0.44045 train_acc= 0.54044 val_roc= 0.89930 val_ap= 0.91707 time= 0.08505\n",
      "Epoch: 0134 log_lik= 0.42880204 train_kl= -0.01196 train_loss= 0.44076 train_acc= 0.53904 val_roc= 0.90037 val_ap= 0.91821 time= 0.08294\n",
      "Epoch: 0135 log_lik= 0.42793733 train_kl= -0.01197 train_loss= 0.43991 train_acc= 0.53896 val_roc= 0.90016 val_ap= 0.91896 time= 0.08297\n",
      "Epoch: 0136 log_lik= 0.42796883 train_kl= -0.01198 train_loss= 0.43995 train_acc= 0.54014 val_roc= 0.89909 val_ap= 0.91837 time= 0.08314\n",
      "Epoch: 0137 log_lik= 0.4276453 train_kl= -0.01199 train_loss= 0.43963 train_acc= 0.54121 val_roc= 0.89906 val_ap= 0.91767 time= 0.08194\n",
      "Epoch: 0138 log_lik= 0.42744377 train_kl= -0.01199 train_loss= 0.43943 train_acc= 0.53952 val_roc= 0.89929 val_ap= 0.91781 time= 0.08105\n",
      "Epoch: 0139 log_lik= 0.42705935 train_kl= -0.01199 train_loss= 0.43904 train_acc= 0.53988 val_roc= 0.89967 val_ap= 0.91822 time= 0.08405\n",
      "Epoch: 0140 log_lik= 0.42713276 train_kl= -0.01200 train_loss= 0.43914 train_acc= 0.53989 val_roc= 0.89920 val_ap= 0.91806 time= 0.08296\n",
      "Epoch: 0141 log_lik= 0.42668977 train_kl= -0.01202 train_loss= 0.43871 train_acc= 0.54065 val_roc= 0.89878 val_ap= 0.91760 time= 0.08499\n",
      "Epoch: 0142 log_lik= 0.42630678 train_kl= -0.01202 train_loss= 0.43832 train_acc= 0.53945 val_roc= 0.89893 val_ap= 0.91704 time= 0.08381\n",
      "Epoch: 0143 log_lik= 0.42568818 train_kl= -0.01202 train_loss= 0.43771 train_acc= 0.53978 val_roc= 0.89978 val_ap= 0.91796 time= 0.08184\n",
      "Epoch: 0144 log_lik= 0.42582136 train_kl= -0.01204 train_loss= 0.43786 train_acc= 0.54028 val_roc= 0.89870 val_ap= 0.91749 time= 0.08208\n",
      "Epoch: 0145 log_lik= 0.42585427 train_kl= -0.01205 train_loss= 0.43790 train_acc= 0.54019 val_roc= 0.89886 val_ap= 0.91744 time= 0.08233\n",
      "Epoch: 0146 log_lik= 0.4254795 train_kl= -0.01206 train_loss= 0.43754 train_acc= 0.54031 val_roc= 0.89956 val_ap= 0.91826 time= 0.08196\n",
      "Epoch: 0147 log_lik= 0.4251673 train_kl= -0.01206 train_loss= 0.43723 train_acc= 0.54046 val_roc= 0.89975 val_ap= 0.91790 time= 0.08104\n",
      "Epoch: 0148 log_lik= 0.42496994 train_kl= -0.01207 train_loss= 0.43704 train_acc= 0.53955 val_roc= 0.89956 val_ap= 0.91741 time= 0.08199\n",
      "Epoch: 0149 log_lik= 0.42506242 train_kl= -0.01210 train_loss= 0.43716 train_acc= 0.54027 val_roc= 0.89877 val_ap= 0.91740 time= 0.08300\n",
      "Epoch: 0150 log_lik= 0.42458647 train_kl= -0.01211 train_loss= 0.43669 train_acc= 0.54098 val_roc= 0.89917 val_ap= 0.91824 time= 0.08100\n",
      "Epoch: 0151 log_lik= 0.4245944 train_kl= -0.01211 train_loss= 0.43671 train_acc= 0.54051 val_roc= 0.89985 val_ap= 0.91802 time= 0.08273\n",
      "Epoch: 0152 log_lik= 0.42377993 train_kl= -0.01212 train_loss= 0.43590 train_acc= 0.54038 val_roc= 0.89919 val_ap= 0.91746 time= 0.08363\n",
      "Epoch: 0153 log_lik= 0.4241102 train_kl= -0.01213 train_loss= 0.43624 train_acc= 0.54044 val_roc= 0.89852 val_ap= 0.91712 time= 0.08264\n",
      "Epoch: 0154 log_lik= 0.42390198 train_kl= -0.01214 train_loss= 0.43604 train_acc= 0.53988 val_roc= 0.89893 val_ap= 0.91791 time= 0.08198\n",
      "Epoch: 0155 log_lik= 0.42360616 train_kl= -0.01216 train_loss= 0.43577 train_acc= 0.53985 val_roc= 0.89956 val_ap= 0.91784 time= 0.08505\n",
      "Epoch: 0156 log_lik= 0.4231066 train_kl= -0.01217 train_loss= 0.43527 train_acc= 0.54077 val_roc= 0.89886 val_ap= 0.91700 time= 0.08298\n",
      "Epoch: 0157 log_lik= 0.4233396 train_kl= -0.01217 train_loss= 0.43551 train_acc= 0.54043 val_roc= 0.89825 val_ap= 0.91661 time= 0.08399\n",
      "Epoch: 0158 log_lik= 0.4230441 train_kl= -0.01218 train_loss= 0.43522 train_acc= 0.54066 val_roc= 0.89784 val_ap= 0.91635 time= 0.08304\n",
      "Epoch: 0159 log_lik= 0.42294344 train_kl= -0.01218 train_loss= 0.43513 train_acc= 0.54092 val_roc= 0.89861 val_ap= 0.91687 time= 0.08098\n",
      "Epoch: 0160 log_lik= 0.42262885 train_kl= -0.01221 train_loss= 0.43483 train_acc= 0.53998 val_roc= 0.89861 val_ap= 0.91685 time= 0.08298\n",
      "Epoch: 0161 log_lik= 0.4220964 train_kl= -0.01222 train_loss= 0.43431 train_acc= 0.54127 val_roc= 0.89714 val_ap= 0.91497 time= 0.08104\n",
      "Epoch: 0162 log_lik= 0.42195034 train_kl= -0.01221 train_loss= 0.43416 train_acc= 0.54082 val_roc= 0.89695 val_ap= 0.91494 time= 0.08497\n",
      "Epoch: 0163 log_lik= 0.4218695 train_kl= -0.01222 train_loss= 0.43409 train_acc= 0.54092 val_roc= 0.89780 val_ap= 0.91601 time= 0.08297\n",
      "Epoch: 0164 log_lik= 0.42142555 train_kl= -0.01224 train_loss= 0.43366 train_acc= 0.54129 val_roc= 0.89802 val_ap= 0.91547 time= 0.08105\n",
      "Epoch: 0165 log_lik= 0.4216577 train_kl= -0.01226 train_loss= 0.43392 train_acc= 0.54030 val_roc= 0.89598 val_ap= 0.91370 time= 0.08502\n",
      "Epoch: 0166 log_lik= 0.4212307 train_kl= -0.01224 train_loss= 0.43348 train_acc= 0.54142 val_roc= 0.89507 val_ap= 0.91298 time= 0.08297\n",
      "Epoch: 0167 log_lik= 0.4215726 train_kl= -0.01225 train_loss= 0.43382 train_acc= 0.54115 val_roc= 0.89620 val_ap= 0.91434 time= 0.08111\n",
      "Epoch: 0168 log_lik= 0.42099303 train_kl= -0.01226 train_loss= 0.43325 train_acc= 0.54131 val_roc= 0.89696 val_ap= 0.91482 time= 0.08393\n",
      "Epoch: 0169 log_lik= 0.421027 train_kl= -0.01227 train_loss= 0.43330 train_acc= 0.54057 val_roc= 0.89573 val_ap= 0.91322 time= 0.08906\n",
      "Epoch: 0170 log_lik= 0.42071125 train_kl= -0.01228 train_loss= 0.43299 train_acc= 0.54112 val_roc= 0.89432 val_ap= 0.91222 time= 0.08093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0171 log_lik= 0.4204045 train_kl= -0.01227 train_loss= 0.43268 train_acc= 0.54132 val_roc= 0.89516 val_ap= 0.91302 time= 0.08297\n",
      "Epoch: 0172 log_lik= 0.42036864 train_kl= -0.01228 train_loss= 0.43264 train_acc= 0.54125 val_roc= 0.89572 val_ap= 0.91337 time= 0.08297\n",
      "Epoch: 0173 log_lik= 0.42034078 train_kl= -0.01228 train_loss= 0.43262 train_acc= 0.54180 val_roc= 0.89517 val_ap= 0.91323 time= 0.08282\n",
      "Epoch: 0174 log_lik= 0.42041367 train_kl= -0.01227 train_loss= 0.43268 train_acc= 0.53993 val_roc= 0.89403 val_ap= 0.91227 time= 0.08221\n",
      "Epoch: 0175 log_lik= 0.41987067 train_kl= -0.01228 train_loss= 0.43215 train_acc= 0.54205 val_roc= 0.89338 val_ap= 0.91163 time= 0.08805\n",
      "Epoch: 0176 log_lik= 0.4198651 train_kl= -0.01228 train_loss= 0.43215 train_acc= 0.54095 val_roc= 0.89497 val_ap= 0.91285 time= 0.07994\n",
      "Epoch: 0177 log_lik= 0.4200222 train_kl= -0.01227 train_loss= 0.43229 train_acc= 0.54060 val_roc= 0.89550 val_ap= 0.91283 time= 0.08205\n",
      "Epoch: 0178 log_lik= 0.41965562 train_kl= -0.01228 train_loss= 0.43194 train_acc= 0.54174 val_roc= 0.89401 val_ap= 0.91245 time= 0.08194\n",
      "Epoch: 0179 log_lik= 0.41949555 train_kl= -0.01228 train_loss= 0.43178 train_acc= 0.54209 val_roc= 0.89313 val_ap= 0.91217 time= 0.08024\n",
      "Epoch: 0180 log_lik= 0.41950205 train_kl= -0.01228 train_loss= 0.43179 train_acc= 0.54245 val_roc= 0.89449 val_ap= 0.91215 time= 0.08380\n",
      "Epoch: 0181 log_lik= 0.4192192 train_kl= -0.01229 train_loss= 0.43151 train_acc= 0.54177 val_roc= 0.89458 val_ap= 0.91233 time= 0.08302\n",
      "Epoch: 0182 log_lik= 0.41913274 train_kl= -0.01228 train_loss= 0.43142 train_acc= 0.54151 val_roc= 0.89388 val_ap= 0.91219 time= 0.08195\n",
      "Epoch: 0183 log_lik= 0.41901234 train_kl= -0.01229 train_loss= 0.43131 train_acc= 0.54250 val_roc= 0.89322 val_ap= 0.91152 time= 0.08257\n",
      "Epoch: 0184 log_lik= 0.4191474 train_kl= -0.01230 train_loss= 0.43145 train_acc= 0.54275 val_roc= 0.89452 val_ap= 0.91254 time= 0.08181\n",
      "Epoch: 0185 log_lik= 0.41879824 train_kl= -0.01230 train_loss= 0.43110 train_acc= 0.54198 val_roc= 0.89403 val_ap= 0.91223 time= 0.08205\n",
      "Epoch: 0186 log_lik= 0.4185323 train_kl= -0.01230 train_loss= 0.43083 train_acc= 0.54265 val_roc= 0.89398 val_ap= 0.91190 time= 0.08931\n",
      "Epoch: 0187 log_lik= 0.41900405 train_kl= -0.01230 train_loss= 0.43131 train_acc= 0.54206 val_roc= 0.89437 val_ap= 0.91217 time= 0.08267\n",
      "Epoch: 0188 log_lik= 0.41859576 train_kl= -0.01231 train_loss= 0.43091 train_acc= 0.54250 val_roc= 0.89463 val_ap= 0.91289 time= 0.08302\n",
      "Epoch: 0189 log_lik= 0.4185742 train_kl= -0.01231 train_loss= 0.43089 train_acc= 0.54306 val_roc= 0.89468 val_ap= 0.91284 time= 0.08100\n",
      "Epoch: 0190 log_lik= 0.41823784 train_kl= -0.01232 train_loss= 0.43056 train_acc= 0.54264 val_roc= 0.89482 val_ap= 0.91252 time= 0.08198\n",
      "Epoch: 0191 log_lik= 0.41869247 train_kl= -0.01232 train_loss= 0.43101 train_acc= 0.54140 val_roc= 0.89394 val_ap= 0.91235 time= 0.08196\n",
      "Epoch: 0192 log_lik= 0.41870862 train_kl= -0.01231 train_loss= 0.43102 train_acc= 0.54180 val_roc= 0.89504 val_ap= 0.91290 time= 0.08101\n",
      "Epoch: 0193 log_lik= 0.4182109 train_kl= -0.01232 train_loss= 0.43053 train_acc= 0.54253 val_roc= 0.89503 val_ap= 0.91259 time= 0.08110\n",
      "Epoch: 0194 log_lik= 0.4186778 train_kl= -0.01232 train_loss= 0.43099 train_acc= 0.54197 val_roc= 0.89450 val_ap= 0.91329 time= 0.08296\n",
      "Epoch: 0195 log_lik= 0.41835755 train_kl= -0.01231 train_loss= 0.43067 train_acc= 0.54253 val_roc= 0.89429 val_ap= 0.91275 time= 0.08302\n",
      "Epoch: 0196 log_lik= 0.41826937 train_kl= -0.01232 train_loss= 0.43059 train_acc= 0.54151 val_roc= 0.89503 val_ap= 0.91214 time= 0.08296\n",
      "Epoch: 0197 log_lik= 0.4185221 train_kl= -0.01231 train_loss= 0.43083 train_acc= 0.54226 val_roc= 0.89602 val_ap= 0.91389 time= 0.08197\n",
      "Epoch: 0198 log_lik= 0.41822246 train_kl= -0.01229 train_loss= 0.43052 train_acc= 0.54296 val_roc= 0.89474 val_ap= 0.91319 time= 0.07994\n",
      "Epoch: 0199 log_lik= 0.41826025 train_kl= -0.01230 train_loss= 0.43056 train_acc= 0.54269 val_roc= 0.89398 val_ap= 0.91194 time= 0.08100\n",
      "Epoch: 0200 log_lik= 0.41842255 train_kl= -0.01230 train_loss= 0.43072 train_acc= 0.54228 val_roc= 0.89570 val_ap= 0.91323 time= 0.08443\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9236809983833161\n",
      "Test AP score: 0.9319760158097183\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始模型输出 kl散度、loss\n",
    "# VGAE\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66051b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:103: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:106: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\initializations.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:106: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:108: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 0001 log_lik= 1.7882688 train_kl= 0.02679 train_loss= 1.81506 train_acc= 0.49582 val_roc= 0.72271 val_ap= 0.74438 time= 2.03334\n",
      "Epoch: 0002 log_lik= 1.8064908 train_kl= 0.02680 train_loss= 1.83329 train_acc= 0.49009 val_roc= 0.72869 val_ap= 0.76027 time= 0.10102\n",
      "Epoch: 0003 log_lik= 1.7538722 train_kl= 0.02681 train_loss= 1.78068 train_acc= 0.48330 val_roc= 0.77604 val_ap= 0.79144 time= 0.09060\n",
      "Epoch: 0004 log_lik= 1.7637148 train_kl= 0.02682 train_loss= 1.79053 train_acc= 0.48468 val_roc= 0.80682 val_ap= 0.80950 time= 0.08956\n",
      "Epoch: 0005 log_lik= 1.7664732 train_kl= 0.02682 train_loss= 1.79330 train_acc= 0.48429 val_roc= 0.81178 val_ap= 0.81117 time= 0.08166\n",
      "Epoch: 0006 log_lik= 1.7021531 train_kl= 0.02684 train_loss= 1.72900 train_acc= 0.48657 val_roc= 0.82579 val_ap= 0.82874 time= 0.08269\n",
      "Epoch: 0007 log_lik= 1.6693606 train_kl= 0.02687 train_loss= 1.69623 train_acc= 0.49150 val_roc= 0.83822 val_ap= 0.84103 time= 0.08215\n",
      "Epoch: 0008 log_lik= 1.6940535 train_kl= 0.02690 train_loss= 1.72096 train_acc= 0.49308 val_roc= 0.84477 val_ap= 0.84429 time= 0.08164\n",
      "Epoch: 0009 log_lik= 1.6005886 train_kl= 0.02696 train_loss= 1.62754 train_acc= 0.49668 val_roc= 0.84973 val_ap= 0.85088 time= 0.08463\n",
      "Epoch: 0010 log_lik= 1.6089301 train_kl= 0.02702 train_loss= 1.63595 train_acc= 0.49583 val_roc= 0.85311 val_ap= 0.85567 time= 0.08463\n",
      "Epoch: 0011 log_lik= 1.5841532 train_kl= 0.02710 train_loss= 1.61126 train_acc= 0.49451 val_roc= 0.86030 val_ap= 0.85745 time= 0.08563\n",
      "Epoch: 0012 log_lik= 1.5505229 train_kl= 0.02716 train_loss= 1.57768 train_acc= 0.49545 val_roc= 0.86056 val_ap= 0.85493 time= 0.08347\n",
      "Epoch: 0013 log_lik= 1.6057323 train_kl= 0.02721 train_loss= 1.63295 train_acc= 0.49238 val_roc= 0.86663 val_ap= 0.86610 time= 0.08263\n",
      "Epoch: 0014 log_lik= 1.5549377 train_kl= 0.02725 train_loss= 1.58218 train_acc= 0.49852 val_roc= 0.87234 val_ap= 0.87501 time= 0.08171\n",
      "Epoch: 0015 log_lik= 1.5673994 train_kl= 0.02726 train_loss= 1.59466 train_acc= 0.49722 val_roc= 0.87867 val_ap= 0.87833 time= 0.08336\n",
      "Epoch: 0016 log_lik= 1.5016677 train_kl= 0.02724 train_loss= 1.52891 train_acc= 0.49803 val_roc= 0.88305 val_ap= 0.87760 time= 0.08782\n",
      "Epoch: 0017 log_lik= 1.4747753 train_kl= 0.02721 train_loss= 1.50199 train_acc= 0.49848 val_roc= 0.88557 val_ap= 0.87873 time= 0.08164\n",
      "Epoch: 0018 log_lik= 1.5062897 train_kl= 0.02718 train_loss= 1.53347 train_acc= 0.49930 val_roc= 0.89010 val_ap= 0.88457 time= 0.08264\n",
      "Epoch: 0019 log_lik= 1.5219223 train_kl= 0.02715 train_loss= 1.54907 train_acc= 0.49898 val_roc= 0.89274 val_ap= 0.88893 time= 0.08280\n",
      "Epoch: 0020 log_lik= 1.5231272 train_kl= 0.02712 train_loss= 1.55025 train_acc= 0.49997 val_roc= 0.89748 val_ap= 0.89393 time= 0.08227\n",
      "Epoch: 0021 log_lik= 1.4861083 train_kl= 0.02711 train_loss= 1.51322 train_acc= 0.49882 val_roc= 0.90201 val_ap= 0.89898 time= 0.08221\n",
      "Epoch: 0022 log_lik= 1.5056671 train_kl= 0.02711 train_loss= 1.53277 train_acc= 0.49670 val_roc= 0.90380 val_ap= 0.90126 time= 0.08363\n",
      "Epoch: 0023 log_lik= 1.4507637 train_kl= 0.02711 train_loss= 1.47787 train_acc= 0.49554 val_roc= 0.90503 val_ap= 0.90313 time= 0.08067\n",
      "Epoch: 0024 log_lik= 1.4827927 train_kl= 0.02711 train_loss= 1.50991 train_acc= 0.49617 val_roc= 0.90491 val_ap= 0.90574 time= 0.08360\n",
      "Epoch: 0025 log_lik= 1.4715092 train_kl= 0.02712 train_loss= 1.49863 train_acc= 0.50090 val_roc= 0.90507 val_ap= 0.90757 time= 0.08167\n",
      "Epoch: 0026 log_lik= 1.484364 train_kl= 0.02713 train_loss= 1.51150 train_acc= 0.50003 val_roc= 0.90578 val_ap= 0.90879 time= 0.08292\n",
      "Epoch: 0027 log_lik= 1.4857311 train_kl= 0.02715 train_loss= 1.51288 train_acc= 0.50046 val_roc= 0.90701 val_ap= 0.91028 time= 0.08401\n",
      "Epoch: 0028 log_lik= 1.480066 train_kl= 0.02717 train_loss= 1.50724 train_acc= 0.49965 val_roc= 0.90896 val_ap= 0.91288 time= 0.08203\n",
      "Epoch: 0029 log_lik= 1.4795291 train_kl= 0.02719 train_loss= 1.50672 train_acc= 0.50089 val_roc= 0.91051 val_ap= 0.91464 time= 0.08400\n",
      "Epoch: 0030 log_lik= 1.4794097 train_kl= 0.02722 train_loss= 1.50662 train_acc= 0.50248 val_roc= 0.91148 val_ap= 0.91570 time= 0.08194\n",
      "Epoch: 0031 log_lik= 1.4910434 train_kl= 0.02724 train_loss= 1.51828 train_acc= 0.50004 val_roc= 0.91224 val_ap= 0.91678 time= 0.08312\n",
      "Epoch: 0032 log_lik= 1.4402832 train_kl= 0.02725 train_loss= 1.46753 train_acc= 0.49995 val_roc= 0.91233 val_ap= 0.91785 time= 0.08200\n",
      "Epoch: 0033 log_lik= 1.4472575 train_kl= 0.02725 train_loss= 1.47451 train_acc= 0.50145 val_roc= 0.91174 val_ap= 0.91868 time= 0.08193\n",
      "Epoch: 0034 log_lik= 1.4530562 train_kl= 0.02725 train_loss= 1.48030 train_acc= 0.50173 val_roc= 0.91252 val_ap= 0.91984 time= 0.08605\n",
      "Epoch: 0035 log_lik= 1.4759626 train_kl= 0.02724 train_loss= 1.50320 train_acc= 0.50283 val_roc= 0.91302 val_ap= 0.92032 time= 0.08297\n",
      "Epoch: 0036 log_lik= 1.4769967 train_kl= 0.02723 train_loss= 1.50423 train_acc= 0.50162 val_roc= 0.91386 val_ap= 0.92125 time= 0.08513\n",
      "Epoch: 0037 log_lik= 1.4832152 train_kl= 0.02722 train_loss= 1.51044 train_acc= 0.50226 val_roc= 0.91551 val_ap= 0.92239 time= 0.08940\n",
      "Epoch: 0038 log_lik= 1.494397 train_kl= 0.02722 train_loss= 1.52162 train_acc= 0.50184 val_roc= 0.91602 val_ap= 0.92211 time= 0.08199\n",
      "Epoch: 0039 log_lik= 1.4244382 train_kl= 0.02722 train_loss= 1.45166 train_acc= 0.50129 val_roc= 0.91667 val_ap= 0.92338 time= 0.08095\n",
      "Epoch: 0040 log_lik= 1.4344832 train_kl= 0.02721 train_loss= 1.46169 train_acc= 0.50110 val_roc= 0.91733 val_ap= 0.92517 time= 0.08440\n",
      "Epoch: 0041 log_lik= 1.4418197 train_kl= 0.02720 train_loss= 1.46902 train_acc= 0.50111 val_roc= 0.91713 val_ap= 0.92502 time= 0.08265\n",
      "Epoch: 0042 log_lik= 1.4484433 train_kl= 0.02719 train_loss= 1.47564 train_acc= 0.50030 val_roc= 0.91501 val_ap= 0.92255 time= 0.08192\n",
      "Epoch: 0043 log_lik= 1.4344338 train_kl= 0.02719 train_loss= 1.46163 train_acc= 0.50052 val_roc= 0.91330 val_ap= 0.92050 time= 0.08499\n",
      "Epoch: 0044 log_lik= 1.4811893 train_kl= 0.02719 train_loss= 1.50838 train_acc= 0.50033 val_roc= 0.91190 val_ap= 0.91919 time= 0.08202\n",
      "Epoch: 0045 log_lik= 1.429015 train_kl= 0.02720 train_loss= 1.45621 train_acc= 0.50143 val_roc= 0.91132 val_ap= 0.91872 time= 0.08299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0046 log_lik= 1.4456143 train_kl= 0.02720 train_loss= 1.47282 train_acc= 0.50029 val_roc= 0.91243 val_ap= 0.91945 time= 0.08504\n",
      "Epoch: 0047 log_lik= 1.4486187 train_kl= 0.02721 train_loss= 1.47583 train_acc= 0.50186 val_roc= 0.91433 val_ap= 0.92136 time= 0.08296\n",
      "Epoch: 0048 log_lik= 1.4848201 train_kl= 0.02722 train_loss= 1.51204 train_acc= 0.50152 val_roc= 0.91609 val_ap= 0.92245 time= 0.08298\n",
      "Epoch: 0049 log_lik= 1.436134 train_kl= 0.02723 train_loss= 1.46337 train_acc= 0.50160 val_roc= 0.91709 val_ap= 0.92338 time= 0.08305\n",
      "Epoch: 0050 log_lik= 1.4613389 train_kl= 0.02724 train_loss= 1.48858 train_acc= 0.50183 val_roc= 0.91660 val_ap= 0.92301 time= 0.08198\n",
      "Epoch: 0051 log_lik= 1.4674987 train_kl= 0.02725 train_loss= 1.49475 train_acc= 0.50220 val_roc= 0.91516 val_ap= 0.92201 time= 0.08302\n",
      "Epoch: 0052 log_lik= 1.447152 train_kl= 0.02726 train_loss= 1.47441 train_acc= 0.50088 val_roc= 0.91314 val_ap= 0.92093 time= 0.08501\n",
      "Epoch: 0053 log_lik= 1.4503248 train_kl= 0.02726 train_loss= 1.47759 train_acc= 0.50138 val_roc= 0.91161 val_ap= 0.92002 time= 0.08299\n",
      "Epoch: 0054 log_lik= 1.4753325 train_kl= 0.02725 train_loss= 1.50259 train_acc= 0.50212 val_roc= 0.91117 val_ap= 0.91920 time= 0.08300\n",
      "Epoch: 0055 log_lik= 1.4212875 train_kl= 0.02725 train_loss= 1.44853 train_acc= 0.50197 val_roc= 0.91143 val_ap= 0.91906 time= 0.08300\n",
      "Epoch: 0056 log_lik= 1.4467388 train_kl= 0.02724 train_loss= 1.47398 train_acc= 0.50049 val_roc= 0.91258 val_ap= 0.92020 time= 0.08299\n",
      "Epoch: 0057 log_lik= 1.4971287 train_kl= 0.02723 train_loss= 1.52436 train_acc= 0.50169 val_roc= 0.91326 val_ap= 0.92111 time= 0.08499\n",
      "Epoch: 0058 log_lik= 1.4547476 train_kl= 0.02722 train_loss= 1.48197 train_acc= 0.50072 val_roc= 0.91414 val_ap= 0.92190 time= 0.08354\n",
      "Epoch: 0059 log_lik= 1.4580487 train_kl= 0.02722 train_loss= 1.48527 train_acc= 0.50164 val_roc= 0.91375 val_ap= 0.92188 time= 0.08294\n",
      "Epoch: 0060 log_lik= 1.4396356 train_kl= 0.02722 train_loss= 1.46686 train_acc= 0.50150 val_roc= 0.91311 val_ap= 0.92190 time= 0.08194\n",
      "Epoch: 0061 log_lik= 1.4293674 train_kl= 0.02722 train_loss= 1.45659 train_acc= 0.50300 val_roc= 0.91278 val_ap= 0.92143 time= 0.08505\n",
      "Epoch: 0062 log_lik= 1.4355066 train_kl= 0.02722 train_loss= 1.46273 train_acc= 0.50195 val_roc= 0.91298 val_ap= 0.92187 time= 0.08199\n",
      "Epoch: 0063 log_lik= 1.4350182 train_kl= 0.02722 train_loss= 1.46224 train_acc= 0.50190 val_roc= 0.91330 val_ap= 0.92201 time= 0.08301\n",
      "Epoch: 0064 log_lik= 1.4450202 train_kl= 0.02722 train_loss= 1.47224 train_acc= 0.50205 val_roc= 0.91450 val_ap= 0.92270 time= 0.08302\n",
      "Epoch: 0065 log_lik= 1.414032 train_kl= 0.02722 train_loss= 1.44125 train_acc= 0.50157 val_roc= 0.91625 val_ap= 0.92369 time= 0.08300\n",
      "Epoch: 0066 log_lik= 1.4376988 train_kl= 0.02723 train_loss= 1.46493 train_acc= 0.50011 val_roc= 0.91703 val_ap= 0.92401 time= 0.08395\n",
      "Epoch: 0067 log_lik= 1.438585 train_kl= 0.02724 train_loss= 1.46582 train_acc= 0.50164 val_roc= 0.91680 val_ap= 0.92440 time= 0.08249\n",
      "Epoch: 0068 log_lik= 1.4410377 train_kl= 0.02725 train_loss= 1.46829 train_acc= 0.50294 val_roc= 0.91493 val_ap= 0.92420 time= 0.08252\n",
      "Epoch: 0069 log_lik= 1.4127437 train_kl= 0.02726 train_loss= 1.44000 train_acc= 0.50221 val_roc= 0.91349 val_ap= 0.92376 time= 0.08301\n",
      "Epoch: 0070 log_lik= 1.4197116 train_kl= 0.02727 train_loss= 1.44698 train_acc= 0.50149 val_roc= 0.91350 val_ap= 0.92423 time= 0.08143\n",
      "Epoch: 0071 log_lik= 1.403249 train_kl= 0.02727 train_loss= 1.43052 train_acc= 0.50312 val_roc= 0.91388 val_ap= 0.92416 time= 0.08334\n",
      "Epoch: 0072 log_lik= 1.4169402 train_kl= 0.02726 train_loss= 1.44420 train_acc= 0.50226 val_roc= 0.91534 val_ap= 0.92472 time= 0.08262\n",
      "Epoch: 0073 log_lik= 1.4471772 train_kl= 0.02726 train_loss= 1.47444 train_acc= 0.50214 val_roc= 0.91600 val_ap= 0.92439 time= 0.08961\n",
      "Epoch: 0074 log_lik= 1.4743085 train_kl= 0.02725 train_loss= 1.50156 train_acc= 0.50250 val_roc= 0.91554 val_ap= 0.92364 time= 0.08498\n",
      "Epoch: 0075 log_lik= 1.4217606 train_kl= 0.02725 train_loss= 1.44901 train_acc= 0.50236 val_roc= 0.91493 val_ap= 0.92344 time= 0.08210\n",
      "Epoch: 0076 log_lik= 1.4678211 train_kl= 0.02724 train_loss= 1.49507 train_acc= 0.50120 val_roc= 0.91323 val_ap= 0.92283 time= 0.08101\n",
      "Epoch: 0077 log_lik= 1.4412818 train_kl= 0.02724 train_loss= 1.46853 train_acc= 0.50218 val_roc= 0.91141 val_ap= 0.92206 time= 0.08296\n",
      "Epoch: 0078 log_lik= 1.4321622 train_kl= 0.02725 train_loss= 1.45941 train_acc= 0.50324 val_roc= 0.91045 val_ap= 0.92164 time= 0.08395\n",
      "Epoch: 0079 log_lik= 1.3820449 train_kl= 0.02725 train_loss= 1.40930 train_acc= 0.50212 val_roc= 0.91047 val_ap= 0.92136 time= 0.08345\n",
      "Epoch: 0080 log_lik= 1.4102154 train_kl= 0.02726 train_loss= 1.43747 train_acc= 0.50249 val_roc= 0.91117 val_ap= 0.92156 time= 0.08401\n",
      "Epoch: 0081 log_lik= 1.4259901 train_kl= 0.02726 train_loss= 1.45325 train_acc= 0.50251 val_roc= 0.91227 val_ap= 0.92205 time= 0.08308\n",
      "Epoch: 0082 log_lik= 1.496161 train_kl= 0.02727 train_loss= 1.52343 train_acc= 0.50272 val_roc= 0.91227 val_ap= 0.92167 time= 0.08192\n",
      "Epoch: 0083 log_lik= 1.4437436 train_kl= 0.02727 train_loss= 1.47101 train_acc= 0.50228 val_roc= 0.91165 val_ap= 0.92065 time= 0.08300\n",
      "Epoch: 0084 log_lik= 1.407303 train_kl= 0.02727 train_loss= 1.43457 train_acc= 0.50251 val_roc= 0.91146 val_ap= 0.92045 time= 0.08397\n",
      "Epoch: 0085 log_lik= 1.437952 train_kl= 0.02727 train_loss= 1.46522 train_acc= 0.50315 val_roc= 0.91180 val_ap= 0.92089 time= 0.08252\n",
      "Epoch: 0086 log_lik= 1.4183443 train_kl= 0.02727 train_loss= 1.44561 train_acc= 0.50312 val_roc= 0.91236 val_ap= 0.92184 time= 0.08193\n",
      "Epoch: 0087 log_lik= 1.410587 train_kl= 0.02727 train_loss= 1.43785 train_acc= 0.50365 val_roc= 0.91311 val_ap= 0.92273 time= 0.08242\n",
      "Epoch: 0088 log_lik= 1.4465886 train_kl= 0.02726 train_loss= 1.47385 train_acc= 0.50282 val_roc= 0.91401 val_ap= 0.92361 time= 0.09098\n",
      "Epoch: 0089 log_lik= 1.4422209 train_kl= 0.02726 train_loss= 1.46949 train_acc= 0.50253 val_roc= 0.91456 val_ap= 0.92362 time= 0.08204\n",
      "Epoch: 0090 log_lik= 1.4286318 train_kl= 0.02727 train_loss= 1.45590 train_acc= 0.50241 val_roc= 0.91382 val_ap= 0.92317 time= 0.08199\n",
      "Epoch: 0091 log_lik= 1.4369451 train_kl= 0.02727 train_loss= 1.46421 train_acc= 0.50165 val_roc= 0.91336 val_ap= 0.92294 time= 0.08198\n",
      "Epoch: 0092 log_lik= 1.4283824 train_kl= 0.02727 train_loss= 1.45565 train_acc= 0.50248 val_roc= 0.91184 val_ap= 0.92217 time= 0.08403\n",
      "Epoch: 0093 log_lik= 1.4321984 train_kl= 0.02727 train_loss= 1.45947 train_acc= 0.50203 val_roc= 0.91139 val_ap= 0.92209 time= 0.08294\n",
      "Epoch: 0094 log_lik= 1.398079 train_kl= 0.02728 train_loss= 1.42536 train_acc= 0.50224 val_roc= 0.91252 val_ap= 0.92262 time= 0.08264\n",
      "Epoch: 0095 log_lik= 1.3973843 train_kl= 0.02728 train_loss= 1.42466 train_acc= 0.50216 val_roc= 0.91460 val_ap= 0.92345 time= 0.08463\n",
      "Epoch: 0096 log_lik= 1.4081557 train_kl= 0.02728 train_loss= 1.43544 train_acc= 0.50324 val_roc= 0.91612 val_ap= 0.92383 time= 0.08178\n",
      "Epoch: 0097 log_lik= 1.4087913 train_kl= 0.02728 train_loss= 1.43607 train_acc= 0.50284 val_roc= 0.91714 val_ap= 0.92464 time= 0.08500\n",
      "Epoch: 0098 log_lik= 1.4477383 train_kl= 0.02728 train_loss= 1.47502 train_acc= 0.50240 val_roc= 0.91717 val_ap= 0.92520 time= 0.08103\n",
      "Epoch: 0099 log_lik= 1.432963 train_kl= 0.02728 train_loss= 1.46024 train_acc= 0.50240 val_roc= 0.91638 val_ap= 0.92589 time= 0.08290\n",
      "Epoch: 0100 log_lik= 1.4319535 train_kl= 0.02728 train_loss= 1.45923 train_acc= 0.50154 val_roc= 0.91560 val_ap= 0.92583 time= 0.08909\n",
      "Epoch: 0101 log_lik= 1.4428335 train_kl= 0.02728 train_loss= 1.47011 train_acc= 0.50198 val_roc= 0.91399 val_ap= 0.92564 time= 0.08197\n",
      "Epoch: 0102 log_lik= 1.4054493 train_kl= 0.02727 train_loss= 1.43272 train_acc= 0.50190 val_roc= 0.91383 val_ap= 0.92594 time= 0.08296\n",
      "Epoch: 0103 log_lik= 1.4235098 train_kl= 0.02726 train_loss= 1.45077 train_acc= 0.50313 val_roc= 0.91415 val_ap= 0.92582 time= 0.08404\n",
      "Epoch: 0104 log_lik= 1.4032376 train_kl= 0.02726 train_loss= 1.43050 train_acc= 0.50295 val_roc= 0.91470 val_ap= 0.92593 time= 0.08100\n",
      "Epoch: 0105 log_lik= 1.4509761 train_kl= 0.02726 train_loss= 1.47824 train_acc= 0.50154 val_roc= 0.91454 val_ap= 0.92570 time= 0.08294\n",
      "Epoch: 0106 log_lik= 1.4062383 train_kl= 0.02726 train_loss= 1.43350 train_acc= 0.50217 val_roc= 0.91463 val_ap= 0.92611 time= 0.08312\n",
      "Epoch: 0107 log_lik= 1.4592366 train_kl= 0.02726 train_loss= 1.48650 train_acc= 0.50226 val_roc= 0.91324 val_ap= 0.92522 time= 0.08393\n",
      "Epoch: 0108 log_lik= 1.4488354 train_kl= 0.02726 train_loss= 1.47609 train_acc= 0.50287 val_roc= 0.91230 val_ap= 0.92485 time= 0.08513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0109 log_lik= 1.4104122 train_kl= 0.02726 train_loss= 1.43767 train_acc= 0.50277 val_roc= 0.91227 val_ap= 0.92474 time= 0.08195\n",
      "Epoch: 0110 log_lik= 1.4020631 train_kl= 0.02726 train_loss= 1.42932 train_acc= 0.50174 val_roc= 0.91288 val_ap= 0.92581 time= 0.08304\n",
      "Epoch: 0111 log_lik= 1.4376241 train_kl= 0.02726 train_loss= 1.46489 train_acc= 0.50220 val_roc= 0.91334 val_ap= 0.92629 time= 0.08199\n",
      "Epoch: 0112 log_lik= 1.4154751 train_kl= 0.02726 train_loss= 1.44274 train_acc= 0.50246 val_roc= 0.91346 val_ap= 0.92632 time= 0.08298\n",
      "Epoch: 0113 log_lik= 1.4272214 train_kl= 0.02727 train_loss= 1.45449 train_acc= 0.50268 val_roc= 0.91272 val_ap= 0.92556 time= 0.08296\n",
      "Epoch: 0114 log_lik= 1.413888 train_kl= 0.02727 train_loss= 1.44116 train_acc= 0.50296 val_roc= 0.91125 val_ap= 0.92401 time= 0.08203\n",
      "Epoch: 0115 log_lik= 1.4090276 train_kl= 0.02727 train_loss= 1.43630 train_acc= 0.50159 val_roc= 0.90893 val_ap= 0.92200 time= 0.08301\n",
      "Epoch: 0116 log_lik= 1.4208944 train_kl= 0.02727 train_loss= 1.44817 train_acc= 0.50194 val_roc= 0.90736 val_ap= 0.92055 time= 0.08303\n",
      "Epoch: 0117 log_lik= 1.4033927 train_kl= 0.02727 train_loss= 1.43067 train_acc= 0.50145 val_roc= 0.90755 val_ap= 0.92009 time= 0.08398\n",
      "Epoch: 0118 log_lik= 1.4038358 train_kl= 0.02728 train_loss= 1.43111 train_acc= 0.50200 val_roc= 0.90899 val_ap= 0.92083 time= 0.08494\n",
      "Epoch: 0119 log_lik= 1.4720649 train_kl= 0.02728 train_loss= 1.49934 train_acc= 0.50086 val_roc= 0.91064 val_ap= 0.92227 time= 0.08404\n",
      "Epoch: 0120 log_lik= 1.414564 train_kl= 0.02728 train_loss= 1.44185 train_acc= 0.50191 val_roc= 0.91217 val_ap= 0.92328 time= 0.08107\n",
      "Epoch: 0121 log_lik= 1.4246656 train_kl= 0.02728 train_loss= 1.45195 train_acc= 0.50290 val_roc= 0.91287 val_ap= 0.92351 time= 0.08313\n",
      "Epoch: 0122 log_lik= 1.414726 train_kl= 0.02728 train_loss= 1.44201 train_acc= 0.50292 val_roc= 0.91239 val_ap= 0.92267 time= 0.08179\n",
      "Epoch: 0123 log_lik= 1.4284358 train_kl= 0.02728 train_loss= 1.45572 train_acc= 0.50293 val_roc= 0.91216 val_ap= 0.92271 time= 0.08320\n",
      "Epoch: 0124 log_lik= 1.3929926 train_kl= 0.02729 train_loss= 1.42028 train_acc= 0.50268 val_roc= 0.91152 val_ap= 0.92268 time= 0.08984\n",
      "Epoch: 0125 log_lik= 1.430038 train_kl= 0.02729 train_loss= 1.45733 train_acc= 0.50239 val_roc= 0.91010 val_ap= 0.92128 time= 0.08200\n",
      "Epoch: 0126 log_lik= 1.4006205 train_kl= 0.02729 train_loss= 1.42791 train_acc= 0.50244 val_roc= 0.90961 val_ap= 0.92054 time= 0.08301\n",
      "Epoch: 0127 log_lik= 1.3843161 train_kl= 0.02730 train_loss= 1.41161 train_acc= 0.50290 val_roc= 0.90927 val_ap= 0.92035 time= 0.08964\n",
      "Epoch: 0128 log_lik= 1.445398 train_kl= 0.02730 train_loss= 1.47269 train_acc= 0.50226 val_roc= 0.90921 val_ap= 0.92064 time= 0.08465\n",
      "Epoch: 0129 log_lik= 1.4223347 train_kl= 0.02730 train_loss= 1.44963 train_acc= 0.50299 val_roc= 0.90982 val_ap= 0.92150 time= 0.08915\n",
      "Epoch: 0130 log_lik= 1.4404472 train_kl= 0.02730 train_loss= 1.46774 train_acc= 0.50323 val_roc= 0.91081 val_ap= 0.92246 time= 0.08861\n",
      "Epoch: 0131 log_lik= 1.4186807 train_kl= 0.02730 train_loss= 1.44598 train_acc= 0.50311 val_roc= 0.91125 val_ap= 0.92291 time= 0.09196\n",
      "Epoch: 0132 log_lik= 1.4248005 train_kl= 0.02730 train_loss= 1.45210 train_acc= 0.50136 val_roc= 0.91091 val_ap= 0.92284 time= 0.08416\n",
      "Epoch: 0133 log_lik= 1.4072516 train_kl= 0.02730 train_loss= 1.43455 train_acc= 0.50237 val_roc= 0.91005 val_ap= 0.92193 time= 0.08883\n",
      "Epoch: 0134 log_lik= 1.4182928 train_kl= 0.02730 train_loss= 1.44559 train_acc= 0.50248 val_roc= 0.90846 val_ap= 0.92055 time= 0.08093\n",
      "Epoch: 0135 log_lik= 1.4441682 train_kl= 0.02730 train_loss= 1.47146 train_acc= 0.50210 val_roc= 0.90708 val_ap= 0.91933 time= 0.08202\n",
      "Epoch: 0136 log_lik= 1.4079137 train_kl= 0.02729 train_loss= 1.43521 train_acc= 0.50161 val_roc= 0.90708 val_ap= 0.91956 time= 0.08305\n",
      "Epoch: 0137 log_lik= 1.3845087 train_kl= 0.02729 train_loss= 1.41180 train_acc= 0.50292 val_roc= 0.90679 val_ap= 0.91978 time= 0.09096\n",
      "Epoch: 0138 log_lik= 1.4318904 train_kl= 0.02728 train_loss= 1.45917 train_acc= 0.50314 val_roc= 0.90650 val_ap= 0.92049 time= 0.08397\n",
      "Epoch: 0139 log_lik= 1.4191895 train_kl= 0.02728 train_loss= 1.44647 train_acc= 0.50303 val_roc= 0.90637 val_ap= 0.92057 time= 0.08417\n",
      "Epoch: 0140 log_lik= 1.4214476 train_kl= 0.02728 train_loss= 1.44873 train_acc= 0.50157 val_roc= 0.90666 val_ap= 0.92067 time= 0.08186\n",
      "Epoch: 0141 log_lik= 1.3914347 train_kl= 0.02728 train_loss= 1.41872 train_acc= 0.50248 val_roc= 0.90642 val_ap= 0.92024 time= 0.08302\n",
      "Epoch: 0142 log_lik= 1.3857671 train_kl= 0.02728 train_loss= 1.41305 train_acc= 0.50202 val_roc= 0.90640 val_ap= 0.91981 time= 0.08200\n",
      "Epoch: 0143 log_lik= 1.4353766 train_kl= 0.02728 train_loss= 1.46265 train_acc= 0.50207 val_roc= 0.90535 val_ap= 0.91864 time= 0.08200\n",
      "Epoch: 0144 log_lik= 1.4367169 train_kl= 0.02728 train_loss= 1.46399 train_acc= 0.50134 val_roc= 0.90397 val_ap= 0.91776 time= 0.08499\n",
      "Epoch: 0145 log_lik= 1.4178288 train_kl= 0.02728 train_loss= 1.44511 train_acc= 0.50228 val_roc= 0.90294 val_ap= 0.91754 time= 0.08245\n",
      "Epoch: 0146 log_lik= 1.391357 train_kl= 0.02728 train_loss= 1.41864 train_acc= 0.50239 val_roc= 0.90174 val_ap= 0.91712 time= 0.08307\n",
      "Epoch: 0147 log_lik= 1.4269732 train_kl= 0.02728 train_loss= 1.45425 train_acc= 0.50260 val_roc= 0.90067 val_ap= 0.91598 time= 0.08348\n",
      "Epoch: 0148 log_lik= 1.4036174 train_kl= 0.02728 train_loss= 1.43090 train_acc= 0.50239 val_roc= 0.90074 val_ap= 0.91604 time= 0.08250\n",
      "Epoch: 0149 log_lik= 1.4135684 train_kl= 0.02729 train_loss= 1.44086 train_acc= 0.50099 val_roc= 0.90186 val_ap= 0.91715 time= 0.08400\n",
      "Epoch: 0150 log_lik= 1.3526418 train_kl= 0.02729 train_loss= 1.37994 train_acc= 0.50249 val_roc= 0.90374 val_ap= 0.91864 time= 0.08396\n",
      "Epoch: 0151 log_lik= 1.4012775 train_kl= 0.02729 train_loss= 1.42857 train_acc= 0.50217 val_roc= 0.90501 val_ap= 0.91925 time= 0.08218\n",
      "Epoch: 0152 log_lik= 1.3833402 train_kl= 0.02729 train_loss= 1.41063 train_acc= 0.50231 val_roc= 0.90616 val_ap= 0.91941 time= 0.08281\n",
      "Epoch: 0153 log_lik= 1.4108044 train_kl= 0.02729 train_loss= 1.43809 train_acc= 0.50295 val_roc= 0.90663 val_ap= 0.91925 time= 0.08184\n",
      "Epoch: 0154 log_lik= 1.4296966 train_kl= 0.02729 train_loss= 1.45699 train_acc= 0.50199 val_roc= 0.90627 val_ap= 0.91901 time= 0.08113\n",
      "Epoch: 0155 log_lik= 1.4204962 train_kl= 0.02729 train_loss= 1.44779 train_acc= 0.50316 val_roc= 0.90609 val_ap= 0.91897 time= 0.08257\n",
      "Epoch: 0156 log_lik= 1.4152124 train_kl= 0.02729 train_loss= 1.44250 train_acc= 0.50254 val_roc= 0.90632 val_ap= 0.91888 time= 0.08363\n",
      "Epoch: 0157 log_lik= 1.388816 train_kl= 0.02730 train_loss= 1.41611 train_acc= 0.50262 val_roc= 0.90682 val_ap= 0.91978 time= 0.08385\n",
      "Epoch: 0158 log_lik= 1.4254887 train_kl= 0.02730 train_loss= 1.45279 train_acc= 0.50279 val_roc= 0.90776 val_ap= 0.92089 time= 0.08303\n",
      "Epoch: 0159 log_lik= 1.4006666 train_kl= 0.02730 train_loss= 1.42797 train_acc= 0.50334 val_roc= 0.90811 val_ap= 0.92168 time= 0.08997\n",
      "Epoch: 0160 log_lik= 1.4232128 train_kl= 0.02731 train_loss= 1.45052 train_acc= 0.50228 val_roc= 0.90838 val_ap= 0.92250 time= 0.08238\n",
      "Epoch: 0161 log_lik= 1.4079299 train_kl= 0.02731 train_loss= 1.43524 train_acc= 0.50255 val_roc= 0.90820 val_ap= 0.92210 time= 0.08404\n",
      "Epoch: 0162 log_lik= 1.3935652 train_kl= 0.02731 train_loss= 1.42087 train_acc= 0.50322 val_roc= 0.90849 val_ap= 0.92168 time= 0.08497\n",
      "Epoch: 0163 log_lik= 1.4193268 train_kl= 0.02730 train_loss= 1.44663 train_acc= 0.50223 val_roc= 0.90883 val_ap= 0.92134 time= 0.08154\n",
      "Epoch: 0164 log_lik= 1.4252622 train_kl= 0.02730 train_loss= 1.45256 train_acc= 0.50298 val_roc= 0.90911 val_ap= 0.92130 time= 0.08295\n",
      "Epoch: 0165 log_lik= 1.4128017 train_kl= 0.02730 train_loss= 1.44010 train_acc= 0.50331 val_roc= 0.90860 val_ap= 0.92087 time= 0.08303\n",
      "Epoch: 0166 log_lik= 1.3964894 train_kl= 0.02730 train_loss= 1.42379 train_acc= 0.50267 val_roc= 0.90768 val_ap= 0.92058 time= 0.08310\n",
      "Epoch: 0167 log_lik= 1.3928269 train_kl= 0.02730 train_loss= 1.42012 train_acc= 0.50253 val_roc= 0.90718 val_ap= 0.92147 time= 0.08387\n",
      "Epoch: 0168 log_lik= 1.4117631 train_kl= 0.02729 train_loss= 1.43905 train_acc= 0.50335 val_roc= 0.90830 val_ap= 0.92335 time= 0.08849\n",
      "Epoch: 0169 log_lik= 1.4228334 train_kl= 0.02729 train_loss= 1.45012 train_acc= 0.50333 val_roc= 0.90919 val_ap= 0.92427 time= 0.08466\n",
      "Epoch: 0170 log_lik= 1.443639 train_kl= 0.02729 train_loss= 1.47093 train_acc= 0.50271 val_roc= 0.91115 val_ap= 0.92548 time= 0.08291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0171 log_lik= 1.3783937 train_kl= 0.02729 train_loss= 1.40569 train_acc= 0.50287 val_roc= 0.91155 val_ap= 0.92563 time= 0.08302\n",
      "Epoch: 0172 log_lik= 1.4243071 train_kl= 0.02730 train_loss= 1.45161 train_acc= 0.50250 val_roc= 0.91159 val_ap= 0.92564 time= 0.08300\n",
      "Epoch: 0173 log_lik= 1.4116455 train_kl= 0.02730 train_loss= 1.43895 train_acc= 0.50159 val_roc= 0.91061 val_ap= 0.92545 time= 0.08203\n",
      "Epoch: 0174 log_lik= 1.4255654 train_kl= 0.02730 train_loss= 1.45287 train_acc= 0.50168 val_roc= 0.90942 val_ap= 0.92465 time= 0.08300\n",
      "Epoch: 0175 log_lik= 1.4177856 train_kl= 0.02730 train_loss= 1.44509 train_acc= 0.50360 val_roc= 0.90766 val_ap= 0.92295 time= 0.08297\n",
      "Epoch: 0176 log_lik= 1.4017575 train_kl= 0.02730 train_loss= 1.42906 train_acc= 0.50304 val_roc= 0.90504 val_ap= 0.92025 time= 0.08208\n",
      "Epoch: 0177 log_lik= 1.3702575 train_kl= 0.02730 train_loss= 1.39756 train_acc= 0.50260 val_roc= 0.90406 val_ap= 0.91837 time= 0.08202\n",
      "Epoch: 0178 log_lik= 1.4267672 train_kl= 0.02730 train_loss= 1.45407 train_acc= 0.50166 val_roc= 0.90554 val_ap= 0.91844 time= 0.08300\n",
      "Epoch: 0179 log_lik= 1.4064105 train_kl= 0.02730 train_loss= 1.43371 train_acc= 0.50269 val_roc= 0.90591 val_ap= 0.91767 time= 0.08407\n",
      "Epoch: 0180 log_lik= 1.3979942 train_kl= 0.02730 train_loss= 1.42529 train_acc= 0.50061 val_roc= 0.90561 val_ap= 0.91698 time= 0.08492\n",
      "Epoch: 0181 log_lik= 1.4143777 train_kl= 0.02730 train_loss= 1.44167 train_acc= 0.50161 val_roc= 0.90528 val_ap= 0.91687 time= 0.08194\n",
      "Epoch: 0182 log_lik= 1.3736715 train_kl= 0.02729 train_loss= 1.40096 train_acc= 0.50296 val_roc= 0.90484 val_ap= 0.91676 time= 0.08105\n",
      "Epoch: 0183 log_lik= 1.3985604 train_kl= 0.02729 train_loss= 1.42585 train_acc= 0.50246 val_roc= 0.90357 val_ap= 0.91623 time= 0.08300\n",
      "Epoch: 0184 log_lik= 1.4222877 train_kl= 0.02729 train_loss= 1.44958 train_acc= 0.50299 val_roc= 0.90220 val_ap= 0.91533 time= 0.08103\n",
      "Epoch: 0185 log_lik= 1.4109944 train_kl= 0.02729 train_loss= 1.43829 train_acc= 0.50280 val_roc= 0.90116 val_ap= 0.91503 time= 0.08299\n",
      "Epoch: 0186 log_lik= 1.3906167 train_kl= 0.02730 train_loss= 1.41791 train_acc= 0.50141 val_roc= 0.90055 val_ap= 0.91403 time= 0.08194\n",
      "Epoch: 0187 log_lik= 1.3914062 train_kl= 0.02730 train_loss= 1.41871 train_acc= 0.50250 val_roc= 0.90156 val_ap= 0.91414 time= 0.08405\n",
      "Epoch: 0188 log_lik= 1.389706 train_kl= 0.02731 train_loss= 1.41701 train_acc= 0.50246 val_roc= 0.90280 val_ap= 0.91375 time= 0.08204\n",
      "Epoch: 0189 log_lik= 1.4334042 train_kl= 0.02731 train_loss= 1.46071 train_acc= 0.50247 val_roc= 0.90405 val_ap= 0.91378 time= 0.08193\n",
      "Epoch: 0190 log_lik= 1.4138498 train_kl= 0.02731 train_loss= 1.44116 train_acc= 0.50159 val_roc= 0.90445 val_ap= 0.91408 time= 0.08215\n",
      "Epoch: 0191 log_lik= 1.42945 train_kl= 0.02731 train_loss= 1.45676 train_acc= 0.50140 val_roc= 0.90351 val_ap= 0.91485 time= 0.08201\n",
      "Epoch: 0192 log_lik= 1.379628 train_kl= 0.02730 train_loss= 1.40693 train_acc= 0.50223 val_roc= 0.90165 val_ap= 0.91520 time= 0.08397\n",
      "Epoch: 0193 log_lik= 1.3786201 train_kl= 0.02730 train_loss= 1.40592 train_acc= 0.50232 val_roc= 0.90074 val_ap= 0.91573 time= 0.08299\n",
      "Epoch: 0194 log_lik= 1.3929775 train_kl= 0.02730 train_loss= 1.42027 train_acc= 0.50207 val_roc= 0.90104 val_ap= 0.91688 time= 0.08301\n",
      "Epoch: 0195 log_lik= 1.4059368 train_kl= 0.02729 train_loss= 1.43323 train_acc= 0.50180 val_roc= 0.90259 val_ap= 0.91811 time= 0.08305\n",
      "Epoch: 0196 log_lik= 1.3853037 train_kl= 0.02729 train_loss= 1.41259 train_acc= 0.50207 val_roc= 0.90406 val_ap= 0.91891 time= 0.08203\n",
      "Epoch: 0197 log_lik= 1.339462 train_kl= 0.02729 train_loss= 1.36675 train_acc= 0.50241 val_roc= 0.90493 val_ap= 0.91954 time= 0.08197\n",
      "Epoch: 0198 log_lik= 1.3752635 train_kl= 0.02728 train_loss= 1.40255 train_acc= 0.50178 val_roc= 0.90504 val_ap= 0.91975 time= 0.08300\n",
      "Epoch: 0199 log_lik= 1.402976 train_kl= 0.02728 train_loss= 1.43025 train_acc= 0.50226 val_roc= 0.90356 val_ap= 0.91924 time= 0.08202\n",
      "Epoch: 0200 log_lik= 1.3863286 train_kl= 0.02727 train_loss= 1.41360 train_acc= 0.50142 val_roc= 0.90286 val_ap= 0.91879 time= 0.08300\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.8947031098660925\n",
      "Test AP score: 0.9101186529998867\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CMGAE\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc64068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Epoch: 0001 log_lik= 1.9547576 train_kl= 0.00361 train_loss= 1.95837 train_acc= 0.49563 val_roc= 0.70799 val_ap= 0.73597 time= 1.98253\n",
      "Epoch: 0002 log_lik= 1.9217292 train_kl= 0.00363 train_loss= 1.92535 train_acc= 0.49174 val_roc= 0.72518 val_ap= 0.75095 time= 0.09460\n",
      "Epoch: 0003 log_lik= 1.9133462 train_kl= 0.00364 train_loss= 1.91699 train_acc= 0.48625 val_roc= 0.77114 val_ap= 0.78351 time= 0.08265\n",
      "Epoch: 0004 log_lik= 1.9111722 train_kl= 0.00366 train_loss= 1.91483 train_acc= 0.48843 val_roc= 0.79628 val_ap= 0.80041 time= 0.08265\n",
      "Epoch: 0005 log_lik= 1.8802978 train_kl= 0.00368 train_loss= 1.88398 train_acc= 0.48415 val_roc= 0.82656 val_ap= 0.82264 time= 0.07866\n",
      "Epoch: 0006 log_lik= 1.8136827 train_kl= 0.00372 train_loss= 1.81740 train_acc= 0.48496 val_roc= 0.84674 val_ap= 0.84212 time= 0.07966\n",
      "Epoch: 0007 log_lik= 1.7846243 train_kl= 0.00377 train_loss= 1.78840 train_acc= 0.48476 val_roc= 0.85927 val_ap= 0.85653 time= 0.08066\n",
      "Epoch: 0008 log_lik= 1.7060969 train_kl= 0.00384 train_loss= 1.70994 train_acc= 0.48868 val_roc= 0.86516 val_ap= 0.86253 time= 0.07966\n",
      "Epoch: 0009 log_lik= 1.6875446 train_kl= 0.00393 train_loss= 1.69147 train_acc= 0.49326 val_roc= 0.86925 val_ap= 0.86748 time= 0.08066\n",
      "Epoch: 0010 log_lik= 1.6305028 train_kl= 0.00405 train_loss= 1.63455 train_acc= 0.49472 val_roc= 0.86633 val_ap= 0.86486 time= 0.07966\n",
      "Epoch: 0011 log_lik= 1.6261435 train_kl= 0.00418 train_loss= 1.63032 train_acc= 0.50034 val_roc= 0.87199 val_ap= 0.87056 time= 0.07767\n",
      "Epoch: 0012 log_lik= 1.6447245 train_kl= 0.00430 train_loss= 1.64903 train_acc= 0.50033 val_roc= 0.87951 val_ap= 0.87929 time= 0.07974\n",
      "Epoch: 0013 log_lik= 1.6074051 train_kl= 0.00439 train_loss= 1.61179 train_acc= 0.49659 val_roc= 0.88502 val_ap= 0.88512 time= 0.08663\n",
      "Epoch: 0014 log_lik= 1.6276771 train_kl= 0.00442 train_loss= 1.63210 train_acc= 0.49809 val_roc= 0.88810 val_ap= 0.88857 time= 0.08165\n",
      "Epoch: 0015 log_lik= 1.5893186 train_kl= 0.00443 train_loss= 1.59375 train_acc= 0.50125 val_roc= 0.89161 val_ap= 0.89187 time= 0.08165\n",
      "Epoch: 0016 log_lik= 1.5710411 train_kl= 0.00440 train_loss= 1.57544 train_acc= 0.50147 val_roc= 0.89446 val_ap= 0.89569 time= 0.09261\n",
      "Epoch: 0017 log_lik= 1.5754143 train_kl= 0.00435 train_loss= 1.57976 train_acc= 0.50076 val_roc= 0.89443 val_ap= 0.89715 time= 0.09559\n",
      "Epoch: 0018 log_lik= 1.6030835 train_kl= 0.00429 train_loss= 1.60737 train_acc= 0.50025 val_roc= 0.89239 val_ap= 0.89608 time= 0.07966\n",
      "Epoch: 0019 log_lik= 1.5750402 train_kl= 0.00423 train_loss= 1.57927 train_acc= 0.49955 val_roc= 0.89047 val_ap= 0.89432 time= 0.08663\n",
      "Epoch: 0020 log_lik= 1.5873996 train_kl= 0.00418 train_loss= 1.59158 train_acc= 0.50016 val_roc= 0.89333 val_ap= 0.89729 time= 0.07966\n",
      "Epoch: 0021 log_lik= 1.5712168 train_kl= 0.00414 train_loss= 1.57536 train_acc= 0.50137 val_roc= 0.89552 val_ap= 0.90028 time= 0.08066\n",
      "Epoch: 0022 log_lik= 1.5771219 train_kl= 0.00413 train_loss= 1.58125 train_acc= 0.50099 val_roc= 0.89800 val_ap= 0.90455 time= 0.08165\n",
      "Epoch: 0023 log_lik= 1.5808184 train_kl= 0.00412 train_loss= 1.58494 train_acc= 0.50088 val_roc= 0.90076 val_ap= 0.90817 time= 0.07966\n",
      "Epoch: 0024 log_lik= 1.5362177 train_kl= 0.00413 train_loss= 1.54035 train_acc= 0.50040 val_roc= 0.90179 val_ap= 0.90894 time= 0.07966\n",
      "Epoch: 0025 log_lik= 1.5577579 train_kl= 0.00416 train_loss= 1.56192 train_acc= 0.50031 val_roc= 0.90094 val_ap= 0.90738 time= 0.09161\n",
      "Epoch: 0026 log_lik= 1.5355282 train_kl= 0.00421 train_loss= 1.53974 train_acc= 0.49971 val_roc= 0.90149 val_ap= 0.90834 time= 0.08066\n",
      "Epoch: 0027 log_lik= 1.5511434 train_kl= 0.00426 train_loss= 1.55540 train_acc= 0.50091 val_roc= 0.90272 val_ap= 0.91046 time= 0.08564\n",
      "Epoch: 0028 log_lik= 1.5275947 train_kl= 0.00430 train_loss= 1.53190 train_acc= 0.50168 val_roc= 0.90390 val_ap= 0.91270 time= 0.08467\n",
      "Epoch: 0029 log_lik= 1.5508244 train_kl= 0.00434 train_loss= 1.55516 train_acc= 0.50103 val_roc= 0.90412 val_ap= 0.91261 time= 0.07966\n",
      "Epoch: 0030 log_lik= 1.5467441 train_kl= 0.00437 train_loss= 1.55111 train_acc= 0.50014 val_roc= 0.90373 val_ap= 0.91125 time= 0.08663\n",
      "Epoch: 0031 log_lik= 1.5456216 train_kl= 0.00438 train_loss= 1.55000 train_acc= 0.50147 val_roc= 0.90316 val_ap= 0.91031 time= 0.08265\n",
      "Epoch: 0032 log_lik= 1.5176545 train_kl= 0.00437 train_loss= 1.52203 train_acc= 0.50126 val_roc= 0.90356 val_ap= 0.91067 time= 0.08862\n",
      "Epoch: 0033 log_lik= 1.5361413 train_kl= 0.00436 train_loss= 1.54050 train_acc= 0.50097 val_roc= 0.90428 val_ap= 0.91207 time= 0.07966\n",
      "Epoch: 0034 log_lik= 1.5160981 train_kl= 0.00433 train_loss= 1.52043 train_acc= 0.50125 val_roc= 0.90423 val_ap= 0.91248 time= 0.07867\n",
      "Epoch: 0035 log_lik= 1.533744 train_kl= 0.00431 train_loss= 1.53806 train_acc= 0.50153 val_roc= 0.90360 val_ap= 0.91203 time= 0.07468\n",
      "Epoch: 0036 log_lik= 1.519616 train_kl= 0.00429 train_loss= 1.52391 train_acc= 0.50146 val_roc= 0.90288 val_ap= 0.91135 time= 0.07767\n",
      "Epoch: 0037 log_lik= 1.5283974 train_kl= 0.00428 train_loss= 1.53268 train_acc= 0.50195 val_roc= 0.90196 val_ap= 0.91104 time= 0.08564\n",
      "Epoch: 0038 log_lik= 1.520454 train_kl= 0.00427 train_loss= 1.52472 train_acc= 0.50102 val_roc= 0.90162 val_ap= 0.91136 time= 0.08364\n",
      "Epoch: 0039 log_lik= 1.5059506 train_kl= 0.00427 train_loss= 1.51022 train_acc= 0.50074 val_roc= 0.90256 val_ap= 0.91239 time= 0.08763\n",
      "Epoch: 0040 log_lik= 1.5200121 train_kl= 0.00426 train_loss= 1.52428 train_acc= 0.50022 val_roc= 0.90403 val_ap= 0.91354 time= 0.09610\n",
      "Epoch: 0041 log_lik= 1.5082096 train_kl= 0.00427 train_loss= 1.51248 train_acc= 0.50101 val_roc= 0.90556 val_ap= 0.91462 time= 0.10057\n",
      "Epoch: 0042 log_lik= 1.4973731 train_kl= 0.00429 train_loss= 1.50166 train_acc= 0.50143 val_roc= 0.90762 val_ap= 0.91627 time= 0.09858\n",
      "Epoch: 0043 log_lik= 1.5232406 train_kl= 0.00432 train_loss= 1.52756 train_acc= 0.50072 val_roc= 0.90903 val_ap= 0.91696 time= 0.08365\n",
      "Epoch: 0044 log_lik= 1.5141203 train_kl= 0.00434 train_loss= 1.51846 train_acc= 0.50078 val_roc= 0.90958 val_ap= 0.91760 time= 0.08082\n",
      "Epoch: 0045 log_lik= 1.5217052 train_kl= 0.00435 train_loss= 1.52605 train_acc= 0.50180 val_roc= 0.90909 val_ap= 0.91735 time= 0.08265\n",
      "Epoch: 0046 log_lik= 1.4990318 train_kl= 0.00435 train_loss= 1.50339 train_acc= 0.50110 val_roc= 0.90882 val_ap= 0.91730 time= 0.07867\n",
      "Epoch: 0047 log_lik= 1.479241 train_kl= 0.00435 train_loss= 1.48359 train_acc= 0.50182 val_roc= 0.90734 val_ap= 0.91613 time= 0.07867\n",
      "Epoch: 0048 log_lik= 1.4916928 train_kl= 0.00435 train_loss= 1.49604 train_acc= 0.50081 val_roc= 0.90849 val_ap= 0.91763 time= 0.08663\n",
      "Epoch: 0049 log_lik= 1.5126672 train_kl= 0.00434 train_loss= 1.51701 train_acc= 0.50157 val_roc= 0.90974 val_ap= 0.91900 time= 0.08165\n",
      "Epoch: 0050 log_lik= 1.5391835 train_kl= 0.00433 train_loss= 1.54352 train_acc= 0.50162 val_roc= 0.91116 val_ap= 0.92010 time= 0.08265\n",
      "Epoch: 0051 log_lik= 1.4930822 train_kl= 0.00433 train_loss= 1.49741 train_acc= 0.50176 val_roc= 0.91143 val_ap= 0.92038 time= 0.08093\n",
      "Epoch: 0052 log_lik= 1.5032682 train_kl= 0.00434 train_loss= 1.50760 train_acc= 0.50054 val_roc= 0.91181 val_ap= 0.92067 time= 0.08165\n",
      "Epoch: 0053 log_lik= 1.5120502 train_kl= 0.00433 train_loss= 1.51638 train_acc= 0.50150 val_roc= 0.91091 val_ap= 0.92001 time= 0.09360\n",
      "Epoch: 0054 log_lik= 1.5122011 train_kl= 0.00434 train_loss= 1.51654 train_acc= 0.50168 val_roc= 0.91029 val_ap= 0.91926 time= 0.08862\n",
      "Epoch: 0055 log_lik= 1.4950292 train_kl= 0.00434 train_loss= 1.49937 train_acc= 0.50167 val_roc= 0.90971 val_ap= 0.91830 time= 0.08763\n",
      "Epoch: 0056 log_lik= 1.5125446 train_kl= 0.00435 train_loss= 1.51689 train_acc= 0.50165 val_roc= 0.91110 val_ap= 0.91942 time= 0.07867\n",
      "Epoch: 0057 log_lik= 1.4842211 train_kl= 0.00435 train_loss= 1.48857 train_acc= 0.50214 val_roc= 0.91245 val_ap= 0.92051 time= 0.07968\n",
      "Epoch: 0058 log_lik= 1.4914734 train_kl= 0.00435 train_loss= 1.49582 train_acc= 0.50178 val_roc= 0.91258 val_ap= 0.92032 time= 0.07667\n",
      "Epoch: 0059 log_lik= 1.4795194 train_kl= 0.00435 train_loss= 1.48387 train_acc= 0.50173 val_roc= 0.91275 val_ap= 0.92042 time= 0.08471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0060 log_lik= 1.5097331 train_kl= 0.00436 train_loss= 1.51409 train_acc= 0.50080 val_roc= 0.91132 val_ap= 0.91872 time= 0.07867\n",
      "Epoch: 0061 log_lik= 1.5180867 train_kl= 0.00436 train_loss= 1.52245 train_acc= 0.50152 val_roc= 0.90867 val_ap= 0.91643 time= 0.08169\n",
      "Epoch: 0062 log_lik= 1.49929 train_kl= 0.00436 train_loss= 1.50365 train_acc= 0.50149 val_roc= 0.90718 val_ap= 0.91471 time= 0.07966\n",
      "Epoch: 0063 log_lik= 1.478772 train_kl= 0.00437 train_loss= 1.48314 train_acc= 0.50044 val_roc= 0.90987 val_ap= 0.91777 time= 0.07818\n",
      "Epoch: 0064 log_lik= 1.5232705 train_kl= 0.00437 train_loss= 1.52764 train_acc= 0.50023 val_roc= 0.91116 val_ap= 0.91905 time= 0.07867\n",
      "Epoch: 0065 log_lik= 1.4672207 train_kl= 0.00437 train_loss= 1.47159 train_acc= 0.50143 val_roc= 0.91100 val_ap= 0.91867 time= 0.07867\n",
      "Epoch: 0066 log_lik= 1.5002456 train_kl= 0.00438 train_loss= 1.50462 train_acc= 0.50017 val_roc= 0.91138 val_ap= 0.91931 time= 0.08165\n",
      "Epoch: 0067 log_lik= 1.522953 train_kl= 0.00437 train_loss= 1.52733 train_acc= 0.50007 val_roc= 0.91132 val_ap= 0.91957 time= 0.07867\n",
      "Epoch: 0068 log_lik= 1.4707493 train_kl= 0.00437 train_loss= 1.47512 train_acc= 0.50149 val_roc= 0.91143 val_ap= 0.91940 time= 0.07867\n",
      "Epoch: 0069 log_lik= 1.5205384 train_kl= 0.00436 train_loss= 1.52490 train_acc= 0.50133 val_roc= 0.91055 val_ap= 0.91923 time= 0.08464\n",
      "Epoch: 0070 log_lik= 1.4746099 train_kl= 0.00436 train_loss= 1.47897 train_acc= 0.50125 val_roc= 0.90987 val_ap= 0.91916 time= 0.08464\n",
      "Epoch: 0071 log_lik= 1.5142325 train_kl= 0.00436 train_loss= 1.51859 train_acc= 0.50093 val_roc= 0.90977 val_ap= 0.91997 time= 0.07568\n",
      "Epoch: 0072 log_lik= 1.4910467 train_kl= 0.00435 train_loss= 1.49540 train_acc= 0.50015 val_roc= 0.90987 val_ap= 0.92011 time= 0.07767\n",
      "Epoch: 0073 log_lik= 1.5035316 train_kl= 0.00434 train_loss= 1.50787 train_acc= 0.50048 val_roc= 0.91161 val_ap= 0.92162 time= 0.08065\n",
      "Epoch: 0074 log_lik= 1.4959656 train_kl= 0.00434 train_loss= 1.50031 train_acc= 0.50087 val_roc= 0.91281 val_ap= 0.92284 time= 0.08066\n",
      "Epoch: 0075 log_lik= 1.4815699 train_kl= 0.00435 train_loss= 1.48592 train_acc= 0.50075 val_roc= 0.91357 val_ap= 0.92409 time= 0.08066\n",
      "Epoch: 0076 log_lik= 1.487197 train_kl= 0.00435 train_loss= 1.49155 train_acc= 0.49945 val_roc= 0.91428 val_ap= 0.92517 time= 0.07867\n",
      "Epoch: 0077 log_lik= 1.5107691 train_kl= 0.00437 train_loss= 1.51513 train_acc= 0.50006 val_roc= 0.91506 val_ap= 0.92597 time= 0.08962\n",
      "Epoch: 0078 log_lik= 1.4892282 train_kl= 0.00438 train_loss= 1.49361 train_acc= 0.50103 val_roc= 0.91333 val_ap= 0.92476 time= 0.08579\n",
      "Epoch: 0079 log_lik= 1.4837538 train_kl= 0.00439 train_loss= 1.48815 train_acc= 0.50076 val_roc= 0.91353 val_ap= 0.92440 time= 0.08663\n",
      "Epoch: 0080 log_lik= 1.4908417 train_kl= 0.00441 train_loss= 1.49525 train_acc= 0.50096 val_roc= 0.91383 val_ap= 0.92419 time= 0.08066\n",
      "Epoch: 0081 log_lik= 1.47256 train_kl= 0.00442 train_loss= 1.47698 train_acc= 0.50095 val_roc= 0.91635 val_ap= 0.92596 time= 0.07966\n",
      "Epoch: 0082 log_lik= 1.477884 train_kl= 0.00442 train_loss= 1.48231 train_acc= 0.50167 val_roc= 0.91687 val_ap= 0.92638 time= 0.08364\n",
      "Epoch: 0083 log_lik= 1.4833889 train_kl= 0.00442 train_loss= 1.48781 train_acc= 0.50069 val_roc= 0.91728 val_ap= 0.92712 time= 0.07568\n",
      "Epoch: 0084 log_lik= 1.4894822 train_kl= 0.00441 train_loss= 1.49389 train_acc= 0.50233 val_roc= 0.91716 val_ap= 0.92838 time= 0.07468\n",
      "Epoch: 0085 log_lik= 1.491371 train_kl= 0.00439 train_loss= 1.49576 train_acc= 0.50229 val_roc= 0.91635 val_ap= 0.92820 time= 0.07767\n",
      "Epoch: 0086 log_lik= 1.4933168 train_kl= 0.00438 train_loss= 1.49769 train_acc= 0.50197 val_roc= 0.91463 val_ap= 0.92647 time= 0.08962\n",
      "Epoch: 0087 log_lik= 1.4977341 train_kl= 0.00437 train_loss= 1.50210 train_acc= 0.50102 val_roc= 0.91388 val_ap= 0.92543 time= 0.08763\n",
      "Epoch: 0088 log_lik= 1.4685743 train_kl= 0.00436 train_loss= 1.47293 train_acc= 0.50124 val_roc= 0.91482 val_ap= 0.92581 time= 0.08292\n",
      "Epoch: 0089 log_lik= 1.4727427 train_kl= 0.00436 train_loss= 1.47710 train_acc= 0.50166 val_roc= 0.91654 val_ap= 0.92669 time= 0.08165\n",
      "Epoch: 0090 log_lik= 1.4914404 train_kl= 0.00436 train_loss= 1.49580 train_acc= 0.50228 val_roc= 0.91821 val_ap= 0.92751 time= 0.08564\n",
      "Epoch: 0091 log_lik= 1.4760243 train_kl= 0.00437 train_loss= 1.48040 train_acc= 0.50228 val_roc= 0.91785 val_ap= 0.92667 time= 0.08066\n",
      "Epoch: 0092 log_lik= 1.4813931 train_kl= 0.00438 train_loss= 1.48578 train_acc= 0.50138 val_roc= 0.91768 val_ap= 0.92618 time= 0.08066\n",
      "Epoch: 0093 log_lik= 1.5077337 train_kl= 0.00439 train_loss= 1.51213 train_acc= 0.50066 val_roc= 0.91647 val_ap= 0.92511 time= 0.08663\n",
      "Epoch: 0094 log_lik= 1.5002993 train_kl= 0.00440 train_loss= 1.50470 train_acc= 0.50218 val_roc= 0.91485 val_ap= 0.92373 time= 0.07900\n",
      "Epoch: 0095 log_lik= 1.4764134 train_kl= 0.00441 train_loss= 1.48083 train_acc= 0.50106 val_roc= 0.91401 val_ap= 0.92299 time= 0.07668\n",
      "Epoch: 0096 log_lik= 1.4907997 train_kl= 0.00442 train_loss= 1.49522 train_acc= 0.50123 val_roc= 0.91622 val_ap= 0.92470 time= 0.07667\n",
      "Epoch: 0097 log_lik= 1.4834929 train_kl= 0.00443 train_loss= 1.48792 train_acc= 0.50245 val_roc= 0.91917 val_ap= 0.92710 time= 0.08066\n",
      "Epoch: 0098 log_lik= 1.4563377 train_kl= 0.00443 train_loss= 1.46077 train_acc= 0.50249 val_roc= 0.91975 val_ap= 0.92795 time= 0.08066\n",
      "Epoch: 0099 log_lik= 1.4902923 train_kl= 0.00443 train_loss= 1.49472 train_acc= 0.50091 val_roc= 0.92021 val_ap= 0.92816 time= 0.07867\n",
      "Epoch: 0100 log_lik= 1.4857461 train_kl= 0.00441 train_loss= 1.49016 train_acc= 0.50033 val_roc= 0.91910 val_ap= 0.92700 time= 0.07966\n",
      "Epoch: 0101 log_lik= 1.4658116 train_kl= 0.00440 train_loss= 1.47021 train_acc= 0.50134 val_roc= 0.91636 val_ap= 0.92464 time= 0.08265\n",
      "Epoch: 0102 log_lik= 1.4599578 train_kl= 0.00438 train_loss= 1.46434 train_acc= 0.50179 val_roc= 0.91421 val_ap= 0.92310 time= 0.09062\n",
      "Epoch: 0103 log_lik= 1.4709991 train_kl= 0.00437 train_loss= 1.47537 train_acc= 0.50110 val_roc= 0.91475 val_ap= 0.92412 time= 0.07966\n",
      "Epoch: 0104 log_lik= 1.4902604 train_kl= 0.00437 train_loss= 1.49463 train_acc= 0.50098 val_roc= 0.91573 val_ap= 0.92549 time= 0.07966\n",
      "Epoch: 0105 log_lik= 1.5012659 train_kl= 0.00436 train_loss= 1.50562 train_acc= 0.50153 val_roc= 0.91775 val_ap= 0.92767 time= 0.08962\n",
      "Epoch: 0106 log_lik= 1.437845 train_kl= 0.00436 train_loss= 1.44220 train_acc= 0.50189 val_roc= 0.91905 val_ap= 0.92895 time= 0.08564\n",
      "Epoch: 0107 log_lik= 1.4692185 train_kl= 0.00436 train_loss= 1.47358 train_acc= 0.50195 val_roc= 0.92069 val_ap= 0.93028 time= 0.08364\n",
      "Epoch: 0108 log_lik= 1.4704393 train_kl= 0.00437 train_loss= 1.47481 train_acc= 0.50145 val_roc= 0.92183 val_ap= 0.93111 time= 0.07966\n",
      "Epoch: 0109 log_lik= 1.4860382 train_kl= 0.00438 train_loss= 1.49042 train_acc= 0.50112 val_roc= 0.92210 val_ap= 0.93121 time= 0.08066\n",
      "Epoch: 0110 log_lik= 1.4759849 train_kl= 0.00439 train_loss= 1.48038 train_acc= 0.50189 val_roc= 0.92327 val_ap= 0.93219 time= 0.08116\n",
      "Epoch: 0111 log_lik= 1.493069 train_kl= 0.00441 train_loss= 1.49748 train_acc= 0.50205 val_roc= 0.92384 val_ap= 0.93303 time= 0.07966\n",
      "Epoch: 0112 log_lik= 1.4827236 train_kl= 0.00442 train_loss= 1.48715 train_acc= 0.50216 val_roc= 0.92426 val_ap= 0.93324 time= 0.07966\n",
      "Epoch: 0113 log_lik= 1.4946774 train_kl= 0.00443 train_loss= 1.49911 train_acc= 0.50159 val_roc= 0.92393 val_ap= 0.93263 time= 0.08365\n",
      "Epoch: 0114 log_lik= 1.4402423 train_kl= 0.00444 train_loss= 1.44469 train_acc= 0.50209 val_roc= 0.92327 val_ap= 0.93161 time= 0.08862\n",
      "Epoch: 0115 log_lik= 1.4667345 train_kl= 0.00445 train_loss= 1.47118 train_acc= 0.50126 val_roc= 0.92326 val_ap= 0.93108 time= 0.08216\n",
      "Epoch: 0116 log_lik= 1.4748635 train_kl= 0.00444 train_loss= 1.47930 train_acc= 0.50168 val_roc= 0.92322 val_ap= 0.93074 time= 0.07966\n",
      "Epoch: 0117 log_lik= 1.4890444 train_kl= 0.00443 train_loss= 1.49347 train_acc= 0.50150 val_roc= 0.92310 val_ap= 0.93050 time= 0.08863\n",
      "Epoch: 0118 log_lik= 1.43688 train_kl= 0.00441 train_loss= 1.44129 train_acc= 0.50196 val_roc= 0.92316 val_ap= 0.93037 time= 0.08066\n",
      "Epoch: 0119 log_lik= 1.4599274 train_kl= 0.00440 train_loss= 1.46433 train_acc= 0.50176 val_roc= 0.92332 val_ap= 0.93033 time= 0.07966\n",
      "Epoch: 0120 log_lik= 1.4892431 train_kl= 0.00439 train_loss= 1.49363 train_acc= 0.50107 val_roc= 0.92343 val_ap= 0.93062 time= 0.08165\n",
      "Epoch: 0121 log_lik= 1.4917777 train_kl= 0.00438 train_loss= 1.49616 train_acc= 0.50219 val_roc= 0.92288 val_ap= 0.93016 time= 0.09360\n",
      "Epoch: 0122 log_lik= 1.4506549 train_kl= 0.00438 train_loss= 1.45503 train_acc= 0.50222 val_roc= 0.92301 val_ap= 0.93021 time= 0.07966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0123 log_lik= 1.4940664 train_kl= 0.00437 train_loss= 1.49844 train_acc= 0.50125 val_roc= 0.92382 val_ap= 0.93113 time= 0.08663\n",
      "Epoch: 0124 log_lik= 1.4898301 train_kl= 0.00438 train_loss= 1.49421 train_acc= 0.50169 val_roc= 0.92414 val_ap= 0.93120 time= 0.07867\n",
      "Epoch: 0125 log_lik= 1.4788806 train_kl= 0.00439 train_loss= 1.48327 train_acc= 0.50209 val_roc= 0.92462 val_ap= 0.93131 time= 0.08663\n",
      "Epoch: 0126 log_lik= 1.4867296 train_kl= 0.00440 train_loss= 1.49113 train_acc= 0.50224 val_roc= 0.92436 val_ap= 0.93069 time= 0.08763\n",
      "Epoch: 0127 log_lik= 1.4585826 train_kl= 0.00442 train_loss= 1.46300 train_acc= 0.50189 val_roc= 0.92445 val_ap= 0.93032 time= 0.08069\n",
      "Epoch: 0128 log_lik= 1.4519733 train_kl= 0.00443 train_loss= 1.45640 train_acc= 0.50177 val_roc= 0.92549 val_ap= 0.93131 time= 0.08763\n",
      "Epoch: 0129 log_lik= 1.4767454 train_kl= 0.00443 train_loss= 1.48118 train_acc= 0.50139 val_roc= 0.92627 val_ap= 0.93272 time= 0.09460\n",
      "Epoch: 0130 log_lik= 1.4653059 train_kl= 0.00443 train_loss= 1.46974 train_acc= 0.50178 val_roc= 0.92656 val_ap= 0.93320 time= 0.08862\n",
      "Epoch: 0131 log_lik= 1.4741739 train_kl= 0.00444 train_loss= 1.47861 train_acc= 0.50217 val_roc= 0.92578 val_ap= 0.93239 time= 0.07667\n",
      "Epoch: 0132 log_lik= 1.4633968 train_kl= 0.00444 train_loss= 1.46783 train_acc= 0.50151 val_roc= 0.92446 val_ap= 0.93080 time= 0.07767\n",
      "Epoch: 0133 log_lik= 1.4688529 train_kl= 0.00443 train_loss= 1.47328 train_acc= 0.50254 val_roc= 0.92398 val_ap= 0.93026 time= 0.07867\n",
      "Epoch: 0134 log_lik= 1.4705065 train_kl= 0.00443 train_loss= 1.47493 train_acc= 0.50170 val_roc= 0.92502 val_ap= 0.93136 time= 0.08066\n",
      "Epoch: 0135 log_lik= 1.462688 train_kl= 0.00443 train_loss= 1.46712 train_acc= 0.50147 val_roc= 0.92552 val_ap= 0.93231 time= 0.08364\n",
      "Epoch: 0136 log_lik= 1.449906 train_kl= 0.00443 train_loss= 1.45433 train_acc= 0.50273 val_roc= 0.92611 val_ap= 0.93315 time= 0.07966\n",
      "Epoch: 0137 log_lik= 1.4754325 train_kl= 0.00442 train_loss= 1.47985 train_acc= 0.50219 val_roc= 0.92599 val_ap= 0.93336 time= 0.07966\n",
      "Epoch: 0138 log_lik= 1.4679985 train_kl= 0.00441 train_loss= 1.47241 train_acc= 0.50237 val_roc= 0.92478 val_ap= 0.93245 time= 0.07966\n",
      "Epoch: 0139 log_lik= 1.4510314 train_kl= 0.00440 train_loss= 1.45543 train_acc= 0.50163 val_roc= 0.92355 val_ap= 0.93159 time= 0.07866\n",
      "Epoch: 0140 log_lik= 1.4862492 train_kl= 0.00440 train_loss= 1.49065 train_acc= 0.50115 val_roc= 0.92265 val_ap= 0.93054 time= 0.07867\n",
      "Epoch: 0141 log_lik= 1.4653044 train_kl= 0.00440 train_loss= 1.46970 train_acc= 0.50053 val_roc= 0.92309 val_ap= 0.93088 time= 0.08663\n",
      "Epoch: 0142 log_lik= 1.4535512 train_kl= 0.00440 train_loss= 1.45795 train_acc= 0.50208 val_roc= 0.92287 val_ap= 0.93074 time= 0.08066\n",
      "Epoch: 0143 log_lik= 1.4613078 train_kl= 0.00441 train_loss= 1.46572 train_acc= 0.50161 val_roc= 0.92212 val_ap= 0.93009 time= 0.07818\n",
      "Epoch: 0144 log_lik= 1.4926908 train_kl= 0.00442 train_loss= 1.49711 train_acc= 0.50036 val_roc= 0.92245 val_ap= 0.93037 time= 0.07767\n",
      "Epoch: 0145 log_lik= 1.4501611 train_kl= 0.00442 train_loss= 1.45458 train_acc= 0.50067 val_roc= 0.92218 val_ap= 0.92989 time= 0.08663\n",
      "Epoch: 0146 log_lik= 1.455629 train_kl= 0.00442 train_loss= 1.46004 train_acc= 0.50255 val_roc= 0.92025 val_ap= 0.92802 time= 0.07966\n",
      "Epoch: 0147 log_lik= 1.462936 train_kl= 0.00441 train_loss= 1.46734 train_acc= 0.50204 val_roc= 0.91892 val_ap= 0.92667 time= 0.09360\n",
      "Epoch: 0148 log_lik= 1.4850842 train_kl= 0.00440 train_loss= 1.48949 train_acc= 0.50074 val_roc= 0.91876 val_ap= 0.92659 time= 0.08066\n",
      "Epoch: 0149 log_lik= 1.4640795 train_kl= 0.00440 train_loss= 1.46848 train_acc= 0.50169 val_roc= 0.91898 val_ap= 0.92620 time= 0.07966\n",
      "Epoch: 0150 log_lik= 1.4631218 train_kl= 0.00440 train_loss= 1.46752 train_acc= 0.50225 val_roc= 0.91905 val_ap= 0.92628 time= 0.07966\n",
      "Epoch: 0151 log_lik= 1.4754019 train_kl= 0.00441 train_loss= 1.47981 train_acc= 0.50280 val_roc= 0.91933 val_ap= 0.92661 time= 0.08663\n",
      "Epoch: 0152 log_lik= 1.4680103 train_kl= 0.00442 train_loss= 1.47243 train_acc= 0.50201 val_roc= 0.91862 val_ap= 0.92517 time= 0.09460\n",
      "Epoch: 0153 log_lik= 1.4644673 train_kl= 0.00444 train_loss= 1.46890 train_acc= 0.50111 val_roc= 0.91827 val_ap= 0.92482 time= 0.08066\n",
      "Epoch: 0154 log_lik= 1.4761888 train_kl= 0.00443 train_loss= 1.48062 train_acc= 0.50065 val_roc= 0.91894 val_ap= 0.92533 time= 0.08165\n",
      "Epoch: 0155 log_lik= 1.4768516 train_kl= 0.00443 train_loss= 1.48128 train_acc= 0.50064 val_roc= 0.91901 val_ap= 0.92600 time= 0.08066\n",
      "Epoch: 0156 log_lik= 1.4712831 train_kl= 0.00443 train_loss= 1.47571 train_acc= 0.50249 val_roc= 0.91793 val_ap= 0.92498 time= 0.08464\n",
      "Epoch: 0157 log_lik= 1.4910059 train_kl= 0.00443 train_loss= 1.49544 train_acc= 0.50135 val_roc= 0.91754 val_ap= 0.92444 time= 0.07867\n",
      "Epoch: 0158 log_lik= 1.4484063 train_kl= 0.00443 train_loss= 1.45284 train_acc= 0.50188 val_roc= 0.91801 val_ap= 0.92487 time= 0.07866\n",
      "Epoch: 0159 log_lik= 1.4767362 train_kl= 0.00444 train_loss= 1.48117 train_acc= 0.50167 val_roc= 0.91884 val_ap= 0.92585 time= 0.07767\n",
      "Epoch: 0160 log_lik= 1.4954878 train_kl= 0.00444 train_loss= 1.49993 train_acc= 0.50102 val_roc= 0.91937 val_ap= 0.92591 time= 0.08666\n",
      "Epoch: 0161 log_lik= 1.4574628 train_kl= 0.00444 train_loss= 1.46190 train_acc= 0.50183 val_roc= 0.92077 val_ap= 0.92687 time= 0.07966\n",
      "Epoch: 0162 log_lik= 1.4694458 train_kl= 0.00444 train_loss= 1.47389 train_acc= 0.50148 val_roc= 0.92168 val_ap= 0.92786 time= 0.07966\n",
      "Epoch: 0163 log_lik= 1.4786776 train_kl= 0.00444 train_loss= 1.48312 train_acc= 0.50147 val_roc= 0.92183 val_ap= 0.92811 time= 0.07966\n",
      "Epoch: 0164 log_lik= 1.4713079 train_kl= 0.00443 train_loss= 1.47574 train_acc= 0.50233 val_roc= 0.92138 val_ap= 0.92765 time= 0.07866\n",
      "Epoch: 0165 log_lik= 1.446787 train_kl= 0.00443 train_loss= 1.45121 train_acc= 0.50130 val_roc= 0.92197 val_ap= 0.92799 time= 0.08066\n",
      "Epoch: 0166 log_lik= 1.4486699 train_kl= 0.00442 train_loss= 1.45309 train_acc= 0.50227 val_roc= 0.92255 val_ap= 0.92827 time= 0.07867\n",
      "Epoch: 0167 log_lik= 1.4612737 train_kl= 0.00441 train_loss= 1.46569 train_acc= 0.50165 val_roc= 0.92215 val_ap= 0.92785 time= 0.07867\n",
      "Epoch: 0168 log_lik= 1.4354372 train_kl= 0.00441 train_loss= 1.43985 train_acc= 0.50283 val_roc= 0.92242 val_ap= 0.92814 time= 0.07468\n",
      "Epoch: 0169 log_lik= 1.4764091 train_kl= 0.00441 train_loss= 1.48081 train_acc= 0.50208 val_roc= 0.92307 val_ap= 0.92911 time= 0.07767\n",
      "Epoch: 0170 log_lik= 1.460097 train_kl= 0.00440 train_loss= 1.46450 train_acc= 0.50137 val_roc= 0.92384 val_ap= 0.93020 time= 0.08564\n",
      "Epoch: 0171 log_lik= 1.4578813 train_kl= 0.00441 train_loss= 1.46229 train_acc= 0.50221 val_roc= 0.92327 val_ap= 0.92980 time= 0.08663\n",
      "Epoch: 0172 log_lik= 1.4629686 train_kl= 0.00441 train_loss= 1.46738 train_acc= 0.50187 val_roc= 0.92273 val_ap= 0.92956 time= 0.08165\n",
      "Epoch: 0173 log_lik= 1.4781055 train_kl= 0.00442 train_loss= 1.48252 train_acc= 0.50131 val_roc= 0.92367 val_ap= 0.93068 time= 0.07917\n",
      "Epoch: 0174 log_lik= 1.4471077 train_kl= 0.00443 train_loss= 1.45154 train_acc= 0.50208 val_roc= 0.92429 val_ap= 0.93150 time= 0.07966\n",
      "Epoch: 0175 log_lik= 1.4605136 train_kl= 0.00444 train_loss= 1.46495 train_acc= 0.50217 val_roc= 0.92450 val_ap= 0.93192 time= 0.09611\n",
      "Epoch: 0176 log_lik= 1.4869986 train_kl= 0.00444 train_loss= 1.49144 train_acc= 0.50277 val_roc= 0.92456 val_ap= 0.93180 time= 0.07966\n",
      "Epoch: 0177 log_lik= 1.4564544 train_kl= 0.00445 train_loss= 1.46090 train_acc= 0.50228 val_roc= 0.92382 val_ap= 0.93098 time= 0.08126\n",
      "Epoch: 0178 log_lik= 1.4676617 train_kl= 0.00446 train_loss= 1.47212 train_acc= 0.50239 val_roc= 0.92338 val_ap= 0.93028 time= 0.07966\n",
      "Epoch: 0179 log_lik= 1.480774 train_kl= 0.00446 train_loss= 1.48523 train_acc= 0.50159 val_roc= 0.92351 val_ap= 0.93040 time= 0.08166\n",
      "Epoch: 0180 log_lik= 1.4679402 train_kl= 0.00446 train_loss= 1.47240 train_acc= 0.50206 val_roc= 0.92439 val_ap= 0.93195 time= 0.07867\n",
      "Epoch: 0181 log_lik= 1.4842491 train_kl= 0.00445 train_loss= 1.48870 train_acc= 0.50269 val_roc= 0.92455 val_ap= 0.93231 time= 0.07867\n",
      "Epoch: 0182 log_lik= 1.4628211 train_kl= 0.00445 train_loss= 1.46727 train_acc= 0.50171 val_roc= 0.92445 val_ap= 0.93226 time= 0.08564\n",
      "Epoch: 0183 log_lik= 1.4640819 train_kl= 0.00445 train_loss= 1.46853 train_acc= 0.50143 val_roc= 0.92460 val_ap= 0.93221 time= 0.07867\n",
      "Epoch: 0184 log_lik= 1.4571203 train_kl= 0.00444 train_loss= 1.46156 train_acc= 0.50286 val_roc= 0.92462 val_ap= 0.93237 time= 0.07966\n",
      "Epoch: 0185 log_lik= 1.4779047 train_kl= 0.00443 train_loss= 1.48234 train_acc= 0.50239 val_roc= 0.92427 val_ap= 0.93234 time= 0.08165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0186 log_lik= 1.4530809 train_kl= 0.00442 train_loss= 1.45750 train_acc= 0.50235 val_roc= 0.92390 val_ap= 0.93190 time= 0.08663\n",
      "Epoch: 0187 log_lik= 1.4605777 train_kl= 0.00441 train_loss= 1.46499 train_acc= 0.50198 val_roc= 0.92358 val_ap= 0.93120 time= 0.08663\n",
      "Epoch: 0188 log_lik= 1.4768237 train_kl= 0.00441 train_loss= 1.48123 train_acc= 0.50120 val_roc= 0.92339 val_ap= 0.93106 time= 0.08165\n",
      "Epoch: 0189 log_lik= 1.4783033 train_kl= 0.00441 train_loss= 1.48271 train_acc= 0.50285 val_roc= 0.92340 val_ap= 0.93116 time= 0.08165\n",
      "Epoch: 0190 log_lik= 1.4608469 train_kl= 0.00441 train_loss= 1.46526 train_acc= 0.50228 val_roc= 0.92351 val_ap= 0.93115 time= 0.07966\n",
      "Epoch: 0191 log_lik= 1.451297 train_kl= 0.00442 train_loss= 1.45571 train_acc= 0.50258 val_roc= 0.92320 val_ap= 0.93072 time= 0.08066\n",
      "Epoch: 0192 log_lik= 1.4468403 train_kl= 0.00443 train_loss= 1.45127 train_acc= 0.50255 val_roc= 0.92322 val_ap= 0.93107 time= 0.08464\n",
      "Epoch: 0193 log_lik= 1.4789962 train_kl= 0.00443 train_loss= 1.48343 train_acc= 0.50201 val_roc= 0.92320 val_ap= 0.93118 time= 0.07690\n",
      "Epoch: 0194 log_lik= 1.4654416 train_kl= 0.00444 train_loss= 1.46988 train_acc= 0.50193 val_roc= 0.92192 val_ap= 0.92981 time= 0.07767\n",
      "Epoch: 0195 log_lik= 1.4614121 train_kl= 0.00445 train_loss= 1.46586 train_acc= 0.50217 val_roc= 0.92114 val_ap= 0.92918 time= 0.08663\n",
      "Epoch: 0196 log_lik= 1.462687 train_kl= 0.00445 train_loss= 1.46714 train_acc= 0.50299 val_roc= 0.92085 val_ap= 0.92900 time= 0.08017\n",
      "Epoch: 0197 log_lik= 1.4482653 train_kl= 0.00445 train_loss= 1.45271 train_acc= 0.50189 val_roc= 0.92141 val_ap= 0.92967 time= 0.08066\n",
      "Epoch: 0198 log_lik= 1.472614 train_kl= 0.00444 train_loss= 1.47706 train_acc= 0.50190 val_roc= 0.92257 val_ap= 0.93142 time= 0.08165\n",
      "Epoch: 0199 log_lik= 1.4798152 train_kl= 0.00444 train_loss= 1.48425 train_acc= 0.50191 val_roc= 0.92218 val_ap= 0.93110 time= 0.07966\n",
      "Epoch: 0200 log_lik= 1.4775801 train_kl= 0.00444 train_loss= 1.48202 train_acc= 0.50181 val_roc= 0.92231 val_ap= 0.93080 time= 0.08166\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9136496368762353\n",
      "Test AP score: 0.9148204205135209\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABcIUlEQVR4nO3deXhU1fnA8e87k4QQspCFPUAS9n2LoAgI4m7dpWqpotZardatal1qaxf91VZbtRt1wX2rihV3QERAWcO+JmxZIIRsJISQdd7fH/cmJCEzmUBmJiTn8zzzMHPnzr1vzgz3vefcc88RVcUwDMNovxyBDsAwDMMILJMIDMMw2jmTCAzDMNo5kwgMwzDaOZMIDMMw2jmTCAzDMNo5kwiMdk1EvhCRWYGOw1sislhEbgl0HEbbYhKBccoRkZI6D5eIHK3zemZztqWqF6rqaycYx946+z4gIq+KSPiJbOsE93+jiCzz1/6MtsskAuOUo6rhNQ8gA7ikzrK3atYTkSA/hHOJHcdoYAzwsB/2aRgtyiQCo80QkakikiUivxKRA8ArIhItIp+KSK6IFNrP4+t8prappeYMW0SettfdIyIXerNvVT0AfIWVEGq2fbqIfC8ih0Rkg4hMrfPejSKyW0QO2/uZaS9/XETerLNegohow6QmIkOA2cAZdo3kkL38IhHZam93n4jc38xiNNohkwiMtqY7EAP0BW7F+o2/Yr/uAxwF/uHh8xOAHUAc8GfgZRGRpnZqJ5cLgZ32617AZ8Af7XjuBz4UkS4i0gl4HrhQVSOAicD65vyRqroNuA1YbteEOttvvQz8zN7ucGBRc7ZrtE8mERhtjQv4raqWq+pRVc1X1Q9VtVRVDwNPAGd5+Hy6qr6oqtXAa0APoJuH9f8nIoeBTOAg8Ft7+Y+Bz1X1c1V1qeoCYA1wUZ04h4tIR1XNVtUtJ/wX11cJDBWRSFUtVNW1LbRdow0zicBoa3JVtazmhYiEich/RCRdRIqBJUBnEXG6+fyBmieqWmo/9XQB+HL77HsqMBirJgFWDWSG3Sx0yG66mQT0UNUjwDVYZ/TZIvKZiAxu9l/auKuwkk26iHwrIme00HaNNswkAqOtaTic7i+BQcAEVY0EptjLm2zuadZOVb8FXgWethdlAm+oauc6j06q+id7/a9U9VysGsd24EX7c0eAsDqb7u5pt43EsVpVLwO6Av8D/nvif5XRXphEYLR1EVjXBQ6JSAzHmm584VngXBEZDbwJXCIi54uIU0RC7YvZ8SLSTUQuta8VlAMlQLW9jfXAFBHpIyJReO6FlAPEi0gIgIiEiMhMEYlS1UqguM52DcMtkwiMtu5ZoCOQB6wAvvTVjlQ1F3gdeExVM4HLgEeAXKwawgNY/+ccWDWV/UAB1jWLn9vbWAC8B2wEUoBPPexyEbAFOCAiefay64G9djPYbVjXKgzDIzET0xiGYbRvpkZgGIbRzjWZCETkThGJ9kcwhmEYhv95UyPoDqwWkf+KyAXe3FxjGIZhnDq8ukZgH/zPA24CkrG6pL2sqrt8G55hGIbha14NyqWqao/dcgCoAqKBD0Rkgao+6MsAG4qLi9OEhAR/7tIwDOOUl5KSkqeqXRp7r8lEICJ3AbOwut+9BDygqpUi4gDSAL8mgoSEBNasWePPXRqGYZzyRCTd3Xve1AjigCtVtd5GVNUlIj842eAMwzCMwPLmYvHnWDe9ACAiESIyAWpHQDxlVLvMPROGYRgNeZMI/o11C3yNI/ayU8qXmw8w5vfzySkua3plwzCMdsSbpiHROl2L7CYhf8z81KJ6x3SkuKyKZWl5XDUuvukPGIbhF5WVlWRlZVFWZk7SWkJoaCjx8fEEBwd7/RlvDui77QvGNbWAnwO7TyC+gBrSPZLYTiF8t9MkAsNoTbKysoiIiCAhIQFzm9LJUVXy8/PJysoiMTHR68950zR0G9YMSvuALKwZnG49oSgDyOEQJvaPY9nOPMz4SobRepSVlREbG2uSQAsQEWJjY5tdu2oyEajqQVW9VlW7qmo3Vf2Rqh484UgDaFL/WA4eLiftYEnTKxuG4TcmCbScEylLb+4jCAV+AgwDQmuWq+rNzd5bgE0aYN1LsSwtj4HdIgIcjWEYRuvgTdPQG1jjDZ0PfAvEA4d9GZSv9OrckaQunXht+V72Hzoa6HAMw2gF8vPzGT16NKNHj6Z79+706tWr9nVFRYXHz65Zs4a77rqrWftLSEggLy+v6RX9yJuLxf1VdYaIXKaqr4nI28BXvg7MV/5y9ShufGUVV//7e2Yk9+bqcfH0jglr+oOGYbRJsbGxrF+/HoDHH3+c8PBw7r///tr3q6qqCApq/FCZnJxMcnKyP8L0KW9qBJX2v4dEZDgQBST4LCIfG9c3mnd+ejpdIjrw90Vp3P5WSqBDMgyjlbnxxhu57777mDZtGr/61a9YtWoVEydOZMyYMUycOJEdO3YAsHjxYn7wA2uAhccff5ybb76ZqVOnkpSUxPPPP+/1/tLT05k+fTojR45k+vTpZGRkAPD+++8zfPhwRo0axZQp1nTbW7ZsYfz48YwePZqRI0eSlpZ20n+vNzWCF+z5CH4NzAPCgcdOes8BNLxXFB/fOYnZ3+7iT19s50BRGd2jQpv+oGEYPvW7T7awdX9xi25zaM9IfnvJsGZ/LjU1lYULF+J0OikuLmbJkiUEBQWxcOFCHnnkET788MPjPrN9+3a++eYbDh8+zKBBg7j99tu96s9/5513csMNNzBr1izmzJnDXXfdxf/+9z9+//vf89VXX9GrVy8OHToEwOzZs7n77ruZOXMmFRUVVFef/LTUHmsE9sByxapaqKpLVDXJ7j30n5PecyswdZB18fjb1FOyE5RhGD40Y8YMnE4nAEVFRcyYMYPhw4dz7733smXLlkY/c/HFF9OhQwfi4uLo2rUrOTk5Xu1r+fLl/OhHPwLg+uuvZ9myZQCceeaZ3Hjjjbz44ou1B/wzzjiDJ598kqeeeor09HQ6dux4sn+q5xqBfRfxnVjzD7Q5g7pF0D0ylG+253LNaX0CHY5htHsncubuK506dap9/thjjzFt2jQ++ugj9u7dy9SpUxv9TIcOHWqfO51OqqqqTmjfNV1AZ8+ezcqVK/nss88YPXo069ev50c/+hETJkzgs88+4/zzz+ell17i7LPPPqH91PDmGsECEblfRHqLSEzN46T22kqICNMGd+G7nXlUVrsCHY5hGK1UUVERvXr1AuDVV19t8e1PnDiRd999F4C33nqLSZMmAbBr1y4mTJjA73//e+Li4sjMzGT37t0kJSVx1113cemll7Jx48aT3r83ieBm4A5gCZBiP9rMhABnDezK4fIq1mceCnQohmG0Ug8++CAPP/wwZ555Zou0yY8cOZL4+Hji4+O57777eP7553nllVcYOXIkb7zxBs899xwADzzwACNGjGD48OFMmTKFUaNG8d577zF8+HBGjx7N9u3bueGGG046Hq+mqmxNkpOTtSUnpskqLGXSU9/wf1eO4LrxpnnIMPxt27ZtDBkyJNBhtCmNlamIpKhqo31dvbmzuNF0o6qvn1CErUyPqI4EOYTMgtJAh2IYhhEQ3nQfPa3O81BgOrAWaBOJwOkQ4qM7kmESgWEY7VSTiUBVf1H3tYhEYQ070Wb0jgkzNQLDCCBVNQPPtZATae735mJxQ6XAgBP4XKvVOybM1AgMI0BCQ0PJz883w8O3gJr5CEJDm3eDrDfXCD4Bar4hBzCUNnZfQZ+YMApLKykuqyQy1PtZfQzDOHnx8fFkZWWRm5sb6FDahJoZyprDm2sET9d5XgWkq2pWs/bSyvWxB53LLChlWM+oAEdjGO1LcHBws2bTMlqeN4kgA8hW1TIAEekoIgmqutenkfmRSQSGYbRn3lwjeB+oe9tttb2szagZhtpcJzAMoz3yJhEEqWrt7Az28xDfheR/UR2DieoYbBKBYRjtkjeJIFdELq15ISKXAa1rep0W0CcmjIwCM2uZYRjtjzeJ4DbgERHJEJEM4FfAz5r6kIjMEZGDIrLZzftRIvKJiGwQkS0iclPzQm9Z/bp0Yuv+Ilwu04XNMIz2pclEoKq7VPV0rG6jw1R1oqru9GLbrwIXeHj/DmCrqo4CpgLPiEjAmpymDe5KXkkF67MOBSoEwzCMgGgyEYjIkyLSWVVLVPWwiESLyB+b+pyqLgEKPK0CRIh1O2G4ve6JDd7dAqYO7IrTISzc6t1EEoZhGG2FN01DF6rqoZoXqloIXNQC+/4HMATYD2wC7lbVRicFEJFbRWSNiKzx1U0nUWHBjE+IYeE2kwgMw2hfvEkEThGpnXZHRDoCHTys763zgfVAT2A08A8RiWxsRVV9QVWTVTW5S5cuLbDrxp0ztBupOSWk5x/x2T4MwzBaG28SwZvA1yLyExG5GVhAy4w8ehMwVy07gT3A4BbY7gmbMiAOgNV7CwMZhmEYhl95M/ron0VkI3AOIMAfVPWrFth3BtaQ1ktFpBswCNjdAts9YTU3lmUfMt1IDcNoP7wZYgJV/RL4UkQ6AVeIyGeqerGnz4jIO1i9geJEJAv4LRBsb2828AfgVRHZhJVgfqWqAb0/ITTYSUynEPYXlQUyDMMwDL/yZvTREKyLwz/C6g76ITC7qc+p6nVNvL8fOM+7MP2nR1QoB4pMjcAwjPbDbSIQkXOB67Au6n6DNRnNeFUN6I1fvtYjqiNZhWaoCcMw2g9PF4u/AvoBk1T1x6r6CfUHn2uTekSFst9cIzAMox3xlAjGASuAhSKyQER+Ajj9E1bg9OgcSnFZFUfKA3Zvm2EYhl+5TQSquk5Vf6Wq/YDHgTFAiIh8ISK3+itAf+sZ1RGAbHOdwDCMdsKrOYtV9TtVvRPoBTwLnOHLoAKpR5Q112e26TlkGEY74VX30Rr2EBBf2Y82qWdnu0ZwyCQCwzDaB69qBO1Jt0irRrDfNA0ZhtFOmETQQEiQg7jwDqZGYBhGu+F1IhCRIXWen+6bcFqHnp1DTY3AMIx2oznXCJ4WkShgHnALMNA3IQVej6hQUnNKAh2GYRiGX7itEYhIQt1hoe2xhf6LNUbQw36ILWDO7B/HnrwjrNydH+hQDMMwfM5T09CHWIPBASAidwHXYM0dcIdvwwqsHyb3Ji48hH8u3hXoUAzDMHzOUyIIVtUisKarBC4EzlXVbUCUP4ILlNBgJzdPSmRJai6bsooCHY5hGIZPeUoEu0TkFRFZAPwMuElVS+teNG7LZo7vC8CSNN9MjWkYhtFaeLpYfA3wQ6ACa8KYhSJyEGsWsVl+iC2gosKC6R4Zyq5cc9HYMIy2zW0iUNUKrGkqARCRZGAEkFZ3Mvu2rF/XTuzONfMXG4bRtnl9H4Gqlqnq6vaSBACS4sLZlVuCqgY6FMMwDJ8xdxZ70K9LJw6XVZFbUh7oUAzDMHzGJAIP+nUNBzDNQ4ZhtGlNJgIR6SciHeznU0XkLhHp7PPIWoGkLlYiqLlgXFZZzc6D5uKxYRhtizc1gg+BahHpD7wMJAJv+zSqVqJHZCgdg53sOmjVCB6ft4Uf/H0ppRVm9jLDMNoObxKBS1WrgCuAZ1X1XqCHb8NqHRwOIalLJ3bllpBddJQP12ZRVulifcahQIdmGIbRYrxJBJUich3WvQOf2suCfRdS65LUJZxN+4r4/SdbcSmIwKq9BRytqCYlvTDQ4RmGYZw0bxLBTVhTUz6hqntEJJE69xe0dTMn9EGALzYf4NJRPRnSPZLVewt46svtzJj9PQcPm3kLDMM4tUlz+siLSDTQW1U3+i4kz5KTk3XNmjV+3efRimrmbz3A5AFdeG5hKv9dk4VD4EhFNf+eOZYLR7SLljLDME5hIpKiqsmNvedNr6HFIhIpIjHABuAVEflrSwfZmnUMcXLZ6F7EdArhtMQYjlZWc6SiGqdDWGOahwzDOMV50zQUparFwJXAK6o6DjinqQ+JyBwROSgimz2sM1VE1ovIFhH51vuwA2d8QgwA4/pGM65vdKOJwOUydyIbhnHq8CYRBIlID6wB6D5tauU6XgUucPemfS/Cv4BLVXUYMKMZ2w6YrpGhPHjBIH57yVCS+0azZV8RRyuqa99flpbHsN9+RUZ+aQCjNAzD8J43ieD3wFfALlVdLSJJQFpTH1LVJUCBh1V+BMxV1Qx7/YNexNIq/Hxqf0bGdyY5IZoql7Ih61Dtex+uzeJoZTWfb84OXICGYRjN0GQiUNX3VXWkqt5uv96tqle1wL4HAtH2NYgUEbnB3YoicquIrBGRNbm5rWd+gLF9ogFYvcfKdxVVLr7elgPAV1sO4HIpG7MOmUHrDMNo1by5WBwvIh/Z7f05IvKhiMS3wL6DgHHAxcD5wGMiMrCxFVX1BVVNVtXkLl26tMCuW0bnsBDG9Y3mvTWZVFa7WLE7n+KyKsb06cy6jEM88tEmLv3Hd7yfklX7mZLyKkrKzZ3JhmG0Ht40Db0CzAN6Ar2AT+xlJysL+FJVj6hqHrAEGNUC2/Wr28/qR1bhUeat388Xmw8QFuLk95cOB+Dd1ZkEOYS/zk8ls6CUh+du4rQ/LmTWnFUBjtowDOMYbxJBF1V9RVWr7MerQEucln8MTBaRIBEJAyYA21pgu341fUhXhvSI5LGPN/POqgzOG9qN4b0i6delEwmxYbxwwzgOFJcx7enFfJiSRY/OoazPPERZZXXTGzcMw/ADT1NV1sgTkR8D79ivrwPym/qQiLwDTAXiRCQL+C320BSqOltVt4nIl8BGwAW8pKpuu5q2ViLCgxcM4rH/beb2s3pz06RERIQ3fjKBYKeDLhEduHpcPBkFpTx5xXB2HizhtjfXsi27mDH2NQbDMIxAavLOYhHpA/wDa5gJBb4H7qrp7eNvgbizuCVlFZYy6alv+MPlw5kxLp6yymo6h4UEOizDMNo4T3cWN1kjsA/4lzbY4NPA/S0TXvvSq3NHosOC2ZxVxOo9BaSkF/LtA1NxOgRVa8RTwzAMf/KmaagxP8QkghMiIgzvFcWynXnkFJdR5VIWbT/ImvRCvtpygK/vO4t3VmXw1wWpXDu+D1MHdqF/13BiwzsEOnTDMNqoE00E5rT1JAzvFcXStDwAOocF87eFaaTmHKbapXybmsvLy/agwH++3cW/F++iY7CT5Q+fbZqQDMPwCbe9hkQkxs0jFpMITsqIXlEATOofx/Wn92VbdjFhwU6iw4L542fb2Jtfyq8vHsr3D03nz1eP5GhlNV9vO0hRaSUfpGTV3qDmqzGNFu84yJl/WkTR0UqfbN8wjNbFU40gBevicGMH/QrfhNM+JCdEEx0WzK1Tkkjq0okXluzmnnMHsq/wKHO+20OnECcXjehOWEgQV4+N56/zU/lqywHWZRby5ooMYsNDQOGBDzbwyo3jGREfxZHyKjp1ONEKXn1vr8xg36GjbMw6xOQBrecGPsMwfMPtkUNVE/0ZSHvSNSKUdb85r/b1qkfPITI0iG3Zh5nz3R5+MLInYSHWV+NwCOcN68Z7qzOpOf9/eekecg+Xk1dSwcMfbeSCYd15ZkEq9583iJ9P7YdI4xW2A0VldInogNPDBemS8ioWp1rDeGzZX9yiiWD/oaPEdAohNNgJgKpy8HA53SJDW2wfhmE0nzc3lBk+FtUxGBFhaM9Inrt2NPedV3+kjfOHdae8ykVVtYsfJsezbGceO3IOc/HIHmzeV8zT81Pp1bkjf/lqB0/P3wFYYx29sXxv7TbS848w+c+L+PlbKazeW8DZTy/m0437j4vl6205VFS5CHIIW/YXn/DflF9SzvYDxz5fUeXi/GeXMPvbXbXL3l6VwaSnFnGwuIzMglLueGutGX7DMAKgZdoSjBZz2ehexy0bnxhDl4gOTO4fx0MXDuHj9fuJj+7I89eOITI0mA5BDh69eAiPzN3EvxbvYmiPKH75/nrKKl1EhAZz+ZhefLJhP5XVyldbcvhqizUw3r8X7+IHI3vW29dnG7PpHhnK8F5RbNlfVLs8s6CULfuL6BDkZOqgLqjC4tSDTBnQhSBn/fOJPXlHmPniCvKOVLDswWl0jQwlNecwh8uq2LzPSg6qyhvL06msVrZkF7M79wifbcrm6nHxjOnTmWfmWzWcqDD302NXVLkICTLnMoZxskwiOAUEOx18cfdkwjsEERrs5KVZybVNPP935Yja9R67ZCiLU3O54+21dApxMrp3Zx6eu4nBPSL4dGM2yX2juWRUT77flceQHpE8uzCNzfuKGG5fvM4rKWfxjlxmnt6HqI7BfL09hyPlVWzLLmbWnFUcseddeHrGKFyqPPjBRp6ZMYqrxh0bg/BIeRXX/Gc5ldVWDWbOd3t56MLBbN5nJZXduSUAbMwqYvuBwwCk5Rxm50Fr+Zb9ReQUl/HGinR6x3Tk1in9Gi2Tg8VlnP3Mt/z+smFcObYlxkA83uZ9RWzLLubqcfFum9sMoy3w6nRKRJwi0lNE+tQ8fB2YUV9ceIfatvXJA7owuHvkcetEhgbz2A+GAvDA+YN44YZxRIQGMWvOKrYfOMwPRvZg1sQE/nN9MjdNTKRDkIP3VmfWfv691ZlUVLuYOaEPw3tGoWoNnDdrziq6RYYy9+cTGdYzkue+TuUfi3YCMH/rgXoxLE3L5eDhcp6/bgwXj+zJmyvSKTpaySY7EaQXlFJR5eK9NZmEBjuIDgsmNaekNils2V/M2gxr1rf312SRnn+E619eydK0+sOPz123j5LyKl5cusdnw3z/5uPNPPDBRu5+d/1JjQ1VVlnNvA37qTYz1xmtlDfDUP8CyAEWAJ/Zj+bMVGb40aWjerL0wWnMmphA14hQ/jlzLPklFYjARSN61K4XFRbMhcO78791+zhSXkVVtYu3VqRzZv9Y+neNYFgvK9H84dOtxISH8M6tpzO2TzS/PG8gmQVHySgoZWC3cL5Nza03Q9ui7QeJCA3i9KRYbjsriZLyKt5emcHmfUWIQLVL2Z1Xwqcb9nPh8B4M6xnF9gPFpObUTQSHCAlykHawhB+/vJKlaXnc+MpqPrCH81ZVPkjJIiTIwbZsa/3m2Jh1iDveWsucZXvcrrP/0FHWZhxiVHwU8zbs55Xv9jZrHzVUlYfnbuKud9axYrfnIbrqJrSt+4sprTix6yVHyqsoLmu7XX9dLuWTDftPuHyM43lTI7gbGKSqw1R1hP0Y6evAjBPXOyastinjtIQYnr12NA+eP5iuDXrnXH9GAofLq/ggJYsvNh9gf1EZN5yRAED3yFBiO4UQ1TGYV28aX9uzZ9qgroxPiGFc32h+ffFQyipdvLUynXveXce27GK+2ZHLlIFdCHY6GNYzion9Ynl9+V62HTjM6YmxAHy0dh/FZVVMH9KVAd3C2byvmLJKF4lxncgoKGXnwRJuOjOB0GAHmQVHeeqqEZyeFMMjczexJ+8IG7KK2HmwhAfPH0R4hyBety+Kf78zj38t3nlcDaHwSAX/+XYXt72RwulPfs2l//iOzzZl86/Fu2rvxSg4UsHd764js8CaYvTzTdYMc89eO4bTEqL5ICWTapfy/NdprNl7bOK9A0VlvLR0N/kl5Y1+F698t5eP1u0DrGsn7mQXHWXSU9/w0tLdfJuay0XPL+W179OPWy815zCFR4713na5lLdWpnPLa2t4Y4W1/l3vrOO0Py7k8XlbOBKAi++zv93FT19vufHAPl6/j//ZZQhWR4hfvLOu9sTAOHneXCPIBIqaXMtotRpeEK4xrm80Y/t05sWluymrrGZw9wimD+4KWENhPH/dGGLDQ+jXJbz2MyLC6z8ZD4BDhIjQIP74mTV6+NK0PPKPVHD2oK616984MYFb30gB4LLRPVm+O593V2ciAhP7xXG47NiB6qqxvXh6fioAZw3oQlynDhypqOKa0/owbXBXpj/zLb/6YCMV1S46BDn44Wm92X+ojDnf7WFf4VHWZhTiUuuGvZpur19vy+GhuZvIPVxO39gwJiTFkNw3GkR47H+b2ZB1iDF9onlp6W4+Xr+foxXVvHBDMp9tymZoj0gS4zpx5dh4Hp67iUfmbuK9NZk8/7Xw5BUj+OFpvXlp6W5eWraHvy5I5ZkZo7iwTq2r6Gglz8zfwdRBXVi+K5/0/PqJoOBIBY/M3cRPJifyv3X72HfoKH/8bBsRodZ/y23Z9Xtt7c07wg/+voyrx8Xz5BXWtaH3UzJ59KPNBDuFPXklzBzfh5V7CogL78Dry/dyoKiMm85M4PFPtvLUVSMYGd/Zm5+MV/61eCdlFdXcd96gesvfW53J3vwjHC6rJCL02MX+LfuLWLTtIHee3d/ray7FZZU8+tFmOodZnR5UlX8ttnqerd5bWHvi0tL+74ttZBUc5Z8zxx73nqoyd+0+JvaPpUdUR5/s39+8qRHsBhaLyMMicl/Nw9eBGf5xy+QksgqPUlxWxfPXjanXA+jM/nGNXosIDXYSGuwkJMjBtaf1ZlTvzjw9YxSFpVYT1FmDjt17MH1IN+Kjrf8s4xNj6BkVStHRSob1jCSmUwgDu1lJxiHHekw5BEb17sxPpyRxzzlWV9quEaE8cP4gVu0tYFduCX++eiSRocE8fNFgfnXBYDbvL+LCET3oHhnK3xftJLvoKLe/mcJPXltDbKcQPr9rMt8+MI3nrh3D9WckcMnIHjgdwsJtORSXVfLG8nSiOgYzf2sOD8/dxLqMQ1w80jqoXzyyh3U9ZU0m4xNjmJAUw0NzN5J7uJzvduUzvFckfWLCeOLzbfWuA7y9MoMjFdXcf94g+sSEkZ5fWq8cn12YypdbDvCTV1fz3upMfphs9ZiqrHYxoGs4afYFdLAOPr/+32YqqlxsrdOtd/6WHOKjO3L39AHsyj3Cxn1FlJRXcfc5A3jkoiF8ueUA1764gm3Zxbyz6tj1oKpq10ndOa6qvLx0D/9avIuc4rLa5en5R9iTdwRVaq8L1Xjqyx08syCVlPRCj9s+WFzG5n1FFJVW8t6qTErKq8gqPEp+STnLduaxaV8REaFBpNSpmW3ZX8Q7qzI83m2vqhxupMmsosrFJ3Wu4ZRXVfP2igw+35xNgV37Oni4jKv//T0LtuawNC2PX76/gT9/ueO4baWkF3KgqKzesrUZhby5Ip2NdeY2r/H9rjw+Xr/vuOV1fbYxmzveXssLS3ax8+Bhj+ueKG9qBBn2I8R+GG3IeUO7cdGI7pwzpBsDu0U0+/OPXjy09vmR8ip255YQV2eAPKdDuPecgby9KoOE2E706xrO/qIyzuwfB0D/rtY+E+I60TsmjK4RHYgN79DoXdIzJ/QlIjSIif3iapuqgp0Obp/aj59MSiQkyMGcZXv4/adbmfb0YlSti+Y/nZx0XDfTzmEhJPeNZsHWHKqqlcPlVXx4+xnc/e762gmGrj+jL2BdhD9/WHe+2JzNk1cMp7JaufC5pby3OoNt2cXcf95A+neN4LY3U5i/5QAXjuhBeVU1c77bw6T+cQzvFUXf2E71EkFqzmHeWpnBRSO6s2pPIeVVLn553iCiOgZTWFrBnGV7eH15OtUuxekQXl62h2U78+ga0YGdB0tQVcoqXSzbmcd14/swurc1t8WbdvPQyPgoBnWLILOglPSCUlRhwdYc/nj5cJwO4cnPt/PfNZn8744z2XmwhG+2H+R3lw2r7ZBQXFZJRIeg2jP3dRmF3PPeeh65aAjnD+vOzoMl5NsHybdXZnDvuVbCXpJ67KL+hswiJvazvufMgtLaC/6vL08nOSGG0ooqfvzSSpK6hPPHy4cTGuzk7ZUZPPLRJrvcgwh2OojpFELBkQo27ivirRXpdI3owE8nJ/HE59vYd+gohUcquO6FFRwur+LbHbn87ZrRdAxxHvf7+WLzAe55bz3z7jyz3gnOvxbv5NmFaQQ7HVwwvDvL0vI4bDepLUnN5bLRPXn0o82sSS8k7f0NdI2wft+fbczm1xcPqR0QsuBIBde9uIIpA7rw0ixrtOd3VmXw8Fzr7+kRFcriB6bSIehYbE9+vo2t+4vpG9uJ0b07HxfzM/N38PdFO+kcFsxnG7MpLK3kVxcMPm69k+XNMNS/a/G9Gq1GkNPBv2aOa5FtzZqY0Ojyq8bF13Yx7dclnKVpeUzub9UaojoG0zumI8N7Wl1YH7pwcL3mhLqcDuGKMY13Fa050F83vg9vr8qgb0wYj186jN4xYW7jPXdoN/742TZSc0q4cHh3xvWN4bWbx5N7uJzTk2Lrrfv4pcO4dUoS/btGoKokxIbVNlGc0S+O0b070ycmjJeW7eGC4d355ze7yD1czl9/aM2+2jc2jGU7c8kuOso1/1lBRkGp1ax2+QiOlFeRV3LsDuseUR3p3zWc8ioXWYWlfLoxm798tYNzh3ZjyoA4Hvt4C/uLyti2v5jyKhfTh3RlRLxVfvM27Cc02EH/LuGICL+7bHjt8m9T17E+s5ARvTozd10WJeVV3PDySg4Ul+FS60x4QLcI3liezoHiMsYnxPDCDePoHBbCmysySM8v5bY3U3ji8hFUu1wADO4ewdurMrhjWn9Cghx8m5pLH7vM654Bv7c6EwEuHNGdzzdZB9C/LkhlXeYh1mYcIjXnMDOSe/OHT7YyqX8cP5rQh1e/38uqPQU8f90Y7n53HSt25bMkLY8fT+jLGf2s72fe+v28sGQXkR2DuXlSIs8vSuOf3+zk/vPrN1cBLNuZR0WViz9/uYM5N54GwL5DR2tvcly84yAXDO/OZxuza5PQNzsOUu1SFmzNYdYZfXlvTSZpB0v4xdn9+fuinbyfksXPpiQhIryzKoOKKhffph7kUGkFwU4Hz8xPJblvNDdMTOCud9bx3urM2uas/JLy2vtqHvxgA/PunFSbiMHqVv33RTu5cmwv/nTlSKvG7fbXfHLcJgIReVZV7xGRT4Dj6luqemkjHzMMj84a1IW1GYUkJxybne31myfUtouf7D0BHUOcLLzvLK/WvWJML7ZlH+bcod04b2g3wEpUda+J1IjpFEJMJ6tCLCJcMLwHs7/dRXiHIEbFR+F0CLdMTuQ3H2/h6tnLSUkv5Opx8Uyyaz4JsWGUVbp4c0U6GQWl/OLs/lw4vEftdhsmrJqa0nc783l6/g4uHtmD564ZXdtDKvXAYb7enkN4hyAmJMYSEuSgX5dO7Mo9wri+0cfd5Dd1UBeCncL8LTkUHKnkUGklP52cyMvL9pDcN4bTEqP55ze7ate9fEwv5izbw9Wzl/P2Tycwf8sBLhnVk0OlFTzx2VZGxnemZ1Qov7pwMDe9spqXl+1h1sS+fL8rn6vGxlNYWsE6O9aswlLeXZ3J1EFdeeD8wXy+6QBnP/MtJeVV/HxqP0bGd+axjzfz2P820yMqlOevG0NMpxAuGNadzMJS+sZ24rmFqby10jrQnju0G4O7R9ApxMlTX24nvEMQb/xkPEldwtmQdYiP1u3jvnMH4nAI2UVHWbwjl2tP6836jEMEO4VF2w/y5eZshvSI5IEPNgLW9bLFO3Ipr6pmwdYczh/eHZcqC7bkMH9LDuMTY/jNJcM4LTGG73bmce85A1m1p4C/LkjlbwtSuXB4d1btKaBPTBgZBaV8timbg8Xl5JWU88IN4xjTuzNvLN/LP7/ZyQ+TexMa7GTZTmsE4runD+C5r9M46y/fMLFfHLmHy7n33IEs3JaD0yE8ctEQQoIcPh2KxVON4A3736d9tnej3Zk2qCvT6lxMBkiM6xSQWGLDO/CMfcbeXBeN6M7sb3cxPjGm9qD74wl9qXYpT325nVHxUfzx8uG1TSt9Yq2/8d1VmSTEhvHL844/Y62rf1crGf37252owr3nDCDI6ai9prI1u5gFW3M4a2CX2trQ6N7R7Mo9Uju6bV2RocGc0S+O/67JZOWeAmI7hfDgBYOZOaEvPTqHEuJ0EB0WQr+u4bXfz5QBcVw/ZxXX/sdqdpkxLp6enTty3t++ZfnufK4c04upA7tw0YjuPD1/B/M27OdoZTUXj+zBpqwiPt2YzbwN+/n9J1sor3Jx1/QBJMZ14j/Xj+P7nXlEhAZz9zkDCHY6mD6kK0tSc0mM61SbcB0Ooa9dbqPiOzN33T46hwVzWoKV6Mb0iWbZzjyeuGI4SXbyvmJML+5+dz2r9xYwtm80t725lg2Zh+jXJZztB4q5ZXISn2/K5rY31wLQMdjJHy4bTrVLeWjuJh79aDOHy6u4bHRPDpVWMnftPrpEdOAf143B6RB+MLJnbeeLBy8YzCvf7SE02MnctVm4FP5z/Tj+8tUO/rYgjfwj5Vw8sgdj7Slpf3neIK59YQV3vbOOf80cy9K0PDqHBXPX9AGclhDDf5bs4vtdeRytqOaBDzZQVlHNlAFx9ZpafcXToHMp9r/f+jwKwzjFjOgVxRVjenHJqGO9hBwO4aYzE7lsdC862hfUayTEWmf8+Ucq+MHIHsdtr6GojsF0jehAZsFRBnWLqK0hdA4LoWuE1SMor6SCK8ceG5JkdJ/OfLg2i5HxxycCgMcuHsJPX1/D+sxDzDqjL8FOBwl1kvAtk5PqrT+xfxx3TuvPc1+nEdsphIn9YglyOrhiTDwfrs3i9KRYRIQ/XTWSzfuWsTu3hH/PHGstt7dx1zvrSIrrxAs3JNcmt/OHdef8Yd3r7ctKBt3clsfI+CjmrtvH9MHdahPvL87uz7lDu9UbluXcod0IC3Hy9qoMFmzNYUPmIUTgT19sw6UwITGG28/qx7epuWQUlPLD5N50jwolu+goAB+kZHHu0G5M6h/HkYpqzhnSlZ9P639c12uwahHj+loH+ctH92LpzlzOGdKNnQdL+MtXO7h4RA+emXHsROP0pFh+e8lQfvfJVn72Rgob9xVxZv84nA5h0oA4Jg2wao/f7DjITa+sBuChi4a4LZOW1OQ1AhEZAPwfMBSoLQ1VTXL7IcNo40SEv10zutH3as5o6+rVuSNBDqHKpfV6VXkyoFs4Bw+X17sREGBgtwiW7cyjS0QHzhp4bFvnDunG19tymDKw8e0P6BbBJ7+YxBsr0rnayya4O8/uz4asQyTXaW66//yBVFZbTTRg1TY+vH0ipRVVx87ge3fm9KQYkvvGcOfZ/eslxRMxISkWEfhBncQ7ISmWCQ2u5YSFBHHBsO7Mte87uGpsPPlHrKFTAEb37kx0pxAuH1N/TK8eUR0Z3D2C/YeO8oRdkwvvEMRLs07zKr66B/JbJicyvFcUk/vHHTf17E1nJlLtUp6Zn8rRymrOamR032mDunLRiO58vyufcz0kx5bkzeT1y4DfAn8DLgFusj/3W9+Hd7xTffJ6o/066y/fkH2ojPW/Pbd2mHFPfvvxZl5bns7C+6bU1ggAfvfJFl75bi+3ndWPhy5s+R4krdXB4rJGz8wb2n/oKAu35TC4eyTJfaP5aN0+fvn+BvrEhLHkwWluP7fjwGEqq121Y2/5Ul5JOYu2H+Ty0b0aHTixvKqaotJKr/5eb53U5PVAR1X9WkREVdOBx0VkKVZyMAzDS6cnxlJaWe1VEgC4/oy+9I3tVC8JAIzpE82bK9KZkeybwfZaK28Pij07d6x3o9m5w7oR8pGDMX06e/zcoO7N7z59ouLCO/DD5N5u3+8Q5KRr5MnVoprDmxrBd8Bk4ANgEbAP+JOqer7a5SOmRmC0dy6XkneknK4RZkIfby3flU/vmI7ER7vvTtzWnWyN4B4gDLgL+AMwDZjVYtEZhtEsDoeYJNBMNfcdGI3zmAhExAn8UFUfAEqwrg8YhmEYbYjbsYZEJEhVq4FxYmblMAzDaLPcXiMQkbWqOlZEngEGAO8DtcMnqupc/4R4XFy5wPHj83onDshrwXBaUmuNzcTVPK01Lmi9sZm4mudE4+qrqo32LfbmGkEMkA+cjTXUhNj/BiQRuPtDvCEia9xdLAm01hqbiat5Wmtc0HpjM3E1jy/i8pQIutrDTW/mWAKoYebcMwzDaCM8JQInEA6NDnhnEoFhGEYb4SkRZKvq7/0WiX+8EOgAPGitsZm4mqe1xgWtNzYTV/O0eFyeLhavU9UxLb1DwzAMo3XxlAhiVLWg0TcNwzCMNqPJISYMwzCMts2byevbBBG5QER2iMhOEXkogHH0FpFvRGSbiGwRkbvt5Y+LyD4RWW8/LgpAbHtFZJO9/zX2shgRWSAiafa/0U1txwdxDapTLutFpFhE7glEmYnIHBE5KCKb6yxzW0Yi8rD9m9shIuf7Oa6/iMh2EdkoIh+JSGd7eYKIHK1TbrP9HJfb781f5eUhtvfqxLVXRNbby/1SZh6OD779jalqm39g9YDaBSQBIcAGYGiAYukBjLWfRwCpWHM9PA7cH+By2gvENVj2Z+Ah+/lDwFOt4Ls8APQNRJkBU4CxwOamysj+XjcAHYBE+zfo9GNc5wFB9vOn6sSVUHe9AJRXo9+bP8vLXWwN3n8G+I0/y8zD8cGnv7H2UiMYD+xU1d2qWgG8C1wWiEBUNVtV19rPDwPbgF6ePxVQlwGv2c9fAy4PXCgATAd2qTUkut+p6hKg4bUzd2V0GfCuqpar6h5gJ9Zv0S9xqep8Va2yX64A/D5utZvycsdv5dVUbPawOj8E3vHV/t3E5O744NPfWHtJBL2AzDqvs2gFB18RSQDGACvtRXfa1fg5gWiCwbo/ZL6IpIjIrfaybqqaDdaPFOjq9tP+cS31/3MGuszAfRm1pt/dzcAXdV4nisg6EflWRCYHIJ7GvrfWVF6TgRxVTauzzK9l1uD44NPfWHtJBK3upjgRCQc+BO5R1WLg30A/YDSQjVUt9bczVXUscCFwh4hMCUAMbolICHAp1rhX0DrKzJNW8bsTkUeBKuAte1E20Eet7uH3AW+LSKQfQ3L3vbWK8rJdR/0TDr+WWSPHB7erNrKs2WXWXhJBFlB3OqB4YH+AYkFEgrG+5LfUHrxPVXNUtVpVXcCL+LBK7I6q7rf/PQh8ZMeQIyI97Lh7AAf9HVcdFwJrVTUHWkeZ2dyVUcB/dyIyC/gBMFPtRmW7GSHffp6C1a480F8xefjeAl5eYI28DFwJvFezzJ9l1tjxAR//xtpLIlgNDBCRRPus8lpgXiACsdseXwa2qepf6yyvO0P5FVhjPPkzrk4iElHzHOtC42ascqqZiGgW8LE/42qg3llaoMusDndlNA+4VkQ6iEgi1ii+q/wVlIhcAPwKuFRVS+ss7yLWXCOISJId124/xuXuewtoedVxDrBdVbNqFvirzNwdH/D1b8zXV8FbywO4COsK/C7g0QDGMQmr6rYRWG8/LgLeADbZy+cBPfwcVxJW74MNwJaaMgJiga+BNPvfmACVWxjWKLhRdZb5vcywElE2UIl1NvYTT2UEPGr/5nYAF/o5rp1Y7cc1v7PZ9rpX2d/xBmAtcImf43L7vfmrvNzFZi9/Fbitwbp+KTMPxwef/sbMDWWGYRjtXHtpGjIMwzDcaDIRiMhAEfm65u47ERkpIr/2fWiGYRiGP3hTI3gReBirHQ1V3Yh1sdUwDMNoA7yZqjJMVVdJ/fnrq9yt7GtxcXGakJAQqN0bhmGcklJSUvL0JOYszhORftg3KYjI1VhX2gMiISGBNWvWBGr3hmEYpyQRcTssizeJ4A6sGXEGi8g+YA8ws4ViMwzjFJCSXsiK3fmcnhTLuL6BGsnD8BWPicC+geJ2VT3HvsnIodZASIZhnISaA2t0WAib9xchCj07h7JxXzEOYHxSDGkHSxBgWM8oCksrvD4IH7dt4Mqx8bWfTUkv5MO1mZRXuugeFcqBojJCg5319gPw4dosRMGlyn/XZOFSJcgpTBvUlS4RHWq3aW0v67j9nGoaK7eGZe9uneY8b0651V2nub+D5mjyPgIRWaSqZ7foXk9CcnKymqYhozlay9lsTRwHD5fx5vIMqpt5D49ThMvH9KSi2kWQOBjcI4It+4uocilj+kSz6+BhDhSVsWxnPlUubfBZmJAYS7W6WL23EJeHXTsEVJsesMYpkJwQzZq9hVTbKwc5hCkD44gN78CY3tE+O3B5o+H3XvO6c8dgtuwvQkQY1jOKjVmH2HmwhHWZh6h2UzBOhzB1YBzf7Mj1WHbecAqM6tOZDRmH6pXb2YOtBFuTNPYfKmVJal69/QnQIdjBW7ec3uwyFZEUVU1u9D0vEsEzWLctvw8cqVmux8bA8CuTCIwa7g7wdc+iIjoE8dKyPVS7FKdDmDwglp6dw7hyrDUic83ZXWFphcczwcb2G+wU1mceQhDO7BdL4dFKt+u/tSKdjzfsd3ug8bexksrpjm0UaDjDHXsBmFs9mbV6/PA5YyWVK51LPa7jidMh/HRSIsXlVY2eKTdWzp6+24Zn5ElxnViXWYiqMK5vZ7bsL+bAoTJW7S2gyqU4HMLk/rEsScs76YN4a+AUuO+8QdwxrX+zPneyieCVRharqt7crChaiEkE7VtNs0Zm/lGW786vPcDfYh9ocovLWbQ9p/ZMyx0HgNDkgSHI3nZEx2BOT4pl2/4ifjNvi9vPBTmE3182nEHdI6yz/+IyXl+Rjqf/Zo0dlDe7Euo9j5ESVriG1DsIuztAN7W9SnVwfdDXOHDVG7qyEieLqseQRxSbXQmMdaTRm4Oc5kzFieu4dU4kKbhTkyyKyirJyC9l5Z6Cet9tRMdgio9W1ib1k9Gw3IDa8oqREo/fQ0s8P5lycwiEBAWgRtDamETQ/qSkF/JBSiZpOSWkpBcGbFxiwfvxfR1NJJmxkspE5zYmxocw4cDbCNWNjidcQxVcOJhfPY4wyoiSI4xw7K09QFfh4PvqYQRTxWnOHQSJq8EG6vwdUn+x1F3FzXqNbIYqdTKveiL7iWN85CEqHKF8eagXQ9gDuE9gLcWbBFr3+RhHGolygLGONJz2X1KNA0HtB94P6uzpy2pCtTr41jWSdO3BmMhDVEhH5hUl1Su3mri3kci50QfoGBwEPUbiKs0neujZDD7tnGbv92RrBPHA34EzsYpkGXC31hmZz59MIvCtmmYVl0tJjOvExiyr3XRCUuxxFy+jw0JYl1nA0XIXA7qFszO3hNBgp9u24YYXvpq6qDa0RySLd+SycFvOSR38BevAjEC1y/06Y+wDS1MHrvoHoD0IyiZXUpMHvWRHKrf22c8AMuh74KtjBx8/aHjAb2q/3qzj7nP1Xqt1sH2x6iIi5Sjg/Rm0u1rQ6Y5thHOEnwZ9jrNBrcaTxhJbw9j99X00tm93pOGroFCYNQ96N2/U9ZNNBAuAt7FGDAT4MdbY5uc2K4oWYhJB8zXWQ2VYr+MPvpn5pXy3Mw83x8pmq20m6RbOi0v3MH/LgRbbNoDTbt9xubTef6TTgtK4I3p17VlUWMEWukWGUho7jB2701nnGMacjK5UVblwOITfjCphbOEXDMmZh2gVipM13Wawc18uSv2q/LWOr/lj8CvWmXiDo4Z10HPyWOWNvOuaDhxrhugjB5kUtBWHVrdgCZwMAYcTzrgTyougJBdSvwJX5fGrOoJhoD0nurt1PGgyibtZQbGSyKtV53OEjvSUPK5yLrUSqNT+FW6TXHOTX/0ttmLihLMfhcm/bN7HTjIRrFfV0U0t8xeTCLxTc/AvPlrJS0v3NLuHSkup1+zQAuoe5LsNHk+n/M0cKC6nNGYYmr2ByMp8Bh/+HoenlOMIInvYLeTl5RLvLCR6/xJwHX+zfE3M1Tj5unoMoi6mO9fhEPdn8opV9V9YPRYROMe5DkcTzT7HHZQR6D4KDqy3nneIhOX/AFc19UrS3QHaEQxjr6+/jYbPj+ZDwuT6Z5WZq2DD28evP+q6Y+vVrFOSC2kLoLoSWjS9W3x6dl633Gr+BofDKv/QSOgY677cTvb5yZabOMDZISA1goVY43PXTAhyHXCTqk5vVhQtpK0ngpW785m7Ngunw8HwXs3vw1xYWkFhaTmvLEs/4YO/u7bXulX1mqaUmotuW119GOXYTRUOt80kdS/SedMMAFaTTrIzjTuiVxJfvZ/E0o2eD/I+UlOS3hyc6pa6+/XrHPxDI48/KDeUuQr2Lq1/kGrsAN1wuS81FpM3CazFNJFA3T1vWG57lzZd/i2pqXJrbgL30skmgj7AP4AzsL7N77GuEbi9XdmX2koiqNs9DuC/qzNISS9kZ+6R49Z1OoSJSbEs25XnsfdJU9wdiOv2nIiUUn4a9DkOjm/CUEBxsj3hBhylOQw6+CViH5Qbnvm7FFx2M0mq9uY65yKucC7FKe7/ABcOdvSdSfHhYgSBHiOJO7CEpPxvEZ9X1+2DiiqoiyYPXI6g+gcgbw56dc/UT+I/9CnlRA96x5VnnUag5iRQo5bpNdSKLN5xkBe+3c3KPQW41G7rVF9Uro8ZK6n8LOhTznWmNHpAdeHAupZa/6B+sqxmEsEB9dp1fa6m6h/e1fsmlrHXW2eKUP/A1bDtvO66DQ9ANWfla9+sv35NLP46U28r6iaRo/nH/jUH/xNysjWC17BqAIfs19HAM+Y+guZ7/utU/rogzeM6nppPvOmTXHPeVNM98aIuuQwpXBSw3hAtzt1B3l3Vv6GmmljcfaY5zS6BaKYxjCacbCJYp6pjmlrm5rMXAM8BTuAlVf1Tg/ejgTlAP6AMuFlVPU5Afqomgnkb9nHXO+s99H3eQxLZjHdur9980kQfZhdOdkRM4HBw12P9jKsLGJjxHtKMrnXH7aBh22vDs2lxWM0jA87z/sxbnDDoQu/Xh+P3Yw6shnFCPCUCb0YfdYhItKoW2huL8eZz9oB1/wTOxZoYerWIzFPVrXVWewRYr6pXiMhge/2AXIT2pYVbc3jg/Y2MlVTeDnmCECrrH6CdgBzfFQ4avjiek2qGHv7eelHwP88rN3YgrtuLoW7Picaq34Mvrl9V91RFH3yxdVa87u1j277oGUi+0f36jbUlm6YAw/A5bxLBM8D3IvKB/XoG8IQXnxsP7FTV3QAi8i5wGVA3EQwF/g9AVbeLSIKIdFPVHG//gNbA3WiPAC8v283nmw4Ayr3B79OBSkTc93H2TROOfYbv7kDsbc+J3uO9PyDXrDvqRy2/bcMwWlSTiUBVXxeRNUDNCKRXNjird6cXkFnndRYwocE6G4ArgWUiMh7oC8QD9RKBiNwK3ArQp08fL3btPynphfzlpdcZ69pCpj1WyQrXEH64aiCjJZUJso3bncpM59fEO/Ltnjf1D/j1Dv7etIHXPZs/7gafE+hd4cuDsDnAG0ar5zYRiEgYUKmqlaq6VUSqgYuAwdQ/q3e7iUaWNWzx/hPwnIisBzYB62hkGkxVfQFrchySk5NbVTenPV/9gzcdf8XpsHrc1NwN+WLVRdwUNJ8OVBw789ea29wFBl9kHeybc6GzMQ1vAjK9KwzDaCZPNYIvgZ8AaSLSH1gOvAX8QETGq+pDTWw7C+hd53U8sL/uCqpaDNwEINakyHvsR6uXkl7I8kXzuH3fM7UXd1WtG6BEXdwW9KnVJVOOJQBrLXuskDPvaZmDtDnjNgzjJDk8vBetqjV9HWcB76jqL4ALgYu92PZqYICIJIpICHAtMK/uCiLS2X4P4BZgiZ0cWrWU9EL+9uLLXLvn0XpJgJq2f7EHOauzHHEgzhBIvumEbg83DMPwFU81grpNMGcDfwFQ1QqRhmPcNvJh1SoRuRP4CqtfzBxV3SIit9nvzwaGAK/bzU5bsWogrV7mvCd53fmC1RqvYHV6dFKUdAlxez8FVzVq30ClCDL4Iug1zjTVGIbRKnlKBBtF5GlgH9AfmA/WWby3G1fVz4HPGyybXef5cqzZz04Z6xf9l0vzXqht9qlW2B0xHtdZv7LGCLfb7GXd2+CqsmoBLdUMZBiG4QOeEsFPgbuBBOA8VS21lw8FnvZxXK3S9lUL6b3kl/Xa/hEnA6954tiBvrndJg3DMALMbSJQ1aNYvXoaLv8ea+C5dmX76oUkfXYNIVKFAlVqNQftm/gHEho70JuLuIZhnCK8uaHMAA5tWUCw3bO1GmFr6FhCznnkhKaMMwzDaE089Roy6uhUvNu+JiBUEmySgGEYbYanG8pCgQhVzW2wvCtQrKplvg4u4OyhF9JLHAzLn49iDdm8cfhDTDBJwDCMNsJT09DzWDeVzW2w/FxgEnC7r4JqFdIWom//ENVqekPtBWJRxVWaH+joDMMwWoynpqFJqtowCaCqbwFTfBdSK7BrMRXvzUK0GgfHCqlKhUqCiB56tqdPG4ZhnFI81Qg8DYTZdq8tZKxE37yCYJfr2Dy1Ys0gtjvitGP3CxiGYbQRng7oB+0RQesRkdOA3EbWbxOKvvg9oi7rwjDgQqhSoYIQNva/3SQBwzDaHE81ggeA/4rIq0CKvSwZuAFr3KC2Z9c3RGYvq50gvpogHq+aRYyUkCLDeGDMtMDGZxiG4QOebihbJSITgJ8DN9qLtwATVPWgH2Lzu9wlL9IFwO4m+qFrKiMuvYfC0goeSIplXN/oQIdoGIbR4jzeUKaqOSLyf1hjDSmwqy13Gy3IPUCcWvMJVBJE0aCruX1C65oIxzAMo6V5uo8gCHgSa76ADKzrCfEi8grwqKpWuvvsKamqgt5l21lYPYb1DLSagiZfEOioDMMwfM5TjeAvQASQpKqHAUQkEmvAuaexBqRrO9a8TJjrCPsiRxN22p2mKcgwjHbDUyL4ATBQVWvnJVDVYhG5HdhOW0oEmavQrx4FhR+XvU1Q/x9B7/6BjsowDMMvPHUf1bpJoM5Cax6WtmTvUtBqa2YxV5X12jAMo53wlAi2isgNDReKyI+xagRtxvaQ4aDgUihXJ9tDRwU6JMMwDL/x1DR0BzBXRG7Guo9AgdOAjsAVfojNb9YfdDFY4NOq03nNdQHTShIYHOigDMMw/MTTfQT7gAkicjYwDGvIiS9U9Wt/Becv40IyAHiu+iqygnrzcFJsgCMyDMPwnyYnplHVRcCimtf2nMV3qOoTPozLryKLtnNUQxg5ahxPnZFkegsZhtGuuL1GICK9ReQFEflURG4RkTAReQZIA7r6L0Tf0+yNbNc+PHjhMJMEDMNodzxdLH4d2A/8HatpaAXQExihqm2n66gq0Ye2EOSAbkUbAh2NYRiG33lqGopR1cft51+JSA5wmqqW+z4sP9o2jw56lGHsQl6/DGbNM5POG4bRrnicV0BEokUkRkRigANAWJ3XbULV9i8AcKBodYW5h8AwjHbHU40gCqvbaN0Jatba/yqQ5Kug/CmtpANDgCp1UKlO0kNHma6jhmG0K566jyb4MY6AySgoo586ebbqKlbpUHMPgWEY7U6T3UfburiqA2RpF/7jupzgIIe5h8AwjHan3SeCzuXZFAR3575pgzjdjDhqGEY71O4TQXRlNgejz+KOaWa0UcMw2iePvYbqEpEhdZ6f7ptw/KugsIAYinFG9w10KIZhGAHjdSIAnhaRZSLyINbNZk0SkQtEZIeI7BSRhxp5P0pEPhGRDSKyRURuakY8Jy1zzw4Awrv38+duDcMwWhVPQ0wk2DOSAaCqFwP/Bf4APNzUhkXECfwTuBAYClwnIkMbrHYHsFVVRwFTgWdEJKS5f8SJ2rZtMwAlob38tUvDMIxWx1ON4EPq3EMgIncB1wCjsQ7gTRkP7FTV3apaAbwLXNZgHQUiRESAcKAAqPI6+pOQkl7IdjsR3LeggJT0Qn/s1jAMo9XxlAiCVbUIQESexDqzP1dVt2HdbNaUXkBmnddZ9rK6/gEMwRrTaBNwt6q6vIz9pKzYnc9oUqlUB92rD7Bid74/dmsYhtHqeOo1tEtEXgHigbHAMFUtrXvRuAnSyLKGU1yeD6wHzgb6AQtEZKmqFtfbkMitwK0Affr08XL3nk3puJuhzhU4cPFG8JOkh48ATM8hwzDaH081gmuAr4EXsQ7YC0Vkkb3suAu/jcgCetd5HY915l/XTcBctewE9sDxN/aq6guqmqyqyV26dPFi102LyV2FAxciEOqoZnCZGXnUMIz2ydMQExXAmzWvRSQZGAGkqeohL7a9GhggIonAPuBa4EcN1skApgNLRaQbMAjY3Zw/4ESt1iFcBiiCOEMgYbI/dmsYhtHqeH1DmaqWYR3cvV2/SkTuBL4CnMAcVd0iIrfZ78/G6oH0qohswmpK+pWq5jXnDzhRK/NCuVyAwRfBmfeYoaeNdq+yspKsrCzKysoCHYpxEkJDQ4mPjyc4ONjrz/j0zmJV/Rz4vMGy2XWe7wfO82UMjUnZW0Du3s1Ww9jpPzdJwDCArKwsIiIiSEhIwOrIZ5xqVJX8/HyysrJITEz0+nPNuaGsTUhJL+RHL62kV/U+ADYcbZlrDoZxqisrKyM2NtYkgVOYiBAbG9vsWp2nG8pCReQeEfmHiPxMRNrEuEQrdudTUeUiSfZzWDuyLLvd5ULDcMskgVPfiXyHno6CrwHJWP37LwSeObGwWpfTk2JxOoQkyWYPPTm9X1ygQzIMwwgoT4lgqKr+WFX/A1wNtIluNeP6RnPB8O70c2TTq98IM+y0YbQC+fn5jB49mtGjR9O9e3d69epV+7qiosLjZ9esWcNdd93V7H2uW7cOEeGrr76qt9zpdDJ69GiGDx/OjBkzKC0tPe6zc+bMYcSIEYwcOZLhw4fz8ccfN3v/rYmn5p7Kmid2DyA/hOMf/UpS6Cn50LlzoEMxjFNaSnohK3bnn/RcHrGxsaxfvx6Axx9/nPDwcO6///7a96uqqggKavxwlZycTHJycrP3+c477zBp0iTeeecdzj///NrlHTt2rI1l5syZzJ49m/vuu6/2/aysLJ544gnWrl1LVFQUJSUl5ObmNnv/dVVXV+N0Ok9qGyfDUyIYJSI1d/gK0NF+LYCqaqT7j7Zimau4Y789Zt6Gt2HMTNNryDAa+N0nW9i6v9jjOofLKtl+4DAuBYfA4O4RRIS677I4tGckv71kmNcx3HjjjcTExLBu3TrGjh3LNddcwz333MPRo0fp2LEjr7zyCoMGDWLx4sU8/fTTfPrppzz++ONkZGSwe/duMjIyuOeeexqtLagqH3zwAQsWLGDy5MmUlZURGhp63HqTJ09m48aN9ZYdPHiQiIgIwsPDAQgPD699vnPnTm677TZyc3NxOp28//77JCUl8eCDD/LFF18gIvz617/mmmuuYfHixfzud7+jR48erF+/nk2bNvHQQw+xePFiysvLueOOO/jZz37mdXmdDE83lAUuPfnS3qU4a8a1c1XB3qUmERjGCSguq8JlDxrjUuu1p0RwIlJTU1m4cCFOp5Pi4mKWLFlCUFAQCxcu5JFHHuHDDz887jPbt2/nm2++4fDhwwwaNIjbb7/9uD713333HYmJifTr14+pU6fy+eefc+WVV9Zbp6qqii+++IILLrig3vJRo0bRrVs3EhMTmT59OldeeSWXXHIJYNUgHnroIa644grKyspwuVzMnTuX9evXs2HDBvLy8jjttNOYMmUKAKtWrWLz5s0kJibywgsvEBUVxerVqykvL+fMM8/kvPPOa1Y30BPVrJ5AItIJuBz4kT0s9SmnPH4iog6cUg3mjmLDaJQ3Z+4p6YXMfGkFlVUugoMcPHftmBa/5jZjxozaJpOioiJmzZpFWloaIkJlZWWjn7n44ovp0KEDHTp0oGvXruTk5BAfH19vnXfeeYdrr70WgGuvvZY33nijNhEcPXqU0aNHA1aN4Cc/+Um9zzqdTr788ktWr17N119/zb333ktKSgq//OUv2bdvH1dccQVAbQ1j2bJlXHfddTidTrp168ZZZ53F6tWriYyMZPz48bUH+vnz57Nx40Y++OCD2r83LS2tdSQCe36Ai7CGh7gAa3jq2R4/1IplR4xkWfVZ/DhoEcz8wNQGDOMEjesbzVu3nN4i1wjc6dSpU+3zxx57jGnTpvHRRx+xd+9epk6d2uhnOnToUPvc6XRSVVV/ZPvq6mo+/PBD5s2bxxNPPFF7E9bhw4eJiIiod43AHRFh/PjxjB8/nnPPPZebbrqp3nWEulQbjrXZ+N+nqvz973+vd73CXzzdR3CuiMzBGgjuauANoEBVb1LVT/wVYEvbd+goFYRQHdQJEk1twDBOxri+0dwxrb9fet8VFRXRq5c1kv2rr756wttZuHAho0aNIjMzk71795Kens5VV13F//73P68+v3//ftauXVv7ev369fTt25fIyEji4+Nrt1NeXk5paSlTpkzhvffeo7q6mtzcXJYsWcL48cefgJ5//vn8+9//rq3ppKamcuTIkRP+O5vDU/fRr7CGhp5kdyP9BPDLXAG+tO/QUWKkGFeYuX/AME4lDz74IA8//DBnnnkm1dXVJ7ydd955p7b5psZVV13F22+/7dXnKysruf/++xk8eDCjR4/mvffe47nnngPgjTfe4Pnnn2fkyJFMnDiRAwcOcMUVVzBy5EhGjRrF2WefzZ///Ge6d+9+3HZvueUWhg4dytixYxk+fDg/+9nPjqvN+Iq4q7aIyBisEUOvxhoR9F3gN6oa0Jnek5OTdc2aNSf8+WcXpjJuyc2c2TsEx08XtWBkhnFq27ZtG0OGeDvdiNGaNfZdikiKqjbaz9ZtjUBV16nqr1S1H/A4MAYIEZEv7IliTkkbsw7RRYopdnQOdCiGYRitglcD7ajqd6p6J9ZUk88CZ/gyKF9JSS9k8Y5cOlPMwvRqM0+xYRgGzRx9VFVdqvqVqt7kq4B8acXuPFyqxFBMnivCzFNsGIZBOxuGulfnMCIpJUSqOeTozOlJsYEOyTAMI+DaxNDS3jpUWkGMPWrGNWeNIdEMOGcYhuFdIhARJ9Ct7vqqmuGroHxl+e58hkSUQwUk9glo5yfDMIxWo8mmIRH5BZADLAA+sx+f+jiuFrdmbwGLd+QyNMoe0raTuY/AMFoLfw9DnZCQUDuM9FlnnUV6enrte1lZWVx22WUMGDCAfv36cffdd9eLYdWqVUyZMoVBgwYxePBgbrnlluOGqi4tLWXmzJmMGDGC4cOHM2nSJEpKSpoVoz95c43gbmCQqg5T1RH2Y6SvA2tJNdNTlle5OJCdZS00icAwTl7mKlj6jPXvSagZhnr9+vXcdttt3HvvvbWvQ0JCPN5YlZyczPPPP9/sfX7zzTds3LiRqVOn8sc//hGwhnm48sorufzyy0lLSyM1NZWSkhIeffRRAHJycpgxYwZPPfUUO3bsYNu2bVxwwQUcPny43rafe+45unXrxqZNm9i8eTMvv/xysyaTb4wvby7zpmkoEyjyWQR+sGJ3PpVV1k3RMdhD65o7iw3DvS8eggObPK9TXgw5m0FdIA7oNhw6eBidvvsIuPBPXofgy2Go6zrjjDNqE8miRYsIDQ3lppusjpFOp5O//e1vJCYm8rvf/Y5//vOfzJo1izPOsHrQiwhXX331cdvMzs6mb99jzc+DBg2qff7666/z9NNPIyKMHDmSN954g/T0dG6++WZyc3Pp0qULr7zyCn369DmuDH7+859zxx13kJubS1hYGC+++CKDBw/2ukzd8SYR7AYWi8hnQHnNQlX960nv3U9OT4qlQ7CDyioXcY7DVAeH4ww+fuxxwzCaoazISgJg/VtW5DkRnABfDUNd15dffsnll18OwJYtWxg3bly99yMjI+nTpw87d+5k8+bNzJo1q8m4b775Zs477zw++OADpk+fzqxZsxgwYABbtmzhiSee4LvvviMuLo6CggIA7rzzTm644QZmzZrFnDlzuOuuu2rHLKpbBtOnT2f27NkMGDCAlStX8vOf/5xFi05+hARvEkGG/QixH6ecuqMkXp5WgrMgyKrKmpFHDaNx3py5Z66C1y6F6gprSPerXmrx/1O+GoYaYNq0aeTk5NC1a9d6TUONzcbobrk7o0ePZvfu3cyfP5+FCxdy2mmnsXz5chYtWsTVV19NXJzVIhETEwPA8uXLmTt3LgDXX389Dz744HFlUFJSwvfff8+MGTNq3ysvL6clNJkIVPV3LbKnABvXN5pxjjRY8p119vLapTBrnkkGhnGieo+3/g/tXWrN6+GD/0u+GIa6xjfffEOnTp248cYb+c1vfsNf//pXhg0bdlwto7i4mMzMTPr168ewYcNISUnhsssuazL28PBwrrzySq688kocDgeff/45wcHBXiWUuuvUlIHL5aJz585NDpF9IjwNQ/2s/e8nIjKv4aPFI/GHvUuPVWWrK6zXhmGcuN7jYfIv/XJC1VLDUNfVsWNHnn32WV5//XUKCgqYPn06paWlvP7664A1d8Evf/lLbrzxRsLCwrjzzjt57bXXWLlyZe023nzzTQ4cOFBvu9999x2FhdYQNhUVFWzdupW+ffsyffp0/vvf/5Kfb41qUNM0NHHiRN59910A3nrrLSZNmnRcrJGRkSQmJvL+++8DVi1lw4YNLVIOnnoNvWH/+zTwTCOPU0+PUfYTMbOTGcYppqWGoW6oR48eXHfddfzzn/9ERPjoo494//33GTBgAAMHDiQ0NJQnn3wSgG7duvHuu+9y//33M2jQIIYMGcLSpUuJjKx/bWTXrl2cddZZjBgxgjFjxpCcnMxVV13FsGHDePTRRznrrLMYNWpU7WQ2zz//PK+88krtxeOaYa0beuutt3j55ZcZNWoUw4YN4+OPP26RMnA7DHVrdVLDUKcthLeugjHXw9gbTLOQYdRhhqFuO5o7DLU3U1UOAP4PGArUdrVR1aSTCzUAslZZ3dwu+BN0CA90NIZhGK2CNzeUvQL8G6gCpgGvc6zZ6NSSuQq6DjNJwDAMow5vEkFHVf0aqxkpXVUfB872bVg+kL4c0r+HaDPGkGG4c6o1FRvHO5Hv0JtEUCYiDiBNRO4UkSuArs3eUyBlroI3LoPqckj96qRvhzeMtig0NJT8/HyTDE5hqkp+fj6hoc27YdabG8ruAcKAu4A/YDUPNX1rHSAiFwDPAU7gJVX9U4P3HwBm1ollCNBFVQu82b7X9i6FKnvQKHVZr82FYsOoJz4+nqysLHJzcwMdinESQkNDG72BzhOPicAefvqHqvoAUAJ4PTOZ/dl/AucCWcBqEZmnqltr1lHVvwB/sde/BLi3xZMAWN1Eg0KP3QFpuo0axnGCg4NJTEwMdBhGALhNBCISpKpVIjJORESbX18cD+xU1d329t4FLgO2uln/OuCdZu7DO364A9IwDONU5alGsAoYC6wDPhaR94EjNW+q6twmtt0La+TSGlnAhMZWFJEw4ALgTjfv3wrcCtCnT58mdutG7/EmARiGYTTCm2sEMUA+Vk8hBcT+t6lE0NiAGu5qFZcA37lrFlLVF4AXwLqhzIuYDcMwDC95SgRdReQ+YDPHEkANbw7GWUDvOq/jgf1u1r0WL5uFUlJS8kQkvek1GxUH5J3gZ32ttcZm4mqe1hoXtN7YTFzNc6Jxue077ykROIFwmndmX9dqYICIJAL7sA72P2q4kohEAWcBP/Zim6hqF2/Wa4yIrHF3i3WgtdbYTFzN01rjgtYbm4mreXwRl6dEkK2qvz/RDdsXmu8EvsJKKnNUdYuI3Ga/P9te9QpgvqoecbMpwzAMw4c8JQLvZ2FwQ1U/Bz5vsGx2g9evAq+e7L4MwzCME+PpzuLpfovCf14IdAAetNbYTFzN01rjgtYbm4mreVo8rlNuGGrDMAyjZXkz1pBhGIbRhplEYBiG0c61m0QgIheIyA4R2SkiDwUwjt4i8o2IbBORLSJyt738cRHZJyLr7cdFAYhtr4hssve/xl4WIyILRCTN/jc6AHENqlMu60WkWETuCUSZicgcETkoIpvrLHNbRiLysP2b2yEi5/s5rr+IyHYR2SgiH4lIZ3t5gogcrVNus91u2Ddxuf3e/FVeHmJ7r05ce0Vkvb3cL2Xm4fjg29+Yqrb5B1b31V1AEhACbACGBiiWHsBY+3kEkIo1+9vjwP0BLqe9QFyDZX8GHrKfPwQ81Qq+ywNYN8f4vcyAKVhDr2xuqozs73UD0AFItH+DTj/GdR4QZD9/qk5cCXXXC0B5Nfq9+bO83MXW4P1ngN/4s8w8HB98+htrLzWC2gHwVLUCqBkAz+9UNVtV19rPDwPbsMZlaq0uA16zn78GXB64UACrN9suVT3Ru8tPiqouARoOheKujC4D3lXVclXdA+zE+i36JS5Vna+qVfbLFVh39/uVm/Jyx2/l1VRsIiLAD/HVQJjuY3J3fPDpb6y9JILGBsAL+MFXRBKAMcBKe9GddjV+TiCaYLDuGJ8vIin2QH8A3VQ1G6wfKYGflKjhcCSBLjNwX0at6Xd3M/BFndeJIrJORL4VkUCMy97Y99aaymsykKOqaXWW+bXMGhwffPobay+J4ESHyfAZEQkHPgTuUdVirHmh+wGjgWysaqm/namqY4ELgTtEZEoAYnBLREKAS4H37UWtocw8aRW/OxF5FGvO8bfsRdlAH1UdA9wHvC0ikX4Myd331irKy9ZwWHy/llkjxwe3qzayrNll1l4SQXMGwPM5EQnG+pLfUns4b1XNUdVqVXUBL+LDKrE7qrrf/vcg8JEdQ46I9LDj7gEc9HdcdVwIrFXVHGgdZWZzV0YB/92JyCzgB8BMtRuV7WaEfPt5Cla78kB/xeThewt4eYE1FwtwJfBezTJ/llljxwd8/BtrL4mgdgA8+6zyWmBeIAKx2x5fBrap6l/rLO9RZ7UrsEZ99WdcnUQkouY51oXGzVjlVDM16SzgY3/G1UC9s7RAl1kd7spoHnCtiHQQa/DFAVjzfPiFWFPF/gq4VFVL6yzvItYMgohIkh3Xbj/G5e57C2h51XEOsF1Vs2oW+KvM3B0f8PVvzNdXwVvLA7gI6wr8LuDRAMYxCavqthFYbz8uAt4ANtnL5wE9/BxXElbvgw3AlpoyAmKBr4E0+9+YAJVbGNa8GFF1lvm9zLASUTZQiXU29hNPZQQ8av/mdgAX+jmunVjtxzW/s9n2ulfZ3/EGYC1wiZ/jcvu9+au83MVmL38VuK3Bun4pMw/HB5/+xswQE4ZhGO1ce2kaMgzDMNwwicAwDKOdM4nAMAyjnTOJwDAMo50zicAwDKOdM4nAaNdEpFrqj2zaYiPT2iNWBureBsPwmqc5iw2jPTiqqqMDHYRhBJKpERhGI+yx6J8SkVX2o7+9vK+IfG0PmPa1iPSxl3cTa8z/DfZjor0pp4i8aI8tP19EOtrr9xORL+0B/paKyGB7+QwR2WxvY0lA/nij3TGJwGjvOjZoGrqmznvFqjoe+AfwrL3sH8DrqjoSaxC35+3lzwPfquoorDHut9jLBwD/VNVhwCGsO1TBmoD8F6o6Drgf+Je9/DfA+fZ2Lm3ZP9UwGmfuLDbaNREpUdXwRpbvBc5W1d32IGAHVDVWRPKwhkSotJdnq2qciOQC8apaXmcbCcACVR1gv/4VEIyVVHKxhgSo0UFVh4g181U/4L/AXLUHOjMMXzLXCAzDPXXz3N06jSmv87wa6IhVEz/U2LUJVb1NRCYAFwPrRWS0SQaGr5mmIcNw75o6/y63n3+PNXotwExgmf38a+B2ABFxehqrXq3x5feIyAx7fRGRUfbzfqq6UlV/A+RRf4hhw/AJkwiM9q7hNYI/1Xmvg4isBO4G7rWX3QXcJCIbgevt97D/nSYim4AUYFgT+50J/EREakZ7rZk69S8issnudroEa7RLw/Apc43AMBphXyNIVtW8QMdiGL5magSGYRjtnKkRGIZhtHOmRmAYhtHOmURgGIbRzplEYBiG0c6ZRGAYhtHOmURgGIbRzv0/X/mNEEmZJ0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 改进损失函数后的 GCMAE\n",
    "# 约束 En 与 He 10倍关系\n",
    "# KL 散度 Ex、En、He 预先设置为0、1、0\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f277c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "训练次数: 1 Epoch: 0001 log_lik= 1.8298507 train_kl= -0.00004 train_loss= 1.82989 train_acc= 0.49735 val_roc= 0.69625 val_ap= 0.71667 time= 2.14096\n",
      "训练次数: 1 Epoch: 0002 log_lik= 1.5372671 train_kl= -0.00017 train_loss= 1.53744 train_acc= 0.48986 val_roc= 0.70565 val_ap= 0.72950 time= 0.09659\n",
      "训练次数: 1 Epoch: 0003 log_lik= 1.3999476 train_kl= -0.00032 train_loss= 1.40026 train_acc= 0.47440 val_roc= 0.70283 val_ap= 0.72751 time= 0.07718\n",
      "训练次数: 1 Epoch: 0004 log_lik= 1.2297577 train_kl= -0.00057 train_loss= 1.23032 train_acc= 0.45063 val_roc= 0.70666 val_ap= 0.73009 time= 0.08265\n",
      "训练次数: 1 Epoch: 0005 log_lik= 1.0923176 train_kl= -0.00097 train_loss= 1.09329 train_acc= 0.41711 val_roc= 0.71205 val_ap= 0.73404 time= 0.07667\n",
      "训练次数: 1 Epoch: 0006 log_lik= 0.94515383 train_kl= -0.00153 train_loss= 0.94669 train_acc= 0.38683 val_roc= 0.71906 val_ap= 0.73911 time= 0.09061\n",
      "训练次数: 1 Epoch: 0007 log_lik= 0.84781563 train_kl= -0.00228 train_loss= 0.85010 train_acc= 0.36051 val_roc= 0.73238 val_ap= 0.74633 time= 0.07867\n",
      "训练次数: 1 Epoch: 0008 log_lik= 0.7719642 train_kl= -0.00320 train_loss= 0.77516 train_acc= 0.36204 val_roc= 0.74905 val_ap= 0.75727 time= 0.09161\n",
      "训练次数: 1 Epoch: 0009 log_lik= 0.72359824 train_kl= -0.00426 train_loss= 0.72786 train_acc= 0.33853 val_roc= 0.76466 val_ap= 0.76981 time= 0.08265\n",
      "训练次数: 1 Epoch: 0010 log_lik= 0.70167583 train_kl= -0.00541 train_loss= 0.70709 train_acc= 0.28837 val_roc= 0.76657 val_ap= 0.76576 time= 0.07867\n",
      "训练次数: 1 Epoch: 0011 log_lik= 0.69353914 train_kl= -0.00658 train_loss= 0.70012 train_acc= 0.21079 val_roc= 0.77743 val_ap= 0.77309 time= 0.08364\n",
      "训练次数: 1 Epoch: 0012 log_lik= 0.68061656 train_kl= -0.00768 train_loss= 0.68830 train_acc= 0.21140 val_roc= 0.79459 val_ap= 0.79195 time= 0.09261\n",
      "训练次数: 1 Epoch: 0013 log_lik= 0.663713 train_kl= -0.00869 train_loss= 0.67241 train_acc= 0.26455 val_roc= 0.79994 val_ap= 0.80012 time= 0.08365\n",
      "训练次数: 1 Epoch: 0014 log_lik= 0.64708644 train_kl= -0.00964 train_loss= 0.65673 train_acc= 0.30586 val_roc= 0.80045 val_ap= 0.79961 time= 0.07868\n",
      "训练次数: 1 Epoch: 0015 log_lik= 0.63322234 train_kl= -0.01053 train_loss= 0.64376 train_acc= 0.30737 val_roc= 0.79893 val_ap= 0.79904 time= 0.07676\n",
      "训练次数: 1 Epoch: 0016 log_lik= 0.6201766 train_kl= -0.01134 train_loss= 0.63151 train_acc= 0.33870 val_roc= 0.79453 val_ap= 0.79465 time= 0.08889\n",
      "训练次数: 1 Epoch: 0017 log_lik= 0.60610586 train_kl= -0.01201 train_loss= 0.61812 train_acc= 0.39484 val_roc= 0.79265 val_ap= 0.79164 time= 0.08803\n",
      "训练次数: 1 Epoch: 0018 log_lik= 0.59313095 train_kl= -0.01260 train_loss= 0.60573 train_acc= 0.45084 val_roc= 0.79406 val_ap= 0.79393 time= 0.08992\n",
      "训练次数: 1 Epoch: 0019 log_lik= 0.5871878 train_kl= -0.01312 train_loss= 0.60031 train_acc= 0.47085 val_roc= 0.79896 val_ap= 0.80001 time= 0.07512\n",
      "训练次数: 1 Epoch: 0020 log_lik= 0.5830787 train_kl= -0.01359 train_loss= 0.59666 train_acc= 0.47958 val_roc= 0.80601 val_ap= 0.80572 time= 0.09993\n",
      "训练次数: 1 Epoch: 0021 log_lik= 0.5793291 train_kl= -0.01398 train_loss= 0.59331 train_acc= 0.47975 val_roc= 0.81431 val_ap= 0.81378 time= 0.07693\n",
      "训练次数: 1 Epoch: 0022 log_lik= 0.57008 train_kl= -0.01430 train_loss= 0.58438 train_acc= 0.48023 val_roc= 0.82255 val_ap= 0.82288 time= 0.09099\n",
      "训练次数: 1 Epoch: 0023 log_lik= 0.5567324 train_kl= -0.01456 train_loss= 0.57129 train_acc= 0.48711 val_roc= 0.82903 val_ap= 0.83096 time= 0.07796\n",
      "训练次数: 1 Epoch: 0024 log_lik= 0.5430591 train_kl= -0.01477 train_loss= 0.55782 train_acc= 0.49709 val_roc= 0.83338 val_ap= 0.83435 time= 0.08603\n",
      "训练次数: 1 Epoch: 0025 log_lik= 0.5330138 train_kl= -0.01493 train_loss= 0.54795 train_acc= 0.50338 val_roc= 0.83509 val_ap= 0.83455 time= 0.10700\n",
      "训练次数: 1 Epoch: 0026 log_lik= 0.5288673 train_kl= -0.01507 train_loss= 0.54394 train_acc= 0.50301 val_roc= 0.83504 val_ap= 0.83434 time= 0.07696\n",
      "训练次数: 1 Epoch: 0027 log_lik= 0.5285716 train_kl= -0.01518 train_loss= 0.54375 train_acc= 0.50068 val_roc= 0.83468 val_ap= 0.83243 time= 0.08004\n",
      "训练次数: 1 Epoch: 0028 log_lik= 0.5290141 train_kl= -0.01524 train_loss= 0.54426 train_acc= 0.49836 val_roc= 0.83644 val_ap= 0.83404 time= 0.09136\n",
      "训练次数: 1 Epoch: 0029 log_lik= 0.5269731 train_kl= -0.01527 train_loss= 0.54225 train_acc= 0.49794 val_roc= 0.84163 val_ap= 0.84129 time= 0.07663\n",
      "训练次数: 1 Epoch: 0030 log_lik= 0.5234567 train_kl= -0.01528 train_loss= 0.53873 train_acc= 0.49498 val_roc= 0.84821 val_ap= 0.85083 time= 0.07506\n",
      "训练次数: 1 Epoch: 0031 log_lik= 0.5190527 train_kl= -0.01526 train_loss= 0.53431 train_acc= 0.49342 val_roc= 0.85356 val_ap= 0.85799 time= 0.07898\n",
      "训练次数: 1 Epoch: 0032 log_lik= 0.5146382 train_kl= -0.01522 train_loss= 0.52986 train_acc= 0.49608 val_roc= 0.85826 val_ap= 0.86417 time= 0.08194\n",
      "训练次数: 1 Epoch: 0033 log_lik= 0.5079743 train_kl= -0.01519 train_loss= 0.52316 train_acc= 0.50037 val_roc= 0.86341 val_ap= 0.87055 time= 0.09103\n",
      "训练次数: 1 Epoch: 0034 log_lik= 0.5030283 train_kl= -0.01516 train_loss= 0.51819 train_acc= 0.50380 val_roc= 0.86732 val_ap= 0.87494 time= 0.07598\n",
      "训练次数: 1 Epoch: 0035 log_lik= 0.49865314 train_kl= -0.01513 train_loss= 0.51378 train_acc= 0.50761 val_roc= 0.87014 val_ap= 0.87808 time= 0.08834\n",
      "训练次数: 1 Epoch: 0036 log_lik= 0.49589875 train_kl= -0.01509 train_loss= 0.51099 train_acc= 0.50857 val_roc= 0.87025 val_ap= 0.87857 time= 0.07869\n",
      "训练次数: 1 Epoch: 0037 log_lik= 0.49467164 train_kl= -0.01503 train_loss= 0.50970 train_acc= 0.51149 val_roc= 0.87130 val_ap= 0.87980 time= 0.07593\n",
      "训练次数: 1 Epoch: 0038 log_lik= 0.49369442 train_kl= -0.01496 train_loss= 0.50865 train_acc= 0.51048 val_roc= 0.87289 val_ap= 0.88151 time= 0.07735\n",
      "训练次数: 1 Epoch: 0039 log_lik= 0.49111584 train_kl= -0.01487 train_loss= 0.50599 train_acc= 0.51293 val_roc= 0.87513 val_ap= 0.88343 time= 0.07568\n",
      "训练次数: 1 Epoch: 0040 log_lik= 0.4876917 train_kl= -0.01477 train_loss= 0.50246 train_acc= 0.51747 val_roc= 0.87717 val_ap= 0.88532 time= 0.08708\n",
      "训练次数: 1 Epoch: 0041 log_lik= 0.48460075 train_kl= -0.01465 train_loss= 0.49926 train_acc= 0.51935 val_roc= 0.87901 val_ap= 0.88702 time= 0.08045\n",
      "训练次数: 1 Epoch: 0042 log_lik= 0.48399714 train_kl= -0.01453 train_loss= 0.49852 train_acc= 0.51898 val_roc= 0.87964 val_ap= 0.88712 time= 0.07997\n",
      "训练次数: 1 Epoch: 0043 log_lik= 0.48253626 train_kl= -0.01440 train_loss= 0.49693 train_acc= 0.51789 val_roc= 0.88171 val_ap= 0.88983 time= 0.07901\n",
      "训练次数: 1 Epoch: 0044 log_lik= 0.47991902 train_kl= -0.01427 train_loss= 0.49419 train_acc= 0.51879 val_roc= 0.88251 val_ap= 0.89122 time= 0.07599\n",
      "训练次数: 1 Epoch: 0045 log_lik= 0.4771857 train_kl= -0.01415 train_loss= 0.49134 train_acc= 0.52183 val_roc= 0.88358 val_ap= 0.89285 time= 0.07700\n",
      "训练次数: 1 Epoch: 0046 log_lik= 0.47418413 train_kl= -0.01404 train_loss= 0.48823 train_acc= 0.52229 val_roc= 0.88425 val_ap= 0.89411 time= 0.07799\n",
      "训练次数: 1 Epoch: 0047 log_lik= 0.47236767 train_kl= -0.01394 train_loss= 0.48631 train_acc= 0.52326 val_roc= 0.88532 val_ap= 0.89455 time= 0.07602\n",
      "训练次数: 1 Epoch: 0048 log_lik= 0.47051618 train_kl= -0.01384 train_loss= 0.48436 train_acc= 0.52444 val_roc= 0.88557 val_ap= 0.89535 time= 0.08006\n",
      "训练次数: 1 Epoch: 0049 log_lik= 0.4688542 train_kl= -0.01373 train_loss= 0.48258 train_acc= 0.52447 val_roc= 0.88609 val_ap= 0.89651 time= 0.07691\n",
      "训练次数: 1 Epoch: 0050 log_lik= 0.46829206 train_kl= -0.01361 train_loss= 0.48190 train_acc= 0.52441 val_roc= 0.88752 val_ap= 0.89786 time= 0.07805\n",
      "训练次数: 1 Epoch: 0051 log_lik= 0.467163 train_kl= -0.01350 train_loss= 0.48066 train_acc= 0.52390 val_roc= 0.88823 val_ap= 0.89868 time= 0.08403\n",
      "训练次数: 1 Epoch: 0052 log_lik= 0.46591693 train_kl= -0.01338 train_loss= 0.47930 train_acc= 0.52458 val_roc= 0.88826 val_ap= 0.89856 time= 0.07796\n",
      "训练次数: 1 Epoch: 0053 log_lik= 0.46538374 train_kl= -0.01328 train_loss= 0.47866 train_acc= 0.52499 val_roc= 0.88801 val_ap= 0.89851 time= 0.08900\n",
      "训练次数: 1 Epoch: 0054 log_lik= 0.46473947 train_kl= -0.01317 train_loss= 0.47791 train_acc= 0.52397 val_roc= 0.88913 val_ap= 0.89941 time= 0.07403\n",
      "训练次数: 1 Epoch: 0055 log_lik= 0.46380863 train_kl= -0.01307 train_loss= 0.47688 train_acc= 0.52406 val_roc= 0.89008 val_ap= 0.90056 time= 0.07796\n",
      "训练次数: 1 Epoch: 0056 log_lik= 0.46282446 train_kl= -0.01298 train_loss= 0.47580 train_acc= 0.52322 val_roc= 0.89077 val_ap= 0.90095 time= 0.07505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 1 Epoch: 0057 log_lik= 0.46192908 train_kl= -0.01289 train_loss= 0.47482 train_acc= 0.52447 val_roc= 0.89179 val_ap= 0.90178 time= 0.07704\n",
      "训练次数: 1 Epoch: 0058 log_lik= 0.460523 train_kl= -0.01280 train_loss= 0.47332 train_acc= 0.52355 val_roc= 0.89291 val_ap= 0.90277 time= 0.07648\n",
      "训练次数: 1 Epoch: 0059 log_lik= 0.46005306 train_kl= -0.01271 train_loss= 0.47276 train_acc= 0.52350 val_roc= 0.89388 val_ap= 0.90390 time= 0.08889\n",
      "训练次数: 1 Epoch: 0060 log_lik= 0.45869896 train_kl= -0.01261 train_loss= 0.47131 train_acc= 0.52421 val_roc= 0.89459 val_ap= 0.90437 time= 0.09013\n",
      "训练次数: 1 Epoch: 0061 log_lik= 0.45778263 train_kl= -0.01253 train_loss= 0.47031 train_acc= 0.52462 val_roc= 0.89566 val_ap= 0.90527 time= 0.07690\n",
      "训练次数: 1 Epoch: 0062 log_lik= 0.45682323 train_kl= -0.01244 train_loss= 0.46927 train_acc= 0.52450 val_roc= 0.89748 val_ap= 0.90668 time= 0.07604\n",
      "训练次数: 1 Epoch: 0063 log_lik= 0.45621604 train_kl= -0.01236 train_loss= 0.46858 train_acc= 0.52353 val_roc= 0.89808 val_ap= 0.90651 time= 0.08295\n",
      "训练次数: 1 Epoch: 0064 log_lik= 0.45565808 train_kl= -0.01228 train_loss= 0.46794 train_acc= 0.52405 val_roc= 0.89756 val_ap= 0.90573 time= 0.08000\n",
      "训练次数: 1 Epoch: 0065 log_lik= 0.45448256 train_kl= -0.01220 train_loss= 0.46669 train_acc= 0.52557 val_roc= 0.89818 val_ap= 0.90614 time= 0.08205\n",
      "训练次数: 1 Epoch: 0066 log_lik= 0.45332816 train_kl= -0.01214 train_loss= 0.46547 train_acc= 0.52573 val_roc= 0.89997 val_ap= 0.90764 time= 0.08402\n",
      "训练次数: 1 Epoch: 0067 log_lik= 0.45262763 train_kl= -0.01208 train_loss= 0.46471 train_acc= 0.52522 val_roc= 0.90101 val_ap= 0.90827 time= 0.07596\n",
      "训练次数: 1 Epoch: 0068 log_lik= 0.45243907 train_kl= -0.01202 train_loss= 0.46446 train_acc= 0.52582 val_roc= 0.90019 val_ap= 0.90661 time= 0.07699\n",
      "训练次数: 1 Epoch: 0069 log_lik= 0.45125023 train_kl= -0.01196 train_loss= 0.46321 train_acc= 0.52610 val_roc= 0.90052 val_ap= 0.90676 time= 0.07892\n",
      "训练次数: 1 Epoch: 0070 log_lik= 0.4512037 train_kl= -0.01191 train_loss= 0.46311 train_acc= 0.52645 val_roc= 0.90179 val_ap= 0.90792 time= 0.07491\n",
      "训练次数: 1 Epoch: 0071 log_lik= 0.44975582 train_kl= -0.01187 train_loss= 0.46162 train_acc= 0.52710 val_roc= 0.90314 val_ap= 0.90931 time= 0.07495\n",
      "训练次数: 1 Epoch: 0072 log_lik= 0.44921494 train_kl= -0.01183 train_loss= 0.46104 train_acc= 0.52695 val_roc= 0.90358 val_ap= 0.90873 time= 0.08010\n",
      "训练次数: 1 Epoch: 0073 log_lik= 0.44831613 train_kl= -0.01178 train_loss= 0.46010 train_acc= 0.52796 val_roc= 0.90397 val_ap= 0.90933 time= 0.07503\n",
      "训练次数: 1 Epoch: 0074 log_lik= 0.4486918 train_kl= -0.01174 train_loss= 0.46043 train_acc= 0.52784 val_roc= 0.90520 val_ap= 0.91069 time= 0.08893\n",
      "训练次数: 1 Epoch: 0075 log_lik= 0.44795513 train_kl= -0.01172 train_loss= 0.45967 train_acc= 0.52885 val_roc= 0.90630 val_ap= 0.91181 time= 0.07913\n",
      "训练次数: 1 Epoch: 0076 log_lik= 0.44692445 train_kl= -0.01170 train_loss= 0.45863 train_acc= 0.52896 val_roc= 0.90707 val_ap= 0.91210 time= 0.07795\n",
      "训练次数: 1 Epoch: 0077 log_lik= 0.4463849 train_kl= -0.01169 train_loss= 0.45807 train_acc= 0.52948 val_roc= 0.90753 val_ap= 0.91271 time= 0.07599\n",
      "训练次数: 1 Epoch: 0078 log_lik= 0.44586426 train_kl= -0.01166 train_loss= 0.45753 train_acc= 0.52941 val_roc= 0.90820 val_ap= 0.91376 time= 0.07805\n",
      "训练次数: 1 Epoch: 0079 log_lik= 0.4451079 train_kl= -0.01164 train_loss= 0.45675 train_acc= 0.52961 val_roc= 0.90896 val_ap= 0.91476 time= 0.08694\n",
      "训练次数: 1 Epoch: 0080 log_lik= 0.44471133 train_kl= -0.01163 train_loss= 0.45634 train_acc= 0.52967 val_roc= 0.91003 val_ap= 0.91532 time= 0.07516\n",
      "训练次数: 1 Epoch: 0081 log_lik= 0.44395918 train_kl= -0.01162 train_loss= 0.45558 train_acc= 0.52957 val_roc= 0.91084 val_ap= 0.91583 time= 0.07498\n",
      "训练次数: 1 Epoch: 0082 log_lik= 0.4435662 train_kl= -0.01161 train_loss= 0.45518 train_acc= 0.52986 val_roc= 0.91207 val_ap= 0.91735 time= 0.08004\n",
      "训练次数: 1 Epoch: 0083 log_lik= 0.4432525 train_kl= -0.01161 train_loss= 0.45487 train_acc= 0.53074 val_roc= 0.91271 val_ap= 0.91859 time= 0.07797\n",
      "训练次数: 1 Epoch: 0084 log_lik= 0.44255078 train_kl= -0.01161 train_loss= 0.45416 train_acc= 0.52969 val_roc= 0.91330 val_ap= 0.91927 time= 0.07699\n",
      "训练次数: 1 Epoch: 0085 log_lik= 0.44186592 train_kl= -0.01161 train_loss= 0.45348 train_acc= 0.53082 val_roc= 0.91420 val_ap= 0.91956 time= 0.07898\n",
      "训练次数: 1 Epoch: 0086 log_lik= 0.44159564 train_kl= -0.01162 train_loss= 0.45321 train_acc= 0.53134 val_roc= 0.91499 val_ap= 0.92078 time= 0.09198\n",
      "训练次数: 1 Epoch: 0087 log_lik= 0.44114915 train_kl= -0.01163 train_loss= 0.45278 train_acc= 0.53080 val_roc= 0.91524 val_ap= 0.92140 time= 0.07701\n",
      "训练次数: 1 Epoch: 0088 log_lik= 0.4400793 train_kl= -0.01165 train_loss= 0.45173 train_acc= 0.53255 val_roc= 0.91613 val_ap= 0.92249 time= 0.07608\n",
      "训练次数: 1 Epoch: 0089 log_lik= 0.44027662 train_kl= -0.01166 train_loss= 0.45194 train_acc= 0.53137 val_roc= 0.91671 val_ap= 0.92262 time= 0.07397\n",
      "训练次数: 1 Epoch: 0090 log_lik= 0.44005328 train_kl= -0.01168 train_loss= 0.45173 train_acc= 0.53182 val_roc= 0.91667 val_ap= 0.92293 time= 0.07998\n",
      "训练次数: 1 Epoch: 0091 log_lik= 0.43900073 train_kl= -0.01169 train_loss= 0.45069 train_acc= 0.53198 val_roc= 0.91694 val_ap= 0.92422 time= 0.07804\n",
      "训练次数: 1 Epoch: 0092 log_lik= 0.43851364 train_kl= -0.01171 train_loss= 0.45023 train_acc= 0.53169 val_roc= 0.91728 val_ap= 0.92478 time= 0.07621\n",
      "训练次数: 1 Epoch: 0093 log_lik= 0.43839866 train_kl= -0.01173 train_loss= 0.45013 train_acc= 0.53250 val_roc= 0.91804 val_ap= 0.92538 time= 0.07569\n",
      "训练次数: 1 Epoch: 0094 log_lik= 0.4376546 train_kl= -0.01175 train_loss= 0.44940 train_acc= 0.53273 val_roc= 0.91797 val_ap= 0.92571 time= 0.07508\n",
      "训练次数: 1 Epoch: 0095 log_lik= 0.43730965 train_kl= -0.01176 train_loss= 0.44907 train_acc= 0.53330 val_roc= 0.91758 val_ap= 0.92615 time= 0.07593\n",
      "训练次数: 1 Epoch: 0096 log_lik= 0.43711394 train_kl= -0.01178 train_loss= 0.44889 train_acc= 0.53197 val_roc= 0.91814 val_ap= 0.92643 time= 0.07604\n",
      "训练次数: 1 Epoch: 0097 log_lik= 0.43688995 train_kl= -0.01180 train_loss= 0.44869 train_acc= 0.53357 val_roc= 0.91915 val_ap= 0.92671 time= 0.07797\n",
      "训练次数: 1 Epoch: 0098 log_lik= 0.43709907 train_kl= -0.01181 train_loss= 0.44891 train_acc= 0.53441 val_roc= 0.91853 val_ap= 0.92736 time= 0.09100\n",
      "训练次数: 1 Epoch: 0099 log_lik= 0.4360725 train_kl= -0.01182 train_loss= 0.44790 train_acc= 0.53378 val_roc= 0.91755 val_ap= 0.92709 time= 0.07703\n",
      "训练次数: 1 Epoch: 0100 log_lik= 0.4363583 train_kl= -0.01184 train_loss= 0.44819 train_acc= 0.53397 val_roc= 0.91830 val_ap= 0.92681 time= 0.07495\n",
      "Optimization Finished!\n",
      "训练次数: 1 ROC score: 0.9244191279988765\n",
      "训练次数: 1 AP score: 0.9337065989772533\n",
      "训练次数: 2 Epoch: 0001 log_lik= 1.7548112 train_kl= -0.00003 train_loss= 1.75485 train_acc= 0.49654 val_roc= 0.68879 val_ap= 0.72524 time= 1.36675\n",
      "训练次数: 2 Epoch: 0002 log_lik= 1.374039 train_kl= -0.00020 train_loss= 1.37424 train_acc= 0.46869 val_roc= 0.69913 val_ap= 0.73324 time= 0.09212\n",
      "训练次数: 2 Epoch: 0003 log_lik= 1.1063627 train_kl= -0.00067 train_loss= 1.10703 train_acc= 0.41357 val_roc= 0.71614 val_ap= 0.74506 time= 0.08996\n",
      "训练次数: 2 Epoch: 0004 log_lik= 0.9319814 train_kl= -0.00136 train_loss= 0.93334 train_acc= 0.35986 val_roc= 0.74797 val_ap= 0.77094 time= 0.09062\n",
      "训练次数: 2 Epoch: 0005 log_lik= 0.7953591 train_kl= -0.00226 train_loss= 0.79762 train_acc= 0.33974 val_roc= 0.76229 val_ap= 0.78654 time= 0.07767\n",
      "训练次数: 2 Epoch: 0006 log_lik= 0.7409352 train_kl= -0.00339 train_loss= 0.74433 train_acc= 0.30543 val_roc= 0.76693 val_ap= 0.78821 time= 0.08962\n",
      "训练次数: 2 Epoch: 0007 log_lik= 0.7160153 train_kl= -0.00466 train_loss= 0.72068 train_acc= 0.26211 val_roc= 0.79273 val_ap= 0.80464 time= 0.08564\n",
      "训练次数: 2 Epoch: 0008 log_lik= 0.70187896 train_kl= -0.00598 train_loss= 0.70786 train_acc= 0.24642 val_roc= 0.81220 val_ap= 0.82151 time= 0.09360\n",
      "训练次数: 2 Epoch: 0009 log_lik= 0.6904877 train_kl= -0.00728 train_loss= 0.69777 train_acc= 0.20780 val_roc= 0.80740 val_ap= 0.81575 time= 0.07966\n",
      "训练次数: 2 Epoch: 0010 log_lik= 0.68052024 train_kl= -0.00854 train_loss= 0.68906 train_acc= 0.13318 val_roc= 0.83124 val_ap= 0.83377 time= 0.07568\n",
      "训练次数: 2 Epoch: 0011 log_lik= 0.6633442 train_kl= -0.00967 train_loss= 0.67301 train_acc= 0.15082 val_roc= 0.85501 val_ap= 0.85664 time= 0.07369\n",
      "训练次数: 2 Epoch: 0012 log_lik= 0.6384873 train_kl= -0.01069 train_loss= 0.64918 train_acc= 0.20943 val_roc= 0.86493 val_ap= 0.86622 time= 0.07977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0013 log_lik= 0.61900866 train_kl= -0.01164 train_loss= 0.63065 train_acc= 0.24623 val_roc= 0.86772 val_ap= 0.86797 time= 0.09299\n",
      "训练次数: 2 Epoch: 0014 log_lik= 0.59836835 train_kl= -0.01249 train_loss= 0.61086 train_acc= 0.30181 val_roc= 0.86844 val_ap= 0.86818 time= 0.08097\n",
      "训练次数: 2 Epoch: 0015 log_lik= 0.5723809 train_kl= -0.01322 train_loss= 0.58560 train_acc= 0.38671 val_roc= 0.87107 val_ap= 0.87066 time= 0.07840\n",
      "训练次数: 2 Epoch: 0016 log_lik= 0.55485404 train_kl= -0.01385 train_loss= 0.56870 train_acc= 0.42969 val_roc= 0.87525 val_ap= 0.87350 time= 0.07809\n",
      "训练次数: 2 Epoch: 0017 log_lik= 0.54907066 train_kl= -0.01441 train_loss= 0.56348 train_acc= 0.44598 val_roc= 0.87983 val_ap= 0.87718 time= 0.07959\n",
      "训练次数: 2 Epoch: 0018 log_lik= 0.54915124 train_kl= -0.01491 train_loss= 0.56406 train_acc= 0.45993 val_roc= 0.88544 val_ap= 0.88359 time= 0.07570\n",
      "训练次数: 2 Epoch: 0019 log_lik= 0.54474896 train_kl= -0.01534 train_loss= 0.56009 train_acc= 0.47425 val_roc= 0.89079 val_ap= 0.88945 time= 0.07780\n",
      "训练次数: 2 Epoch: 0020 log_lik= 0.5395697 train_kl= -0.01569 train_loss= 0.55526 train_acc= 0.48416 val_roc= 0.89461 val_ap= 0.89341 time= 0.08986\n",
      "训练次数: 2 Epoch: 0021 log_lik= 0.5306194 train_kl= -0.01594 train_loss= 0.54656 train_acc= 0.49261 val_roc= 0.89676 val_ap= 0.89630 time= 0.07707\n",
      "训练次数: 2 Epoch: 0022 log_lik= 0.5191479 train_kl= -0.01612 train_loss= 0.53526 train_acc= 0.50527 val_roc= 0.89887 val_ap= 0.89912 time= 0.07795\n",
      "训练次数: 2 Epoch: 0023 log_lik= 0.513152 train_kl= -0.01623 train_loss= 0.52938 train_acc= 0.50899 val_roc= 0.90222 val_ap= 0.90414 time= 0.07746\n",
      "训练次数: 2 Epoch: 0024 log_lik= 0.5096434 train_kl= -0.01630 train_loss= 0.52595 train_acc= 0.50653 val_roc= 0.90363 val_ap= 0.90612 time= 0.09705\n",
      "训练次数: 2 Epoch: 0025 log_lik= 0.5059568 train_kl= -0.01633 train_loss= 0.52229 train_acc= 0.50667 val_roc= 0.90482 val_ap= 0.90876 time= 0.07597\n",
      "训练次数: 2 Epoch: 0026 log_lik= 0.4998605 train_kl= -0.01633 train_loss= 0.51619 train_acc= 0.51309 val_roc= 0.90565 val_ap= 0.90943 time= 0.07605\n",
      "训练次数: 2 Epoch: 0027 log_lik= 0.49491143 train_kl= -0.01630 train_loss= 0.51121 train_acc= 0.51970 val_roc= 0.90658 val_ap= 0.91024 time= 0.08602\n",
      "训练次数: 2 Epoch: 0028 log_lik= 0.49368235 train_kl= -0.01626 train_loss= 0.50995 train_acc= 0.52113 val_roc= 0.90676 val_ap= 0.91023 time= 0.07892\n",
      "训练次数: 2 Epoch: 0029 log_lik= 0.4944445 train_kl= -0.01622 train_loss= 0.51067 train_acc= 0.51816 val_roc= 0.90630 val_ap= 0.90981 time= 0.08412\n",
      "训练次数: 2 Epoch: 0030 log_lik= 0.49417582 train_kl= -0.01618 train_loss= 0.51036 train_acc= 0.51491 val_roc= 0.90752 val_ap= 0.91125 time= 0.07689\n",
      "训练次数: 2 Epoch: 0031 log_lik= 0.49053565 train_kl= -0.01615 train_loss= 0.50668 train_acc= 0.51842 val_roc= 0.91000 val_ap= 0.91380 time= 0.07855\n",
      "训练次数: 2 Epoch: 0032 log_lik= 0.48593006 train_kl= -0.01611 train_loss= 0.50204 train_acc= 0.52361 val_roc= 0.91253 val_ap= 0.91703 time= 0.07849\n",
      "训练次数: 2 Epoch: 0033 log_lik= 0.4829607 train_kl= -0.01607 train_loss= 0.49903 train_acc= 0.52659 val_roc= 0.91473 val_ap= 0.92043 time= 0.08486\n",
      "训练次数: 2 Epoch: 0034 log_lik= 0.48031896 train_kl= -0.01602 train_loss= 0.49634 train_acc= 0.52615 val_roc= 0.91605 val_ap= 0.92254 time= 0.08799\n",
      "训练次数: 2 Epoch: 0035 log_lik= 0.47704965 train_kl= -0.01598 train_loss= 0.49303 train_acc= 0.52594 val_roc= 0.91823 val_ap= 0.92460 time= 0.07608\n",
      "训练次数: 2 Epoch: 0036 log_lik= 0.47339687 train_kl= -0.01592 train_loss= 0.48932 train_acc= 0.52906 val_roc= 0.91996 val_ap= 0.92565 time= 0.07500\n",
      "训练次数: 2 Epoch: 0037 log_lik= 0.47164696 train_kl= -0.01587 train_loss= 0.48752 train_acc= 0.52960 val_roc= 0.92079 val_ap= 0.92685 time= 0.08898\n",
      "训练次数: 2 Epoch: 0038 log_lik= 0.469671 train_kl= -0.01582 train_loss= 0.48549 train_acc= 0.53195 val_roc= 0.92128 val_ap= 0.92748 time= 0.07498\n",
      "训练次数: 2 Epoch: 0039 log_lik= 0.46745333 train_kl= -0.01576 train_loss= 0.48322 train_acc= 0.53416 val_roc= 0.92226 val_ap= 0.92743 time= 0.08003\n",
      "训练次数: 2 Epoch: 0040 log_lik= 0.4657597 train_kl= -0.01570 train_loss= 0.48146 train_acc= 0.53733 val_roc= 0.92310 val_ap= 0.92736 time= 0.07903\n",
      "训练次数: 2 Epoch: 0041 log_lik= 0.46471506 train_kl= -0.01562 train_loss= 0.48033 train_acc= 0.53837 val_roc= 0.92271 val_ap= 0.92673 time= 0.07600\n",
      "训练次数: 2 Epoch: 0042 log_lik= 0.46364063 train_kl= -0.01551 train_loss= 0.47915 train_acc= 0.53859 val_roc= 0.92252 val_ap= 0.92703 time= 0.07801\n",
      "训练次数: 2 Epoch: 0043 log_lik= 0.46330085 train_kl= -0.01540 train_loss= 0.47870 train_acc= 0.53904 val_roc= 0.92367 val_ap= 0.92795 time= 0.07812\n",
      "训练次数: 2 Epoch: 0044 log_lik= 0.4608934 train_kl= -0.01528 train_loss= 0.47617 train_acc= 0.54054 val_roc= 0.92374 val_ap= 0.92773 time= 0.07691\n",
      "训练次数: 2 Epoch: 0045 log_lik= 0.45911747 train_kl= -0.01514 train_loss= 0.47426 train_acc= 0.54256 val_roc= 0.92387 val_ap= 0.92784 time= 0.09248\n",
      "训练次数: 2 Epoch: 0046 log_lik= 0.45804977 train_kl= -0.01501 train_loss= 0.47306 train_acc= 0.54290 val_roc= 0.92400 val_ap= 0.92800 time= 0.08903\n",
      "训练次数: 2 Epoch: 0047 log_lik= 0.45734358 train_kl= -0.01487 train_loss= 0.47221 train_acc= 0.54141 val_roc= 0.92446 val_ap= 0.92895 time= 0.07791\n",
      "训练次数: 2 Epoch: 0048 log_lik= 0.45647454 train_kl= -0.01473 train_loss= 0.47120 train_acc= 0.54041 val_roc= 0.92427 val_ap= 0.92910 time= 0.08864\n",
      "训练次数: 2 Epoch: 0049 log_lik= 0.4555484 train_kl= -0.01459 train_loss= 0.47014 train_acc= 0.54067 val_roc= 0.92459 val_ap= 0.92921 time= 0.09062\n",
      "训练次数: 2 Epoch: 0050 log_lik= 0.4544853 train_kl= -0.01446 train_loss= 0.46894 train_acc= 0.54115 val_roc= 0.92487 val_ap= 0.92978 time= 0.08080\n",
      "训练次数: 2 Epoch: 0051 log_lik= 0.4535091 train_kl= -0.01434 train_loss= 0.46785 train_acc= 0.53962 val_roc= 0.92528 val_ap= 0.93063 time= 0.08100\n",
      "训练次数: 2 Epoch: 0052 log_lik= 0.4531529 train_kl= -0.01423 train_loss= 0.46738 train_acc= 0.53847 val_roc= 0.92543 val_ap= 0.93121 time= 0.07705\n",
      "训练次数: 2 Epoch: 0053 log_lik= 0.4522318 train_kl= -0.01413 train_loss= 0.46636 train_acc= 0.53794 val_roc= 0.92640 val_ap= 0.93221 time= 0.07886\n",
      "训练次数: 2 Epoch: 0054 log_lik= 0.4515024 train_kl= -0.01403 train_loss= 0.46553 train_acc= 0.53852 val_roc= 0.92693 val_ap= 0.93273 time= 0.08211\n",
      "训练次数: 2 Epoch: 0055 log_lik= 0.45064464 train_kl= -0.01393 train_loss= 0.46458 train_acc= 0.53763 val_roc= 0.92776 val_ap= 0.93444 time= 0.07500\n",
      "训练次数: 2 Epoch: 0056 log_lik= 0.44946823 train_kl= -0.01384 train_loss= 0.46331 train_acc= 0.53851 val_roc= 0.92849 val_ap= 0.93568 time= 0.07804\n",
      "训练次数: 2 Epoch: 0057 log_lik= 0.44868115 train_kl= -0.01375 train_loss= 0.46243 train_acc= 0.53908 val_roc= 0.92946 val_ap= 0.93673 time= 0.08003\n",
      "训练次数: 2 Epoch: 0058 log_lik= 0.44779253 train_kl= -0.01366 train_loss= 0.46145 train_acc= 0.53911 val_roc= 0.92993 val_ap= 0.93738 time= 0.08294\n",
      "训练次数: 2 Epoch: 0059 log_lik= 0.4471347 train_kl= -0.01358 train_loss= 0.46071 train_acc= 0.53898 val_roc= 0.93001 val_ap= 0.93784 time= 0.08102\n",
      "训练次数: 2 Epoch: 0060 log_lik= 0.44625178 train_kl= -0.01349 train_loss= 0.45975 train_acc= 0.53973 val_roc= 0.92991 val_ap= 0.93785 time= 0.08304\n",
      "训练次数: 2 Epoch: 0061 log_lik= 0.4455678 train_kl= -0.01341 train_loss= 0.45898 train_acc= 0.53987 val_roc= 0.93017 val_ap= 0.93790 time= 0.07800\n",
      "训练次数: 2 Epoch: 0062 log_lik= 0.44502163 train_kl= -0.01334 train_loss= 0.45836 train_acc= 0.53992 val_roc= 0.93001 val_ap= 0.93741 time= 0.07702\n",
      "训练次数: 2 Epoch: 0063 log_lik= 0.44421682 train_kl= -0.01324 train_loss= 0.45746 train_acc= 0.53969 val_roc= 0.92990 val_ap= 0.93747 time= 0.08594\n",
      "训练次数: 2 Epoch: 0064 log_lik= 0.44403112 train_kl= -0.01315 train_loss= 0.45718 train_acc= 0.53894 val_roc= 0.92933 val_ap= 0.93784 time= 0.07703\n",
      "训练次数: 2 Epoch: 0065 log_lik= 0.44354707 train_kl= -0.01305 train_loss= 0.45660 train_acc= 0.53911 val_roc= 0.92961 val_ap= 0.93827 time= 0.07600\n",
      "训练次数: 2 Epoch: 0066 log_lik= 0.44302848 train_kl= -0.01296 train_loss= 0.45599 train_acc= 0.53896 val_roc= 0.92981 val_ap= 0.93837 time= 0.07898\n",
      "训练次数: 2 Epoch: 0067 log_lik= 0.44236463 train_kl= -0.01288 train_loss= 0.45524 train_acc= 0.53930 val_roc= 0.93020 val_ap= 0.93877 time= 0.07700\n",
      "训练次数: 2 Epoch: 0068 log_lik= 0.44222158 train_kl= -0.01280 train_loss= 0.45502 train_acc= 0.53845 val_roc= 0.92978 val_ap= 0.93875 time= 0.08501\n",
      "训练次数: 2 Epoch: 0069 log_lik= 0.44143394 train_kl= -0.01272 train_loss= 0.45416 train_acc= 0.53779 val_roc= 0.92868 val_ap= 0.93836 time= 0.08198\n",
      "训练次数: 2 Epoch: 0070 log_lik= 0.44087094 train_kl= -0.01264 train_loss= 0.45352 train_acc= 0.53781 val_roc= 0.92893 val_ap= 0.93826 time= 0.07803\n",
      "训练次数: 2 Epoch: 0071 log_lik= 0.4404409 train_kl= -0.01258 train_loss= 0.45302 train_acc= 0.53860 val_roc= 0.93003 val_ap= 0.93921 time= 0.08005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0072 log_lik= 0.43995595 train_kl= -0.01254 train_loss= 0.45249 train_acc= 0.53865 val_roc= 0.92987 val_ap= 0.93948 time= 0.07799\n",
      "训练次数: 2 Epoch: 0073 log_lik= 0.43960148 train_kl= -0.01249 train_loss= 0.45209 train_acc= 0.53793 val_roc= 0.92930 val_ap= 0.93917 time= 0.07494\n",
      "训练次数: 2 Epoch: 0074 log_lik= 0.43935612 train_kl= -0.01243 train_loss= 0.45179 train_acc= 0.53819 val_roc= 0.92923 val_ap= 0.93923 time= 0.07712\n",
      "训练次数: 2 Epoch: 0075 log_lik= 0.438741 train_kl= -0.01238 train_loss= 0.45113 train_acc= 0.53820 val_roc= 0.92967 val_ap= 0.93986 time= 0.07393\n",
      "训练次数: 2 Epoch: 0076 log_lik= 0.4381042 train_kl= -0.01235 train_loss= 0.45046 train_acc= 0.53833 val_roc= 0.92951 val_ap= 0.94010 time= 0.07503\n",
      "训练次数: 2 Epoch: 0077 log_lik= 0.43781215 train_kl= -0.01232 train_loss= 0.45013 train_acc= 0.53819 val_roc= 0.93014 val_ap= 0.94062 time= 0.08398\n",
      "训练次数: 2 Epoch: 0078 log_lik= 0.4372748 train_kl= -0.01229 train_loss= 0.44956 train_acc= 0.53979 val_roc= 0.93026 val_ap= 0.94079 time= 0.08800\n",
      "训练次数: 2 Epoch: 0079 log_lik= 0.43681863 train_kl= -0.01226 train_loss= 0.44908 train_acc= 0.53997 val_roc= 0.93011 val_ap= 0.94106 time= 0.09044\n",
      "训练次数: 2 Epoch: 0080 log_lik= 0.43655035 train_kl= -0.01223 train_loss= 0.44878 train_acc= 0.54034 val_roc= 0.92978 val_ap= 0.94095 time= 0.07570\n",
      "训练次数: 2 Epoch: 0081 log_lik= 0.4361817 train_kl= -0.01220 train_loss= 0.44839 train_acc= 0.54073 val_roc= 0.92991 val_ap= 0.94068 time= 0.07381\n",
      "训练次数: 2 Epoch: 0082 log_lik= 0.4357305 train_kl= -0.01217 train_loss= 0.44790 train_acc= 0.53923 val_roc= 0.93003 val_ap= 0.94109 time= 0.07808\n",
      "训练次数: 2 Epoch: 0083 log_lik= 0.4352215 train_kl= -0.01215 train_loss= 0.44737 train_acc= 0.54149 val_roc= 0.92981 val_ap= 0.94167 time= 0.07391\n",
      "训练次数: 2 Epoch: 0084 log_lik= 0.43522146 train_kl= -0.01212 train_loss= 0.44734 train_acc= 0.54055 val_roc= 0.92959 val_ap= 0.94134 time= 0.09605\n",
      "训练次数: 2 Epoch: 0085 log_lik= 0.43476138 train_kl= -0.01210 train_loss= 0.44686 train_acc= 0.54178 val_roc= 0.92956 val_ap= 0.94073 time= 0.08099\n",
      "训练次数: 2 Epoch: 0086 log_lik= 0.43432146 train_kl= -0.01208 train_loss= 0.44640 train_acc= 0.54128 val_roc= 0.92919 val_ap= 0.94075 time= 0.07898\n",
      "训练次数: 2 Epoch: 0087 log_lik= 0.43400618 train_kl= -0.01206 train_loss= 0.44607 train_acc= 0.54151 val_roc= 0.92884 val_ap= 0.94107 time= 0.07503\n",
      "训练次数: 2 Epoch: 0088 log_lik= 0.43339923 train_kl= -0.01204 train_loss= 0.44544 train_acc= 0.54210 val_roc= 0.92871 val_ap= 0.94099 time= 0.07899\n",
      "训练次数: 2 Epoch: 0089 log_lik= 0.43323478 train_kl= -0.01202 train_loss= 0.44525 train_acc= 0.54065 val_roc= 0.92867 val_ap= 0.94066 time= 0.07702\n",
      "训练次数: 2 Epoch: 0090 log_lik= 0.43330228 train_kl= -0.01200 train_loss= 0.44531 train_acc= 0.54115 val_roc= 0.92860 val_ap= 0.94051 time= 0.08100\n",
      "训练次数: 2 Epoch: 0091 log_lik= 0.43310145 train_kl= -0.01201 train_loss= 0.44511 train_acc= 0.54223 val_roc= 0.92855 val_ap= 0.94045 time= 0.07553\n",
      "训练次数: 2 Epoch: 0092 log_lik= 0.43282035 train_kl= -0.01200 train_loss= 0.44482 train_acc= 0.54223 val_roc= 0.92891 val_ap= 0.94080 time= 0.07794\n",
      "训练次数: 2 Epoch: 0093 log_lik= 0.43242502 train_kl= -0.01199 train_loss= 0.44442 train_acc= 0.54142 val_roc= 0.92841 val_ap= 0.94083 time= 0.07602\n",
      "训练次数: 2 Epoch: 0094 log_lik= 0.43223527 train_kl= -0.01200 train_loss= 0.44423 train_acc= 0.54098 val_roc= 0.92828 val_ap= 0.94090 time= 0.07893\n",
      "训练次数: 2 Epoch: 0095 log_lik= 0.4316965 train_kl= -0.01201 train_loss= 0.44370 train_acc= 0.54277 val_roc= 0.92816 val_ap= 0.94037 time= 0.07607\n",
      "训练次数: 2 Epoch: 0096 log_lik= 0.43143708 train_kl= -0.01202 train_loss= 0.44345 train_acc= 0.54230 val_roc= 0.92839 val_ap= 0.94055 time= 0.08994\n",
      "训练次数: 2 Epoch: 0097 log_lik= 0.43118933 train_kl= -0.01203 train_loss= 0.44322 train_acc= 0.54192 val_roc= 0.92848 val_ap= 0.94088 time= 0.07760\n",
      "训练次数: 2 Epoch: 0098 log_lik= 0.43116662 train_kl= -0.01203 train_loss= 0.44320 train_acc= 0.54168 val_roc= 0.92841 val_ap= 0.94079 time= 0.07768\n",
      "训练次数: 2 Epoch: 0099 log_lik= 0.43120414 train_kl= -0.01203 train_loss= 0.44324 train_acc= 0.54108 val_roc= 0.92799 val_ap= 0.94037 time= 0.07775\n",
      "训练次数: 2 Epoch: 0100 log_lik= 0.43033332 train_kl= -0.01205 train_loss= 0.44238 train_acc= 0.54346 val_roc= 0.92790 val_ap= 0.94013 time= 0.07706\n",
      "Optimization Finished!\n",
      "训练次数: 2 ROC score: 0.9267991459300252\n",
      "训练次数: 2 AP score: 0.9364155377465491\n",
      "训练次数: 3 Epoch: 0001 log_lik= 1.7865689 train_kl= -0.00004 train_loss= 1.78661 train_acc= 0.49598 val_roc= 0.70715 val_ap= 0.72660 time= 1.39271\n",
      "训练次数: 3 Epoch: 0002 log_lik= 1.4640303 train_kl= -0.00017 train_loss= 1.46420 train_acc= 0.48299 val_roc= 0.70216 val_ap= 0.72896 time= 0.07718\n",
      "训练次数: 3 Epoch: 0003 log_lik= 1.2708563 train_kl= -0.00045 train_loss= 1.27131 train_acc= 0.45300 val_roc= 0.70193 val_ap= 0.72871 time= 0.07619\n",
      "训练次数: 3 Epoch: 0004 log_lik= 1.104852 train_kl= -0.00088 train_loss= 1.10573 train_acc= 0.41445 val_roc= 0.70296 val_ap= 0.72781 time= 0.07568\n",
      "训练次数: 3 Epoch: 0005 log_lik= 0.98977154 train_kl= -0.00144 train_loss= 0.99121 train_acc= 0.36534 val_roc= 0.70812 val_ap= 0.73072 time= 0.08564\n",
      "训练次数: 3 Epoch: 0006 log_lik= 0.88822216 train_kl= -0.00211 train_loss= 0.89033 train_acc= 0.32132 val_roc= 0.71982 val_ap= 0.74192 time= 0.07568\n",
      "训练次数: 3 Epoch: 0007 log_lik= 0.8054872 train_kl= -0.00289 train_loss= 0.80837 train_acc= 0.30156 val_roc= 0.73784 val_ap= 0.75876 time= 0.07670\n",
      "训练次数: 3 Epoch: 0008 log_lik= 0.7483019 train_kl= -0.00376 train_loss= 0.75206 train_acc= 0.29282 val_roc= 0.76345 val_ap= 0.78108 time= 0.07627\n",
      "训练次数: 3 Epoch: 0009 log_lik= 0.70920205 train_kl= -0.00467 train_loss= 0.71388 train_acc= 0.31147 val_roc= 0.78415 val_ap= 0.79289 time= 0.07468\n",
      "训练次数: 3 Epoch: 0010 log_lik= 0.69878095 train_kl= -0.00562 train_loss= 0.70440 train_acc= 0.32745 val_roc= 0.78428 val_ap= 0.78232 time= 0.08415\n",
      "训练次数: 3 Epoch: 0011 log_lik= 0.6872679 train_kl= -0.00658 train_loss= 0.69384 train_acc= 0.29302 val_roc= 0.78729 val_ap= 0.78076 time= 0.08963\n",
      "训练次数: 3 Epoch: 0012 log_lik= 0.68373644 train_kl= -0.00751 train_loss= 0.69124 train_acc= 0.21089 val_roc= 0.81399 val_ap= 0.80667 time= 0.07568\n",
      "训练次数: 3 Epoch: 0013 log_lik= 0.6719254 train_kl= -0.00830 train_loss= 0.68022 train_acc= 0.20478 val_roc= 0.83545 val_ap= 0.82962 time= 0.07568\n",
      "训练次数: 3 Epoch: 0014 log_lik= 0.6555015 train_kl= -0.00897 train_loss= 0.66447 train_acc= 0.21806 val_roc= 0.83910 val_ap= 0.83181 time= 0.07469\n",
      "训练次数: 3 Epoch: 0015 log_lik= 0.6379687 train_kl= -0.00956 train_loss= 0.64753 train_acc= 0.26877 val_roc= 0.83915 val_ap= 0.82892 time= 0.08165\n",
      "训练次数: 3 Epoch: 0016 log_lik= 0.61569256 train_kl= -0.01010 train_loss= 0.62579 train_acc= 0.33891 val_roc= 0.84058 val_ap= 0.82554 time= 0.07667\n",
      "训练次数: 3 Epoch: 0017 log_lik= 0.5956669 train_kl= -0.01061 train_loss= 0.60628 train_acc= 0.39966 val_roc= 0.84403 val_ap= 0.82914 time= 0.07491\n",
      "训练次数: 3 Epoch: 0018 log_lik= 0.5805257 train_kl= -0.01110 train_loss= 0.59162 train_acc= 0.43598 val_roc= 0.85045 val_ap= 0.83902 time= 0.08705\n",
      "训练次数: 3 Epoch: 0019 log_lik= 0.5657184 train_kl= -0.01154 train_loss= 0.57725 train_acc= 0.45766 val_roc= 0.85427 val_ap= 0.84662 time= 0.07698\n",
      "训练次数: 3 Epoch: 0020 log_lik= 0.5569112 train_kl= -0.01194 train_loss= 0.56885 train_acc= 0.46632 val_roc= 0.86284 val_ap= 0.85653 time= 0.07692\n",
      "训练次数: 3 Epoch: 0021 log_lik= 0.5502698 train_kl= -0.01230 train_loss= 0.56257 train_acc= 0.47632 val_roc= 0.87121 val_ap= 0.86562 time= 0.07507\n",
      "训练次数: 3 Epoch: 0022 log_lik= 0.54341316 train_kl= -0.01261 train_loss= 0.55602 train_acc= 0.48240 val_roc= 0.87867 val_ap= 0.87534 time= 0.09752\n",
      "训练次数: 3 Epoch: 0023 log_lik= 0.5379128 train_kl= -0.01288 train_loss= 0.55079 train_acc= 0.48457 val_roc= 0.88180 val_ap= 0.88255 time= 0.07493\n",
      "训练次数: 3 Epoch: 0024 log_lik= 0.5321126 train_kl= -0.01313 train_loss= 0.54524 train_acc= 0.48714 val_roc= 0.88443 val_ap= 0.88635 time= 0.08900\n",
      "训练次数: 3 Epoch: 0025 log_lik= 0.5235436 train_kl= -0.01333 train_loss= 0.53687 train_acc= 0.49509 val_roc= 0.88866 val_ap= 0.89057 time= 0.08997\n",
      "训练次数: 3 Epoch: 0026 log_lik= 0.5116112 train_kl= -0.01348 train_loss= 0.52509 train_acc= 0.50260 val_roc= 0.89422 val_ap= 0.89508 time= 0.08405\n",
      "训练次数: 3 Epoch: 0027 log_lik= 0.50748056 train_kl= -0.01358 train_loss= 0.52107 train_acc= 0.50086 val_roc= 0.89815 val_ap= 0.89824 time= 0.07690\n",
      "训练次数: 3 Epoch: 0028 log_lik= 0.5029068 train_kl= -0.01365 train_loss= 0.51656 train_acc= 0.50061 val_roc= 0.90030 val_ap= 0.90162 time= 0.07404\n",
      "训练次数: 3 Epoch: 0029 log_lik= 0.49621907 train_kl= -0.01367 train_loss= 0.50989 train_acc= 0.50403 val_roc= 0.90003 val_ap= 0.90243 time= 0.07606\n",
      "训练次数: 3 Epoch: 0030 log_lik= 0.49120897 train_kl= -0.01367 train_loss= 0.50487 train_acc= 0.50887 val_roc= 0.89978 val_ap= 0.90229 time= 0.07607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0031 log_lik= 0.48862258 train_kl= -0.01365 train_loss= 0.50227 train_acc= 0.51282 val_roc= 0.89909 val_ap= 0.90068 time= 0.07589\n",
      "训练次数: 3 Epoch: 0032 log_lik= 0.48715082 train_kl= -0.01365 train_loss= 0.50080 train_acc= 0.51680 val_roc= 0.89779 val_ap= 0.89839 time= 0.08801\n",
      "训练次数: 3 Epoch: 0033 log_lik= 0.48542154 train_kl= -0.01366 train_loss= 0.49908 train_acc= 0.51798 val_roc= 0.89601 val_ap= 0.89714 time= 0.08310\n",
      "训练次数: 3 Epoch: 0034 log_lik= 0.4846509 train_kl= -0.01369 train_loss= 0.49834 train_acc= 0.51730 val_roc= 0.89504 val_ap= 0.89752 time= 0.07497\n",
      "训练次数: 3 Epoch: 0035 log_lik= 0.48195082 train_kl= -0.01373 train_loss= 0.49568 train_acc= 0.51887 val_roc= 0.89602 val_ap= 0.89894 time= 0.07501\n",
      "训练次数: 3 Epoch: 0036 log_lik= 0.47865486 train_kl= -0.01375 train_loss= 0.49241 train_acc= 0.52199 val_roc= 0.89844 val_ap= 0.90128 time= 0.07400\n",
      "训练次数: 3 Epoch: 0037 log_lik= 0.47370917 train_kl= -0.01376 train_loss= 0.48747 train_acc= 0.52675 val_roc= 0.90144 val_ap= 0.90309 time= 0.08903\n",
      "训练次数: 3 Epoch: 0038 log_lik= 0.4707797 train_kl= -0.01375 train_loss= 0.48453 train_acc= 0.52851 val_roc= 0.90393 val_ap= 0.90560 time= 0.07602\n",
      "训练次数: 3 Epoch: 0039 log_lik= 0.46986052 train_kl= -0.01374 train_loss= 0.48360 train_acc= 0.52829 val_roc= 0.90496 val_ap= 0.90773 time= 0.08394\n",
      "训练次数: 3 Epoch: 0040 log_lik= 0.46792763 train_kl= -0.01373 train_loss= 0.48165 train_acc= 0.52795 val_roc= 0.90571 val_ap= 0.90936 time= 0.07900\n",
      "训练次数: 3 Epoch: 0041 log_lik= 0.46623155 train_kl= -0.01370 train_loss= 0.47993 train_acc= 0.52952 val_roc= 0.90685 val_ap= 0.91093 time= 0.07497\n",
      "训练次数: 3 Epoch: 0042 log_lik= 0.46366674 train_kl= -0.01364 train_loss= 0.47730 train_acc= 0.53008 val_roc= 0.90711 val_ap= 0.91059 time= 0.08745\n",
      "训练次数: 3 Epoch: 0043 log_lik= 0.46198308 train_kl= -0.01356 train_loss= 0.47554 train_acc= 0.53071 val_roc= 0.90724 val_ap= 0.91109 time= 0.07606\n",
      "训练次数: 3 Epoch: 0044 log_lik= 0.4609385 train_kl= -0.01348 train_loss= 0.47441 train_acc= 0.53136 val_roc= 0.90718 val_ap= 0.91096 time= 0.07697\n",
      "训练次数: 3 Epoch: 0045 log_lik= 0.4596399 train_kl= -0.01339 train_loss= 0.47303 train_acc= 0.53241 val_roc= 0.90811 val_ap= 0.91171 time= 0.07477\n",
      "训练次数: 3 Epoch: 0046 log_lik= 0.4584317 train_kl= -0.01331 train_loss= 0.47174 train_acc= 0.53190 val_roc= 0.90872 val_ap= 0.91261 time= 0.07569\n",
      "训练次数: 3 Epoch: 0047 log_lik= 0.45733804 train_kl= -0.01323 train_loss= 0.47057 train_acc= 0.53294 val_roc= 0.90882 val_ap= 0.91281 time= 0.09161\n",
      "训练次数: 3 Epoch: 0048 log_lik= 0.45582148 train_kl= -0.01314 train_loss= 0.46896 train_acc= 0.53361 val_roc= 0.90889 val_ap= 0.91260 time= 0.07396\n",
      "训练次数: 3 Epoch: 0049 log_lik= 0.45486003 train_kl= -0.01306 train_loss= 0.46792 train_acc= 0.53417 val_roc= 0.90960 val_ap= 0.91328 time= 0.07501\n",
      "训练次数: 3 Epoch: 0050 log_lik= 0.45372796 train_kl= -0.01298 train_loss= 0.46671 train_acc= 0.53384 val_roc= 0.91068 val_ap= 0.91477 time= 0.08997\n",
      "训练次数: 3 Epoch: 0051 log_lik= 0.45345092 train_kl= -0.01290 train_loss= 0.46635 train_acc= 0.53441 val_roc= 0.91171 val_ap= 0.91624 time= 0.07597\n",
      "训练次数: 3 Epoch: 0052 log_lik= 0.45193982 train_kl= -0.01282 train_loss= 0.46476 train_acc= 0.53436 val_roc= 0.91235 val_ap= 0.91664 time= 0.09101\n",
      "训练次数: 3 Epoch: 0053 log_lik= 0.45124793 train_kl= -0.01274 train_loss= 0.46399 train_acc= 0.53368 val_roc= 0.91253 val_ap= 0.91738 time= 0.09598\n",
      "训练次数: 3 Epoch: 0054 log_lik= 0.44993526 train_kl= -0.01266 train_loss= 0.46260 train_acc= 0.53459 val_roc= 0.91285 val_ap= 0.91807 time= 0.08201\n",
      "训练次数: 3 Epoch: 0055 log_lik= 0.44969955 train_kl= -0.01260 train_loss= 0.46230 train_acc= 0.53499 val_roc= 0.91373 val_ap= 0.91920 time= 0.09146\n",
      "训练次数: 3 Epoch: 0056 log_lik= 0.44832054 train_kl= -0.01255 train_loss= 0.46087 train_acc= 0.53637 val_roc= 0.91414 val_ap= 0.91996 time= 0.09103\n",
      "训练次数: 3 Epoch: 0057 log_lik= 0.4478939 train_kl= -0.01251 train_loss= 0.46040 train_acc= 0.53723 val_roc= 0.91440 val_ap= 0.92022 time= 0.07594\n",
      "训练次数: 3 Epoch: 0058 log_lik= 0.44722632 train_kl= -0.01246 train_loss= 0.45969 train_acc= 0.53611 val_roc= 0.91453 val_ap= 0.92019 time= 0.07551\n",
      "训练次数: 3 Epoch: 0059 log_lik= 0.4465898 train_kl= -0.01240 train_loss= 0.45899 train_acc= 0.53691 val_roc= 0.91463 val_ap= 0.92042 time= 0.07951\n",
      "训练次数: 3 Epoch: 0060 log_lik= 0.44612235 train_kl= -0.01235 train_loss= 0.45847 train_acc= 0.53751 val_roc= 0.91498 val_ap= 0.92085 time= 0.08863\n",
      "训练次数: 3 Epoch: 0061 log_lik= 0.44543922 train_kl= -0.01231 train_loss= 0.45775 train_acc= 0.53838 val_roc= 0.91570 val_ap= 0.92134 time= 0.08984\n",
      "训练次数: 3 Epoch: 0062 log_lik= 0.44486165 train_kl= -0.01227 train_loss= 0.45713 train_acc= 0.53920 val_roc= 0.91623 val_ap= 0.92193 time= 0.07800\n",
      "训练次数: 3 Epoch: 0063 log_lik= 0.44385675 train_kl= -0.01223 train_loss= 0.45608 train_acc= 0.53963 val_roc= 0.91690 val_ap= 0.92252 time= 0.07697\n",
      "训练次数: 3 Epoch: 0064 log_lik= 0.44402334 train_kl= -0.01219 train_loss= 0.45621 train_acc= 0.54013 val_roc= 0.91714 val_ap= 0.92285 time= 0.07900\n",
      "训练次数: 3 Epoch: 0065 log_lik= 0.44323012 train_kl= -0.01214 train_loss= 0.45537 train_acc= 0.54086 val_roc= 0.91765 val_ap= 0.92301 time= 0.07705\n",
      "训练次数: 3 Epoch: 0066 log_lik= 0.4427658 train_kl= -0.01210 train_loss= 0.45486 train_acc= 0.54105 val_roc= 0.91848 val_ap= 0.92362 time= 0.07597\n",
      "训练次数: 3 Epoch: 0067 log_lik= 0.44177768 train_kl= -0.01206 train_loss= 0.45384 train_acc= 0.54202 val_roc= 0.91904 val_ap= 0.92419 time= 0.07506\n",
      "训练次数: 3 Epoch: 0068 log_lik= 0.44090414 train_kl= -0.01203 train_loss= 0.45294 train_acc= 0.54202 val_roc= 0.91982 val_ap= 0.92506 time= 0.07598\n",
      "训练次数: 3 Epoch: 0069 log_lik= 0.44083485 train_kl= -0.01200 train_loss= 0.45284 train_acc= 0.54145 val_roc= 0.92038 val_ap= 0.92565 time= 0.07603\n",
      "训练次数: 3 Epoch: 0070 log_lik= 0.4403494 train_kl= -0.01198 train_loss= 0.45233 train_acc= 0.54103 val_roc= 0.92057 val_ap= 0.92576 time= 0.07600\n",
      "训练次数: 3 Epoch: 0071 log_lik= 0.43944317 train_kl= -0.01195 train_loss= 0.45140 train_acc= 0.54207 val_roc= 0.92090 val_ap= 0.92610 time= 0.08195\n",
      "训练次数: 3 Epoch: 0072 log_lik= 0.4394091 train_kl= -0.01194 train_loss= 0.45135 train_acc= 0.54108 val_roc= 0.92116 val_ap= 0.92619 time= 0.07904\n",
      "训练次数: 3 Epoch: 0073 log_lik= 0.43916938 train_kl= -0.01192 train_loss= 0.45109 train_acc= 0.54116 val_roc= 0.92112 val_ap= 0.92589 time= 0.07496\n",
      "训练次数: 3 Epoch: 0074 log_lik= 0.43889725 train_kl= -0.01191 train_loss= 0.45081 train_acc= 0.54121 val_roc= 0.92174 val_ap= 0.92718 time= 0.07654\n",
      "训练次数: 3 Epoch: 0075 log_lik= 0.4382776 train_kl= -0.01191 train_loss= 0.45018 train_acc= 0.54241 val_roc= 0.92187 val_ap= 0.92676 time= 0.08389\n",
      "训练次数: 3 Epoch: 0076 log_lik= 0.43763113 train_kl= -0.01189 train_loss= 0.44952 train_acc= 0.54166 val_roc= 0.92222 val_ap= 0.92666 time= 0.07703\n",
      "训练次数: 3 Epoch: 0077 log_lik= 0.4372692 train_kl= -0.01189 train_loss= 0.44916 train_acc= 0.54117 val_roc= 0.92252 val_ap= 0.92740 time= 0.07698\n",
      "训练次数: 3 Epoch: 0078 log_lik= 0.43674082 train_kl= -0.01189 train_loss= 0.44863 train_acc= 0.54181 val_roc= 0.92288 val_ap= 0.92782 time= 0.08098\n",
      "训练次数: 3 Epoch: 0079 log_lik= 0.4366948 train_kl= -0.01188 train_loss= 0.44858 train_acc= 0.54233 val_roc= 0.92356 val_ap= 0.92871 time= 0.07500\n",
      "训练次数: 3 Epoch: 0080 log_lik= 0.4359686 train_kl= -0.01188 train_loss= 0.44785 train_acc= 0.54102 val_roc= 0.92397 val_ap= 0.92891 time= 0.07502\n",
      "训练次数: 3 Epoch: 0081 log_lik= 0.43559292 train_kl= -0.01188 train_loss= 0.44747 train_acc= 0.54133 val_roc= 0.92446 val_ap= 0.92928 time= 0.09609\n",
      "训练次数: 3 Epoch: 0082 log_lik= 0.43514493 train_kl= -0.01188 train_loss= 0.44702 train_acc= 0.54215 val_roc= 0.92494 val_ap= 0.92974 time= 0.07585\n",
      "训练次数: 3 Epoch: 0083 log_lik= 0.43453094 train_kl= -0.01188 train_loss= 0.44641 train_acc= 0.54264 val_roc= 0.92514 val_ap= 0.92992 time= 0.08002\n",
      "训练次数: 3 Epoch: 0084 log_lik= 0.43394852 train_kl= -0.01188 train_loss= 0.44583 train_acc= 0.54223 val_roc= 0.92526 val_ap= 0.92962 time= 0.07804\n",
      "训练次数: 3 Epoch: 0085 log_lik= 0.43406695 train_kl= -0.01187 train_loss= 0.44594 train_acc= 0.54180 val_roc= 0.92557 val_ap= 0.92994 time= 0.07698\n",
      "训练次数: 3 Epoch: 0086 log_lik= 0.4333194 train_kl= -0.01187 train_loss= 0.44519 train_acc= 0.54240 val_roc= 0.92562 val_ap= 0.93016 time= 0.07803\n",
      "训练次数: 3 Epoch: 0087 log_lik= 0.43359077 train_kl= -0.01188 train_loss= 0.44547 train_acc= 0.54076 val_roc= 0.92604 val_ap= 0.93021 time= 0.08202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0088 log_lik= 0.43306577 train_kl= -0.01187 train_loss= 0.44493 train_acc= 0.54251 val_roc= 0.92621 val_ap= 0.93028 time= 0.07898\n",
      "训练次数: 3 Epoch: 0089 log_lik= 0.43232992 train_kl= -0.01187 train_loss= 0.44420 train_acc= 0.54251 val_roc= 0.92679 val_ap= 0.93063 time= 0.07702\n",
      "训练次数: 3 Epoch: 0090 log_lik= 0.43221012 train_kl= -0.01187 train_loss= 0.44408 train_acc= 0.54326 val_roc= 0.92748 val_ap= 0.93130 time= 0.08003\n",
      "训练次数: 3 Epoch: 0091 log_lik= 0.43174565 train_kl= -0.01188 train_loss= 0.44362 train_acc= 0.54289 val_roc= 0.92713 val_ap= 0.93084 time= 0.07697\n",
      "训练次数: 3 Epoch: 0092 log_lik= 0.4319841 train_kl= -0.01188 train_loss= 0.44386 train_acc= 0.54298 val_roc= 0.92670 val_ap= 0.93010 time= 0.07799\n",
      "训练次数: 3 Epoch: 0093 log_lik= 0.43108836 train_kl= -0.01188 train_loss= 0.44297 train_acc= 0.54253 val_roc= 0.92783 val_ap= 0.93114 time= 0.07700\n",
      "训练次数: 3 Epoch: 0094 log_lik= 0.43096104 train_kl= -0.01189 train_loss= 0.44285 train_acc= 0.54291 val_roc= 0.92818 val_ap= 0.93127 time= 0.07799\n",
      "训练次数: 3 Epoch: 0095 log_lik= 0.43081307 train_kl= -0.01190 train_loss= 0.44271 train_acc= 0.54232 val_roc= 0.92796 val_ap= 0.93122 time= 0.07605\n",
      "训练次数: 3 Epoch: 0096 log_lik= 0.43045574 train_kl= -0.01191 train_loss= 0.44237 train_acc= 0.54276 val_roc= 0.92836 val_ap= 0.93108 time= 0.07402\n",
      "训练次数: 3 Epoch: 0097 log_lik= 0.43027362 train_kl= -0.01191 train_loss= 0.44218 train_acc= 0.54308 val_roc= 0.92797 val_ap= 0.93100 time= 0.08193\n",
      "训练次数: 3 Epoch: 0098 log_lik= 0.430028 train_kl= -0.01192 train_loss= 0.44195 train_acc= 0.54248 val_roc= 0.92805 val_ap= 0.93163 time= 0.07714\n",
      "训练次数: 3 Epoch: 0099 log_lik= 0.42991802 train_kl= -0.01195 train_loss= 0.44187 train_acc= 0.54070 val_roc= 0.92868 val_ap= 0.93107 time= 0.07502\n",
      "训练次数: 3 Epoch: 0100 log_lik= 0.42959365 train_kl= -0.01194 train_loss= 0.44153 train_acc= 0.54212 val_roc= 0.92887 val_ap= 0.93116 time= 0.07527\n",
      "Optimization Finished!\n",
      "训练次数: 3 ROC score: 0.9185933049843552\n",
      "训练次数: 3 AP score: 0.916557249240193\n",
      "训练次数: 4 Epoch: 0001 log_lik= 1.6605418 train_kl= -0.00004 train_loss= 1.66058 train_acc= 0.49933 val_roc= 0.70114 val_ap= 0.73373 time= 1.41319\n",
      "训练次数: 4 Epoch: 0002 log_lik= 1.3667763 train_kl= -0.00036 train_loss= 1.36714 train_acc= 0.45349 val_roc= 0.68983 val_ap= 0.73225 time= 0.09312\n",
      "训练次数: 4 Epoch: 0003 log_lik= 1.1456807 train_kl= -0.00099 train_loss= 1.14667 train_acc= 0.35913 val_roc= 0.69186 val_ap= 0.73634 time= 0.09112\n",
      "训练次数: 4 Epoch: 0004 log_lik= 1.0074272 train_kl= -0.00164 train_loss= 1.00906 train_acc= 0.30402 val_roc= 0.70242 val_ap= 0.74529 time= 0.09161\n",
      "训练次数: 4 Epoch: 0005 log_lik= 0.86292785 train_kl= -0.00218 train_loss= 0.86510 train_acc= 0.31216 val_roc= 0.72908 val_ap= 0.76473 time= 0.08168\n",
      "训练次数: 4 Epoch: 0006 log_lik= 0.7795334 train_kl= -0.00278 train_loss= 0.78231 train_acc= 0.36991 val_roc= 0.76435 val_ap= 0.78656 time= 0.08166\n",
      "训练次数: 4 Epoch: 0007 log_lik= 0.7409381 train_kl= -0.00354 train_loss= 0.74448 train_acc= 0.39720 val_roc= 0.76293 val_ap= 0.78148 time= 0.07767\n",
      "训练次数: 4 Epoch: 0008 log_lik= 0.7158778 train_kl= -0.00444 train_loss= 0.72032 train_acc= 0.36516 val_roc= 0.74830 val_ap= 0.77040 time= 0.08265\n",
      "训练次数: 4 Epoch: 0009 log_lik= 0.7112586 train_kl= -0.00541 train_loss= 0.71667 train_acc= 0.21768 val_roc= 0.75265 val_ap= 0.77599 time= 0.08962\n",
      "训练次数: 4 Epoch: 0010 log_lik= 0.718353 train_kl= -0.00636 train_loss= 0.72471 train_acc= 0.12535 val_roc= 0.77867 val_ap= 0.79836 time= 0.08771\n",
      "训练次数: 4 Epoch: 0011 log_lik= 0.70803535 train_kl= -0.00720 train_loss= 0.71523 train_acc= 0.10852 val_roc= 0.81146 val_ap= 0.82312 time= 0.07453\n",
      "训练次数: 4 Epoch: 0012 log_lik= 0.68326783 train_kl= -0.00794 train_loss= 0.69121 train_acc= 0.14091 val_roc= 0.83167 val_ap= 0.83464 time= 0.07475\n",
      "训练次数: 4 Epoch: 0013 log_lik= 0.66559184 train_kl= -0.00862 train_loss= 0.67421 train_acc= 0.19235 val_roc= 0.83921 val_ap= 0.83869 time= 0.09261\n",
      "训练次数: 4 Epoch: 0014 log_lik= 0.6482353 train_kl= -0.00925 train_loss= 0.65749 train_acc= 0.25551 val_roc= 0.84630 val_ap= 0.84543 time= 0.07670\n",
      "训练次数: 4 Epoch: 0015 log_lik= 0.62952095 train_kl= -0.00984 train_loss= 0.63936 train_acc= 0.27954 val_roc= 0.85327 val_ap= 0.85502 time= 0.09581\n",
      "训练次数: 4 Epoch: 0016 log_lik= 0.60686445 train_kl= -0.01039 train_loss= 0.61725 train_acc= 0.32517 val_roc= 0.85541 val_ap= 0.85952 time= 0.07686\n",
      "训练次数: 4 Epoch: 0017 log_lik= 0.58829087 train_kl= -0.01088 train_loss= 0.59917 train_acc= 0.37183 val_roc= 0.85561 val_ap= 0.86066 time= 0.08296\n",
      "训练次数: 4 Epoch: 0018 log_lik= 0.5743097 train_kl= -0.01133 train_loss= 0.58564 train_acc= 0.41280 val_roc= 0.85592 val_ap= 0.86052 time= 0.07901\n",
      "训练次数: 4 Epoch: 0019 log_lik= 0.564794 train_kl= -0.01174 train_loss= 0.57654 train_acc= 0.44679 val_roc= 0.85576 val_ap= 0.86108 time= 0.07747\n",
      "训练次数: 4 Epoch: 0020 log_lik= 0.5622736 train_kl= -0.01210 train_loss= 0.57437 train_acc= 0.46782 val_roc= 0.86131 val_ap= 0.86737 time= 0.07785\n",
      "训练次数: 4 Epoch: 0021 log_lik= 0.55804604 train_kl= -0.01236 train_loss= 0.57041 train_acc= 0.48312 val_roc= 0.86725 val_ap= 0.87403 time= 0.07451\n",
      "训练次数: 4 Epoch: 0022 log_lik= 0.5536596 train_kl= -0.01253 train_loss= 0.56619 train_acc= 0.48926 val_roc= 0.87142 val_ap= 0.87906 time= 0.09200\n",
      "训练次数: 4 Epoch: 0023 log_lik= 0.5484767 train_kl= -0.01262 train_loss= 0.56110 train_acc= 0.49059 val_roc= 0.87510 val_ap= 0.88258 time= 0.09202\n",
      "训练次数: 4 Epoch: 0024 log_lik= 0.5409444 train_kl= -0.01265 train_loss= 0.55359 train_acc= 0.49210 val_roc= 0.87851 val_ap= 0.88665 time= 0.07998\n",
      "训练次数: 4 Epoch: 0025 log_lik= 0.5301531 train_kl= -0.01262 train_loss= 0.54277 train_acc= 0.49583 val_roc= 0.88180 val_ap= 0.89099 time= 0.07906\n",
      "训练次数: 4 Epoch: 0026 log_lik= 0.5189255 train_kl= -0.01255 train_loss= 0.53148 train_acc= 0.49893 val_roc= 0.88485 val_ap= 0.89506 time= 0.07999\n",
      "训练次数: 4 Epoch: 0027 log_lik= 0.5102799 train_kl= -0.01248 train_loss= 0.52276 train_acc= 0.50025 val_roc= 0.88943 val_ap= 0.89920 time= 0.07499\n",
      "训练次数: 4 Epoch: 0028 log_lik= 0.503495 train_kl= -0.01243 train_loss= 0.51592 train_acc= 0.50114 val_roc= 0.89291 val_ap= 0.90089 time= 0.09000\n",
      "训练次数: 4 Epoch: 0029 log_lik= 0.49704668 train_kl= -0.01241 train_loss= 0.50946 train_acc= 0.50551 val_roc= 0.89527 val_ap= 0.90264 time= 0.07801\n",
      "训练次数: 4 Epoch: 0030 log_lik= 0.49393904 train_kl= -0.01241 train_loss= 0.50635 train_acc= 0.50971 val_roc= 0.89719 val_ap= 0.90427 time= 0.07801\n",
      "训练次数: 4 Epoch: 0031 log_lik= 0.49153325 train_kl= -0.01242 train_loss= 0.50396 train_acc= 0.50958 val_roc= 0.89832 val_ap= 0.90678 time= 0.07699\n",
      "训练次数: 4 Epoch: 0032 log_lik= 0.49065736 train_kl= -0.01241 train_loss= 0.50307 train_acc= 0.50796 val_roc= 0.89912 val_ap= 0.90867 time= 0.07808\n",
      "训练次数: 4 Epoch: 0033 log_lik= 0.48836926 train_kl= -0.01237 train_loss= 0.50074 train_acc= 0.50955 val_roc= 0.90042 val_ap= 0.91002 time= 0.08301\n",
      "训练次数: 4 Epoch: 0034 log_lik= 0.48455375 train_kl= -0.01231 train_loss= 0.49686 train_acc= 0.51270 val_roc= 0.90146 val_ap= 0.91063 time= 0.07898\n",
      "训练次数: 4 Epoch: 0035 log_lik= 0.4808632 train_kl= -0.01223 train_loss= 0.49309 train_acc= 0.51800 val_roc= 0.90350 val_ap= 0.91319 time= 0.07599\n",
      "训练次数: 4 Epoch: 0036 log_lik= 0.477101 train_kl= -0.01215 train_loss= 0.48925 train_acc= 0.52231 val_roc= 0.90503 val_ap= 0.91514 time= 0.07999\n",
      "训练次数: 4 Epoch: 0037 log_lik= 0.47497094 train_kl= -0.01208 train_loss= 0.48705 train_acc= 0.52201 val_roc= 0.90622 val_ap= 0.91690 time= 0.09101\n",
      "训练次数: 4 Epoch: 0038 log_lik= 0.4740871 train_kl= -0.01202 train_loss= 0.48611 train_acc= 0.52231 val_roc= 0.90715 val_ap= 0.91825 time= 0.07699\n",
      "训练次数: 4 Epoch: 0039 log_lik= 0.4735222 train_kl= -0.01195 train_loss= 0.48548 train_acc= 0.51880 val_roc= 0.90805 val_ap= 0.91962 time= 0.07401\n",
      "训练次数: 4 Epoch: 0040 log_lik= 0.47195455 train_kl= -0.01189 train_loss= 0.48384 train_acc= 0.52016 val_roc= 0.90831 val_ap= 0.92046 time= 0.07701\n",
      "训练次数: 4 Epoch: 0041 log_lik= 0.4701446 train_kl= -0.01183 train_loss= 0.48197 train_acc= 0.52370 val_roc= 0.90976 val_ap= 0.92191 time= 0.07500\n",
      "训练次数: 4 Epoch: 0042 log_lik= 0.46838334 train_kl= -0.01178 train_loss= 0.48017 train_acc= 0.52597 val_roc= 0.91113 val_ap= 0.92324 time= 0.09199\n",
      "训练次数: 4 Epoch: 0043 log_lik= 0.46678904 train_kl= -0.01174 train_loss= 0.47853 train_acc= 0.52546 val_roc= 0.91232 val_ap= 0.92409 time= 0.07549\n",
      "训练次数: 4 Epoch: 0044 log_lik= 0.46512312 train_kl= -0.01169 train_loss= 0.47681 train_acc= 0.52607 val_roc= 0.91263 val_ap= 0.92475 time= 0.07398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 Epoch: 0045 log_lik= 0.46406782 train_kl= -0.01161 train_loss= 0.47568 train_acc= 0.52623 val_roc= 0.91243 val_ap= 0.92483 time= 0.07803\n",
      "训练次数: 4 Epoch: 0046 log_lik= 0.46190667 train_kl= -0.01153 train_loss= 0.47344 train_acc= 0.52648 val_roc= 0.91242 val_ap= 0.92516 time= 0.09297\n",
      "训练次数: 4 Epoch: 0047 log_lik= 0.46020758 train_kl= -0.01145 train_loss= 0.47166 train_acc= 0.52663 val_roc= 0.91255 val_ap= 0.92523 time= 0.07400\n",
      "训练次数: 4 Epoch: 0048 log_lik= 0.4593822 train_kl= -0.01138 train_loss= 0.47077 train_acc= 0.52563 val_roc= 0.91253 val_ap= 0.92550 time= 0.08000\n",
      "训练次数: 4 Epoch: 0049 log_lik= 0.45736036 train_kl= -0.01132 train_loss= 0.46868 train_acc= 0.52667 val_roc= 0.91229 val_ap= 0.92565 time= 0.07595\n",
      "训练次数: 4 Epoch: 0050 log_lik= 0.45692202 train_kl= -0.01127 train_loss= 0.46819 train_acc= 0.52661 val_roc= 0.91174 val_ap= 0.92548 time= 0.07812\n",
      "训练次数: 4 Epoch: 0051 log_lik= 0.4549103 train_kl= -0.01123 train_loss= 0.46614 train_acc= 0.52859 val_roc= 0.91143 val_ap= 0.92551 time= 0.07570\n",
      "训练次数: 4 Epoch: 0052 log_lik= 0.45465815 train_kl= -0.01120 train_loss= 0.46585 train_acc= 0.52768 val_roc= 0.91097 val_ap= 0.92549 time= 0.08331\n",
      "训练次数: 4 Epoch: 0053 log_lik= 0.45291018 train_kl= -0.01117 train_loss= 0.46408 train_acc= 0.52812 val_roc= 0.91044 val_ap= 0.92576 time= 0.08066\n",
      "训练次数: 4 Epoch: 0054 log_lik= 0.4531366 train_kl= -0.01115 train_loss= 0.46428 train_acc= 0.52720 val_roc= 0.91023 val_ap= 0.92565 time= 0.07673\n",
      "训练次数: 4 Epoch: 0055 log_lik= 0.45181233 train_kl= -0.01113 train_loss= 0.46295 train_acc= 0.52733 val_roc= 0.90970 val_ap= 0.92500 time= 0.08564\n",
      "训练次数: 4 Epoch: 0056 log_lik= 0.45085034 train_kl= -0.01113 train_loss= 0.46198 train_acc= 0.52884 val_roc= 0.90976 val_ap= 0.92519 time= 0.07569\n",
      "训练次数: 4 Epoch: 0057 log_lik= 0.44961196 train_kl= -0.01114 train_loss= 0.46075 train_acc= 0.52894 val_roc= 0.90944 val_ap= 0.92536 time= 0.07503\n",
      "训练次数: 4 Epoch: 0058 log_lik= 0.44930226 train_kl= -0.01114 train_loss= 0.46045 train_acc= 0.52805 val_roc= 0.90980 val_ap= 0.92617 time= 0.08165\n",
      "训练次数: 4 Epoch: 0059 log_lik= 0.44828284 train_kl= -0.01116 train_loss= 0.45944 train_acc= 0.52794 val_roc= 0.91062 val_ap= 0.92731 time= 0.07867\n",
      "训练次数: 4 Epoch: 0060 log_lik= 0.4479412 train_kl= -0.01119 train_loss= 0.45913 train_acc= 0.52884 val_roc= 0.91133 val_ap= 0.92812 time= 0.07568\n",
      "训练次数: 4 Epoch: 0061 log_lik= 0.44637394 train_kl= -0.01122 train_loss= 0.45760 train_acc= 0.52934 val_roc= 0.91193 val_ap= 0.92856 time= 0.07867\n",
      "训练次数: 4 Epoch: 0062 log_lik= 0.44582543 train_kl= -0.01125 train_loss= 0.45708 train_acc= 0.52908 val_roc= 0.91276 val_ap= 0.92925 time= 0.07540\n",
      "训练次数: 4 Epoch: 0063 log_lik= 0.44526008 train_kl= -0.01128 train_loss= 0.45654 train_acc= 0.52989 val_roc= 0.91347 val_ap= 0.93001 time= 0.07836\n",
      "训练次数: 4 Epoch: 0064 log_lik= 0.44403273 train_kl= -0.01129 train_loss= 0.45533 train_acc= 0.53079 val_roc= 0.91447 val_ap= 0.93114 time= 0.07984\n",
      "训练次数: 4 Epoch: 0065 log_lik= 0.44320953 train_kl= -0.01130 train_loss= 0.45451 train_acc= 0.53095 val_roc= 0.91540 val_ap= 0.93229 time= 0.08265\n",
      "训练次数: 4 Epoch: 0066 log_lik= 0.44248214 train_kl= -0.01131 train_loss= 0.45379 train_acc= 0.53070 val_roc= 0.91623 val_ap= 0.93347 time= 0.08464\n",
      "训练次数: 4 Epoch: 0067 log_lik= 0.4426944 train_kl= -0.01132 train_loss= 0.45402 train_acc= 0.52958 val_roc= 0.91671 val_ap= 0.93398 time= 0.07469\n",
      "训练次数: 4 Epoch: 0068 log_lik= 0.44163817 train_kl= -0.01135 train_loss= 0.45299 train_acc= 0.52960 val_roc= 0.91798 val_ap= 0.93525 time= 0.07667\n",
      "训练次数: 4 Epoch: 0069 log_lik= 0.44109792 train_kl= -0.01138 train_loss= 0.45248 train_acc= 0.53081 val_roc= 0.91930 val_ap= 0.93620 time= 0.07469\n",
      "训练次数: 4 Epoch: 0070 log_lik= 0.4403788 train_kl= -0.01141 train_loss= 0.45179 train_acc= 0.52960 val_roc= 0.91923 val_ap= 0.93629 time= 0.07568\n",
      "训练次数: 4 Epoch: 0071 log_lik= 0.43981507 train_kl= -0.01144 train_loss= 0.45125 train_acc= 0.53002 val_roc= 0.91889 val_ap= 0.93609 time= 0.07490\n",
      "训练次数: 4 Epoch: 0072 log_lik= 0.43991837 train_kl= -0.01145 train_loss= 0.45137 train_acc= 0.52899 val_roc= 0.92021 val_ap= 0.93748 time= 0.07411\n",
      "训练次数: 4 Epoch: 0073 log_lik= 0.43884745 train_kl= -0.01147 train_loss= 0.45032 train_acc= 0.53004 val_roc= 0.92128 val_ap= 0.93854 time= 0.08092\n",
      "训练次数: 4 Epoch: 0074 log_lik= 0.43891886 train_kl= -0.01148 train_loss= 0.45040 train_acc= 0.53027 val_roc= 0.92079 val_ap= 0.93789 time= 0.07595\n",
      "训练次数: 4 Epoch: 0075 log_lik= 0.43779296 train_kl= -0.01148 train_loss= 0.44928 train_acc= 0.52996 val_roc= 0.92047 val_ap= 0.93741 time= 0.08000\n",
      "训练次数: 4 Epoch: 0076 log_lik= 0.43785995 train_kl= -0.01149 train_loss= 0.44935 train_acc= 0.52931 val_roc= 0.92111 val_ap= 0.93791 time= 0.07606\n",
      "训练次数: 4 Epoch: 0077 log_lik= 0.43733725 train_kl= -0.01151 train_loss= 0.44885 train_acc= 0.53020 val_roc= 0.92255 val_ap= 0.93927 time= 0.07705\n",
      "训练次数: 4 Epoch: 0078 log_lik= 0.43697232 train_kl= -0.01153 train_loss= 0.44850 train_acc= 0.53053 val_roc= 0.92251 val_ap= 0.93912 time= 0.07798\n",
      "训练次数: 4 Epoch: 0079 log_lik= 0.43690845 train_kl= -0.01155 train_loss= 0.44845 train_acc= 0.53049 val_roc= 0.92168 val_ap= 0.93822 time= 0.07498\n",
      "训练次数: 4 Epoch: 0080 log_lik= 0.4361555 train_kl= -0.01157 train_loss= 0.44772 train_acc= 0.52984 val_roc= 0.92236 val_ap= 0.93872 time= 0.07797\n",
      "训练次数: 4 Epoch: 0081 log_lik= 0.4356957 train_kl= -0.01159 train_loss= 0.44729 train_acc= 0.53090 val_roc= 0.92332 val_ap= 0.93978 time= 0.07904\n",
      "训练次数: 4 Epoch: 0082 log_lik= 0.4355989 train_kl= -0.01161 train_loss= 0.44721 train_acc= 0.53087 val_roc= 0.92369 val_ap= 0.93998 time= 0.07505\n",
      "训练次数: 4 Epoch: 0083 log_lik= 0.43532354 train_kl= -0.01163 train_loss= 0.44695 train_acc= 0.53124 val_roc= 0.92309 val_ap= 0.93956 time= 0.08098\n",
      "训练次数: 4 Epoch: 0084 log_lik= 0.4346575 train_kl= -0.01164 train_loss= 0.44630 train_acc= 0.53169 val_roc= 0.92244 val_ap= 0.93844 time= 0.09502\n",
      "训练次数: 4 Epoch: 0085 log_lik= 0.43428394 train_kl= -0.01165 train_loss= 0.44594 train_acc= 0.53202 val_roc= 0.92294 val_ap= 0.93883 time= 0.07999\n",
      "训练次数: 4 Epoch: 0086 log_lik= 0.4338708 train_kl= -0.01167 train_loss= 0.44554 train_acc= 0.53257 val_roc= 0.92382 val_ap= 0.93968 time= 0.07599\n",
      "训练次数: 4 Epoch: 0087 log_lik= 0.43338653 train_kl= -0.01169 train_loss= 0.44507 train_acc= 0.53205 val_roc= 0.92408 val_ap= 0.93982 time= 0.07403\n",
      "训练次数: 4 Epoch: 0088 log_lik= 0.43307337 train_kl= -0.01170 train_loss= 0.44477 train_acc= 0.53357 val_roc= 0.92374 val_ap= 0.93950 time= 0.08002\n",
      "训练次数: 4 Epoch: 0089 log_lik= 0.43299055 train_kl= -0.01172 train_loss= 0.44471 train_acc= 0.53251 val_roc= 0.92404 val_ap= 0.93963 time= 0.08899\n",
      "训练次数: 4 Epoch: 0090 log_lik= 0.4322836 train_kl= -0.01173 train_loss= 0.44402 train_acc= 0.53339 val_roc= 0.92375 val_ap= 0.93934 time= 0.07498\n",
      "训练次数: 4 Epoch: 0091 log_lik= 0.43240985 train_kl= -0.01175 train_loss= 0.44416 train_acc= 0.53251 val_roc= 0.92417 val_ap= 0.93952 time= 0.08001\n",
      "训练次数: 4 Epoch: 0092 log_lik= 0.43202335 train_kl= -0.01177 train_loss= 0.44379 train_acc= 0.53259 val_roc= 0.92401 val_ap= 0.93945 time= 0.09501\n",
      "训练次数: 4 Epoch: 0093 log_lik= 0.43189538 train_kl= -0.01179 train_loss= 0.44368 train_acc= 0.53336 val_roc= 0.92397 val_ap= 0.93940 time= 0.07511\n",
      "训练次数: 4 Epoch: 0094 log_lik= 0.43143883 train_kl= -0.01180 train_loss= 0.44324 train_acc= 0.53281 val_roc= 0.92411 val_ap= 0.93932 time= 0.07493\n",
      "训练次数: 4 Epoch: 0095 log_lik= 0.43099695 train_kl= -0.01181 train_loss= 0.44280 train_acc= 0.53313 val_roc= 0.92395 val_ap= 0.93928 time= 0.07803\n",
      "训练次数: 4 Epoch: 0096 log_lik= 0.43084958 train_kl= -0.01182 train_loss= 0.44267 train_acc= 0.53353 val_roc= 0.92414 val_ap= 0.93939 time= 0.08004\n",
      "训练次数: 4 Epoch: 0097 log_lik= 0.4307162 train_kl= -0.01183 train_loss= 0.44255 train_acc= 0.53238 val_roc= 0.92475 val_ap= 0.93959 time= 0.07898\n",
      "训练次数: 4 Epoch: 0098 log_lik= 0.43077844 train_kl= -0.01183 train_loss= 0.44261 train_acc= 0.53262 val_roc= 0.92417 val_ap= 0.93905 time= 0.07501\n",
      "训练次数: 4 Epoch: 0099 log_lik= 0.4301141 train_kl= -0.01184 train_loss= 0.44196 train_acc= 0.53380 val_roc= 0.92354 val_ap= 0.93881 time= 0.07693\n",
      "训练次数: 4 Epoch: 0100 log_lik= 0.4304556 train_kl= -0.01186 train_loss= 0.44232 train_acc= 0.53249 val_roc= 0.92446 val_ap= 0.93985 time= 0.07652\n",
      "Optimization Finished!\n",
      "训练次数: 4 ROC score: 0.8941630150254385\n",
      "训练次数: 4 AP score: 0.9035241228361783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0001 log_lik= 1.7208997 train_kl= -0.00003 train_loss= 1.72093 train_acc= 0.49826 val_roc= 0.63314 val_ap= 0.67932 time= 1.41334\n",
      "训练次数: 5 Epoch: 0002 log_lik= 1.5107833 train_kl= -0.00024 train_loss= 1.51102 train_acc= 0.47006 val_roc= 0.62355 val_ap= 0.67744 time= 0.09460\n",
      "训练次数: 5 Epoch: 0003 log_lik= 1.3342867 train_kl= -0.00073 train_loss= 1.33502 train_acc= 0.39692 val_roc= 0.62460 val_ap= 0.68000 time= 0.07568\n",
      "训练次数: 5 Epoch: 0004 log_lik= 1.2092116 train_kl= -0.00113 train_loss= 1.21034 train_acc= 0.35025 val_roc= 0.63257 val_ap= 0.68925 time= 0.09261\n",
      "训练次数: 5 Epoch: 0005 log_lik= 1.0781503 train_kl= -0.00138 train_loss= 1.07953 train_acc= 0.35696 val_roc= 0.64630 val_ap= 0.70205 time= 0.07471\n",
      "训练次数: 5 Epoch: 0006 log_lik= 0.9583133 train_kl= -0.00171 train_loss= 0.96003 train_acc= 0.35425 val_roc= 0.66479 val_ap= 0.71809 time= 0.08280\n",
      "训练次数: 5 Epoch: 0007 log_lik= 0.84625685 train_kl= -0.00219 train_loss= 0.84845 train_acc= 0.36790 val_roc= 0.68756 val_ap= 0.73947 time= 0.07868\n",
      "训练次数: 5 Epoch: 0008 log_lik= 0.7716531 train_kl= -0.00289 train_loss= 0.77454 train_acc= 0.36969 val_roc= 0.72303 val_ap= 0.76282 time= 0.08026\n",
      "训练次数: 5 Epoch: 0009 log_lik= 0.7259767 train_kl= -0.00380 train_loss= 0.72978 train_acc= 0.35379 val_roc= 0.72342 val_ap= 0.74780 time= 0.07785\n",
      "训练次数: 5 Epoch: 0010 log_lik= 0.70323145 train_kl= -0.00488 train_loss= 0.70811 train_acc= 0.26008 val_roc= 0.73753 val_ap= 0.75254 time= 0.07699\n",
      "训练次数: 5 Epoch: 0011 log_lik= 0.69435656 train_kl= -0.00596 train_loss= 0.70031 train_acc= 0.20217 val_roc= 0.79505 val_ap= 0.79959 time= 0.07405\n",
      "训练次数: 5 Epoch: 0012 log_lik= 0.6742237 train_kl= -0.00690 train_loss= 0.68113 train_acc= 0.21256 val_roc= 0.82387 val_ap= 0.82521 time= 0.07595\n",
      "训练次数: 5 Epoch: 0013 log_lik= 0.6602895 train_kl= -0.00778 train_loss= 0.66807 train_acc= 0.24647 val_roc= 0.83059 val_ap= 0.82350 time= 0.08210\n",
      "训练次数: 5 Epoch: 0014 log_lik= 0.6473626 train_kl= -0.00863 train_loss= 0.65599 train_acc= 0.26291 val_roc= 0.84557 val_ap= 0.83627 time= 0.09092\n",
      "训练次数: 5 Epoch: 0015 log_lik= 0.6302136 train_kl= -0.00944 train_loss= 0.63966 train_acc= 0.28779 val_roc= 0.84560 val_ap= 0.83704 time= 0.08004\n",
      "训练次数: 5 Epoch: 0016 log_lik= 0.6154976 train_kl= -0.01022 train_loss= 0.62572 train_acc= 0.31070 val_roc= 0.84023 val_ap= 0.83199 time= 0.09097\n",
      "训练次数: 5 Epoch: 0017 log_lik= 0.60492855 train_kl= -0.01093 train_loss= 0.61586 train_acc= 0.34266 val_roc= 0.85122 val_ap= 0.84202 time= 0.07799\n",
      "训练次数: 5 Epoch: 0018 log_lik= 0.58672416 train_kl= -0.01153 train_loss= 0.59825 train_acc= 0.39126 val_roc= 0.85738 val_ap= 0.84675 time= 0.08299\n",
      "训练次数: 5 Epoch: 0019 log_lik= 0.57459277 train_kl= -0.01204 train_loss= 0.58664 train_acc= 0.43135 val_roc= 0.85796 val_ap= 0.84746 time= 0.07728\n",
      "训练次数: 5 Epoch: 0020 log_lik= 0.5680813 train_kl= -0.01250 train_loss= 0.58058 train_acc= 0.45148 val_roc= 0.86238 val_ap= 0.85641 time= 0.07675\n",
      "训练次数: 5 Epoch: 0021 log_lik= 0.5600447 train_kl= -0.01289 train_loss= 0.57294 train_acc= 0.45891 val_roc= 0.86555 val_ap= 0.86319 time= 0.07600\n",
      "训练次数: 5 Epoch: 0022 log_lik= 0.5529409 train_kl= -0.01320 train_loss= 0.56614 train_acc= 0.46616 val_roc= 0.86566 val_ap= 0.86458 time= 0.07800\n",
      "训练次数: 5 Epoch: 0023 log_lik= 0.54642105 train_kl= -0.01340 train_loss= 0.55982 train_acc= 0.48078 val_roc= 0.86718 val_ap= 0.86806 time= 0.07701\n",
      "训练次数: 5 Epoch: 0024 log_lik= 0.53764695 train_kl= -0.01355 train_loss= 0.55119 train_acc= 0.49358 val_roc= 0.86957 val_ap= 0.87037 time= 0.08197\n",
      "训练次数: 5 Epoch: 0025 log_lik= 0.5301973 train_kl= -0.01366 train_loss= 0.54386 train_acc= 0.49819 val_roc= 0.87419 val_ap= 0.87282 time= 0.08110\n",
      "训练次数: 5 Epoch: 0026 log_lik= 0.5254145 train_kl= -0.01377 train_loss= 0.53918 train_acc= 0.49838 val_roc= 0.87801 val_ap= 0.87603 time= 0.07691\n",
      "训练次数: 5 Epoch: 0027 log_lik= 0.5173808 train_kl= -0.01384 train_loss= 0.53122 train_acc= 0.49936 val_roc= 0.87814 val_ap= 0.87604 time= 0.07515\n",
      "训练次数: 5 Epoch: 0028 log_lik= 0.51224256 train_kl= -0.01387 train_loss= 0.52611 train_acc= 0.50067 val_roc= 0.87919 val_ap= 0.87897 time= 0.07893\n",
      "训练次数: 5 Epoch: 0029 log_lik= 0.50666064 train_kl= -0.01386 train_loss= 0.52052 train_acc= 0.50143 val_roc= 0.88041 val_ap= 0.88129 time= 0.07592\n",
      "训练次数: 5 Epoch: 0030 log_lik= 0.5032838 train_kl= -0.01382 train_loss= 0.51710 train_acc= 0.50160 val_roc= 0.88222 val_ap= 0.88352 time= 0.07701\n",
      "训练次数: 5 Epoch: 0031 log_lik= 0.4997828 train_kl= -0.01376 train_loss= 0.51355 train_acc= 0.50326 val_roc= 0.88457 val_ap= 0.88642 time= 0.07803\n",
      "训练次数: 5 Epoch: 0032 log_lik= 0.49630153 train_kl= -0.01370 train_loss= 0.51000 train_acc= 0.50774 val_roc= 0.88645 val_ap= 0.88859 time= 0.07602\n",
      "训练次数: 5 Epoch: 0033 log_lik= 0.49274194 train_kl= -0.01363 train_loss= 0.50637 train_acc= 0.50893 val_roc= 0.88807 val_ap= 0.89113 time= 0.07399\n",
      "训练次数: 5 Epoch: 0034 log_lik= 0.490104 train_kl= -0.01353 train_loss= 0.50364 train_acc= 0.50912 val_roc= 0.89018 val_ap= 0.89471 time= 0.07749\n",
      "训练次数: 5 Epoch: 0035 log_lik= 0.48752034 train_kl= -0.01343 train_loss= 0.50095 train_acc= 0.50903 val_roc= 0.89264 val_ap= 0.89772 time= 0.07605\n",
      "训练次数: 5 Epoch: 0036 log_lik= 0.4849945 train_kl= -0.01333 train_loss= 0.49832 train_acc= 0.51143 val_roc= 0.89546 val_ap= 0.90047 time= 0.07595\n",
      "训练次数: 5 Epoch: 0037 log_lik= 0.48272282 train_kl= -0.01323 train_loss= 0.49595 train_acc= 0.51338 val_roc= 0.89812 val_ap= 0.90299 time= 0.07696\n",
      "训练次数: 5 Epoch: 0038 log_lik= 0.48148158 train_kl= -0.01314 train_loss= 0.49462 train_acc= 0.51223 val_roc= 0.90059 val_ap= 0.90616 time= 0.07448\n",
      "训练次数: 5 Epoch: 0039 log_lik= 0.47912973 train_kl= -0.01305 train_loss= 0.49218 train_acc= 0.51315 val_roc= 0.90241 val_ap= 0.90809 time= 0.07499\n",
      "训练次数: 5 Epoch: 0040 log_lik= 0.47573993 train_kl= -0.01296 train_loss= 0.48870 train_acc= 0.51620 val_roc= 0.90483 val_ap= 0.91110 time= 0.07696\n",
      "训练次数: 5 Epoch: 0041 log_lik= 0.47512063 train_kl= -0.01286 train_loss= 0.48798 train_acc= 0.51630 val_roc= 0.90575 val_ap= 0.91186 time= 0.07704\n",
      "训练次数: 5 Epoch: 0042 log_lik= 0.47311485 train_kl= -0.01277 train_loss= 0.48588 train_acc= 0.51749 val_roc= 0.90565 val_ap= 0.91178 time= 0.07652\n",
      "训练次数: 5 Epoch: 0043 log_lik= 0.47194675 train_kl= -0.01267 train_loss= 0.48462 train_acc= 0.51884 val_roc= 0.90671 val_ap= 0.91265 time= 0.07868\n",
      "训练次数: 5 Epoch: 0044 log_lik= 0.47127873 train_kl= -0.01258 train_loss= 0.48386 train_acc= 0.51831 val_roc= 0.90834 val_ap= 0.91447 time= 0.07783\n",
      "训练次数: 5 Epoch: 0045 log_lik= 0.47015816 train_kl= -0.01247 train_loss= 0.48263 train_acc= 0.51891 val_roc= 0.90976 val_ap= 0.91627 time= 0.07492\n",
      "训练次数: 5 Epoch: 0046 log_lik= 0.46801195 train_kl= -0.01237 train_loss= 0.48038 train_acc= 0.52047 val_roc= 0.91088 val_ap= 0.91744 time= 0.07807\n",
      "训练次数: 5 Epoch: 0047 log_lik= 0.46576476 train_kl= -0.01226 train_loss= 0.47803 train_acc= 0.52111 val_roc= 0.91181 val_ap= 0.91845 time= 0.07905\n",
      "训练次数: 5 Epoch: 0048 log_lik= 0.4645233 train_kl= -0.01216 train_loss= 0.47669 train_acc= 0.52209 val_roc= 0.91223 val_ap= 0.91847 time= 0.08196\n",
      "训练次数: 5 Epoch: 0049 log_lik= 0.46457025 train_kl= -0.01206 train_loss= 0.47663 train_acc= 0.52155 val_roc= 0.91304 val_ap= 0.91917 time= 0.07896\n",
      "训练次数: 5 Epoch: 0050 log_lik= 0.46372226 train_kl= -0.01195 train_loss= 0.47567 train_acc= 0.52188 val_roc= 0.91385 val_ap= 0.92030 time= 0.07446\n",
      "训练次数: 5 Epoch: 0051 log_lik= 0.4619106 train_kl= -0.01183 train_loss= 0.47375 train_acc= 0.52316 val_roc= 0.91378 val_ap= 0.92057 time= 0.08502\n",
      "训练次数: 5 Epoch: 0052 log_lik= 0.46016058 train_kl= -0.01173 train_loss= 0.47189 train_acc= 0.52429 val_roc= 0.91372 val_ap= 0.92032 time= 0.08002\n",
      "训练次数: 5 Epoch: 0053 log_lik= 0.45971888 train_kl= -0.01164 train_loss= 0.47136 train_acc= 0.52365 val_roc= 0.91350 val_ap= 0.92004 time= 0.07798\n",
      "训练次数: 5 Epoch: 0054 log_lik= 0.45941523 train_kl= -0.01157 train_loss= 0.47099 train_acc= 0.52524 val_roc= 0.91373 val_ap= 0.92019 time= 0.08007\n",
      "训练次数: 5 Epoch: 0055 log_lik= 0.45842242 train_kl= -0.01151 train_loss= 0.46993 train_acc= 0.52464 val_roc= 0.91496 val_ap= 0.92144 time= 0.07491\n",
      "训练次数: 5 Epoch: 0056 log_lik= 0.45749342 train_kl= -0.01145 train_loss= 0.46894 train_acc= 0.52480 val_roc= 0.91576 val_ap= 0.92247 time= 0.07705\n",
      "训练次数: 5 Epoch: 0057 log_lik= 0.45705792 train_kl= -0.01139 train_loss= 0.46844 train_acc= 0.52417 val_roc= 0.91570 val_ap= 0.92244 time= 0.07501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0058 log_lik= 0.45626384 train_kl= -0.01133 train_loss= 0.46759 train_acc= 0.52370 val_roc= 0.91482 val_ap= 0.92151 time= 0.07599\n",
      "训练次数: 5 Epoch: 0059 log_lik= 0.45565197 train_kl= -0.01127 train_loss= 0.46693 train_acc= 0.52429 val_roc= 0.91434 val_ap= 0.92099 time= 0.07500\n",
      "训练次数: 5 Epoch: 0060 log_lik= 0.45472243 train_kl= -0.01122 train_loss= 0.46594 train_acc= 0.52486 val_roc= 0.91433 val_ap= 0.92086 time= 0.08132\n",
      "训练次数: 5 Epoch: 0061 log_lik= 0.4546371 train_kl= -0.01117 train_loss= 0.46581 train_acc= 0.52449 val_roc= 0.91537 val_ap= 0.92176 time= 0.08482\n",
      "训练次数: 5 Epoch: 0062 log_lik= 0.45458907 train_kl= -0.01113 train_loss= 0.46572 train_acc= 0.52280 val_roc= 0.91687 val_ap= 0.92380 time= 0.07492\n",
      "训练次数: 5 Epoch: 0063 log_lik= 0.45367724 train_kl= -0.01109 train_loss= 0.46477 train_acc= 0.52427 val_roc= 0.91723 val_ap= 0.92415 time= 0.08300\n",
      "训练次数: 5 Epoch: 0064 log_lik= 0.45289153 train_kl= -0.01106 train_loss= 0.46395 train_acc= 0.52407 val_roc= 0.91738 val_ap= 0.92464 time= 0.08907\n",
      "训练次数: 5 Epoch: 0065 log_lik= 0.45205513 train_kl= -0.01103 train_loss= 0.46309 train_acc= 0.52407 val_roc= 0.91681 val_ap= 0.92416 time= 0.07695\n",
      "训练次数: 5 Epoch: 0066 log_lik= 0.45195743 train_kl= -0.01100 train_loss= 0.46296 train_acc= 0.52466 val_roc= 0.91661 val_ap= 0.92415 time= 0.07998\n",
      "训练次数: 5 Epoch: 0067 log_lik= 0.45146585 train_kl= -0.01097 train_loss= 0.46243 train_acc= 0.52456 val_roc= 0.91693 val_ap= 0.92477 time= 0.08800\n",
      "训练次数: 5 Epoch: 0068 log_lik= 0.4510765 train_kl= -0.01095 train_loss= 0.46202 train_acc= 0.52448 val_roc= 0.91785 val_ap= 0.92568 time= 0.07706\n",
      "训练次数: 5 Epoch: 0069 log_lik= 0.44974008 train_kl= -0.01093 train_loss= 0.46067 train_acc= 0.52536 val_roc= 0.91827 val_ap= 0.92606 time= 0.07548\n",
      "训练次数: 5 Epoch: 0070 log_lik= 0.449642 train_kl= -0.01091 train_loss= 0.46056 train_acc= 0.52477 val_roc= 0.91824 val_ap= 0.92622 time= 0.09004\n",
      "训练次数: 5 Epoch: 0071 log_lik= 0.4491688 train_kl= -0.01089 train_loss= 0.46006 train_acc= 0.52531 val_roc= 0.91778 val_ap= 0.92628 time= 0.08394\n",
      "训练次数: 5 Epoch: 0072 log_lik= 0.44869715 train_kl= -0.01087 train_loss= 0.45957 train_acc= 0.52555 val_roc= 0.91756 val_ap= 0.92653 time= 0.08101\n",
      "训练次数: 5 Epoch: 0073 log_lik= 0.44828156 train_kl= -0.01086 train_loss= 0.45914 train_acc= 0.52590 val_roc= 0.91690 val_ap= 0.92587 time= 0.08300\n",
      "训练次数: 5 Epoch: 0074 log_lik= 0.44799232 train_kl= -0.01086 train_loss= 0.45885 train_acc= 0.52622 val_roc= 0.91722 val_ap= 0.92624 time= 0.09001\n",
      "训练次数: 5 Epoch: 0075 log_lik= 0.4477682 train_kl= -0.01087 train_loss= 0.45863 train_acc= 0.52598 val_roc= 0.91746 val_ap= 0.92661 time= 0.07498\n",
      "训练次数: 5 Epoch: 0076 log_lik= 0.44685525 train_kl= -0.01088 train_loss= 0.45773 train_acc= 0.52662 val_roc= 0.91739 val_ap= 0.92672 time= 0.08924\n",
      "训练次数: 5 Epoch: 0077 log_lik= 0.4467359 train_kl= -0.01088 train_loss= 0.45762 train_acc= 0.52599 val_roc= 0.91709 val_ap= 0.92649 time= 0.07475\n",
      "训练次数: 5 Epoch: 0078 log_lik= 0.44643298 train_kl= -0.01089 train_loss= 0.45732 train_acc= 0.52768 val_roc= 0.91647 val_ap= 0.92571 time= 0.07700\n",
      "训练次数: 5 Epoch: 0079 log_lik= 0.44560975 train_kl= -0.01091 train_loss= 0.45652 train_acc= 0.52669 val_roc= 0.91616 val_ap= 0.92538 time= 0.08333\n",
      "训练次数: 5 Epoch: 0080 log_lik= 0.4456683 train_kl= -0.01093 train_loss= 0.45660 train_acc= 0.52834 val_roc= 0.91654 val_ap= 0.92597 time= 0.07672\n",
      "训练次数: 5 Epoch: 0081 log_lik= 0.44508106 train_kl= -0.01094 train_loss= 0.45602 train_acc= 0.52833 val_roc= 0.91670 val_ap= 0.92657 time= 0.08096\n",
      "训练次数: 5 Epoch: 0082 log_lik= 0.4445885 train_kl= -0.01096 train_loss= 0.45555 train_acc= 0.52795 val_roc= 0.91687 val_ap= 0.92678 time= 0.07611\n",
      "训练次数: 5 Epoch: 0083 log_lik= 0.44427592 train_kl= -0.01098 train_loss= 0.45526 train_acc= 0.52861 val_roc= 0.91668 val_ap= 0.92653 time= 0.07943\n",
      "训练次数: 5 Epoch: 0084 log_lik= 0.44390947 train_kl= -0.01100 train_loss= 0.45491 train_acc= 0.52911 val_roc= 0.91622 val_ap= 0.92642 time= 0.08002\n",
      "训练次数: 5 Epoch: 0085 log_lik= 0.4436309 train_kl= -0.01101 train_loss= 0.45464 train_acc= 0.52905 val_roc= 0.91662 val_ap= 0.92678 time= 0.07695\n",
      "训练次数: 5 Epoch: 0086 log_lik= 0.44319475 train_kl= -0.01104 train_loss= 0.45423 train_acc= 0.53020 val_roc= 0.91687 val_ap= 0.92680 time= 0.07900\n",
      "训练次数: 5 Epoch: 0087 log_lik= 0.44278687 train_kl= -0.01106 train_loss= 0.45385 train_acc= 0.53010 val_roc= 0.91729 val_ap= 0.92716 time= 0.09304\n",
      "训练次数: 5 Epoch: 0088 log_lik= 0.44252822 train_kl= -0.01109 train_loss= 0.45361 train_acc= 0.53033 val_roc= 0.91733 val_ap= 0.92759 time= 0.09394\n",
      "训练次数: 5 Epoch: 0089 log_lik= 0.44231364 train_kl= -0.01110 train_loss= 0.45341 train_acc= 0.52975 val_roc= 0.91671 val_ap= 0.92746 time= 0.07508\n",
      "训练次数: 5 Epoch: 0090 log_lik= 0.44178325 train_kl= -0.01110 train_loss= 0.45289 train_acc= 0.53071 val_roc= 0.91615 val_ap= 0.92713 time= 0.07493\n",
      "训练次数: 5 Epoch: 0091 log_lik= 0.4416039 train_kl= -0.01111 train_loss= 0.45272 train_acc= 0.53069 val_roc= 0.91613 val_ap= 0.92702 time= 0.07705\n",
      "训练次数: 5 Epoch: 0092 log_lik= 0.44078386 train_kl= -0.01113 train_loss= 0.45192 train_acc= 0.53130 val_roc= 0.91680 val_ap= 0.92779 time= 0.07604\n",
      "训练次数: 5 Epoch: 0093 log_lik= 0.4405142 train_kl= -0.01115 train_loss= 0.45166 train_acc= 0.53148 val_roc= 0.91729 val_ap= 0.92827 time= 0.07398\n",
      "训练次数: 5 Epoch: 0094 log_lik= 0.44028735 train_kl= -0.01116 train_loss= 0.45145 train_acc= 0.53238 val_roc= 0.91758 val_ap= 0.92866 time= 0.07697\n",
      "训练次数: 5 Epoch: 0095 log_lik= 0.44023842 train_kl= -0.01117 train_loss= 0.45140 train_acc= 0.53163 val_roc= 0.91694 val_ap= 0.92824 time= 0.07802\n",
      "训练次数: 5 Epoch: 0096 log_lik= 0.43962502 train_kl= -0.01118 train_loss= 0.45080 train_acc= 0.53152 val_roc= 0.91658 val_ap= 0.92767 time= 0.07600\n",
      "训练次数: 5 Epoch: 0097 log_lik= 0.4394476 train_kl= -0.01119 train_loss= 0.45064 train_acc= 0.53251 val_roc= 0.91690 val_ap= 0.92789 time= 0.07807\n",
      "训练次数: 5 Epoch: 0098 log_lik= 0.4397293 train_kl= -0.01120 train_loss= 0.45093 train_acc= 0.53193 val_roc= 0.91709 val_ap= 0.92834 time= 0.07591\n",
      "训练次数: 5 Epoch: 0099 log_lik= 0.43903795 train_kl= -0.01120 train_loss= 0.45024 train_acc= 0.53251 val_roc= 0.91772 val_ap= 0.92887 time= 0.07707\n",
      "训练次数: 5 Epoch: 0100 log_lik= 0.4387424 train_kl= -0.01121 train_loss= 0.44995 train_acc= 0.53284 val_roc= 0.91801 val_ap= 0.92882 time= 0.08603\n",
      "Optimization Finished!\n",
      "训练次数: 5 ROC score: 0.9303709731428839\n",
      "训练次数: 5 AP score: 0.9341659243436699\n",
      "训练次数: 6 Epoch: 0001 log_lik= 1.7276932 train_kl= -0.00006 train_loss= 1.72775 train_acc= 0.49485 val_roc= 0.67509 val_ap= 0.71119 time= 1.42152\n",
      "训练次数: 6 Epoch: 0002 log_lik= 1.3858334 train_kl= -0.00020 train_loss= 1.38603 train_acc= 0.47995 val_roc= 0.67355 val_ap= 0.70729 time= 0.07818\n",
      "训练次数: 6 Epoch: 0003 log_lik= 1.1415648 train_kl= -0.00063 train_loss= 1.14220 train_acc= 0.42776 val_roc= 0.67328 val_ap= 0.70414 time= 0.09114\n",
      "训练次数: 6 Epoch: 0004 log_lik= 0.97487766 train_kl= -0.00136 train_loss= 0.97623 train_acc= 0.35728 val_roc= 0.67656 val_ap= 0.70774 time= 0.09062\n",
      "训练次数: 6 Epoch: 0005 log_lik= 0.85800904 train_kl= -0.00224 train_loss= 0.86025 train_acc= 0.30473 val_roc= 0.68775 val_ap= 0.71722 time= 0.08360\n",
      "训练次数: 6 Epoch: 0006 log_lik= 0.7691554 train_kl= -0.00322 train_loss= 0.77237 train_acc= 0.28987 val_roc= 0.71432 val_ap= 0.73725 time= 0.07823\n",
      "训练次数: 6 Epoch: 0007 log_lik= 0.71874857 train_kl= -0.00428 train_loss= 0.72303 train_acc= 0.31126 val_roc= 0.74908 val_ap= 0.76919 time= 0.07817\n",
      "训练次数: 6 Epoch: 0008 log_lik= 0.70638657 train_kl= -0.00541 train_loss= 0.71180 train_acc= 0.30382 val_roc= 0.75780 val_ap= 0.78511 time= 0.08962\n",
      "训练次数: 6 Epoch: 0009 log_lik= 0.698599 train_kl= -0.00656 train_loss= 0.70516 train_acc= 0.20699 val_roc= 0.76070 val_ap= 0.78991 time= 0.07769\n",
      "训练次数: 6 Epoch: 0010 log_lik= 0.7042294 train_kl= -0.00767 train_loss= 0.71190 train_acc= 0.11820 val_roc= 0.79556 val_ap= 0.81620 time= 0.08165\n",
      "训练次数: 6 Epoch: 0011 log_lik= 0.6849618 train_kl= -0.00864 train_loss= 0.69360 train_acc= 0.14204 val_roc= 0.82066 val_ap= 0.82945 time= 0.08962\n",
      "训练次数: 6 Epoch: 0012 log_lik= 0.6679585 train_kl= -0.00951 train_loss= 0.67747 train_acc= 0.20310 val_roc= 0.82106 val_ap= 0.82667 time= 0.08964\n",
      "训练次数: 6 Epoch: 0013 log_lik= 0.64844894 train_kl= -0.01031 train_loss= 0.65876 train_acc= 0.23816 val_roc= 0.81973 val_ap= 0.82546 time= 0.08965\n",
      "训练次数: 6 Epoch: 0014 log_lik= 0.629721 train_kl= -0.01104 train_loss= 0.64076 train_acc= 0.26280 val_roc= 0.82301 val_ap= 0.82613 time= 0.08766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0015 log_lik= 0.60783035 train_kl= -0.01168 train_loss= 0.61951 train_acc= 0.31587 val_roc= 0.82721 val_ap= 0.82442 time= 0.07593\n",
      "训练次数: 6 Epoch: 0016 log_lik= 0.5846557 train_kl= -0.01224 train_loss= 0.59690 train_acc= 0.40113 val_roc= 0.83021 val_ap= 0.82503 time= 0.09193\n",
      "训练次数: 6 Epoch: 0017 log_lik= 0.56657434 train_kl= -0.01277 train_loss= 0.57934 train_acc= 0.45716 val_roc= 0.83323 val_ap= 0.82715 time= 0.09106\n",
      "训练次数: 6 Epoch: 0018 log_lik= 0.5590066 train_kl= -0.01326 train_loss= 0.57227 train_acc= 0.47395 val_roc= 0.83848 val_ap= 0.83328 time= 0.09096\n",
      "训练次数: 6 Epoch: 0019 log_lik= 0.56071633 train_kl= -0.01370 train_loss= 0.57441 train_acc= 0.46845 val_roc= 0.84519 val_ap= 0.83731 time= 0.07609\n",
      "训练次数: 6 Epoch: 0020 log_lik= 0.5635431 train_kl= -0.01404 train_loss= 0.57759 train_acc= 0.46514 val_roc= 0.85014 val_ap= 0.84175 time= 0.07493\n",
      "训练次数: 6 Epoch: 0021 log_lik= 0.5538507 train_kl= -0.01427 train_loss= 0.56812 train_acc= 0.47583 val_roc= 0.85391 val_ap= 0.84741 time= 0.07505\n",
      "训练次数: 6 Epoch: 0022 log_lik= 0.5363153 train_kl= -0.01440 train_loss= 0.55072 train_acc= 0.49257 val_roc= 0.85582 val_ap= 0.84988 time= 0.09097\n",
      "训练次数: 6 Epoch: 0023 log_lik= 0.52409923 train_kl= -0.01449 train_loss= 0.53859 train_acc= 0.50244 val_roc= 0.85853 val_ap= 0.85339 time= 0.07804\n",
      "训练次数: 6 Epoch: 0024 log_lik= 0.5203561 train_kl= -0.01455 train_loss= 0.53491 train_acc= 0.50171 val_roc= 0.86092 val_ap= 0.85663 time= 0.07609\n",
      "训练次数: 6 Epoch: 0025 log_lik= 0.5197048 train_kl= -0.01460 train_loss= 0.53431 train_acc= 0.49694 val_roc= 0.86297 val_ap= 0.85955 time= 0.08999\n",
      "训练次数: 6 Epoch: 0026 log_lik= 0.5161785 train_kl= -0.01465 train_loss= 0.53083 train_acc= 0.49573 val_roc= 0.86542 val_ap= 0.86219 time= 0.07695\n",
      "训练次数: 6 Epoch: 0027 log_lik= 0.51198655 train_kl= -0.01468 train_loss= 0.52666 train_acc= 0.49969 val_roc= 0.86887 val_ap= 0.86591 time= 0.08458\n",
      "训练次数: 6 Epoch: 0028 log_lik= 0.5078526 train_kl= -0.01469 train_loss= 0.52255 train_acc= 0.50297 val_roc= 0.87302 val_ap= 0.86994 time= 0.07800\n",
      "训练次数: 6 Epoch: 0029 log_lik= 0.5031883 train_kl= -0.01470 train_loss= 0.51788 train_acc= 0.50651 val_roc= 0.87675 val_ap= 0.87362 time= 0.07805\n",
      "训练次数: 6 Epoch: 0030 log_lik= 0.4995407 train_kl= -0.01469 train_loss= 0.51423 train_acc= 0.50697 val_roc= 0.88032 val_ap= 0.87741 time= 0.07791\n",
      "训练次数: 6 Epoch: 0031 log_lik= 0.49663326 train_kl= -0.01467 train_loss= 0.51131 train_acc= 0.50344 val_roc= 0.88378 val_ap= 0.88090 time= 0.07909\n",
      "训练次数: 6 Epoch: 0032 log_lik= 0.494973 train_kl= -0.01465 train_loss= 0.50963 train_acc= 0.50047 val_roc= 0.88697 val_ap= 0.88398 time= 0.09188\n",
      "训练次数: 6 Epoch: 0033 log_lik= 0.49228564 train_kl= -0.01463 train_loss= 0.50692 train_acc= 0.50289 val_roc= 0.88984 val_ap= 0.88684 time= 0.08956\n",
      "训练次数: 6 Epoch: 0034 log_lik= 0.4880319 train_kl= -0.01460 train_loss= 0.50264 train_acc= 0.50759 val_roc= 0.89213 val_ap= 0.88792 time= 0.09561\n",
      "训练次数: 6 Epoch: 0035 log_lik= 0.48554397 train_kl= -0.01457 train_loss= 0.50011 train_acc= 0.51058 val_roc= 0.89456 val_ap= 0.88961 time= 0.08192\n",
      "训练次数: 6 Epoch: 0036 log_lik= 0.4841893 train_kl= -0.01452 train_loss= 0.49871 train_acc= 0.50944 val_roc= 0.89563 val_ap= 0.89032 time= 0.07594\n",
      "训练次数: 6 Epoch: 0037 log_lik= 0.4829463 train_kl= -0.01445 train_loss= 0.49740 train_acc= 0.50991 val_roc= 0.89662 val_ap= 0.89167 time= 0.07599\n",
      "训练次数: 6 Epoch: 0038 log_lik= 0.48080707 train_kl= -0.01437 train_loss= 0.49517 train_acc= 0.51144 val_roc= 0.89753 val_ap= 0.89342 time= 0.07503\n",
      "训练次数: 6 Epoch: 0039 log_lik= 0.4787642 train_kl= -0.01427 train_loss= 0.49303 train_acc= 0.51142 val_roc= 0.89756 val_ap= 0.89387 time= 0.07515\n",
      "训练次数: 6 Epoch: 0040 log_lik= 0.47719875 train_kl= -0.01416 train_loss= 0.49136 train_acc= 0.51162 val_roc= 0.89816 val_ap= 0.89534 time= 0.07866\n",
      "训练次数: 6 Epoch: 0041 log_lik= 0.47531617 train_kl= -0.01405 train_loss= 0.48937 train_acc= 0.51196 val_roc= 0.89805 val_ap= 0.89572 time= 0.07869\n",
      "训练次数: 6 Epoch: 0042 log_lik= 0.47342557 train_kl= -0.01394 train_loss= 0.48736 train_acc= 0.51418 val_roc= 0.89805 val_ap= 0.89642 time= 0.07672\n",
      "训练次数: 6 Epoch: 0043 log_lik= 0.47205973 train_kl= -0.01383 train_loss= 0.48588 train_acc= 0.51548 val_roc= 0.89818 val_ap= 0.89681 time= 0.08481\n",
      "训练次数: 6 Epoch: 0044 log_lik= 0.46998617 train_kl= -0.01371 train_loss= 0.48370 train_acc= 0.51720 val_roc= 0.89855 val_ap= 0.89790 time= 0.07598\n",
      "训练次数: 6 Epoch: 0045 log_lik= 0.46868357 train_kl= -0.01358 train_loss= 0.48227 train_acc= 0.51789 val_roc= 0.89800 val_ap= 0.89813 time= 0.08804\n",
      "训练次数: 6 Epoch: 0046 log_lik= 0.46757922 train_kl= -0.01345 train_loss= 0.48103 train_acc= 0.51825 val_roc= 0.89782 val_ap= 0.89842 time= 0.08496\n",
      "训练次数: 6 Epoch: 0047 log_lik= 0.4663934 train_kl= -0.01333 train_loss= 0.47973 train_acc= 0.51804 val_roc= 0.89787 val_ap= 0.89901 time= 0.07501\n",
      "训练次数: 6 Epoch: 0048 log_lik= 0.46551034 train_kl= -0.01322 train_loss= 0.47873 train_acc= 0.51789 val_roc= 0.89894 val_ap= 0.90105 time= 0.07604\n",
      "训练次数: 6 Epoch: 0049 log_lik= 0.4634317 train_kl= -0.01312 train_loss= 0.47655 train_acc= 0.51945 val_roc= 0.90078 val_ap= 0.90309 time= 0.07499\n",
      "训练次数: 6 Epoch: 0050 log_lik= 0.46294406 train_kl= -0.01301 train_loss= 0.47596 train_acc= 0.52093 val_roc= 0.90170 val_ap= 0.90354 time= 0.07508\n",
      "训练次数: 6 Epoch: 0051 log_lik= 0.46193466 train_kl= -0.01291 train_loss= 0.47485 train_acc= 0.51980 val_roc= 0.90250 val_ap= 0.90418 time= 0.08896\n",
      "训练次数: 6 Epoch: 0052 log_lik= 0.46151143 train_kl= -0.01282 train_loss= 0.47433 train_acc= 0.52023 val_roc= 0.90396 val_ap= 0.90584 time= 0.09101\n",
      "训练次数: 6 Epoch: 0053 log_lik= 0.4603769 train_kl= -0.01273 train_loss= 0.47311 train_acc= 0.51996 val_roc= 0.90515 val_ap= 0.90732 time= 0.09001\n",
      "训练次数: 6 Epoch: 0054 log_lik= 0.4589132 train_kl= -0.01265 train_loss= 0.47157 train_acc= 0.52044 val_roc= 0.90672 val_ap= 0.90868 time= 0.08905\n",
      "训练次数: 6 Epoch: 0055 log_lik= 0.45851967 train_kl= -0.01258 train_loss= 0.47110 train_acc= 0.52035 val_roc= 0.90815 val_ap= 0.91009 time= 0.07793\n",
      "训练次数: 6 Epoch: 0056 log_lik= 0.4581813 train_kl= -0.01250 train_loss= 0.47068 train_acc= 0.51936 val_roc= 0.90935 val_ap= 0.91133 time= 0.07755\n",
      "训练次数: 6 Epoch: 0057 log_lik= 0.45641363 train_kl= -0.01243 train_loss= 0.46885 train_acc= 0.51961 val_roc= 0.91106 val_ap= 0.91286 time= 0.07446\n",
      "训练次数: 6 Epoch: 0058 log_lik= 0.45658484 train_kl= -0.01236 train_loss= 0.46895 train_acc= 0.51940 val_roc= 0.91174 val_ap= 0.91334 time= 0.07998\n",
      "训练次数: 6 Epoch: 0059 log_lik= 0.45486 train_kl= -0.01230 train_loss= 0.46716 train_acc= 0.52012 val_roc= 0.91302 val_ap= 0.91454 time= 0.08207\n",
      "训练次数: 6 Epoch: 0060 log_lik= 0.4538296 train_kl= -0.01224 train_loss= 0.46607 train_acc= 0.52037 val_roc= 0.91435 val_ap= 0.91576 time= 0.07687\n",
      "训练次数: 6 Epoch: 0061 log_lik= 0.45360863 train_kl= -0.01217 train_loss= 0.46578 train_acc= 0.52009 val_roc= 0.91540 val_ap= 0.91687 time= 0.07748\n",
      "训练次数: 6 Epoch: 0062 log_lik= 0.45245662 train_kl= -0.01211 train_loss= 0.46457 train_acc= 0.51948 val_roc= 0.91665 val_ap= 0.91841 time= 0.07905\n",
      "训练次数: 6 Epoch: 0063 log_lik= 0.45251524 train_kl= -0.01206 train_loss= 0.46457 train_acc= 0.52054 val_roc= 0.91717 val_ap= 0.91850 time= 0.07507\n",
      "训练次数: 6 Epoch: 0064 log_lik= 0.4513364 train_kl= -0.01202 train_loss= 0.46335 train_acc= 0.52086 val_roc= 0.91834 val_ap= 0.91944 time= 0.07798\n",
      "训练次数: 6 Epoch: 0065 log_lik= 0.45114166 train_kl= -0.01198 train_loss= 0.46312 train_acc= 0.52028 val_roc= 0.91950 val_ap= 0.92059 time= 0.07903\n",
      "训练次数: 6 Epoch: 0066 log_lik= 0.45056522 train_kl= -0.01194 train_loss= 0.46251 train_acc= 0.52051 val_roc= 0.91992 val_ap= 0.92098 time= 0.07606\n",
      "训练次数: 6 Epoch: 0067 log_lik= 0.45048958 train_kl= -0.01191 train_loss= 0.46240 train_acc= 0.51997 val_roc= 0.92093 val_ap= 0.92175 time= 0.07600\n",
      "训练次数: 6 Epoch: 0068 log_lik= 0.44975734 train_kl= -0.01190 train_loss= 0.46166 train_acc= 0.52057 val_roc= 0.92179 val_ap= 0.92250 time= 0.07492\n",
      "训练次数: 6 Epoch: 0069 log_lik= 0.44904727 train_kl= -0.01189 train_loss= 0.46093 train_acc= 0.52054 val_roc= 0.92319 val_ap= 0.92353 time= 0.07507\n",
      "训练次数: 6 Epoch: 0070 log_lik= 0.4484161 train_kl= -0.01188 train_loss= 0.46029 train_acc= 0.52049 val_roc= 0.92327 val_ap= 0.92354 time= 0.08995\n",
      "训练次数: 6 Epoch: 0071 log_lik= 0.44811228 train_kl= -0.01188 train_loss= 0.45999 train_acc= 0.52131 val_roc= 0.92426 val_ap= 0.92422 time= 0.07900\n",
      "训练次数: 6 Epoch: 0072 log_lik= 0.447627 train_kl= -0.01189 train_loss= 0.45952 train_acc= 0.52183 val_roc= 0.92511 val_ap= 0.92518 time= 0.07998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0073 log_lik= 0.4471254 train_kl= -0.01189 train_loss= 0.45902 train_acc= 0.52300 val_roc= 0.92543 val_ap= 0.92489 time= 0.07703\n",
      "训练次数: 6 Epoch: 0074 log_lik= 0.44663903 train_kl= -0.01189 train_loss= 0.45853 train_acc= 0.52230 val_roc= 0.92563 val_ap= 0.92520 time= 0.07415\n",
      "训练次数: 6 Epoch: 0075 log_lik= 0.4459508 train_kl= -0.01190 train_loss= 0.45785 train_acc= 0.52254 val_roc= 0.92682 val_ap= 0.92624 time= 0.07496\n",
      "训练次数: 6 Epoch: 0076 log_lik= 0.44540203 train_kl= -0.01192 train_loss= 0.45732 train_acc= 0.52390 val_roc= 0.92630 val_ap= 0.92553 time= 0.09501\n",
      "训练次数: 6 Epoch: 0077 log_lik= 0.44517913 train_kl= -0.01193 train_loss= 0.45711 train_acc= 0.52343 val_roc= 0.92576 val_ap= 0.92479 time= 0.08001\n",
      "训练次数: 6 Epoch: 0078 log_lik= 0.44398728 train_kl= -0.01193 train_loss= 0.45592 train_acc= 0.52439 val_roc= 0.92696 val_ap= 0.92602 time= 0.08397\n",
      "训练次数: 6 Epoch: 0079 log_lik= 0.44374496 train_kl= -0.01194 train_loss= 0.45568 train_acc= 0.52466 val_roc= 0.92755 val_ap= 0.92652 time= 0.08293\n",
      "训练次数: 6 Epoch: 0080 log_lik= 0.4431395 train_kl= -0.01195 train_loss= 0.45509 train_acc= 0.52528 val_roc= 0.92755 val_ap= 0.92606 time= 0.08099\n",
      "训练次数: 6 Epoch: 0081 log_lik= 0.4424318 train_kl= -0.01197 train_loss= 0.45440 train_acc= 0.52634 val_roc= 0.92680 val_ap= 0.92552 time= 0.07450\n",
      "训练次数: 6 Epoch: 0082 log_lik= 0.4417243 train_kl= -0.01197 train_loss= 0.45369 train_acc= 0.52678 val_roc= 0.92716 val_ap= 0.92601 time= 0.07789\n",
      "训练次数: 6 Epoch: 0083 log_lik= 0.441603 train_kl= -0.01198 train_loss= 0.45359 train_acc= 0.52605 val_roc= 0.92755 val_ap= 0.92637 time= 0.08000\n",
      "训练次数: 6 Epoch: 0084 log_lik= 0.4401712 train_kl= -0.01200 train_loss= 0.45217 train_acc= 0.52776 val_roc= 0.92786 val_ap= 0.92652 time= 0.07495\n",
      "训练次数: 6 Epoch: 0085 log_lik= 0.44060287 train_kl= -0.01201 train_loss= 0.45261 train_acc= 0.52794 val_roc= 0.92748 val_ap= 0.92611 time= 0.07701\n",
      "训练次数: 6 Epoch: 0086 log_lik= 0.43965438 train_kl= -0.01202 train_loss= 0.45168 train_acc= 0.52807 val_roc= 0.92698 val_ap= 0.92579 time= 0.07702\n",
      "训练次数: 6 Epoch: 0087 log_lik= 0.43971568 train_kl= -0.01203 train_loss= 0.45175 train_acc= 0.52815 val_roc= 0.92589 val_ap= 0.92487 time= 0.08199\n",
      "训练次数: 6 Epoch: 0088 log_lik= 0.439617 train_kl= -0.01204 train_loss= 0.45165 train_acc= 0.52864 val_roc= 0.92596 val_ap= 0.92498 time= 0.08196\n",
      "训练次数: 6 Epoch: 0089 log_lik= 0.438412 train_kl= -0.01205 train_loss= 0.45046 train_acc= 0.52995 val_roc= 0.92659 val_ap= 0.92547 time= 0.08000\n",
      "训练次数: 6 Epoch: 0090 log_lik= 0.43842772 train_kl= -0.01206 train_loss= 0.45049 train_acc= 0.52916 val_roc= 0.92635 val_ap= 0.92569 time= 0.08203\n",
      "训练次数: 6 Epoch: 0091 log_lik= 0.43824065 train_kl= -0.01207 train_loss= 0.45031 train_acc= 0.52971 val_roc= 0.92469 val_ap= 0.92411 time= 0.07900\n",
      "训练次数: 6 Epoch: 0092 log_lik= 0.43737662 train_kl= -0.01206 train_loss= 0.44944 train_acc= 0.52986 val_roc= 0.92385 val_ap= 0.92333 time= 0.07701\n",
      "训练次数: 6 Epoch: 0093 log_lik= 0.4370214 train_kl= -0.01207 train_loss= 0.44909 train_acc= 0.52993 val_roc= 0.92462 val_ap= 0.92461 time= 0.08254\n",
      "训练次数: 6 Epoch: 0094 log_lik= 0.43707964 train_kl= -0.01206 train_loss= 0.44914 train_acc= 0.53017 val_roc= 0.92502 val_ap= 0.92506 time= 0.08092\n",
      "训练次数: 6 Epoch: 0095 log_lik= 0.43633333 train_kl= -0.01207 train_loss= 0.44840 train_acc= 0.53012 val_roc= 0.92430 val_ap= 0.92443 time= 0.08000\n",
      "训练次数: 6 Epoch: 0096 log_lik= 0.43635815 train_kl= -0.01206 train_loss= 0.44842 train_acc= 0.52986 val_roc= 0.92320 val_ap= 0.92367 time= 0.07698\n",
      "训练次数: 6 Epoch: 0097 log_lik= 0.4356144 train_kl= -0.01206 train_loss= 0.44767 train_acc= 0.53117 val_roc= 0.92301 val_ap= 0.92384 time= 0.07700\n",
      "训练次数: 6 Epoch: 0098 log_lik= 0.43559584 train_kl= -0.01206 train_loss= 0.44766 train_acc= 0.53027 val_roc= 0.92303 val_ap= 0.92369 time= 0.07500\n",
      "训练次数: 6 Epoch: 0099 log_lik= 0.43513015 train_kl= -0.01206 train_loss= 0.44719 train_acc= 0.53063 val_roc= 0.92322 val_ap= 0.92402 time= 0.07805\n",
      "训练次数: 6 Epoch: 0100 log_lik= 0.4348455 train_kl= -0.01207 train_loss= 0.44691 train_acc= 0.53111 val_roc= 0.92329 val_ap= 0.92448 time= 0.08299\n",
      "Optimization Finished!\n",
      "训练次数: 6 ROC score: 0.9075393639123029\n",
      "训练次数: 6 AP score: 0.9171554727602627\n",
      "训练次数: 7 Epoch: 0001 log_lik= 1.7730775 train_kl= -0.00004 train_loss= 1.77312 train_acc= 0.49691 val_roc= 0.69658 val_ap= 0.71535 time= 1.39967\n",
      "训练次数: 7 Epoch: 0002 log_lik= 1.497609 train_kl= -0.00016 train_loss= 1.49777 train_acc= 0.48489 val_roc= 0.68257 val_ap= 0.70396 time= 0.09312\n",
      "训练次数: 7 Epoch: 0003 log_lik= 1.3329961 train_kl= -0.00041 train_loss= 1.33341 train_acc= 0.44091 val_roc= 0.67883 val_ap= 0.70312 time= 0.08814\n",
      "训练次数: 7 Epoch: 0004 log_lik= 1.1839542 train_kl= -0.00085 train_loss= 1.18480 train_acc= 0.36599 val_roc= 0.68377 val_ap= 0.70876 time= 0.09064\n",
      "训练次数: 7 Epoch: 0005 log_lik= 1.0707501 train_kl= -0.00136 train_loss= 1.07211 train_acc= 0.30493 val_roc= 0.69376 val_ap= 0.71770 time= 0.07865\n",
      "训练次数: 7 Epoch: 0006 log_lik= 0.9626709 train_kl= -0.00186 train_loss= 0.96453 train_acc= 0.29825 val_roc= 0.70808 val_ap= 0.72936 time= 0.07670\n",
      "训练次数: 7 Epoch: 0007 log_lik= 0.8721587 train_kl= -0.00240 train_loss= 0.87456 train_acc= 0.30272 val_roc= 0.72161 val_ap= 0.73907 time= 0.08365\n",
      "训练次数: 7 Epoch: 0008 log_lik= 0.80576885 train_kl= -0.00302 train_loss= 0.80878 train_acc= 0.30121 val_roc= 0.73725 val_ap= 0.74909 time= 0.07667\n",
      "训练次数: 7 Epoch: 0009 log_lik= 0.7608238 train_kl= -0.00372 train_loss= 0.76455 train_acc= 0.29643 val_roc= 0.74970 val_ap= 0.75838 time= 0.07269\n",
      "训练次数: 7 Epoch: 0010 log_lik= 0.729199 train_kl= -0.00450 train_loss= 0.73370 train_acc= 0.27639 val_roc= 0.75752 val_ap= 0.76333 time= 0.07767\n",
      "训练次数: 7 Epoch: 0011 log_lik= 0.7095615 train_kl= -0.00532 train_loss= 0.71488 train_acc= 0.24835 val_roc= 0.76644 val_ap= 0.76990 time= 0.07667\n",
      "训练次数: 7 Epoch: 0012 log_lik= 0.6980323 train_kl= -0.00612 train_loss= 0.70416 train_acc= 0.23645 val_roc= 0.77750 val_ap= 0.77965 time= 0.08026\n",
      "训练次数: 7 Epoch: 0013 log_lik= 0.68518263 train_kl= -0.00689 train_loss= 0.69207 train_acc= 0.22189 val_roc= 0.79385 val_ap= 0.79260 time= 0.07671\n",
      "训练次数: 7 Epoch: 0014 log_lik= 0.672843 train_kl= -0.00760 train_loss= 0.68044 train_acc= 0.22848 val_roc= 0.81700 val_ap= 0.81341 time= 0.07632\n",
      "训练次数: 7 Epoch: 0015 log_lik= 0.65821975 train_kl= -0.00823 train_loss= 0.66645 train_acc= 0.22397 val_roc= 0.83659 val_ap= 0.83118 time= 0.07864\n",
      "训练次数: 7 Epoch: 0016 log_lik= 0.6414735 train_kl= -0.00879 train_loss= 0.65026 train_acc= 0.24655 val_roc= 0.84745 val_ap= 0.83977 time= 0.07426\n",
      "训练次数: 7 Epoch: 0017 log_lik= 0.62144595 train_kl= -0.00928 train_loss= 0.63073 train_acc= 0.29001 val_roc= 0.84813 val_ap= 0.83976 time= 0.07570\n",
      "训练次数: 7 Epoch: 0018 log_lik= 0.60376954 train_kl= -0.00973 train_loss= 0.61350 train_acc= 0.33183 val_roc= 0.84697 val_ap= 0.83909 time= 0.07273\n",
      "训练次数: 7 Epoch: 0019 log_lik= 0.58849186 train_kl= -0.01015 train_loss= 0.59864 train_acc= 0.37214 val_roc= 0.84570 val_ap= 0.83986 time= 0.07938\n",
      "训练次数: 7 Epoch: 0020 log_lik= 0.5742312 train_kl= -0.01053 train_loss= 0.58476 train_acc= 0.41381 val_roc= 0.84359 val_ap= 0.83841 time= 0.07493\n",
      "训练次数: 7 Epoch: 0021 log_lik= 0.56716037 train_kl= -0.01087 train_loss= 0.57803 train_acc= 0.43826 val_roc= 0.84408 val_ap= 0.84015 time= 0.08464\n",
      "训练次数: 7 Epoch: 0022 log_lik= 0.5622783 train_kl= -0.01116 train_loss= 0.57344 train_acc= 0.45939 val_roc= 0.84581 val_ap= 0.84336 time= 0.07469\n",
      "训练次数: 7 Epoch: 0023 log_lik= 0.5589392 train_kl= -0.01140 train_loss= 0.57034 train_acc= 0.47148 val_roc= 0.84870 val_ap= 0.84711 time= 0.07571\n",
      "训练次数: 7 Epoch: 0024 log_lik= 0.5574422 train_kl= -0.01160 train_loss= 0.56904 train_acc= 0.47776 val_roc= 0.85090 val_ap= 0.84909 time= 0.08367\n",
      "训练次数: 7 Epoch: 0025 log_lik= 0.5536855 train_kl= -0.01175 train_loss= 0.56543 train_acc= 0.48072 val_roc= 0.85345 val_ap= 0.85127 time= 0.07591\n",
      "训练次数: 7 Epoch: 0026 log_lik= 0.5495942 train_kl= -0.01185 train_loss= 0.56144 train_acc= 0.48259 val_roc= 0.85878 val_ap= 0.85725 time= 0.07599\n",
      "训练次数: 7 Epoch: 0027 log_lik= 0.543919 train_kl= -0.01189 train_loss= 0.55581 train_acc= 0.48439 val_roc= 0.86481 val_ap= 0.86498 time= 0.08903\n",
      "训练次数: 7 Epoch: 0028 log_lik= 0.5362034 train_kl= -0.01190 train_loss= 0.54811 train_acc= 0.48295 val_roc= 0.86965 val_ap= 0.87198 time= 0.07998\n",
      "训练次数: 7 Epoch: 0029 log_lik= 0.5284867 train_kl= -0.01189 train_loss= 0.54038 train_acc= 0.48393 val_roc= 0.87241 val_ap= 0.87630 time= 0.07630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0030 log_lik= 0.5210253 train_kl= -0.01186 train_loss= 0.53289 train_acc= 0.48929 val_roc= 0.87594 val_ap= 0.88010 time= 0.07875\n",
      "训练次数: 7 Epoch: 0031 log_lik= 0.5148361 train_kl= -0.01183 train_loss= 0.52666 train_acc= 0.49564 val_roc= 0.87922 val_ap= 0.88385 time= 0.07898\n",
      "训练次数: 7 Epoch: 0032 log_lik= 0.50945085 train_kl= -0.01179 train_loss= 0.52124 train_acc= 0.50144 val_roc= 0.88368 val_ap= 0.89011 time= 0.08799\n",
      "训练次数: 7 Epoch: 0033 log_lik= 0.5063397 train_kl= -0.01176 train_loss= 0.51810 train_acc= 0.50414 val_roc= 0.88616 val_ap= 0.89370 time= 0.07507\n",
      "训练次数: 7 Epoch: 0034 log_lik= 0.50382817 train_kl= -0.01174 train_loss= 0.51557 train_acc= 0.50186 val_roc= 0.88767 val_ap= 0.89629 time= 0.08901\n",
      "训练次数: 7 Epoch: 0035 log_lik= 0.5030618 train_kl= -0.01172 train_loss= 0.51478 train_acc= 0.49916 val_roc= 0.88839 val_ap= 0.89769 time= 0.08299\n",
      "训练次数: 7 Epoch: 0036 log_lik= 0.5018437 train_kl= -0.01171 train_loss= 0.51355 train_acc= 0.49592 val_roc= 0.88962 val_ap= 0.89911 time= 0.07606\n",
      "训练次数: 7 Epoch: 0037 log_lik= 0.5009418 train_kl= -0.01169 train_loss= 0.51263 train_acc= 0.49350 val_roc= 0.89252 val_ap= 0.90127 time= 0.07598\n",
      "训练次数: 7 Epoch: 0038 log_lik= 0.49734408 train_kl= -0.01167 train_loss= 0.50902 train_acc= 0.49505 val_roc= 0.89409 val_ap= 0.90196 time= 0.07500\n",
      "训练次数: 7 Epoch: 0039 log_lik= 0.49354237 train_kl= -0.01165 train_loss= 0.50519 train_acc= 0.49973 val_roc= 0.89503 val_ap= 0.90206 time= 0.09098\n",
      "训练次数: 7 Epoch: 0040 log_lik= 0.48866454 train_kl= -0.01162 train_loss= 0.50029 train_acc= 0.50599 val_roc= 0.89602 val_ap= 0.90236 time= 0.07507\n",
      "训练次数: 7 Epoch: 0041 log_lik= 0.4852816 train_kl= -0.01160 train_loss= 0.49688 train_acc= 0.50873 val_roc= 0.89855 val_ap= 0.90480 time= 0.07298\n",
      "训练次数: 7 Epoch: 0042 log_lik= 0.4824975 train_kl= -0.01159 train_loss= 0.49408 train_acc= 0.50922 val_roc= 0.90092 val_ap= 0.90595 time= 0.07746\n",
      "训练次数: 7 Epoch: 0043 log_lik= 0.4812444 train_kl= -0.01158 train_loss= 0.49282 train_acc= 0.50871 val_roc= 0.90290 val_ap= 0.90669 time= 0.08502\n",
      "训练次数: 7 Epoch: 0044 log_lik= 0.481208 train_kl= -0.01156 train_loss= 0.49277 train_acc= 0.50724 val_roc= 0.90331 val_ap= 0.90670 time= 0.08102\n",
      "训练次数: 7 Epoch: 0045 log_lik= 0.48009616 train_kl= -0.01154 train_loss= 0.49164 train_acc= 0.50727 val_roc= 0.90334 val_ap= 0.90633 time= 0.08302\n",
      "训练次数: 7 Epoch: 0046 log_lik= 0.47778806 train_kl= -0.01150 train_loss= 0.48929 train_acc= 0.50876 val_roc= 0.90303 val_ap= 0.90649 time= 0.07598\n",
      "训练次数: 7 Epoch: 0047 log_lik= 0.4765826 train_kl= -0.01146 train_loss= 0.48804 train_acc= 0.51013 val_roc= 0.90299 val_ap= 0.90668 time= 0.07502\n",
      "训练次数: 7 Epoch: 0048 log_lik= 0.47575432 train_kl= -0.01141 train_loss= 0.48716 train_acc= 0.51076 val_roc= 0.90415 val_ap= 0.90821 time= 0.07699\n",
      "训练次数: 7 Epoch: 0049 log_lik= 0.4743177 train_kl= -0.01136 train_loss= 0.48567 train_acc= 0.51180 val_roc= 0.90549 val_ap= 0.90946 time= 0.07906\n",
      "训练次数: 7 Epoch: 0050 log_lik= 0.47242644 train_kl= -0.01130 train_loss= 0.48373 train_acc= 0.51129 val_roc= 0.90597 val_ap= 0.91007 time= 0.07503\n",
      "训练次数: 7 Epoch: 0051 log_lik= 0.4715026 train_kl= -0.01124 train_loss= 0.48274 train_acc= 0.51259 val_roc= 0.90655 val_ap= 0.91100 time= 0.07800\n",
      "训练次数: 7 Epoch: 0052 log_lik= 0.470894 train_kl= -0.01118 train_loss= 0.48207 train_acc= 0.51252 val_roc= 0.90694 val_ap= 0.91234 time= 0.08246\n",
      "训练次数: 7 Epoch: 0053 log_lik= 0.470557 train_kl= -0.01111 train_loss= 0.48167 train_acc= 0.51305 val_roc= 0.90711 val_ap= 0.91370 time= 0.07650\n",
      "训练次数: 7 Epoch: 0054 log_lik= 0.4690358 train_kl= -0.01105 train_loss= 0.48009 train_acc= 0.51372 val_roc= 0.90760 val_ap= 0.91534 time= 0.07705\n",
      "训练次数: 7 Epoch: 0055 log_lik= 0.46722046 train_kl= -0.01099 train_loss= 0.47821 train_acc= 0.51361 val_roc= 0.90849 val_ap= 0.91710 time= 0.07698\n",
      "训练次数: 7 Epoch: 0056 log_lik= 0.46520147 train_kl= -0.01095 train_loss= 0.47615 train_acc= 0.51437 val_roc= 0.90958 val_ap= 0.91834 time= 0.08398\n",
      "训练次数: 7 Epoch: 0057 log_lik= 0.46402338 train_kl= -0.01091 train_loss= 0.47493 train_acc= 0.51536 val_roc= 0.91148 val_ap= 0.92044 time= 0.08019\n",
      "训练次数: 7 Epoch: 0058 log_lik= 0.4636177 train_kl= -0.01087 train_loss= 0.47449 train_acc= 0.51636 val_roc= 0.91255 val_ap= 0.92154 time= 0.07484\n",
      "训练次数: 7 Epoch: 0059 log_lik= 0.46306354 train_kl= -0.01084 train_loss= 0.47390 train_acc= 0.51642 val_roc= 0.91243 val_ap= 0.92217 time= 0.07705\n",
      "训练次数: 7 Epoch: 0060 log_lik= 0.46128932 train_kl= -0.01081 train_loss= 0.47210 train_acc= 0.51663 val_roc= 0.91172 val_ap= 0.92194 time= 0.07594\n",
      "训练次数: 7 Epoch: 0061 log_lik= 0.46032193 train_kl= -0.01079 train_loss= 0.47111 train_acc= 0.51698 val_roc= 0.91119 val_ap= 0.92173 time= 0.07490\n",
      "训练次数: 7 Epoch: 0062 log_lik= 0.46021238 train_kl= -0.01077 train_loss= 0.47099 train_acc= 0.51753 val_roc= 0.91161 val_ap= 0.92230 time= 0.07605\n",
      "训练次数: 7 Epoch: 0063 log_lik= 0.45928168 train_kl= -0.01076 train_loss= 0.47005 train_acc= 0.51845 val_roc= 0.91249 val_ap= 0.92254 time= 0.07591\n",
      "训练次数: 7 Epoch: 0064 log_lik= 0.45814523 train_kl= -0.01076 train_loss= 0.46891 train_acc= 0.51879 val_roc= 0.91217 val_ap= 0.92238 time= 0.07691\n",
      "训练次数: 7 Epoch: 0065 log_lik= 0.45765263 train_kl= -0.01076 train_loss= 0.46842 train_acc= 0.51981 val_roc= 0.91263 val_ap= 0.92286 time= 0.07503\n",
      "训练次数: 7 Epoch: 0066 log_lik= 0.4569653 train_kl= -0.01077 train_loss= 0.46773 train_acc= 0.51908 val_roc= 0.91298 val_ap= 0.92317 time= 0.07647\n",
      "训练次数: 7 Epoch: 0067 log_lik= 0.45666784 train_kl= -0.01078 train_loss= 0.46744 train_acc= 0.52015 val_roc= 0.91355 val_ap= 0.92372 time= 0.07597\n",
      "训练次数: 7 Epoch: 0068 log_lik= 0.4558861 train_kl= -0.01079 train_loss= 0.46667 train_acc= 0.52023 val_roc= 0.91389 val_ap= 0.92365 time= 0.09006\n",
      "训练次数: 7 Epoch: 0069 log_lik= 0.45527196 train_kl= -0.01079 train_loss= 0.46607 train_acc= 0.52100 val_roc= 0.91391 val_ap= 0.92349 time= 0.07499\n",
      "训练次数: 7 Epoch: 0070 log_lik= 0.45514297 train_kl= -0.01080 train_loss= 0.46594 train_acc= 0.52136 val_roc= 0.91368 val_ap= 0.92302 time= 0.10099\n",
      "训练次数: 7 Epoch: 0071 log_lik= 0.45404762 train_kl= -0.01080 train_loss= 0.46484 train_acc= 0.52184 val_roc= 0.91444 val_ap= 0.92396 time= 0.09103\n",
      "训练次数: 7 Epoch: 0072 log_lik= 0.45377555 train_kl= -0.01080 train_loss= 0.46457 train_acc= 0.52196 val_roc= 0.91514 val_ap= 0.92441 time= 0.07599\n",
      "训练次数: 7 Epoch: 0073 log_lik= 0.45278502 train_kl= -0.01080 train_loss= 0.46358 train_acc= 0.52194 val_roc= 0.91596 val_ap= 0.92509 time= 0.07698\n",
      "训练次数: 7 Epoch: 0074 log_lik= 0.45240572 train_kl= -0.01080 train_loss= 0.46321 train_acc= 0.52251 val_roc= 0.91648 val_ap= 0.92567 time= 0.07998\n",
      "训练次数: 7 Epoch: 0075 log_lik= 0.45201975 train_kl= -0.01080 train_loss= 0.46282 train_acc= 0.52275 val_roc= 0.91675 val_ap= 0.92578 time= 0.07702\n",
      "训练次数: 7 Epoch: 0076 log_lik= 0.4515583 train_kl= -0.01081 train_loss= 0.46237 train_acc= 0.52340 val_roc= 0.91667 val_ap= 0.92558 time= 0.07598\n",
      "训练次数: 7 Epoch: 0077 log_lik= 0.4512033 train_kl= -0.01082 train_loss= 0.46202 train_acc= 0.52392 val_roc= 0.91697 val_ap= 0.92608 time= 0.07499\n",
      "训练次数: 7 Epoch: 0078 log_lik= 0.4502137 train_kl= -0.01084 train_loss= 0.46105 train_acc= 0.52417 val_roc= 0.91807 val_ap= 0.92716 time= 0.07499\n",
      "训练次数: 7 Epoch: 0079 log_lik= 0.44922495 train_kl= -0.01085 train_loss= 0.46008 train_acc= 0.52354 val_roc= 0.91889 val_ap= 0.92785 time= 0.07386\n",
      "训练次数: 7 Epoch: 0080 log_lik= 0.4490866 train_kl= -0.01087 train_loss= 0.45995 train_acc= 0.52479 val_roc= 0.91926 val_ap= 0.92832 time= 0.07705\n",
      "训练次数: 7 Epoch: 0081 log_lik= 0.44879135 train_kl= -0.01088 train_loss= 0.45967 train_acc= 0.52524 val_roc= 0.91933 val_ap= 0.92844 time= 0.07398\n",
      "训练次数: 7 Epoch: 0082 log_lik= 0.44841984 train_kl= -0.01089 train_loss= 0.45931 train_acc= 0.52407 val_roc= 0.91953 val_ap= 0.92827 time= 0.07748\n",
      "训练次数: 7 Epoch: 0083 log_lik= 0.44789362 train_kl= -0.01091 train_loss= 0.45881 train_acc= 0.52463 val_roc= 0.91926 val_ap= 0.92832 time= 0.07635\n",
      "训练次数: 7 Epoch: 0084 log_lik= 0.4476491 train_kl= -0.01094 train_loss= 0.45859 train_acc= 0.52497 val_roc= 0.91937 val_ap= 0.92824 time= 0.08067\n",
      "训练次数: 7 Epoch: 0085 log_lik= 0.44762954 train_kl= -0.01096 train_loss= 0.45859 train_acc= 0.52431 val_roc= 0.91934 val_ap= 0.92795 time= 0.07599\n",
      "训练次数: 7 Epoch: 0086 log_lik= 0.44667837 train_kl= -0.01098 train_loss= 0.45766 train_acc= 0.52506 val_roc= 0.91978 val_ap= 0.92851 time= 0.07697\n",
      "训练次数: 7 Epoch: 0087 log_lik= 0.4465422 train_kl= -0.01100 train_loss= 0.45754 train_acc= 0.52457 val_roc= 0.92031 val_ap= 0.92916 time= 0.07606\n",
      "训练次数: 7 Epoch: 0088 log_lik= 0.44613346 train_kl= -0.01103 train_loss= 0.45716 train_acc= 0.52586 val_roc= 0.92054 val_ap= 0.92900 time= 0.08797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0089 log_lik= 0.4455612 train_kl= -0.01105 train_loss= 0.45661 train_acc= 0.52507 val_roc= 0.92040 val_ap= 0.92832 time= 0.07398\n",
      "训练次数: 7 Epoch: 0090 log_lik= 0.44538596 train_kl= -0.01106 train_loss= 0.45645 train_acc= 0.52534 val_roc= 0.91983 val_ap= 0.92774 time= 0.07701\n",
      "训练次数: 7 Epoch: 0091 log_lik= 0.44553623 train_kl= -0.01108 train_loss= 0.45661 train_acc= 0.52641 val_roc= 0.92015 val_ap= 0.92778 time= 0.09046\n",
      "训练次数: 7 Epoch: 0092 log_lik= 0.4443847 train_kl= -0.01110 train_loss= 0.45549 train_acc= 0.52667 val_roc= 0.92096 val_ap= 0.92873 time= 0.08603\n",
      "训练次数: 7 Epoch: 0093 log_lik= 0.4445088 train_kl= -0.01112 train_loss= 0.45563 train_acc= 0.52590 val_roc= 0.92106 val_ap= 0.92865 time= 0.08507\n",
      "训练次数: 7 Epoch: 0094 log_lik= 0.44409308 train_kl= -0.01114 train_loss= 0.45523 train_acc= 0.52537 val_roc= 0.92109 val_ap= 0.92849 time= 0.07796\n",
      "训练次数: 7 Epoch: 0095 log_lik= 0.44355866 train_kl= -0.01115 train_loss= 0.45471 train_acc= 0.52721 val_roc= 0.92096 val_ap= 0.92810 time= 0.07901\n",
      "训练次数: 7 Epoch: 0096 log_lik= 0.4437749 train_kl= -0.01116 train_loss= 0.45494 train_acc= 0.52776 val_roc= 0.92060 val_ap= 0.92777 time= 0.08196\n",
      "训练次数: 7 Epoch: 0097 log_lik= 0.44317165 train_kl= -0.01118 train_loss= 0.45435 train_acc= 0.52739 val_roc= 0.92070 val_ap= 0.92790 time= 0.07899\n",
      "训练次数: 7 Epoch: 0098 log_lik= 0.443123 train_kl= -0.01119 train_loss= 0.45431 train_acc= 0.52669 val_roc= 0.92082 val_ap= 0.92781 time= 0.08404\n",
      "训练次数: 7 Epoch: 0099 log_lik= 0.44249016 train_kl= -0.01121 train_loss= 0.45370 train_acc= 0.52673 val_roc= 0.92101 val_ap= 0.92743 time= 0.07702\n",
      "训练次数: 7 Epoch: 0100 log_lik= 0.44193888 train_kl= -0.01122 train_loss= 0.45316 train_acc= 0.52773 val_roc= 0.92118 val_ap= 0.92732 time= 0.08191\n",
      "Optimization Finished!\n",
      "训练次数: 7 ROC score: 0.8994055356120535\n",
      "训练次数: 7 AP score: 0.9041697244133016\n",
      "训练次数: 8 Epoch: 0001 log_lik= 1.7339606 train_kl= -0.00004 train_loss= 1.73400 train_acc= 0.49745 val_roc= 0.64831 val_ap= 0.67282 time= 1.40719\n",
      "训练次数: 8 Epoch: 0002 log_lik= 1.4875803 train_kl= -0.00018 train_loss= 1.48776 train_acc= 0.47331 val_roc= 0.65145 val_ap= 0.68064 time= 0.09312\n",
      "训练次数: 8 Epoch: 0003 log_lik= 1.3314986 train_kl= -0.00051 train_loss= 1.33201 train_acc= 0.42859 val_roc= 0.66658 val_ap= 0.69328 time= 0.08965\n",
      "训练次数: 8 Epoch: 0004 log_lik= 1.1572189 train_kl= -0.00082 train_loss= 1.15804 train_acc= 0.38847 val_roc= 0.67912 val_ap= 0.70441 time= 0.07869\n",
      "训练次数: 8 Epoch: 0005 log_lik= 1.0302279 train_kl= -0.00126 train_loss= 1.03149 train_acc= 0.34002 val_roc= 0.69149 val_ap= 0.71597 time= 0.07871\n",
      "训练次数: 8 Epoch: 0006 log_lik= 0.92471486 train_kl= -0.00181 train_loss= 0.92653 train_acc= 0.32429 val_roc= 0.70352 val_ap= 0.73251 time= 0.07768\n",
      "训练次数: 8 Epoch: 0007 log_lik= 0.8336943 train_kl= -0.00249 train_loss= 0.83618 train_acc= 0.31194 val_roc= 0.71753 val_ap= 0.75032 time= 0.08577\n",
      "训练次数: 8 Epoch: 0008 log_lik= 0.7637384 train_kl= -0.00328 train_loss= 0.76702 train_acc= 0.31499 val_roc= 0.73977 val_ap= 0.76586 time= 0.07568\n",
      "训练次数: 8 Epoch: 0009 log_lik= 0.72781545 train_kl= -0.00416 train_loss= 0.73198 train_acc= 0.33240 val_roc= 0.77614 val_ap= 0.79305 time= 0.07867\n",
      "训练次数: 8 Epoch: 0010 log_lik= 0.71139807 train_kl= -0.00509 train_loss= 0.71649 train_acc= 0.35789 val_roc= 0.79196 val_ap= 0.80763 time= 0.07628\n",
      "训练次数: 8 Epoch: 0011 log_lik= 0.69372994 train_kl= -0.00605 train_loss= 0.69978 train_acc= 0.30602 val_roc= 0.78331 val_ap= 0.80184 time= 0.07469\n",
      "训练次数: 8 Epoch: 0012 log_lik= 0.69010675 train_kl= -0.00703 train_loss= 0.69713 train_acc= 0.19073 val_roc= 0.78963 val_ap= 0.80722 time= 0.07864\n",
      "训练次数: 8 Epoch: 0013 log_lik= 0.6913626 train_kl= -0.00788 train_loss= 0.69924 train_acc= 0.12827 val_roc= 0.82110 val_ap= 0.83179 time= 0.07525\n",
      "训练次数: 8 Epoch: 0014 log_lik= 0.6706734 train_kl= -0.00854 train_loss= 0.67922 train_acc= 0.15114 val_roc= 0.84126 val_ap= 0.84519 time= 0.07857\n",
      "训练次数: 8 Epoch: 0015 log_lik= 0.6569528 train_kl= -0.00911 train_loss= 0.66606 train_acc= 0.19745 val_roc= 0.83994 val_ap= 0.83749 time= 0.07869\n",
      "训练次数: 8 Epoch: 0016 log_lik= 0.6448836 train_kl= -0.00965 train_loss= 0.65453 train_acc= 0.24036 val_roc= 0.84072 val_ap= 0.83325 time= 0.07590\n",
      "训练次数: 8 Epoch: 0017 log_lik= 0.6322955 train_kl= -0.01017 train_loss= 0.64246 train_acc= 0.27472 val_roc= 0.84872 val_ap= 0.83868 time= 0.07598\n",
      "训练次数: 8 Epoch: 0018 log_lik= 0.61449295 train_kl= -0.01066 train_loss= 0.62516 train_acc= 0.32552 val_roc= 0.85705 val_ap= 0.84588 time= 0.09406\n",
      "训练次数: 8 Epoch: 0019 log_lik= 0.5930788 train_kl= -0.01115 train_loss= 0.60423 train_acc= 0.36720 val_roc= 0.86027 val_ap= 0.84870 time= 0.07701\n",
      "训练次数: 8 Epoch: 0020 log_lik= 0.57459944 train_kl= -0.01162 train_loss= 0.58622 train_acc= 0.39739 val_roc= 0.86245 val_ap= 0.85009 time= 0.07696\n",
      "训练次数: 8 Epoch: 0021 log_lik= 0.5639562 train_kl= -0.01205 train_loss= 0.57601 train_acc= 0.41814 val_roc= 0.86594 val_ap= 0.85170 time= 0.08299\n",
      "训练次数: 8 Epoch: 0022 log_lik= 0.5568141 train_kl= -0.01239 train_loss= 0.56920 train_acc= 0.44119 val_roc= 0.86841 val_ap= 0.85248 time= 0.08004\n",
      "训练次数: 8 Epoch: 0023 log_lik= 0.55099696 train_kl= -0.01263 train_loss= 0.56363 train_acc= 0.46617 val_roc= 0.87040 val_ap= 0.85425 time= 0.07902\n",
      "训练次数: 8 Epoch: 0024 log_lik= 0.5503935 train_kl= -0.01280 train_loss= 0.56319 train_acc= 0.47656 val_roc= 0.87398 val_ap= 0.85859 time= 0.08097\n",
      "训练次数: 8 Epoch: 0025 log_lik= 0.5430051 train_kl= -0.01292 train_loss= 0.55592 train_acc= 0.48816 val_roc= 0.88104 val_ap= 0.86667 time= 0.08198\n",
      "训练次数: 8 Epoch: 0026 log_lik= 0.53314376 train_kl= -0.01302 train_loss= 0.54617 train_acc= 0.49317 val_roc= 0.88963 val_ap= 0.87675 time= 0.07702\n",
      "训练次数: 8 Epoch: 0027 log_lik= 0.52402884 train_kl= -0.01313 train_loss= 0.53716 train_acc= 0.49302 val_roc= 0.89646 val_ap= 0.88365 time= 0.08803\n",
      "训练次数: 8 Epoch: 0028 log_lik= 0.5186757 train_kl= -0.01321 train_loss= 0.53189 train_acc= 0.49167 val_roc= 0.90048 val_ap= 0.88719 time= 0.07502\n",
      "训练次数: 8 Epoch: 0029 log_lik= 0.5101378 train_kl= -0.01322 train_loss= 0.52336 train_acc= 0.49760 val_roc= 0.90188 val_ap= 0.88747 time= 0.07701\n",
      "训练次数: 8 Epoch: 0030 log_lik= 0.5021627 train_kl= -0.01319 train_loss= 0.51535 train_acc= 0.50312 val_roc= 0.90335 val_ap= 0.88948 time= 0.08001\n",
      "训练次数: 8 Epoch: 0031 log_lik= 0.50045377 train_kl= -0.01317 train_loss= 0.51362 train_acc= 0.50151 val_roc= 0.90549 val_ap= 0.89153 time= 0.07700\n",
      "训练次数: 8 Epoch: 0032 log_lik= 0.4974431 train_kl= -0.01316 train_loss= 0.51060 train_acc= 0.50278 val_roc= 0.90821 val_ap= 0.89448 time= 0.09804\n",
      "训练次数: 8 Epoch: 0033 log_lik= 0.49591744 train_kl= -0.01315 train_loss= 0.50907 train_acc= 0.49886 val_roc= 0.90979 val_ap= 0.89788 time= 0.08901\n",
      "训练次数: 8 Epoch: 0034 log_lik= 0.49319708 train_kl= -0.01311 train_loss= 0.50631 train_acc= 0.49924 val_roc= 0.91070 val_ap= 0.90179 time= 0.07698\n",
      "训练次数: 8 Epoch: 0035 log_lik= 0.48980635 train_kl= -0.01303 train_loss= 0.50284 train_acc= 0.49983 val_roc= 0.91208 val_ap= 0.90493 time= 0.07505\n",
      "训练次数: 8 Epoch: 0036 log_lik= 0.48770344 train_kl= -0.01291 train_loss= 0.50062 train_acc= 0.49971 val_roc= 0.91337 val_ap= 0.90744 time= 0.08699\n",
      "训练次数: 8 Epoch: 0037 log_lik= 0.48684737 train_kl= -0.01279 train_loss= 0.49964 train_acc= 0.50182 val_roc= 0.91457 val_ap= 0.90957 time= 0.08904\n",
      "训练次数: 8 Epoch: 0038 log_lik= 0.4851175 train_kl= -0.01269 train_loss= 0.49781 train_acc= 0.50434 val_roc= 0.91602 val_ap= 0.91124 time= 0.09793\n",
      "训练次数: 8 Epoch: 0039 log_lik= 0.48360246 train_kl= -0.01260 train_loss= 0.49620 train_acc= 0.50487 val_roc= 0.91720 val_ap= 0.91289 time= 0.08505\n",
      "训练次数: 8 Epoch: 0040 log_lik= 0.4816888 train_kl= -0.01252 train_loss= 0.49421 train_acc= 0.50508 val_roc= 0.91891 val_ap= 0.91549 time= 0.08100\n",
      "训练次数: 8 Epoch: 0041 log_lik= 0.4793557 train_kl= -0.01243 train_loss= 0.49179 train_acc= 0.50516 val_roc= 0.91972 val_ap= 0.91667 time= 0.07796\n",
      "训练次数: 8 Epoch: 0042 log_lik= 0.47656545 train_kl= -0.01233 train_loss= 0.48890 train_acc= 0.50738 val_roc= 0.92040 val_ap= 0.91756 time= 0.08008\n",
      "训练次数: 8 Epoch: 0043 log_lik= 0.47308174 train_kl= -0.01222 train_loss= 0.48531 train_acc= 0.50995 val_roc= 0.92108 val_ap= 0.91820 time= 0.07695\n",
      "训练次数: 8 Epoch: 0044 log_lik= 0.47168818 train_kl= -0.01213 train_loss= 0.48382 train_acc= 0.51321 val_roc= 0.92166 val_ap= 0.91842 time= 0.08545\n",
      "训练次数: 8 Epoch: 0045 log_lik= 0.47055227 train_kl= -0.01207 train_loss= 0.48263 train_acc= 0.51274 val_roc= 0.92179 val_ap= 0.91823 time= 0.08803\n",
      "训练次数: 8 Epoch: 0046 log_lik= 0.46867502 train_kl= -0.01203 train_loss= 0.48070 train_acc= 0.51286 val_roc= 0.92183 val_ap= 0.91868 time= 0.08201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 8 Epoch: 0047 log_lik= 0.4669117 train_kl= -0.01198 train_loss= 0.47889 train_acc= 0.51426 val_roc= 0.92187 val_ap= 0.91841 time= 0.07596\n",
      "训练次数: 8 Epoch: 0048 log_lik= 0.465164 train_kl= -0.01192 train_loss= 0.47708 train_acc= 0.51689 val_roc= 0.92210 val_ap= 0.91825 time= 0.08298\n",
      "训练次数: 8 Epoch: 0049 log_lik= 0.46389005 train_kl= -0.01186 train_loss= 0.47575 train_acc= 0.51750 val_roc= 0.92258 val_ap= 0.91870 time= 0.07903\n",
      "训练次数: 8 Epoch: 0050 log_lik= 0.46286684 train_kl= -0.01182 train_loss= 0.47469 train_acc= 0.51978 val_roc= 0.92297 val_ap= 0.91926 time= 0.07796\n",
      "训练次数: 8 Epoch: 0051 log_lik= 0.46214953 train_kl= -0.01179 train_loss= 0.47394 train_acc= 0.51873 val_roc= 0.92345 val_ap= 0.91981 time= 0.09238\n",
      "训练次数: 8 Epoch: 0052 log_lik= 0.46141148 train_kl= -0.01177 train_loss= 0.47318 train_acc= 0.51894 val_roc= 0.92387 val_ap= 0.92040 time= 0.09012\n",
      "训练次数: 8 Epoch: 0053 log_lik= 0.46018466 train_kl= -0.01174 train_loss= 0.47193 train_acc= 0.51990 val_roc= 0.92442 val_ap= 0.92195 time= 0.07767\n",
      "训练次数: 8 Epoch: 0054 log_lik= 0.45970377 train_kl= -0.01171 train_loss= 0.47141 train_acc= 0.52054 val_roc= 0.92494 val_ap= 0.92283 time= 0.07668\n",
      "训练次数: 8 Epoch: 0055 log_lik= 0.45816016 train_kl= -0.01166 train_loss= 0.46982 train_acc= 0.52166 val_roc= 0.92498 val_ap= 0.92380 time= 0.08275\n",
      "训练次数: 8 Epoch: 0056 log_lik= 0.45757854 train_kl= -0.01162 train_loss= 0.46920 train_acc= 0.52256 val_roc= 0.92500 val_ap= 0.92434 time= 0.08484\n",
      "训练次数: 8 Epoch: 0057 log_lik= 0.45652825 train_kl= -0.01158 train_loss= 0.46811 train_acc= 0.52276 val_roc= 0.92501 val_ap= 0.92512 time= 0.08900\n",
      "训练次数: 8 Epoch: 0058 log_lik= 0.45581648 train_kl= -0.01155 train_loss= 0.46736 train_acc= 0.52357 val_roc= 0.92507 val_ap= 0.92580 time= 0.08805\n",
      "训练次数: 8 Epoch: 0059 log_lik= 0.45480838 train_kl= -0.01151 train_loss= 0.46632 train_acc= 0.52314 val_roc= 0.92559 val_ap= 0.92680 time= 0.07702\n",
      "训练次数: 8 Epoch: 0060 log_lik= 0.45417747 train_kl= -0.01147 train_loss= 0.46565 train_acc= 0.52508 val_roc= 0.92609 val_ap= 0.92816 time= 0.07803\n",
      "训练次数: 8 Epoch: 0061 log_lik= 0.45288318 train_kl= -0.01143 train_loss= 0.46431 train_acc= 0.52489 val_roc= 0.92676 val_ap= 0.92925 time= 0.07794\n",
      "训练次数: 8 Epoch: 0062 log_lik= 0.45238766 train_kl= -0.01140 train_loss= 0.46379 train_acc= 0.52614 val_roc= 0.92687 val_ap= 0.93002 time= 0.07598\n",
      "训练次数: 8 Epoch: 0063 log_lik= 0.45135686 train_kl= -0.01139 train_loss= 0.46274 train_acc= 0.52619 val_roc= 0.92709 val_ap= 0.93071 time= 0.09197\n",
      "训练次数: 8 Epoch: 0064 log_lik= 0.45139343 train_kl= -0.01136 train_loss= 0.46276 train_acc= 0.52621 val_roc= 0.92690 val_ap= 0.93085 time= 0.07765\n",
      "训练次数: 8 Epoch: 0065 log_lik= 0.4503723 train_kl= -0.01133 train_loss= 0.46171 train_acc= 0.52713 val_roc= 0.92692 val_ap= 0.93096 time= 0.08392\n",
      "训练次数: 8 Epoch: 0066 log_lik= 0.44963968 train_kl= -0.01130 train_loss= 0.46094 train_acc= 0.52774 val_roc= 0.92705 val_ap= 0.93112 time= 0.07656\n",
      "训练次数: 8 Epoch: 0067 log_lik= 0.44923982 train_kl= -0.01128 train_loss= 0.46052 train_acc= 0.52800 val_roc= 0.92703 val_ap= 0.93134 time= 0.09003\n",
      "训练次数: 8 Epoch: 0068 log_lik= 0.44859785 train_kl= -0.01127 train_loss= 0.45987 train_acc= 0.52838 val_roc= 0.92735 val_ap= 0.93206 time= 0.08390\n",
      "训练次数: 8 Epoch: 0069 log_lik= 0.4477894 train_kl= -0.01127 train_loss= 0.45906 train_acc= 0.52918 val_roc= 0.92748 val_ap= 0.93194 time= 0.08880\n",
      "训练次数: 8 Epoch: 0070 log_lik= 0.44739282 train_kl= -0.01127 train_loss= 0.45866 train_acc= 0.52888 val_roc= 0.92783 val_ap= 0.93228 time= 0.07468\n",
      "训练次数: 8 Epoch: 0071 log_lik= 0.44648835 train_kl= -0.01127 train_loss= 0.45776 train_acc= 0.52958 val_roc= 0.92794 val_ap= 0.93217 time= 0.07469\n",
      "训练次数: 8 Epoch: 0072 log_lik= 0.44663125 train_kl= -0.01126 train_loss= 0.45789 train_acc= 0.52964 val_roc= 0.92797 val_ap= 0.93216 time= 0.09414\n",
      "训练次数: 8 Epoch: 0073 log_lik= 0.44568187 train_kl= -0.01126 train_loss= 0.45694 train_acc= 0.53016 val_roc= 0.92764 val_ap= 0.93188 time= 0.07628\n",
      "训练次数: 8 Epoch: 0074 log_lik= 0.44538367 train_kl= -0.01126 train_loss= 0.45664 train_acc= 0.53069 val_roc= 0.92774 val_ap= 0.93191 time= 0.08894\n",
      "训练次数: 8 Epoch: 0075 log_lik= 0.44483253 train_kl= -0.01126 train_loss= 0.45610 train_acc= 0.53067 val_roc= 0.92790 val_ap= 0.93194 time= 0.07598\n",
      "训练次数: 8 Epoch: 0076 log_lik= 0.4445209 train_kl= -0.01127 train_loss= 0.45579 train_acc= 0.53106 val_roc= 0.92838 val_ap= 0.93214 time= 0.07805\n",
      "训练次数: 8 Epoch: 0077 log_lik= 0.4436463 train_kl= -0.01128 train_loss= 0.45493 train_acc= 0.53216 val_roc= 0.92871 val_ap= 0.93233 time= 0.07599\n",
      "训练次数: 8 Epoch: 0078 log_lik= 0.44311765 train_kl= -0.01128 train_loss= 0.45439 train_acc= 0.53225 val_roc= 0.92857 val_ap= 0.93236 time= 0.07998\n",
      "训练次数: 8 Epoch: 0079 log_lik= 0.44291782 train_kl= -0.01127 train_loss= 0.45419 train_acc= 0.53235 val_roc= 0.92861 val_ap= 0.93207 time= 0.07802\n",
      "训练次数: 8 Epoch: 0080 log_lik= 0.442135 train_kl= -0.01128 train_loss= 0.45342 train_acc= 0.53293 val_roc= 0.92897 val_ap= 0.93274 time= 0.08000\n",
      "训练次数: 8 Epoch: 0081 log_lik= 0.44184938 train_kl= -0.01130 train_loss= 0.45315 train_acc= 0.53333 val_roc= 0.92938 val_ap= 0.93328 time= 0.07496\n",
      "训练次数: 8 Epoch: 0082 log_lik= 0.441056 train_kl= -0.01131 train_loss= 0.45237 train_acc= 0.53478 val_roc= 0.92956 val_ap= 0.93342 time= 0.07645\n",
      "训练次数: 8 Epoch: 0083 log_lik= 0.4409177 train_kl= -0.01131 train_loss= 0.45223 train_acc= 0.53441 val_roc= 0.92959 val_ap= 0.93326 time= 0.07605\n",
      "训练次数: 8 Epoch: 0084 log_lik= 0.4406434 train_kl= -0.01132 train_loss= 0.45196 train_acc= 0.53467 val_roc= 0.92969 val_ap= 0.93358 time= 0.07700\n",
      "训练次数: 8 Epoch: 0085 log_lik= 0.440274 train_kl= -0.01131 train_loss= 0.45159 train_acc= 0.53577 val_roc= 0.92982 val_ap= 0.93380 time= 0.09023\n",
      "训练次数: 8 Epoch: 0086 log_lik= 0.44003573 train_kl= -0.01132 train_loss= 0.45136 train_acc= 0.53554 val_roc= 0.92961 val_ap= 0.93394 time= 0.08663\n",
      "训练次数: 8 Epoch: 0087 log_lik= 0.4397198 train_kl= -0.01133 train_loss= 0.45105 train_acc= 0.53727 val_roc= 0.92949 val_ap= 0.93419 time= 0.07470\n",
      "训练次数: 8 Epoch: 0088 log_lik= 0.4397163 train_kl= -0.01134 train_loss= 0.45106 train_acc= 0.53747 val_roc= 0.92953 val_ap= 0.93406 time= 0.07792\n",
      "训练次数: 8 Epoch: 0089 log_lik= 0.43884853 train_kl= -0.01136 train_loss= 0.45021 train_acc= 0.53726 val_roc= 0.92975 val_ap= 0.93446 time= 0.07607\n",
      "训练次数: 8 Epoch: 0090 log_lik= 0.43874717 train_kl= -0.01138 train_loss= 0.45013 train_acc= 0.53733 val_roc= 0.92948 val_ap= 0.93424 time= 0.07591\n",
      "训练次数: 8 Epoch: 0091 log_lik= 0.438224 train_kl= -0.01140 train_loss= 0.44962 train_acc= 0.53760 val_roc= 0.92909 val_ap= 0.93415 time= 0.09203\n",
      "训练次数: 8 Epoch: 0092 log_lik= 0.43794098 train_kl= -0.01142 train_loss= 0.44936 train_acc= 0.53825 val_roc= 0.92855 val_ap= 0.93388 time= 0.07905\n",
      "训练次数: 8 Epoch: 0093 log_lik= 0.43745878 train_kl= -0.01143 train_loss= 0.44889 train_acc= 0.53814 val_roc= 0.92815 val_ap= 0.93359 time= 0.07693\n",
      "训练次数: 8 Epoch: 0094 log_lik= 0.43735808 train_kl= -0.01145 train_loss= 0.44881 train_acc= 0.53904 val_roc= 0.92810 val_ap= 0.93357 time= 0.08705\n",
      "训练次数: 8 Epoch: 0095 log_lik= 0.4365166 train_kl= -0.01146 train_loss= 0.44798 train_acc= 0.53957 val_roc= 0.92833 val_ap= 0.93375 time= 0.08804\n",
      "训练次数: 8 Epoch: 0096 log_lik= 0.4364774 train_kl= -0.01148 train_loss= 0.44796 train_acc= 0.53960 val_roc= 0.92835 val_ap= 0.93394 time= 0.07491\n",
      "训练次数: 8 Epoch: 0097 log_lik= 0.43598938 train_kl= -0.01151 train_loss= 0.44750 train_acc= 0.54051 val_roc= 0.92757 val_ap= 0.93344 time= 0.07707\n",
      "训练次数: 8 Epoch: 0098 log_lik= 0.43563616 train_kl= -0.01154 train_loss= 0.44718 train_acc= 0.54049 val_roc= 0.92703 val_ap= 0.93310 time= 0.07397\n",
      "训练次数: 8 Epoch: 0099 log_lik= 0.4355737 train_kl= -0.01156 train_loss= 0.44713 train_acc= 0.53999 val_roc= 0.92690 val_ap= 0.93302 time= 0.09066\n",
      "训练次数: 8 Epoch: 0100 log_lik= 0.4350978 train_kl= -0.01158 train_loss= 0.44668 train_acc= 0.54071 val_roc= 0.92735 val_ap= 0.93304 time= 0.09055\n",
      "Optimization Finished!\n",
      "训练次数: 8 ROC score: 0.9152699213981976\n",
      "训练次数: 8 AP score: 0.9192016363266724\n",
      "训练次数: 9 Epoch: 0001 log_lik= 1.7406775 train_kl= -0.00004 train_loss= 1.74072 train_acc= 0.49429 val_roc= 0.66559 val_ap= 0.68154 time= 1.39695\n",
      "训练次数: 9 Epoch: 0002 log_lik= 1.3954196 train_kl= -0.00023 train_loss= 1.39565 train_acc= 0.47330 val_roc= 0.65898 val_ap= 0.67535 time= 0.10307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0003 log_lik= 1.1498979 train_kl= -0.00072 train_loss= 1.15061 train_acc= 0.41685 val_roc= 0.66011 val_ap= 0.67560 time= 0.08862\n",
      "训练次数: 9 Epoch: 0004 log_lik= 1.0124594 train_kl= -0.00138 train_loss= 1.01384 train_acc= 0.34577 val_roc= 0.66703 val_ap= 0.68211 time= 0.09162\n",
      "训练次数: 9 Epoch: 0005 log_lik= 0.90982014 train_kl= -0.00214 train_loss= 0.91196 train_acc= 0.28984 val_roc= 0.68584 val_ap= 0.69855 time= 0.07670\n",
      "训练次数: 9 Epoch: 0006 log_lik= 0.8033645 train_kl= -0.00292 train_loss= 0.80628 train_acc= 0.29466 val_roc= 0.72058 val_ap= 0.73025 time= 0.07769\n",
      "训练次数: 9 Epoch: 0007 log_lik= 0.74714226 train_kl= -0.00376 train_loss= 0.75090 train_acc= 0.31181 val_roc= 0.76043 val_ap= 0.77054 time= 0.07866\n",
      "训练次数: 9 Epoch: 0008 log_lik= 0.7198337 train_kl= -0.00468 train_loss= 0.72451 train_acc= 0.33845 val_roc= 0.78026 val_ap= 0.79464 time= 0.07767\n",
      "训练次数: 9 Epoch: 0009 log_lik= 0.71110713 train_kl= -0.00565 train_loss= 0.71675 train_acc= 0.31717 val_roc= 0.76744 val_ap= 0.78733 time= 0.09062\n",
      "训练次数: 9 Epoch: 0010 log_lik= 0.6982064 train_kl= -0.00663 train_loss= 0.70484 train_acc= 0.20830 val_roc= 0.75780 val_ap= 0.77965 time= 0.08265\n",
      "训练次数: 9 Epoch: 0011 log_lik= 0.7093663 train_kl= -0.00757 train_loss= 0.71693 train_acc= 0.10578 val_roc= 0.78411 val_ap= 0.80351 time= 0.07967\n",
      "训练次数: 9 Epoch: 0012 log_lik= 0.69523406 train_kl= -0.00834 train_loss= 0.70357 train_acc= 0.10158 val_roc= 0.81859 val_ap= 0.83155 time= 0.07782\n",
      "训练次数: 9 Epoch: 0013 log_lik= 0.67389077 train_kl= -0.00899 train_loss= 0.68288 train_acc= 0.14809 val_roc= 0.83024 val_ap= 0.83875 time= 0.07903\n",
      "训练次数: 9 Epoch: 0014 log_lik= 0.6571302 train_kl= -0.00959 train_loss= 0.66672 train_acc= 0.22478 val_roc= 0.82864 val_ap= 0.83666 time= 0.07693\n",
      "训练次数: 9 Epoch: 0015 log_lik= 0.6400109 train_kl= -0.01016 train_loss= 0.65017 train_acc= 0.30409 val_roc= 0.82998 val_ap= 0.83903 time= 0.07804\n",
      "训练次数: 9 Epoch: 0016 log_lik= 0.62339675 train_kl= -0.01069 train_loss= 0.63409 train_acc= 0.34533 val_roc= 0.83387 val_ap= 0.84152 time= 0.08495\n",
      "训练次数: 9 Epoch: 0017 log_lik= 0.6044686 train_kl= -0.01120 train_loss= 0.61567 train_acc= 0.38466 val_roc= 0.83751 val_ap= 0.84329 time= 0.07605\n",
      "训练次数: 9 Epoch: 0018 log_lik= 0.5943878 train_kl= -0.01168 train_loss= 0.60607 train_acc= 0.40236 val_roc= 0.83921 val_ap= 0.84294 time= 0.08598\n",
      "训练次数: 9 Epoch: 0019 log_lik= 0.5858416 train_kl= -0.01212 train_loss= 0.59796 train_acc= 0.42541 val_roc= 0.83955 val_ap= 0.84208 time= 0.07603\n",
      "训练次数: 9 Epoch: 0020 log_lik= 0.5850658 train_kl= -0.01249 train_loss= 0.59755 train_acc= 0.44779 val_roc= 0.83949 val_ap= 0.84312 time= 0.07512\n",
      "训练次数: 9 Epoch: 0021 log_lik= 0.5813586 train_kl= -0.01277 train_loss= 0.59413 train_acc= 0.46282 val_roc= 0.84175 val_ap= 0.84704 time= 0.07384\n",
      "训练次数: 9 Epoch: 0022 log_lik= 0.5747217 train_kl= -0.01297 train_loss= 0.58769 train_acc= 0.47330 val_roc= 0.84434 val_ap= 0.85125 time= 0.07800\n",
      "训练次数: 9 Epoch: 0023 log_lik= 0.5645292 train_kl= -0.01307 train_loss= 0.57760 train_acc= 0.48519 val_roc= 0.84690 val_ap= 0.85389 time= 0.07914\n",
      "训练次数: 9 Epoch: 0024 log_lik= 0.5541891 train_kl= -0.01310 train_loss= 0.56729 train_acc= 0.49048 val_roc= 0.84938 val_ap= 0.85482 time= 0.09392\n",
      "训练次数: 9 Epoch: 0025 log_lik= 0.5487943 train_kl= -0.01310 train_loss= 0.56190 train_acc= 0.48517 val_roc= 0.85360 val_ap= 0.85794 time= 0.08400\n",
      "训练次数: 9 Epoch: 0026 log_lik= 0.54459757 train_kl= -0.01308 train_loss= 0.55768 train_acc= 0.47975 val_roc= 0.85930 val_ap= 0.86228 time= 0.08003\n",
      "训练次数: 9 Epoch: 0027 log_lik= 0.53667825 train_kl= -0.01303 train_loss= 0.54971 train_acc= 0.48243 val_roc= 0.86692 val_ap= 0.86906 time= 0.08301\n",
      "训练次数: 9 Epoch: 0028 log_lik= 0.52854633 train_kl= -0.01296 train_loss= 0.54151 train_acc= 0.48872 val_roc= 0.87312 val_ap= 0.87496 time= 0.07998\n",
      "训练次数: 9 Epoch: 0029 log_lik= 0.5218592 train_kl= -0.01288 train_loss= 0.53474 train_acc= 0.49628 val_roc= 0.87940 val_ap= 0.88004 time= 0.09103\n",
      "训练次数: 9 Epoch: 0030 log_lik= 0.5183195 train_kl= -0.01282 train_loss= 0.53114 train_acc= 0.49952 val_roc= 0.88405 val_ap= 0.88516 time= 0.09704\n",
      "训练次数: 9 Epoch: 0031 log_lik= 0.5165402 train_kl= -0.01276 train_loss= 0.52930 train_acc= 0.50150 val_roc= 0.88823 val_ap= 0.89017 time= 0.07499\n",
      "训练次数: 9 Epoch: 0032 log_lik= 0.5140758 train_kl= -0.01271 train_loss= 0.52678 train_acc= 0.50323 val_roc= 0.89163 val_ap= 0.89495 time= 0.07500\n",
      "训练次数: 9 Epoch: 0033 log_lik= 0.51209956 train_kl= -0.01266 train_loss= 0.52476 train_acc= 0.50580 val_roc= 0.89537 val_ap= 0.89956 time= 0.08802\n",
      "训练次数: 9 Epoch: 0034 log_lik= 0.5079078 train_kl= -0.01261 train_loss= 0.52052 train_acc= 0.50784 val_roc= 0.89815 val_ap= 0.90229 time= 0.07699\n",
      "训练次数: 9 Epoch: 0035 log_lik= 0.5034882 train_kl= -0.01256 train_loss= 0.51605 train_acc= 0.51058 val_roc= 0.90030 val_ap= 0.90349 time= 0.08106\n",
      "训练次数: 9 Epoch: 0036 log_lik= 0.50000006 train_kl= -0.01251 train_loss= 0.51251 train_acc= 0.51598 val_roc= 0.90277 val_ap= 0.90602 time= 0.07604\n",
      "训练次数: 9 Epoch: 0037 log_lik= 0.49550435 train_kl= -0.01248 train_loss= 0.50798 train_acc= 0.51892 val_roc= 0.90598 val_ap= 0.91024 time= 0.07686\n",
      "训练次数: 9 Epoch: 0038 log_lik= 0.49177295 train_kl= -0.01246 train_loss= 0.50423 train_acc= 0.52105 val_roc= 0.90889 val_ap= 0.91483 time= 0.07607\n",
      "训练次数: 9 Epoch: 0039 log_lik= 0.4889233 train_kl= -0.01243 train_loss= 0.50135 train_acc= 0.52037 val_roc= 0.91080 val_ap= 0.91787 time= 0.09194\n",
      "训练次数: 9 Epoch: 0040 log_lik= 0.48688626 train_kl= -0.01239 train_loss= 0.49928 train_acc= 0.51996 val_roc= 0.91239 val_ap= 0.91990 time= 0.09000\n",
      "训练次数: 9 Epoch: 0041 log_lik= 0.48505074 train_kl= -0.01235 train_loss= 0.49740 train_acc= 0.52203 val_roc= 0.91383 val_ap= 0.92136 time= 0.07607\n",
      "训练次数: 9 Epoch: 0042 log_lik= 0.48243505 train_kl= -0.01229 train_loss= 0.49472 train_acc= 0.52369 val_roc= 0.91446 val_ap= 0.92185 time= 0.08209\n",
      "训练次数: 9 Epoch: 0043 log_lik= 0.48014903 train_kl= -0.01223 train_loss= 0.49237 train_acc= 0.52629 val_roc= 0.91490 val_ap= 0.92203 time= 0.07591\n",
      "训练次数: 9 Epoch: 0044 log_lik= 0.4782771 train_kl= -0.01217 train_loss= 0.49044 train_acc= 0.52749 val_roc= 0.91579 val_ap= 0.92304 time= 0.07706\n",
      "训练次数: 9 Epoch: 0045 log_lik= 0.47577485 train_kl= -0.01211 train_loss= 0.48788 train_acc= 0.52650 val_roc= 0.91652 val_ap= 0.92362 time= 0.07699\n",
      "训练次数: 9 Epoch: 0046 log_lik= 0.4753209 train_kl= -0.01205 train_loss= 0.48737 train_acc= 0.52799 val_roc= 0.91703 val_ap= 0.92396 time= 0.07808\n",
      "训练次数: 9 Epoch: 0047 log_lik= 0.47363353 train_kl= -0.01198 train_loss= 0.48562 train_acc= 0.52901 val_roc= 0.91798 val_ap= 0.92534 time= 0.07592\n",
      "训练次数: 9 Epoch: 0048 log_lik= 0.4723175 train_kl= -0.01192 train_loss= 0.48424 train_acc= 0.52812 val_roc= 0.91989 val_ap= 0.92776 time= 0.07546\n",
      "训练次数: 9 Epoch: 0049 log_lik= 0.4699089 train_kl= -0.01186 train_loss= 0.48177 train_acc= 0.52996 val_roc= 0.92115 val_ap= 0.92941 time= 0.07406\n",
      "训练次数: 9 Epoch: 0050 log_lik= 0.46891034 train_kl= -0.01179 train_loss= 0.48070 train_acc= 0.52922 val_roc= 0.92202 val_ap= 0.93062 time= 0.07500\n",
      "训练次数: 9 Epoch: 0051 log_lik= 0.4666336 train_kl= -0.01171 train_loss= 0.47835 train_acc= 0.53025 val_roc= 0.92203 val_ap= 0.93098 time= 0.07689\n",
      "训练次数: 9 Epoch: 0052 log_lik= 0.4655902 train_kl= -0.01165 train_loss= 0.47724 train_acc= 0.52953 val_roc= 0.92219 val_ap= 0.93139 time= 0.08809\n",
      "训练次数: 9 Epoch: 0053 log_lik= 0.4655678 train_kl= -0.01159 train_loss= 0.47715 train_acc= 0.52839 val_roc= 0.92228 val_ap= 0.93166 time= 0.07450\n",
      "训练次数: 9 Epoch: 0054 log_lik= 0.46371123 train_kl= -0.01153 train_loss= 0.47525 train_acc= 0.52752 val_roc= 0.92271 val_ap= 0.93226 time= 0.07675\n",
      "训练次数: 9 Epoch: 0055 log_lik= 0.46245643 train_kl= -0.01148 train_loss= 0.47394 train_acc= 0.52830 val_roc= 0.92369 val_ap= 0.93349 time= 0.08405\n",
      "训练次数: 9 Epoch: 0056 log_lik= 0.4611126 train_kl= -0.01143 train_loss= 0.47255 train_acc= 0.52917 val_roc= 0.92351 val_ap= 0.93360 time= 0.07490\n",
      "训练次数: 9 Epoch: 0057 log_lik= 0.46125805 train_kl= -0.01139 train_loss= 0.47265 train_acc= 0.52896 val_roc= 0.92364 val_ap= 0.93386 time= 0.08386\n",
      "训练次数: 9 Epoch: 0058 log_lik= 0.4595826 train_kl= -0.01135 train_loss= 0.47093 train_acc= 0.52866 val_roc= 0.92335 val_ap= 0.93350 time= 0.07801\n",
      "训练次数: 9 Epoch: 0059 log_lik= 0.4586649 train_kl= -0.01131 train_loss= 0.46998 train_acc= 0.52905 val_roc= 0.92252 val_ap= 0.93287 time= 0.07699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0060 log_lik= 0.4578636 train_kl= -0.01127 train_loss= 0.46914 train_acc= 0.52977 val_roc= 0.92238 val_ap= 0.93293 time= 0.08701\n",
      "训练次数: 9 Epoch: 0061 log_lik= 0.45677593 train_kl= -0.01124 train_loss= 0.46802 train_acc= 0.53134 val_roc= 0.92327 val_ap= 0.93419 time= 0.07907\n",
      "训练次数: 9 Epoch: 0062 log_lik= 0.4554125 train_kl= -0.01121 train_loss= 0.46663 train_acc= 0.52935 val_roc= 0.92404 val_ap= 0.93528 time= 0.07695\n",
      "训练次数: 9 Epoch: 0063 log_lik= 0.45505768 train_kl= -0.01119 train_loss= 0.46625 train_acc= 0.52940 val_roc= 0.92395 val_ap= 0.93561 time= 0.08214\n",
      "训练次数: 9 Epoch: 0064 log_lik= 0.4539218 train_kl= -0.01117 train_loss= 0.46509 train_acc= 0.52948 val_roc= 0.92343 val_ap= 0.93510 time= 0.07793\n",
      "训练次数: 9 Epoch: 0065 log_lik= 0.453406 train_kl= -0.01114 train_loss= 0.46455 train_acc= 0.52995 val_roc= 0.92329 val_ap= 0.93495 time= 0.07702\n",
      "训练次数: 9 Epoch: 0066 log_lik= 0.45239267 train_kl= -0.01112 train_loss= 0.46351 train_acc= 0.53007 val_roc= 0.92281 val_ap= 0.93465 time= 0.07797\n",
      "训练次数: 9 Epoch: 0067 log_lik= 0.45186 train_kl= -0.01111 train_loss= 0.46297 train_acc= 0.53043 val_roc= 0.92364 val_ap= 0.93586 time= 0.07801\n",
      "训练次数: 9 Epoch: 0068 log_lik= 0.45078036 train_kl= -0.01111 train_loss= 0.46189 train_acc= 0.53001 val_roc= 0.92323 val_ap= 0.93580 time= 0.09008\n",
      "训练次数: 9 Epoch: 0069 log_lik= 0.4507717 train_kl= -0.01111 train_loss= 0.46188 train_acc= 0.52920 val_roc= 0.92411 val_ap= 0.93654 time= 0.08394\n",
      "训练次数: 9 Epoch: 0070 log_lik= 0.45005947 train_kl= -0.01111 train_loss= 0.46117 train_acc= 0.52972 val_roc= 0.92447 val_ap= 0.93690 time= 0.07497\n",
      "训练次数: 9 Epoch: 0071 log_lik= 0.44878033 train_kl= -0.01112 train_loss= 0.45990 train_acc= 0.52948 val_roc= 0.92367 val_ap= 0.93676 time= 0.07997\n",
      "训练次数: 9 Epoch: 0072 log_lik= 0.44814286 train_kl= -0.01112 train_loss= 0.45927 train_acc= 0.53002 val_roc= 0.92430 val_ap= 0.93764 time= 0.09001\n",
      "训练次数: 9 Epoch: 0073 log_lik= 0.44782403 train_kl= -0.01114 train_loss= 0.45896 train_acc= 0.53036 val_roc= 0.92544 val_ap= 0.93870 time= 0.07306\n",
      "训练次数: 9 Epoch: 0074 log_lik= 0.44683227 train_kl= -0.01116 train_loss= 0.45799 train_acc= 0.53059 val_roc= 0.92560 val_ap= 0.93896 time= 0.07396\n",
      "训练次数: 9 Epoch: 0075 log_lik= 0.4462075 train_kl= -0.01119 train_loss= 0.45739 train_acc= 0.53137 val_roc= 0.92530 val_ap= 0.93882 time= 0.07471\n",
      "训练次数: 9 Epoch: 0076 log_lik= 0.4453352 train_kl= -0.01122 train_loss= 0.45655 train_acc= 0.53123 val_roc= 0.92641 val_ap= 0.93974 time= 0.08189\n",
      "训练次数: 9 Epoch: 0077 log_lik= 0.44496965 train_kl= -0.01126 train_loss= 0.45623 train_acc= 0.53190 val_roc= 0.92594 val_ap= 0.93976 time= 0.07783\n",
      "训练次数: 9 Epoch: 0078 log_lik= 0.4437759 train_kl= -0.01129 train_loss= 0.45506 train_acc= 0.53240 val_roc= 0.92572 val_ap= 0.94008 time= 0.07402\n",
      "训练次数: 9 Epoch: 0079 log_lik= 0.4436231 train_kl= -0.01132 train_loss= 0.45494 train_acc= 0.53229 val_roc= 0.92703 val_ap= 0.94114 time= 0.07699\n",
      "训练次数: 9 Epoch: 0080 log_lik= 0.44302785 train_kl= -0.01136 train_loss= 0.45439 train_acc= 0.53184 val_roc= 0.92641 val_ap= 0.94071 time= 0.07545\n",
      "训练次数: 9 Epoch: 0081 log_lik= 0.44190603 train_kl= -0.01138 train_loss= 0.45329 train_acc= 0.53296 val_roc= 0.92547 val_ap= 0.93982 time= 0.07902\n",
      "训练次数: 9 Epoch: 0082 log_lik= 0.44133547 train_kl= -0.01142 train_loss= 0.45276 train_acc= 0.53353 val_roc= 0.92562 val_ap= 0.93938 time= 0.07599\n",
      "训练次数: 9 Epoch: 0083 log_lik= 0.44136208 train_kl= -0.01147 train_loss= 0.45283 train_acc= 0.53362 val_roc= 0.92526 val_ap= 0.93937 time= 0.07798\n",
      "训练次数: 9 Epoch: 0084 log_lik= 0.44096193 train_kl= -0.01149 train_loss= 0.45246 train_acc= 0.53396 val_roc= 0.92459 val_ap= 0.93869 time= 0.08215\n",
      "训练次数: 9 Epoch: 0085 log_lik= 0.4397573 train_kl= -0.01152 train_loss= 0.45127 train_acc= 0.53392 val_roc= 0.92429 val_ap= 0.93779 time= 0.07988\n",
      "训练次数: 9 Epoch: 0086 log_lik= 0.43925115 train_kl= -0.01155 train_loss= 0.45080 train_acc= 0.53475 val_roc= 0.92416 val_ap= 0.93765 time= 0.07602\n",
      "训练次数: 9 Epoch: 0087 log_lik= 0.43866584 train_kl= -0.01157 train_loss= 0.45024 train_acc= 0.53454 val_roc= 0.92323 val_ap= 0.93679 time= 0.08108\n",
      "训练次数: 9 Epoch: 0088 log_lik= 0.43893895 train_kl= -0.01160 train_loss= 0.45054 train_acc= 0.53471 val_roc= 0.92274 val_ap= 0.93527 time= 0.07399\n",
      "训练次数: 9 Epoch: 0089 log_lik= 0.43789175 train_kl= -0.01164 train_loss= 0.44953 train_acc= 0.53520 val_roc= 0.92248 val_ap= 0.93494 time= 0.08295\n",
      "训练次数: 9 Epoch: 0090 log_lik= 0.4374489 train_kl= -0.01165 train_loss= 0.44910 train_acc= 0.53562 val_roc= 0.92207 val_ap= 0.93544 time= 0.07693\n",
      "训练次数: 9 Epoch: 0091 log_lik= 0.43756866 train_kl= -0.01166 train_loss= 0.44923 train_acc= 0.53570 val_roc= 0.92239 val_ap= 0.93511 time= 0.07701\n",
      "训练次数: 9 Epoch: 0092 log_lik= 0.43639338 train_kl= -0.01169 train_loss= 0.44808 train_acc= 0.53611 val_roc= 0.92174 val_ap= 0.93435 time= 0.07497\n",
      "训练次数: 9 Epoch: 0093 log_lik= 0.43671146 train_kl= -0.01172 train_loss= 0.44843 train_acc= 0.53426 val_roc= 0.92072 val_ap= 0.93457 time= 0.07609\n",
      "训练次数: 9 Epoch: 0094 log_lik= 0.43655393 train_kl= -0.01171 train_loss= 0.44827 train_acc= 0.53433 val_roc= 0.92161 val_ap= 0.93526 time= 0.07371\n",
      "训练次数: 9 Epoch: 0095 log_lik= 0.4364456 train_kl= -0.01173 train_loss= 0.44817 train_acc= 0.53582 val_roc= 0.92187 val_ap= 0.93520 time= 0.07667\n",
      "训练次数: 9 Epoch: 0096 log_lik= 0.43591347 train_kl= -0.01177 train_loss= 0.44768 train_acc= 0.53417 val_roc= 0.92020 val_ap= 0.93492 time= 0.09060\n",
      "训练次数: 9 Epoch: 0097 log_lik= 0.43518013 train_kl= -0.01178 train_loss= 0.44696 train_acc= 0.53567 val_roc= 0.92114 val_ap= 0.93583 time= 0.08864\n",
      "训练次数: 9 Epoch: 0098 log_lik= 0.43482557 train_kl= -0.01179 train_loss= 0.44662 train_acc= 0.53546 val_roc= 0.92194 val_ap= 0.93647 time= 0.07492\n",
      "训练次数: 9 Epoch: 0099 log_lik= 0.43501532 train_kl= -0.01181 train_loss= 0.44683 train_acc= 0.53637 val_roc= 0.92074 val_ap= 0.93620 time= 0.07817\n",
      "训练次数: 9 Epoch: 0100 log_lik= 0.4342469 train_kl= -0.01183 train_loss= 0.44608 train_acc= 0.53514 val_roc= 0.92079 val_ap= 0.93645 time= 0.07569\n",
      "Optimization Finished!\n",
      "训练次数: 9 ROC score: 0.9185789024552712\n",
      "训练次数: 9 AP score: 0.9265447657155303\n",
      "训练次数: 10 Epoch: 0001 log_lik= 1.8172202 train_kl= -0.00004 train_loss= 1.81726 train_acc= 0.49731 val_roc= 0.65908 val_ap= 0.68980 time= 1.42703\n",
      "训练次数: 10 Epoch: 0002 log_lik= 1.5204246 train_kl= -0.00010 train_loss= 1.52053 train_acc= 0.48681 val_roc= 0.63676 val_ap= 0.67943 time= 0.07917\n",
      "训练次数: 10 Epoch: 0003 log_lik= 1.3692054 train_kl= -0.00040 train_loss= 1.36960 train_acc= 0.42793 val_roc= 0.63588 val_ap= 0.68137 time= 0.08862\n",
      "训练次数: 10 Epoch: 0004 log_lik= 1.2063754 train_kl= -0.00089 train_loss= 1.20726 train_acc= 0.35691 val_roc= 0.64068 val_ap= 0.68496 time= 0.07577\n",
      "训练次数: 10 Epoch: 0005 log_lik= 1.1201357 train_kl= -0.00136 train_loss= 1.12150 train_acc= 0.32170 val_roc= 0.65310 val_ap= 0.69315 time= 0.07663\n",
      "训练次数: 10 Epoch: 0006 log_lik= 0.98452723 train_kl= -0.00180 train_loss= 0.98633 train_acc= 0.33203 val_roc= 0.67967 val_ap= 0.71270 time= 0.08467\n",
      "训练次数: 10 Epoch: 0007 log_lik= 0.88088995 train_kl= -0.00231 train_loss= 0.88320 train_acc= 0.37632 val_roc= 0.71603 val_ap= 0.74575 time= 0.07568\n",
      "训练次数: 10 Epoch: 0008 log_lik= 0.80131036 train_kl= -0.00298 train_loss= 0.80429 train_acc= 0.39401 val_roc= 0.73357 val_ap= 0.75533 time= 0.07867\n",
      "训练次数: 10 Epoch: 0009 log_lik= 0.75033516 train_kl= -0.00379 train_loss= 0.75413 train_acc= 0.39545 val_roc= 0.72836 val_ap= 0.74632 time= 0.07568\n",
      "训练次数: 10 Epoch: 0010 log_lik= 0.7231905 train_kl= -0.00471 train_loss= 0.72790 train_acc= 0.33610 val_roc= 0.72433 val_ap= 0.74346 time= 0.07670\n",
      "训练次数: 10 Epoch: 0011 log_lik= 0.7105368 train_kl= -0.00569 train_loss= 0.71622 train_acc= 0.23433 val_roc= 0.74296 val_ap= 0.75948 time= 0.07433\n",
      "训练次数: 10 Epoch: 0012 log_lik= 0.7053593 train_kl= -0.00661 train_loss= 0.71197 train_acc= 0.16643 val_roc= 0.77650 val_ap= 0.78517 time= 0.07669\n",
      "训练次数: 10 Epoch: 0013 log_lik= 0.6865317 train_kl= -0.00745 train_loss= 0.69398 train_acc= 0.17008 val_roc= 0.81028 val_ap= 0.81018 time= 0.08421\n",
      "训练次数: 10 Epoch: 0014 log_lik= 0.66507953 train_kl= -0.00821 train_loss= 0.67329 train_acc= 0.20252 val_roc= 0.82767 val_ap= 0.81885 time= 0.07576\n",
      "训练次数: 10 Epoch: 0015 log_lik= 0.64432013 train_kl= -0.00891 train_loss= 0.65323 train_acc= 0.25675 val_roc= 0.83128 val_ap= 0.81907 time= 0.07970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0016 log_lik= 0.6263551 train_kl= -0.00958 train_loss= 0.63593 train_acc= 0.29316 val_roc= 0.83430 val_ap= 0.82011 time= 0.07979\n",
      "训练次数: 10 Epoch: 0017 log_lik= 0.60967 train_kl= -0.01020 train_loss= 0.61987 train_acc= 0.32324 val_roc= 0.83783 val_ap= 0.82151 time= 0.07700\n",
      "训练次数: 10 Epoch: 0018 log_lik= 0.59493995 train_kl= -0.01077 train_loss= 0.60571 train_acc= 0.35487 val_roc= 0.84148 val_ap= 0.82321 time= 0.07503\n",
      "训练次数: 10 Epoch: 0019 log_lik= 0.5826192 train_kl= -0.01129 train_loss= 0.59391 train_acc= 0.38924 val_roc= 0.84432 val_ap= 0.82438 time= 0.09249\n",
      "训练次数: 10 Epoch: 0020 log_lik= 0.5736484 train_kl= -0.01175 train_loss= 0.58539 train_acc= 0.42444 val_roc= 0.84732 val_ap= 0.82796 time= 0.07492\n",
      "训练次数: 10 Epoch: 0021 log_lik= 0.565474 train_kl= -0.01214 train_loss= 0.57761 train_acc= 0.45207 val_roc= 0.85248 val_ap= 0.83480 time= 0.07602\n",
      "训练次数: 10 Epoch: 0022 log_lik= 0.55881226 train_kl= -0.01248 train_loss= 0.57129 train_acc= 0.46962 val_roc= 0.85801 val_ap= 0.84392 time= 0.07899\n",
      "训练次数: 10 Epoch: 0023 log_lik= 0.55325073 train_kl= -0.01275 train_loss= 0.56600 train_acc= 0.47350 val_roc= 0.86582 val_ap= 0.85491 time= 0.07698\n",
      "训练次数: 10 Epoch: 0024 log_lik= 0.5455865 train_kl= -0.01297 train_loss= 0.55856 train_acc= 0.47760 val_roc= 0.87257 val_ap= 0.86401 time= 0.07601\n",
      "训练次数: 10 Epoch: 0025 log_lik= 0.5357953 train_kl= -0.01314 train_loss= 0.54894 train_acc= 0.48666 val_roc= 0.87727 val_ap= 0.87054 time= 0.07902\n",
      "训练次数: 10 Epoch: 0026 log_lik= 0.5254363 train_kl= -0.01329 train_loss= 0.53872 train_acc= 0.49271 val_roc= 0.88213 val_ap= 0.87724 time= 0.08052\n",
      "训练次数: 10 Epoch: 0027 log_lik= 0.51833695 train_kl= -0.01340 train_loss= 0.53174 train_acc= 0.49914 val_roc= 0.88421 val_ap= 0.87959 time= 0.08791\n",
      "训练次数: 10 Epoch: 0028 log_lik= 0.5147331 train_kl= -0.01349 train_loss= 0.52822 train_acc= 0.49996 val_roc= 0.88613 val_ap= 0.88246 time= 0.07798\n",
      "训练次数: 10 Epoch: 0029 log_lik= 0.51261693 train_kl= -0.01356 train_loss= 0.52617 train_acc= 0.49708 val_roc= 0.88735 val_ap= 0.88415 time= 0.07796\n",
      "训练次数: 10 Epoch: 0030 log_lik= 0.5092818 train_kl= -0.01360 train_loss= 0.52288 train_acc= 0.49655 val_roc= 0.88663 val_ap= 0.88464 time= 0.07702\n",
      "训练次数: 10 Epoch: 0031 log_lik= 0.5047625 train_kl= -0.01361 train_loss= 0.51837 train_acc= 0.50094 val_roc= 0.88616 val_ap= 0.88520 time= 0.09504\n",
      "训练次数: 10 Epoch: 0032 log_lik= 0.49929574 train_kl= -0.01358 train_loss= 0.51288 train_acc= 0.50793 val_roc= 0.88505 val_ap= 0.88490 time= 0.07406\n",
      "训练次数: 10 Epoch: 0033 log_lik= 0.49721342 train_kl= -0.01354 train_loss= 0.51075 train_acc= 0.50944 val_roc= 0.88569 val_ap= 0.88677 time= 0.08499\n",
      "训练次数: 10 Epoch: 0034 log_lik= 0.4947696 train_kl= -0.01348 train_loss= 0.50825 train_acc= 0.51161 val_roc= 0.88774 val_ap= 0.88933 time= 0.08493\n",
      "训练次数: 10 Epoch: 0035 log_lik= 0.4914976 train_kl= -0.01341 train_loss= 0.50491 train_acc= 0.51302 val_roc= 0.89046 val_ap= 0.89118 time= 0.07999\n",
      "训练次数: 10 Epoch: 0036 log_lik= 0.488526 train_kl= -0.01334 train_loss= 0.50186 train_acc= 0.51268 val_roc= 0.89338 val_ap= 0.89269 time= 0.08009\n",
      "训练次数: 10 Epoch: 0037 log_lik= 0.48732856 train_kl= -0.01327 train_loss= 0.50059 train_acc= 0.51363 val_roc= 0.89565 val_ap= 0.89481 time= 0.09199\n",
      "训练次数: 10 Epoch: 0038 log_lik= 0.48492604 train_kl= -0.01320 train_loss= 0.49813 train_acc= 0.51632 val_roc= 0.89724 val_ap= 0.89612 time= 0.08298\n",
      "训练次数: 10 Epoch: 0039 log_lik= 0.4821185 train_kl= -0.01315 train_loss= 0.49526 train_acc= 0.51880 val_roc= 0.89876 val_ap= 0.89841 time= 0.08998\n",
      "训练次数: 10 Epoch: 0040 log_lik= 0.47936314 train_kl= -0.01310 train_loss= 0.49246 train_acc= 0.52007 val_roc= 0.90026 val_ap= 0.90028 time= 0.08764\n",
      "训练次数: 10 Epoch: 0041 log_lik= 0.47643405 train_kl= -0.01307 train_loss= 0.48950 train_acc= 0.52226 val_roc= 0.90188 val_ap= 0.90239 time= 0.08066\n",
      "训练次数: 10 Epoch: 0042 log_lik= 0.47453055 train_kl= -0.01304 train_loss= 0.48757 train_acc= 0.52196 val_roc= 0.90293 val_ap= 0.90384 time= 0.07468\n",
      "训练次数: 10 Epoch: 0043 log_lik= 0.47279158 train_kl= -0.01301 train_loss= 0.48580 train_acc= 0.52170 val_roc= 0.90422 val_ap= 0.90509 time= 0.07454\n",
      "训练次数: 10 Epoch: 0044 log_lik= 0.47133213 train_kl= -0.01297 train_loss= 0.48430 train_acc= 0.52128 val_roc= 0.90486 val_ap= 0.90645 time= 0.08962\n",
      "训练次数: 10 Epoch: 0045 log_lik= 0.47012308 train_kl= -0.01291 train_loss= 0.48303 train_acc= 0.52222 val_roc= 0.90575 val_ap= 0.90752 time= 0.08366\n",
      "训练次数: 10 Epoch: 0046 log_lik= 0.46715382 train_kl= -0.01283 train_loss= 0.47998 train_acc= 0.52373 val_roc= 0.90707 val_ap= 0.90888 time= 0.07867\n",
      "训练次数: 10 Epoch: 0047 log_lik= 0.46596587 train_kl= -0.01274 train_loss= 0.47871 train_acc= 0.52402 val_roc= 0.90857 val_ap= 0.90994 time= 0.07471\n",
      "训练次数: 10 Epoch: 0048 log_lik= 0.46503893 train_kl= -0.01265 train_loss= 0.47769 train_acc= 0.52360 val_roc= 0.90967 val_ap= 0.91120 time= 0.07688\n",
      "训练次数: 10 Epoch: 0049 log_lik= 0.464061 train_kl= -0.01256 train_loss= 0.47662 train_acc= 0.52372 val_roc= 0.91034 val_ap= 0.91228 time= 0.07791\n",
      "训练次数: 10 Epoch: 0050 log_lik= 0.46333256 train_kl= -0.01248 train_loss= 0.47581 train_acc= 0.52424 val_roc= 0.91048 val_ap= 0.91262 time= 0.07637\n",
      "训练次数: 10 Epoch: 0051 log_lik= 0.4626715 train_kl= -0.01240 train_loss= 0.47507 train_acc= 0.52473 val_roc= 0.91103 val_ap= 0.91377 time= 0.07708\n",
      "训练次数: 10 Epoch: 0052 log_lik= 0.46093044 train_kl= -0.01233 train_loss= 0.47326 train_acc= 0.52615 val_roc= 0.91148 val_ap= 0.91478 time= 0.07609\n",
      "训练次数: 10 Epoch: 0053 log_lik= 0.4600002 train_kl= -0.01225 train_loss= 0.47225 train_acc= 0.52648 val_roc= 0.91161 val_ap= 0.91524 time= 0.07394\n",
      "训练次数: 10 Epoch: 0054 log_lik= 0.45957735 train_kl= -0.01217 train_loss= 0.47175 train_acc= 0.52679 val_roc= 0.91162 val_ap= 0.91608 time= 0.07701\n",
      "训练次数: 10 Epoch: 0055 log_lik= 0.45895064 train_kl= -0.01209 train_loss= 0.47104 train_acc= 0.52693 val_roc= 0.91154 val_ap= 0.91710 time= 0.07702\n",
      "训练次数: 10 Epoch: 0056 log_lik= 0.45822814 train_kl= -0.01201 train_loss= 0.47024 train_acc= 0.52726 val_roc= 0.91168 val_ap= 0.91821 time= 0.07798\n",
      "训练次数: 10 Epoch: 0057 log_lik= 0.45678172 train_kl= -0.01193 train_loss= 0.46871 train_acc= 0.52751 val_roc= 0.91158 val_ap= 0.91893 time= 0.07604\n",
      "训练次数: 10 Epoch: 0058 log_lik= 0.4562185 train_kl= -0.01185 train_loss= 0.46806 train_acc= 0.52800 val_roc= 0.91138 val_ap= 0.91894 time= 0.07898\n",
      "训练次数: 10 Epoch: 0059 log_lik= 0.45532253 train_kl= -0.01177 train_loss= 0.46710 train_acc= 0.52856 val_roc= 0.91181 val_ap= 0.91992 time= 0.07507\n",
      "训练次数: 10 Epoch: 0060 log_lik= 0.45485058 train_kl= -0.01172 train_loss= 0.46657 train_acc= 0.52930 val_roc= 0.91116 val_ap= 0.91931 time= 0.07596\n",
      "训练次数: 10 Epoch: 0061 log_lik= 0.4545688 train_kl= -0.01167 train_loss= 0.46623 train_acc= 0.52911 val_roc= 0.91015 val_ap= 0.91850 time= 0.07594\n",
      "训练次数: 10 Epoch: 0062 log_lik= 0.4536566 train_kl= -0.01162 train_loss= 0.46527 train_acc= 0.52991 val_roc= 0.90974 val_ap= 0.91925 time= 0.07447\n",
      "训练次数: 10 Epoch: 0063 log_lik= 0.45293954 train_kl= -0.01158 train_loss= 0.46452 train_acc= 0.52905 val_roc= 0.90963 val_ap= 0.91929 time= 0.07999\n",
      "训练次数: 10 Epoch: 0064 log_lik= 0.45260116 train_kl= -0.01156 train_loss= 0.46416 train_acc= 0.53090 val_roc= 0.90961 val_ap= 0.91928 time= 0.08901\n",
      "训练次数: 10 Epoch: 0065 log_lik= 0.45211104 train_kl= -0.01154 train_loss= 0.46365 train_acc= 0.53006 val_roc= 0.90919 val_ap= 0.91796 time= 0.07602\n",
      "训练次数: 10 Epoch: 0066 log_lik= 0.45177153 train_kl= -0.01152 train_loss= 0.46329 train_acc= 0.53110 val_roc= 0.90927 val_ap= 0.91781 time= 0.09086\n",
      "训练次数: 10 Epoch: 0067 log_lik= 0.45121324 train_kl= -0.01150 train_loss= 0.46271 train_acc= 0.53155 val_roc= 0.90931 val_ap= 0.91789 time= 0.07817\n",
      "训练次数: 10 Epoch: 0068 log_lik= 0.4506472 train_kl= -0.01148 train_loss= 0.46213 train_acc= 0.53120 val_roc= 0.90918 val_ap= 0.91791 time= 0.07613\n",
      "训练次数: 10 Epoch: 0069 log_lik= 0.45041093 train_kl= -0.01145 train_loss= 0.46186 train_acc= 0.53246 val_roc= 0.90906 val_ap= 0.91783 time= 0.07585\n",
      "训练次数: 10 Epoch: 0070 log_lik= 0.44998428 train_kl= -0.01144 train_loss= 0.46142 train_acc= 0.53121 val_roc= 0.90941 val_ap= 0.91719 time= 0.07602\n",
      "训练次数: 10 Epoch: 0071 log_lik= 0.4490243 train_kl= -0.01142 train_loss= 0.46044 train_acc= 0.53295 val_roc= 0.90918 val_ap= 0.91714 time= 0.08105\n",
      "训练次数: 10 Epoch: 0072 log_lik= 0.44922906 train_kl= -0.01141 train_loss= 0.46064 train_acc= 0.53183 val_roc= 0.90903 val_ap= 0.91677 time= 0.07699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0073 log_lik= 0.44829127 train_kl= -0.01140 train_loss= 0.45969 train_acc= 0.53302 val_roc= 0.90945 val_ap= 0.91733 time= 0.07600\n",
      "训练次数: 10 Epoch: 0074 log_lik= 0.44783103 train_kl= -0.01139 train_loss= 0.45922 train_acc= 0.53314 val_roc= 0.91006 val_ap= 0.91768 time= 0.08004\n",
      "训练次数: 10 Epoch: 0075 log_lik= 0.44771588 train_kl= -0.01138 train_loss= 0.45910 train_acc= 0.53309 val_roc= 0.91052 val_ap= 0.91866 time= 0.07996\n",
      "训练次数: 10 Epoch: 0076 log_lik= 0.4472785 train_kl= -0.01138 train_loss= 0.45866 train_acc= 0.53263 val_roc= 0.91051 val_ap= 0.91883 time= 0.07708\n",
      "训练次数: 10 Epoch: 0077 log_lik= 0.44714028 train_kl= -0.01136 train_loss= 0.45850 train_acc= 0.53319 val_roc= 0.91036 val_ap= 0.91848 time= 0.07895\n",
      "训练次数: 10 Epoch: 0078 log_lik= 0.44715127 train_kl= -0.01135 train_loss= 0.45850 train_acc= 0.53289 val_roc= 0.91109 val_ap= 0.91925 time= 0.07999\n",
      "训练次数: 10 Epoch: 0079 log_lik= 0.44633862 train_kl= -0.01134 train_loss= 0.45768 train_acc= 0.53294 val_roc= 0.91123 val_ap= 0.91936 time= 0.08193\n",
      "训练次数: 10 Epoch: 0080 log_lik= 0.4461443 train_kl= -0.01133 train_loss= 0.45747 train_acc= 0.53302 val_roc= 0.91143 val_ap= 0.91931 time= 0.07606\n",
      "训练次数: 10 Epoch: 0081 log_lik= 0.4455805 train_kl= -0.01132 train_loss= 0.45690 train_acc= 0.53282 val_roc= 0.91224 val_ap= 0.92077 time= 0.08106\n",
      "训练次数: 10 Epoch: 0082 log_lik= 0.44592437 train_kl= -0.01133 train_loss= 0.45726 train_acc= 0.53386 val_roc= 0.91172 val_ap= 0.92027 time= 0.07992\n",
      "训练次数: 10 Epoch: 0083 log_lik= 0.4456171 train_kl= -0.01133 train_loss= 0.45694 train_acc= 0.53438 val_roc= 0.91194 val_ap= 0.92021 time= 0.07606\n",
      "训练次数: 10 Epoch: 0084 log_lik= 0.44490123 train_kl= -0.01133 train_loss= 0.45623 train_acc= 0.53391 val_roc= 0.91281 val_ap= 0.92064 time= 0.07603\n",
      "训练次数: 10 Epoch: 0085 log_lik= 0.44536504 train_kl= -0.01134 train_loss= 0.45670 train_acc= 0.53403 val_roc= 0.91266 val_ap= 0.92073 time= 0.08397\n",
      "训练次数: 10 Epoch: 0086 log_lik= 0.44445324 train_kl= -0.01133 train_loss= 0.45579 train_acc= 0.53455 val_roc= 0.91198 val_ap= 0.92081 time= 0.08001\n",
      "训练次数: 10 Epoch: 0087 log_lik= 0.44474363 train_kl= -0.01133 train_loss= 0.45607 train_acc= 0.53346 val_roc= 0.91195 val_ap= 0.92109 time= 0.07593\n",
      "训练次数: 10 Epoch: 0088 log_lik= 0.44392633 train_kl= -0.01134 train_loss= 0.45527 train_acc= 0.53502 val_roc= 0.91261 val_ap= 0.92202 time= 0.07803\n",
      "训练次数: 10 Epoch: 0089 log_lik= 0.44427633 train_kl= -0.01135 train_loss= 0.45563 train_acc= 0.53513 val_roc= 0.91284 val_ap= 0.92135 time= 0.07599\n",
      "训练次数: 10 Epoch: 0090 log_lik= 0.44323483 train_kl= -0.01135 train_loss= 0.45458 train_acc= 0.53373 val_roc= 0.91219 val_ap= 0.92087 time= 0.07800\n",
      "训练次数: 10 Epoch: 0091 log_lik= 0.44379848 train_kl= -0.01134 train_loss= 0.45514 train_acc= 0.53452 val_roc= 0.91200 val_ap= 0.92077 time= 0.09006\n",
      "训练次数: 10 Epoch: 0092 log_lik= 0.44312483 train_kl= -0.01134 train_loss= 0.45446 train_acc= 0.53523 val_roc= 0.91172 val_ap= 0.92160 time= 0.09293\n",
      "训练次数: 10 Epoch: 0093 log_lik= 0.4432764 train_kl= -0.01135 train_loss= 0.45462 train_acc= 0.53412 val_roc= 0.91217 val_ap= 0.92154 time= 0.09097\n",
      "训练次数: 10 Epoch: 0094 log_lik= 0.442956 train_kl= -0.01134 train_loss= 0.45429 train_acc= 0.53511 val_roc= 0.91263 val_ap= 0.92168 time= 0.09202\n",
      "训练次数: 10 Epoch: 0095 log_lik= 0.44315028 train_kl= -0.01133 train_loss= 0.45448 train_acc= 0.53386 val_roc= 0.91291 val_ap= 0.92189 time= 0.09391\n",
      "训练次数: 10 Epoch: 0096 log_lik= 0.44238365 train_kl= -0.01133 train_loss= 0.45372 train_acc= 0.53587 val_roc= 0.91259 val_ap= 0.92203 time= 0.09001\n",
      "训练次数: 10 Epoch: 0097 log_lik= 0.44240674 train_kl= -0.01133 train_loss= 0.45374 train_acc= 0.53452 val_roc= 0.91139 val_ap= 0.92130 time= 0.11100\n",
      "训练次数: 10 Epoch: 0098 log_lik= 0.4425055 train_kl= -0.01133 train_loss= 0.45383 train_acc= 0.53489 val_roc= 0.91180 val_ap= 0.92133 time= 0.07601\n",
      "训练次数: 10 Epoch: 0099 log_lik= 0.4420691 train_kl= -0.01133 train_loss= 0.45340 train_acc= 0.53587 val_roc= 0.91346 val_ap= 0.92231 time= 0.08096\n",
      "训练次数: 10 Epoch: 0100 log_lik= 0.4417834 train_kl= -0.01133 train_loss= 0.45311 train_acc= 0.53548 val_roc= 0.91386 val_ap= 0.92236 time= 0.08107\n",
      "Optimization Finished!\n",
      "训练次数: 10 ROC score: 0.9109455620406943\n",
      "训练次数: 10 AP score: 0.9127978944933867\n",
      "Average Test ROC score: 0.9146084852500099\n",
      "Average Test AP score: 0.9204238926852998\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABNN0lEQVR4nO3dd3hUZfbA8e9JgdCL9BqUEpr0YC/YcFWwS/kJFlZxLajoyrLrqmt3rbh2FHQBUVFc2+oqFlSkhN47hNB7T0g5vz/eiYQwCZMwM3dmcj7PM08yd+6dezJJ5sx92xFVxRhjjCkszusAjDHGRCZLEMYYY/yyBGGMMcYvSxDGGGP8sgRhjDHGL0sQxhhj/LIEYUwZJiLJIqIikuB1LCbyWIIwEU1EfhSRnSJS3s/2QYW2nSMiGQXui4jcJSILRGS/iGSIyEci0r6Y8/2fiKwRkT0iMk1EGgUY5w2+N9pr/cSUJyL7RGSviCwVkRuLeI6A9w0Vf6+rKbssQZiIJSLJwJmAAr1K8RQvAUOAu4CaQEvgU+CSIs5XGRgF3AJUB+4AMgM810Bgh+9rYRtUtTJQFXgAeEtE2hTxPAX3vce3b6sAYzAmqCxBmEg2AJgKjMb/G2+RRKQFcDvQV1W/V9UsVT2gqmNV9akiDlMgB1itqnmqOkNVtwVwrqbA2bjEcpGI1PX75M6nwE6gqARRcN+vcEnnZN954kRkmIisFJHtIvKhiNT0PZYkImN823eJyIz8OHxXROcXiPdhERnj5+d4HJeQ/+W7ivmX7yrsBRHZIiK7RWSeiLQ71mtiYoMlCBPJBgBjfbci33iLcB6QoarTS3DMIWAO8KGI1CjBcQOANFX9GFgM9Pe3k+8N/grc1cn84p7Qt28voBawwrf5LuByXDJqgEs0r/geGwhUAxoDJwCDgYMl+BlQ1b8CPwN3qGplVb0DuBA4C3f1VR24Dthekuc10csShIlIInIG0BT4UFVnAiuBfiV4ihOAjSU87cvAXOB94Lv8JCEij4vIc8UcNwAY5/t+HEdf7TQQkV3ANuAh4HpVXVrEc+XvexCYCNyrqrN9j90K/FVVM1Q1C3gYuNrXwZyN+5mbq2quqs5U1T0B/tzFyQaqACmAqOpiVS3p62qilCUIE6kGAv8r0MRT+I03B0gsdEwi7g0N3Kfc+oGeTEQqATcDz6jqM8C3HE4SpwHfFXHc6UAzYHyBONuLSMcCu21Q1eqqWlNVO6rq+MLPU3hfXB/ECKBHgceaAhN9TUi7cFcruUBd4N/AN8B4EdkgIs+ISOHXp8RU9XvgX7grlc0i8qaIVD3e5zXRwRKEiTgiUgG4FjhbRDaJyCZch20HEeng2y0dSC50aDNgre/7SUAjEeka4GnjgHhc4kFVhwFpuD6QisDXRRw3EBBgji/Oab7tAwI8r1++K4QHcMnmct/mdcDFvmSTf0tS1fWqmq2qj6hqG1xCu7RADPt9P0O+esWd2k8sI1S1C9AW19R0//H8bCZ6WIIwkehy3CfjNkBH3601rn08/03vA+BGEUn1daS2xCWR8QCquhx4FXjfN3y0nK8jt4+IDCt8QlXdi0sCr4pIXREpB3wPnITrmzjq07iIJOES2S0F4uwI3An0P965Bap6CHgO+Ltv0+vA475OcUSktoj09n1/roi0F5F4YA/uSirXd9wcoI+IJPoS5tXFnHYzcGKBn7GbiHT3XY3sx43qyi3qYBNjVNVudouoG+6N+jk/268FNgEJvvs3AQtxb4grgGFAXIH9BTfMdSFwAFiPSyxtizhvTeBt3zk2A58BHYBfgDF+9u+D6+dILLQ9CdffcClwDq6zPJCf+6h9cZ/8twGX4T7Q3QssBfbi+mWe8O3X17d9vy/2EQVepxNxVzb7gC99j43xPZaMu2rI3/dUYBmuA3wErrN/nu/YbbgBA5W9/huxW3hu4vujMMYYY45gTUzGGGP8sgRhjDHGL0sQxhhj/LIEYYwxxq+YWuK3Vq1ampyc7HUYxhgTNWbOnLlNVWv7eyymEkRycjJpaWleh2GMMVFDRNYW9Zg1MRlTlLFjITkZ4uLc17FjvY7ImLAKaYIQkZ6+oicr/M1eFZEaIjLRt4Tw9PxlhH0zXqeLyFwRWSgij4QyTmOOMnYs3HILrF0Lqu7rLbdYkjBlSsgShG/K/yvAxbglE/r6KZIyHJijqifjllB4ybc9C+ihqh1wSxf0FJFTQhWrMUf561/hwIEjtx044LYbU0aE8goiFVihqqvUrSkzHuhdaJ82uEXVUNUlQLKI1FVnn2+fRN/Npnyb8ElPL9l2Y2JQKBNEQ9zqk/kyfNsKmgtcCSAiqbjljBv57seLyBxgC/Ctqk7DDxG5RUTSRCRt69atwf0JTNnVpEnJthsTg0KZIMTPtsJXAU8BNXyJ4E5gNoeXW85V1Y64hJFaVJlDVX1TVbuqatfatf2O1DKm5B5/3HVOF5SQ4LYbU0aEcphrBq78Yb5GwIaCO6ireHUjgIgIsNp3K7jPLhH5EegJLAhhvMYcdsYZkJcH1arBnj1QsSJkZkJqqteRGRM2obyCmAG0EJFmvrX1++CWT/6diFT3PQYwCJisqnt869xX9+1TATgfWBLCWI050nvvua9z5rhEsXy5SxJ33eVGNRlTBoQsQahqDnAHrgziYlxt4YUiMlhEBvt2aw0sFJEluNFOQ3zb6wM/iMg8XKL5VlW/CFWsxhxBFUaPhh493PwHgPr14ZFH4Ouv4bPPijvamJgRU/UgunbtqlE7k3rsWDeEMj3ddYQ+/jj07+91VGXT5Mlw9tnuKuL66w9vz86GTp1g/35YtAgqVPAuRmOCRERmqqrf0rw2kzoS2KSsyDJ6NFSpAldeeeT2xET4179gzRp46ikvIjMmrCxBeCEnB5YsgY8+gr//Hf74R5uUFSn27YMPP4Rrr4VKlY5+/JxzoE8fePppWLky7OEZE04xtVhfxFGF9eth/nxYsMB9nT8fFi+GrCy3T1yc6wT1xyZlhd/HH7smpBtuKHqfZ5+FL76Au++Gzz8PV2TGhJ0liGDZtevIJJCfFHbtOrxPgwbQvj2cfz60a+e+b93a3db6WVAxIQF++w1OPTVcP4UZPRqaN4fTTy96n4YN3ZXfn//sEsWll4YtPGPCyTqpSyory10BFEwC8+dDRsbhfapWdW/+7dsfTgTt2kHNmv6fM78PomAzU7lybljlrl1w002uzdsmAobW6tVw4onw2GPHbt47dAg6dHBfFy6EpKTwxGhMkBXXSW1XEEWNHsrLg1Wrjm4eWr4ccnPdseXKuU//Z599ZEJo3BjE30TyIuSPViocR+/e8Oij8Pzz8Mkn8MQTLpHExwf/dTDw7rvu91Zw5FJRypWDl1+GCy6Af/4THnww9PEZE2Zl+wrC3yf3+Hj3Br158+HtIu6TZf7VQH4iaNHCjWwJtcWL4fbb4YcfoHNnePVV6N499OctS/Ly4KSTXPPSt98Gftw117hmpsWLD8+ZMCaKFHcFUbYTRHKy/7b/pCQYPPhwQmjb1v+IlnBShQ8+gHvvhY0bYdAgePJJqFXL27hixY8/wrnnug8N/foFfty6dZCSAhdeCBMnhiw8Y0LF5kEUpahRQllZ8MILcPPNbu0dr5MDuKuYPn1g6VIYOtR1prZqBW++ebjJy5TeqFGu7+jyy0t2XOPG8Le/waefulnWxsSQsp0gonFJ5ypV3DDLOXPc1c2tt7pRTjNmeB1Z9Nq7FyZMgOuucwMDSuree11z4113HR6+bEwMKNsJ4vHHj35DqFgxOpZ0btvW9UmMHeuaObp3d81i27d7HVn0mTDB9TfdeGPpji9f3nVYL1/uBhQYEyPKdoLo39810TRt6ppwmjZ196NlDSQR116+dKmbtDVypGt2evvtoiffmaONGuVet1OOo6rtRRe55qnHHrMJjiZmlO0EAS4ZrFnj3lDXrIme5FBQ1aruk+usWW7Y7aBBcNpp7r4p3ooV8PPPbuZ0SYYm+/PCC+7v6L77ghKaMV6zBBFLTj7ZrUT63nsu2XXt6obH7tzpdWSR69133XIngcx9OJbkZBg+3K2x9d13x/98xngsoAQhImeISH7lt9oi0iy0YZlSy5/otWQJ3HknvP46tGzpmlGs2elIeXkuQVxwgVs+Ixjuv9/Np7jzTjfL2pgodswEISIPAQ8Af/FtSgTGBPLkItJTRJaKyAoRGebn8RoiMlFE5onI9Py60yLSWER+EJHFIrJQRIYc/eymWNWrw0svuWamli3dch1nnOFGPxnnhx9cB39pO6f9SUpyr/uSJe6rMVEskCuIK4BewH4AVd0AVDnWQSISD7yCqxTXBugrIm0K7TYcmKOqJwMDgPz/qBxgqKq2Bk4BbvdzrAlEhw6ujX30aNfe3qWLG45ZcBHBsmrUKFdzunfv4D7vJZe4BfweecSt5mtMlAokQRxSN91aAUQk0FljqcAKVV2lqoeA8UDh/8Q2wCQAVV0CJItIXVXdqKqzfNv34kqWBqkNoAyKi4OBA91op9tug1decaN23nuv7NZX3r3brW/Vt29oFtp76SVX9+P++4P/3MaESSAJ4kMReQOoLiJ/BL4D3grguIbAugL3Mzj6TX4ucCWAiKQCTYFGBXcQkWSgEzAtgHOa4tSo4SqizZjh1pYaOBDOOgvmzfM6svD76CM4eDC4zUsFnXgiPPAAvP++W8bDmChUbIIQEQE+ACYAHwOtgL+r6ssBPLe/MYOFP64+BdQQkTnAncBsXPNS/vkr+857t6ruKSLGW0QkTUTStm7dGkBYhs6d4ddf3XyJJUvc/XvucfMokpPdFUdycmyXPB01yg0J7tYtdOcYNsy9jnfc4epZm9gxdmzZ+F9R1WJvwMxj7VPEcacC3xS4/xfgL8XsL8AaoKrvfiLwDXBvoOfs0qWLmhLavl31tttUXWPTkbeKFVXHjPE6wuBbutT9fE8/HfpzffqpO9fzz4f+XCY8xoxx/xsx8r8CpGkR76mBNDFNFZHSfMyaAbQQkWYiUg7oA3xWcAcRqe57DGAQMFlV9/iuXN4GFquqrV0QSjVruuXD69U7+rFYrYsdzLkPx9KrF/TsCQ895FbhNdFv+PAyU0M+kARxLi5JrPQNR50vIsdstFbVHOAO3FXAYuBDVV0oIoNFZLBvt9bAQhFZghvtlD+c9XTgeqCHiMzx3f5Qwp/NlMTmzf63x9qyEbm5LkH07An164f+fCIwYoRbxO+BB0J/PhN8mZluAupjj7ll3Yv6n4i1/xUCqyh3cWmfXFW/Ar4qtO31At//BrTwc9wv+O/DMKHSpIn/2hiRvLJtaUya5IaevvBC+M7ZooVbfiO/IuAZZ4Tv3Kbk9u1zteAnT3a3adMOr9J78sluReW9e48+TsSNELzllvAUEguDY15BqOpaoDpwme9W3bfNxBJ/K9vGx0fHyrYlMWqUG83Vq1d4zzt8uKsdcfvtbviriRw7d8Lnn7shyd27u0mmF17oCnIdPOgGGfznP26l5Llz4bXXjv5fSUpyQ8fvuMMlkS++iIkh5IHMpB4CjAXq+G5jROTOUAdmwqzwyrZVq7rmmFatvI4seHbtclXf+vVzS3SHU6VK7qpl3jz3BmO8s3mzW+L9rrugY0c44QT3gWHECFdr/IEH4JtvXOKYPt3VX+nVy/XXgf9VoEeOhIULXSLJy4PLLnNLuMyd6+mPeryOWXLU199wqqru992vBPymbvZzRClxyVFTtD17XH3m1q3dOP7jXek0ErzxhquZMWOGW8gw3FTdJ9MZM9ykxbp1wx9DWbRu3eHmop9+cq89uKuA005zc4HOOstVj6xQ4fjPl53t1kB7+GGXZG64wfVfNGhw/M8dAsWVHA1kuOp8IKnA/SRg/rGO8+Jmw1yD7NVX3RC+iRO9jiQ4undXbddONS/PuxgWL1ZNTFS98UbvYoh2Y8aoNm2qKuK+FhxempenumyZ6siRqgMGqCYnHx6KWq2a6iWXuOHNv/2mmpUV2jh37FAdOtT9vitWVH34YdV9+0J7zlKgmGGugSSIe3Eznh/23ebgJq55nhAK3yxBBFl2tmrr1qotWoT+nynUFi1yf+7PPut1JKp//rOLZcoUryOJPv7mICQlqQ4cqHrddar16x/eXquW6pVXqr74ours2ao5Od7EvGKF6tVXu5gaNFAdNUo1N9ebWPwoLkEcs4nJdwnSGTgDN7JosqrOPo4rmpCxJqYQ+PJLt/DcSy+5NttoNWyYa0vOyPA/5yOc9u2DlBTXxDR9uhsMYAKTnOx/tB24JdvPPvtwk1FKSmQ1jf76q6tfPn266/t47jno0cPrqIptYgqkD+IUYKG6RfMQkSpAG1WNuLWRLEGEgKrrbJs9G1audCM8ok1Ojhuu27UrfPbZsfcPhw8+gD593CTF227zOproERfnf3SQiBtUEUkJwZ+8PPe7HzbMzZu47DL45z89HQxSXIIIZKLca8C+Avf3+7aZskDEffLeuTN6h7x++62bxXzDDV5Hcti118K557rZt9u2eR1N9Kha1f/2Jk0iPzmAS3B9+7o10J580g0AadfOFZiKwL+DQBKEaIHLDFXNI7AJdiZWdOzoVn4dMQJWr/Y6mpIbPdoNZbz0Uq8jOUwEXn7ZTbgaPtzraKLD+PFumfbCTXIVK0bfh5cKFdxVxIoV8Mc/uivJ5s3dh7H8SXkRIJAEsUpE7hKRRN9tCLAq1IGZCPPYY5CQ4P6oo8mOHfDpp27serlyx9w9rNq2df06I0e6oa+maL/+6q4AzzjDrUJccA7Cm2+63280qlPHJYf58+H0091kvdat3XL0kTDRrqje6/wbbnLceGCL7zYOqHOs47y42SimEPv736Nv9M0rr7iYZ83yOhL/du9WrVdPtVu3iBrZElGWL1c94QQ3mm7bNq+jCa3//U+1fXv3N3vaaW44bohxPKu5quoWVe2jqnV8t36quiWUSctEqPvvdyOAhg6NjE83gRg92pVd7dTJ60j8q1rVNSvMmOE+GZsj7djhSriquhF1J5zgdUShlT8g5K23YNUqOPVU12exZo0n4RSZIETkjyLSwve9iMg7IrLbt6Jr5/CFaCJG5cquqem339xSBZFu4UL3xhtJndP+9OsHZ54Jf/mLe0M0TlYWXHGFe3P89FO36GFZEB8PgwbB8uXw4INu+Y6UFNe8u3t3eGMp6tICWAAk+r7vB8wETgDOB34u6jgvb9bEFAY5Oe4SuFkz1cxMr6Mp3n33qSYkqG7Z4nUkxzZ3rmp8vCveZNyM6Ouvd00tY8d6HY231q1zs8LzJ/+98oqbxBoklLKJKUdV8+skXgq8p6rbVfU7oFKoEpaJcPHxrklk9WpX3zpSZWfDv//tRi7Vru11NMd28slupdfXX4dZs7yOxnuPPup+f//4h7vCKssaNXI1TGbOdENib7/d/b18+WXoS58WlTmAWUB93NpLm4G2BR5bXNRxXt7sCiKMevZUrV49cjsNP//cfeL69FOvIwnczp2qdeqonnJK2e6w/ve/3e9uwABv182KRHl57m+6RQv3GsXF6fGWPqWUVxB/B9JwdaI/U9WFACJyNgEOcxWRniKyVERWiMhR4yNFpIaITPT1a0wXkXYFHntHRLaIyIJAzmXC7Nln3Yqvjz7qdST+jR7trhz+EEWFCKtXh2eegalT3SfGsmjyZLj5ZjjnHNdRGw2T38JJBHr3hgULXF2TvLwjHw9y6dNil9oQkQSgiqruLLCtku+4fUUe6PaLB5YBFwAZuBrVfVV1UYF9/gnsU9VHRCQFeEVVz/M9dhZuBvd7qtruqBP4YUtthNmtt8I778CiRZHVgbhtm1ta+Y474PkoK2mel+fG+q9YAcuWRefSJqW1bJkbtVO7NkyZcrj+gvGvuGVHCieOYpR6qQ1VzSmYHHzb9h8rOfikAitUdZWqHsLNpehdaJ82wCTf8y4BkkWkru/+ZMCGdESyRx5xlbQirdby+++7PohIH73kT1ycK1u5fTv8/e9eRxM+27a54axxca5t3ZLDsRVVDjiIZYIDmUldWg2BdQXuZ/i2FTQXuBJARFKBpkCjkpxERG4RkTQRSdu6detxhGtKrF49lxwmTnRNA5Fi9Gg37+HkiKtpFZhOnVxho1deifqKZAHJzITLL3eFff7zHzjpJK8jig7+ygQHedmRUCYIf42Hha+HngJqiMgc4E5gNlCigr2q+qaqdlXVrrWjYbRKrLn3XrfM8n33leiyNmTmzXOjgG680etIjs9jj7lP0XfcET2TEktD1fU5/Pqr63c57TSvI4oe/kqfBnnZkeImyl0kIlf72d5fRC4I4LkzgMYF7jcCNhTcQVX3qOqNqtoRGADUBqJwNbgyrGJFeOIJNyFt/Hivo3FXD4mJbvZpNKtRA556Cn75BcaM8Tqa0Hn4YRg3zv0NXXed19FEn/793UTCvDz3NchrUhXZSS0iU4HLVHVroe31gImqemqxT+w6uJcB5wHrcZ3U/fJHQ/n2qQ4cUNVDIvJH4ExVHVDg8WTgC+ukjnB5ea7WwvbtbhnjYNT1LY3sbHc1c+aZ8PHH3sQQTHl5rtN27VrXgVvUUtfR6r333CrBN93kFiy0EUueKG0ndcXCyQFAVTcRwEQ5Vc0B7gC+ARYDH6rqQhEZLCKDfbu1BhaKyBLgYmBIgaDfB34DWolIhojcfKxzGo/ExbnqWOnprvKcV776CrZujf7mpXz5HdabN7vJUqGaDOWFH390y0n06OEmB1pyiEjFXUEsw1WOyym0PRFYpKoRNK7RsSsIj/Xq5f7xV6xwyxiH2xVXuHWiMjLc0uSxYOxYNxorp8C/YcWK0b3E9ZIl7sqofn03nLUsDeWNQKW9gvgEeMs37yH/iSoBr/seM+ZIzzzjJuo88kj4z71lC3zxBVx/fewkB3CTnnIKjdsI8mSosNq61Q1nTUx0w1ktOUS04hLE33BLbKwVkZkiMgs3q3qr7zFjjpSS4oZnvvEGLF4c3nOPG+feSAcODO95Qy09vWTbI1lmppsFvGGDqw3erJnXEZljKDJB+CbJDcONRLoBGAg0UdVhengRP2OO9NBDUKkS/PnP4T3v6NGuo7xdQOMZokdRk55UXRt+tMz9yctzTWW//eYW4TvlFK8jMgEobpjrlSJyJa7zuAXQHOgqIlXCFZyJQrVru+aPL76A778Pzzlnz3YTymKlc7ogf5OhKlRwzTTvvgutWrmSlbm53sQXqAcfhA8+gKefhquPGj1vIlRxTUyXFbr1Au4D5olIjzDEZqLVXXe5STtDh4bnjWv0aFdvuk+f0J8r3PxNhnrrLZeA582Dzp3d8s/durlP55HonXfcPIc//tFVJTTRo6hlXou64ZbDmFbS48Jxs+W+I8i4cW754dGjQ3uerCxXr/iaa0J7nkiVl6f64YeqjRq513vgQNVNm7yO6rDvvnNFmy64QPXQIa+jMX5wPDWp/SSUtUBikPOUiTV9+kBqqmtuOnAgdOf54gs3QS8Wm5cCIQLXXOMGBQwb5jrrW7WCESOOHv0UbosWwVVXuXg++siNXDJRpcQJQkRaAVkhiMXEEhG31Pb69W4SXaiMHu3G018QyOovMaxyZXjySZg/H7p3hyFDXPOTV4sobt7s+kmSktxw1mrVvInDHJfiOqk/F5HPCt1+Ab4ChoYvRBO1Tj/dfYJ8+mnYuDH4z795s5s9HWtzH45Hq1bw9dfwySeuwP3ZZ8P//V9oXv+iHDzohrNu3gyff+76TUxUKu4K4lnguQK3Z4FbgdaqOiUMsZlY8NRTcOhQaGobjBnjOsGjse5DKIm4WeWLF8Pf/uaad1q1cld02SEeoZ6XBwMGwPTpbhZ4t26hPZ8JqeLmQfxU6DZZ3UJ73UTklTDGaKJZ8+ZulM0777jmj2BRdc1L3btD69bBe95YUrGiKwm7cKFbwHDoUOjYEX74IXTnHD4cJkxwJWmvuCJ05zFhEVAfhIh0FJFnRGQN8BiwJKRRmdjy4IOuDTqYQxxnzXJ1ee3q4diaN3ed+Z995pp/evRwgwgyMoJ7nrfecs2Jt90G99wT3Oc2niiuD6KliPxdRBYD/8JVhxNVPVdVXw5bhCb61azpksQ337hbMIwaBeXLx+bch1AQgcsuc1cTDz/sKrelpLj1sw4dOv7n//Zblxh69nQjqGx11phQ3GquecDPwM2qusK3bZWqnhjG+ErEVnONYIcOQZs2bhbwnDkQH1/658rKciOXLrrI1Z82Jbd6Ndx9t7uqaNUKXn659CPBFixwAxKSk+Hnn2OvbkWMK+1qrlcBm4AfROQtETkP/2VEjTm2cuVch/WCBa4/4nh89hns3GnNS8ejWTN3FfHll66j/8IL3RIYJV0EcNMmN5y1UiXXjGXJIaYU10k9UVWvA1KAH4F7gLoi8pqIXBim+Ewsueoq90nzwQdh377SP8/o0a5y3PnnBy20MusPf3CDBx57zA0ZTklx6z9lBTDV6cAB12y1bZsbztq48bGPMVHlmJ3UqrpfVceq6qW4utJzgGGBPLmI9BSRpSKyQkSOOkZEaojIRBGZJyLTRaRdoMeaKCTiJs1t3uzavktj40Y3zn/AgONrpjKHJSW5Ge+LF8PFF7uhse3awX//W/QxublufsXMma6Zr0uX8MVrwqeoNTiO9wbEAyuBE4FywFxchbqC+/wTeMj3fQowKdBj/d1sLaYo0aePaoUKquvWlfzYp592aw4tXRr8uIzzzTeqLVu617l3b9VVq47eZ+hQ9/iLL4Y9PBNcBHMtphJIBVao6ipVPQSMB3oX2qcNMAlAVZcAySJSN8BjTbR68kn3CfRvJaw7lT/34bTToGXLkIRmcP0R8+e7PqPvvnODCx55xI0cS04+fCV4wQVu5V4Ts0KZIBrihsbmy/BtK2gucCWAiKTiVoptFOCx+I67RUTSRCRta7QUTynrkpPdWkHvvedqOQRqxgzXDGKd06FXrhw88ICrH92rlxsae/PNsHbt4X1+/dUtDmhiVigThL8RT4XH1D4F1BCROcCdwGwgJ8Bj3UbVN1W1q6p2rV279nGEa8Jq+HA3P2LoUHdlEIhRo9ww2WuvDW1s5rBGjVyhnzp1jv49RXNtbBOQUCaIDFy50nyNgA0Fd1DVPap6o6p2BAYAtYHVgRxrolz16u5T6Q8/uKGWx5KZ6TpDr7zSVgb1QlFX59FYG9sELJQJYgbQQkSaiUg5oA/wWcEdRKS67zGAQcBkVd0TyLEmBtx6q5ukdf/9x15E7j//cauTWvOSN4qqjV3UdhMTQpYgVDUHuAP4BlgMfKiqC0VksIgM9u3WGlgoIktwta+HFHdsqGI1HklMdMNdlyxx6/gUZ9QoN86+h1W79YS/2tgVK7rtJmYVudRGNLKlNqKQKpx7rlsjaMUK/81H69e7T6rDh7vVSY03xo51fQ7p6e738fjjrma2iWqlXWrDmNDLHzK5bZsbVunPv//t6gwMHBje2MyR+veHNWvc72LNGksOZYAlCOO9Ll1cVbgXXjhyGCW4K4xRo1w9g+bNvYnPmDLKEoSJDI8/7q4mhg8/cvvUqbBsmXVOG+MBSxAmMjRu7OZEjBvnylXmGzXKdYZec413sRlTRlmCMJHjgQfchKz8yXMHDrhJWldfDVWqeB2dMWVOgtcBGPO7KlXgH/+AwYPh009decw9e6x5yRiP2BWEiSw33wwNGrjlNPr3d0t6r1/vdVTGlEmWIExk+eAD2L4dcnLc/dxcN+N67Fhv4zKmDLIEYSLLX/96dDUzWxTOGE9YgjCRpajF32xROGPCzhKEiSy2KJwxEcMShIkstiicMRHDEoSJLP37w5tvQtOmbmZ106buvq37Y0zYxdRqriKyFVh7zB39qwVsC2I40cxeiyPZ63Ekez0Oi4XXoqmq+i3HGVMJ4niISFpRS96WNfZaHMlejyPZ63FYrL8W1sRkjDHGL0sQxhhj/LIEcdibXgcQQey1OJK9Hkey1+OwmH4trA/CGGOMX3YFYYwxxi9LEMYYY/wq8wlCRHqKyFIRWSEiw7yOx0si0lhEfhCRxSKyUESGeB2T10QkXkRmi8gXXsfiNRGpLiITRGSJ72/kVK9j8pKI3OP7P1kgIu+LSJLXMQVbmU4QIhIPvAJcDLQB+opIG2+j8lQOMFRVWwOnALeX8dcDYAiw2OsgIsRLwNeqmgJ0oAy/LiLSELgL6Kqq7YB4oI+3UQVfmU4QQCqwQlVXqeohYDzQ2+OYPKOqG1V1lu/7vbg3gIbeRuUdEWkEXAKM9DoWr4lIVeAs4G0AVT2kqrs8Dcp7CUAFEUkAKgIbPI4n6Mp6gmgIrCtwP4My/IZYkIgkA52AaR6H4qUXgT8DeR7HEQlOBLYCo3xNbiNFpJLXQXlFVdcDzwLpwEZgt6r+z9uogq+sJwjxs63Mj/sVkcrAx8DdqrrH63i8ICKXAltUdabXsUSIBKAz8JqqdgL2A2W2z05EauBaG5oBDYBKIvJ/3kYVfGU9QWQAjQvcb0QMXiaWhIgk4pLDWFX9xOt4PHQ60EtE1uCaHnuIyBhvQ/JUBpChqvlXlBNwCaOsOh9YrapbVTUb+AQ4zeOYgq6sJ4gZQAsRaSYi5XCdTJ95HJNnRERwbcyLVfV5r+Pxkqr+RVUbqWoy7u/ie1WNuU+IgVLVTcA6EWnl23QesMjDkLyWDpwiIhV9/zfnEYOd9gleB+AlVc0RkTuAb3CjEN5R1YUeh+Wl04HrgfkiMse3bbiqfuVdSCaC3AmM9X2YWgXc6HE8nlHVaSIyAZiFG/03mxhcdsOW2jDGGONXWW9iMsYYUwRLEMYYY/yyBGGMMcavmOqkrlWrliYnJ3sdhjHGRI2ZM2duK6omdUwliOTkZNLS0rwOo9S27Mnkjvdn869+nahTJebW/TLGRCARWVvUY9bEFEFGTFrOjDU7GDFphdehGGOMJYhIsWzTXj5IW4cqTEhbx5a9mV6HZIwp42KqiSlaZOfmsXTTXman72R2+i5mr9vF6m37f388V5URk1bw2OXtPIzSGFPWWYIIg817Mg8ng/RdzFu/i8xst0BorcrlaVu/Cut2HCAnz01azM5VJqSt467zmltfhDHGM5YggiwzO5eFG3b/ngxmp+9kw27XXFQuPo62DavSL7UpnZpUp1OT6jSsXoEHP12ArNp+xPPYVYQxxmuWII6DqrJux0Fmr9v5ezJYtHEP2bnuSqBRjQp0Sa7JoMYuGbRpUJXyCfFHPc+s9F2/H5MvO1eZtXZnWH4OY4zxxxJECezLymHeOtdnkN9ktH3/IQAqJMbToXE1Bp15Ip0aV6djk+oBNw99NeTM379XVa57cypLN+3lvZtTQ/JzmMDZ0GNTllmCwP+bQF6esmrbPmat3fX7FcKyzXvxdRNwUu1KnJtSxzUVNa5By7qVSYg//kFhIsITV7Tn4pcm8/iXi3nhuo7H/Zym9F78btnvQ4+tuc+UNZYgODz/YNiEebRrVJ3Z6TuZs24XezNzAKialEDHJjW4qG09OjetQcdG1alWMTFk8TSvU5nbzj6JEd+v4KrOjTijRa2QncsUbcueTMbPcEOPP7JBA6YMiqnlvrt27aolnUm9bud+znr6x9/rjAqQUr+q78qgOp2a1ODEWpWIi/NXnTR0MrNz6fniZAC+vvsskhKP7rswoXX3+Nl8OscVGBSgf/cmPHZFe2+DMibIRGSmqnb191iZnyj3xo+rEN97f0KccF23xvx3yJk8cUV7runamOZ1Koc9OQAkJcbz2OXtWbP9AK/+YDOrw23Lnkw+n7fx9/sKfGATGE0ZU6YTxJY9mXw0M+P3foWcPOXT2esj5k3gjBa1uKJTQ177aSUrtuz1Opwy5cXvlpGbd/TIsse+iLmqksYUqUwniBGTlpNXqIktf/5BpPjrJa2pWC6B4RMXEEvNgZFu8vJtfrd/s3ATWTm5YY7GGG+U6QQRDfMPalUuz18uTmH66h18NDPD63DKjGa1KtGgWhIrn/gDa566hDVPXcJbA7qSlZPHM18v9To8Y8KiTI9iKjj/IJJd27UxH8/K4MmvFnN+67rUrFTO65Bi2trt+/l5+TbuvaAl8QX6ny5oU5eBpzbl7V9Wc0bzWpybUsfDKI0JvTJ9BREt4uKEx69oz97MHB7/0trAQ+396euI9w1YKOwvf2hNSr0qDP1oLlv2REZflTGhYgkiSrSsW4Vbzz6Rj2dlMGWl//Zxc/wO5eTxUdo6zkupQ92qR895SEqM51/9OnHgUA73fDiHvDzrFzKxK6QJQkR6ishSEVkhIsP8PF5DRCaKyDwRmS4i7Xzbk3z354rIQhF5JJRxRos7e7SgSc2K/G3iAusoDZFvFm5i+/5D9D+laZH7NK9ThYcva8uvK7bz+uSVYYzOmPAKWYIQkXjgFeBioA3QV0TaFNptODBHVU8GBgAv+bZnAT1UtQPQEegpIqeEKtZokZQYz6OXt2PVtv289qO9MYXC2GlraVyzAmc2L372+nXdGnPJyfV57n/LmJUeOYMajAmmUF5BpAIrVHWVqh4CxgO9C+3TBpgEoKpLgGQRqavOPt8+ib6bXcsDZ7esTa8ODXj1h5Ws2rrv2AeYgK3Yso+pq3bQN7XJMSdH5q+ZVa9qEkPGz2ZPZnaYojQmfEKZIBoC6wrcz/BtK2gucCWAiKQCTYFGvvvxIjIH2AJ8q6rT/J1ERG4RkTQRSdu6dWtwf4II9bdLW1M+MY6/2tyIoHp/ejoJccI1XY7unPanWoVERvTtyIZdmfa7MDEplAnC30ewwv9BTwE1fIngTmA2kAOgqrmq2hGXMFLz+yeOekLVN1W1q6p2rV27drBij2h1qiQx7OIUflu1nU9mrfc6nJiQmZ3Lx7MyuKhdPWpXKR/wcV2a1uSe81vw+dwNNk/FxJxQJogMoOBHsUbAhoI7qOoeVb3RlwgGALWB1YX22QX8CPQMYaxRp2+3JnRuUp3Hv1rMTl9NClN6/12wkV0Hsumf2qTEx952TnNOPfEEHvrPQlZas5+JIaFMEDOAFiLSTETKAX2AzwruICLVfY8BDAImq+oeEaktItV9+1QAzgeWhDDWqBMXJzxxZXv2HMzmyf/a3IjjNXZqOs1qVeLUk04o8bHxccIL13UkKTGOO8fNthFmJmaELEGoag5wB/ANsBj4UFUXishgERns2601sFBEluBGOw3xba8P/CAi83CJ5ltV/SJUsUarlHpVufnMZnyYlsG0QjWtTeCWbtpL2tqd9EttgkjpVu6tVy2Jf17dgUUb9/DUf+2zjIkNIV1qQ1W/Ar4qtO31At//BrTwc9w8oFMoY4sVQ85rwZfzNjJ84ny+GnKm35rXpnjjpq2lXHwcV3VpdFzPc36butxwWjKjfl3DGc1rcV7rukGK0Bhv2EzqKFexXAKPXt6OlVv38+ZPq7wOJ+ocOJTDJ7PX84f29YKyxtWwi1NoXb8q90+Yx2ZbiiOmbdmTybVv/BYx5QFCIaAEISIVRKRVqIMxpXNuqzpc0r4+L/+wgjXb9nsdTlT5Yu5G9mbm0K970TOnSyIpMZ6X+3bi4KFc7vlgzlE1JUzsyC9VHEnlAYLtmAlCRC4D5gBf++53FJHPij3IhN3fL2tD+fg4/vapjccvibHT02lRpzLdkmsE7Tmb16nMI73aMmXldl7/yWa8x6L8YmOqMCGGKw0GcgXxMG5W9C4AVZ0DJIcqIFM6dasmcX/PVvyyYhufzd1w7AMMC9bvZu66XfTrXvrO6aJc07URl55cn+e/taU4Yo2qcvcHc8jKyQMgOzcvZq8iAkkQOaq6O+SRmOPWv3tTOjSuzqNfLGLXAZsbcSzjpqdTPiGOKzsdX+e0PyJuGHL9aknc9f5sdh+0pThiwcINu+n9r1+ZsvLwqMFchQ9mpMfkVUQgCWKBiPQD4kWkhYi8DEwJcVymFOLjhCeuaMfOA9k8/bUNtSzOvqwc/jN7PZd1aEC1iokhOUfVpERG9O3Ext2Z/HXifGv6i2K7DhziwU8XcNnLv7B0817iC71zZucqf/1kgTfBhVAgCeJOoC1uhdVxwG7g7hDGZI5D2wbVuOn0ZN6fvo60NTu8Didi/WfOevYfyqVf95LPnC6Jzk1qcO8FLfli3kY+SrOlOKJNbp4yblo65z77I+OmpzPg1GSSa1UiN+/ofSct2cyiDXvCH2QISXGfanxLdn+jqueHL6TS69q1q6alpXkdhuf2Z+Vw4QuTqVQ+ni/uPJNyCTaauSBV5dKXfyFP4au7zgh6/0NhuXnK9W9PY3b6Lj6/83Sa16kS0vOZ4JiVvpOH/rOQ+et3k9qsJo/0akvr+lX97rth10GufHUKeap8fNtpNK5ZMczRlp6IzFTVrv4eK/adQ1VzgQMiUi0kkZmQqFQ+gUd6tWXZ5n289bPNjShsbsZuFm7YE5LOaX/yl+KoUC6eO9+fQ2a2LcURybbty+L+j+Zy5atT2LI3k5f6dOSDW04pMjkANKhegfduTiUzO5eB70xnR4ysjxbIR8tMYL6IvC0iI/JvoQ7MHJ/z29SlZ9t6jJi0nPTtB7wOJ6KMm7aWiuXiubxjg7Cds27VJJ695mQW21IcESsnN49Rv67m3Gd/5NM56xl89kl8P/QcendsGNAHiZZ1q/D2Dd1Yv+sgN42ewYFDOWGIOrQCSRBfAg8Ck4GZBW4mwj3cqy2J8XH87T82NyLf7oPZfD53I707NqBKUmg6p4vSI6UuN56ezOgpa/hu0eawntsU77eV27lkxC888vkiOjauztd3n8Wwi1OoVL5kqxF1S67JiL6dmJexi9vHziLbX2dFFDlmglDVd4H3OZwYxvm2mQhXr1oSQy9syeRlW/li3kavw4kIn85ez8HsXPqlBmfmdEkNuziFNvWrcv+EuWzaHXvDIqPNxt0HufP92fR9ayr7snJ44/ouvHdTKifVrlzq57yobT0eu7w9Pyzdyl8+ie7Ra4HMpD4HWI6rL/0qsExEzgptWCZYBpyaTPuG1Xjk80Vlfiy+qhuRcnKjarRv5E23WvmEeF7u14nM7DxbisNDWTm5vPbjSs577if+t3ATQ85rwaShZ3NR23pB6Zfq170Jd5/fggkzM/jnN0uDELE3Amlieg64UFXPVtWzgIuAF0IblgmW+DjhySvbs2N/Fs+U8bkRM9fuZOnmvfQrRVGgYDqpdmUe6d2W31bZUhxe+HHpFi5+8Wee/noJZzSvxXf3ns09F7QkKTG4KyEPOa8FfVOb8OqPKxn96+pjHxCBAmlgS1TV31Ogqi4TkfA23prj0q5hNW44rRmjpqzmys6N6NI0eOsORZNx09KpUj6ByzqEr3O6KNd0acTPy7fx/LfLOOXEmnRpWtPrkGLeuh0H+McXi/h20Waa1arE6Bu7cU6rOiE7n4jw2OXt2L4vi0e+WEStKuW59GTv//ZKIpAriDTfCKZzfLe3sE7qqHPvhS2pVzWJv06cH/UdZ6Wxc/8hvpi/kcs7NSxxx2MoiAiPX9GOBtWTuOv9OWW++S+UMrNzeeHbZZz//E/8umIbD/RM4eu7zwxpcsgXHyeM6NuJrk1rcO8Hc5myclvIzxlMgSSI24CFwF24im+LgMHFHmEiTuXyCTzcqy1LNu3lnV+i83L3eHw8K4NDOXkhnzldElWTEhnRpxOb92QyPMo7MyORqvL1gk2c//xPvDRpORe1rcf3Q8/htnNOCmthraTEeEYO6EZyrYrc8t5MFm6InqXtAkkQCcBLqnqlql4BjACsbFkUuqhtPS5oU5cXvlvGuh1lZ26EqjJuejqdm1QvdrKTFzo1qcG9F7bky/kb+WDGOq/DiRkrt+5jwDvTGTxmJpXKJfD+H09hRN9O1KuW5Ek81SomMvrGVKokJXDDqBlR8/8XSIKYBFQocL8C8F0gTy4iPUVkqYisEJFhfh6vISITRWSeiEwXkXa+7Y1F5AcRWSwiC0VkyNHPbkrjkV5tiRPh72VobsTUVTtYtXV/0IoCBdvgs07ijOa1ePjzhazYstfrcKLavqwcnvzvYnq+OJk56bt46LI2fHnXGZx60gleh+ZmW9+UyqGcPAa8M53t+7K8DumYAkkQSaq6L/+O7/tjLjTiW8fpFeBioA3QV0TaFNptODBHVU8GBgAv+bbnAENVtTVwCnC7n2NNKTSoXoF7L2jJD0u38t8Fm7wOJyzGTU+nalICl55c3+tQ/IqLE56/tgMVyyVwx7jZthRHKagq/5mznvOe+5E3flrF5R0b8v1953Dj6c1IKLz0qoda1K3C2wO7smHXQW56Ny3iZ1sH8srtF5HO+XdEpAtwMIDjUoEVqrpKVQ8B44HehfZpg7tCQVWXAMkiUldVN6rqLN/2vcBioGEA5zQBuOG0ZNrUr8rDny1kT2Zsd45u25fF1ws2clWXRkEfxhhMdaom8dw1HViyaa8txRGAgvWgF2/cw3VvTmXI+DnUqZLEJ386jX9e04HaVcp7HaZfXZNr8q9+nZmfsYs/Rfhs60ASxN3ARyLys4j8DHwA3BHAcQ2Bgo2qGRz9Jj8XuBJARFKBpsAR1VtEJBnoBEzzdxIRuUVE0kQkbevWrQGEZRLi43jyyvZs3ZfFc1E8iScQE2ZmkJ2r9I+gzuminJtSh5vPaMboKWv41pbiKFZ+PejrR07jkhE/s3zzXp68sj2f3n46nZtE/jDuC9rU5fEr2vPj0q0M+zhyBygEstTGDCAFN5rpT0BrVQ1kmKu/6YiFX4WngBoiMgdXd2I2rnnJPYFIZeBj4G5V9bvQuqq+qapdVbVr7dq1AwjLAHRoXJ2Bpybz3tS1zFm3y+twQiLPt5Z/arOaUbPE9p97tqJtA1uKozibdx9k/Ix1qMLSzfu4slMjfrjvHPqmNiE+LvSr8wZL39Qm3HN+Sz6elcEzEfpBrcgEISLdRKQegKpmA52Bx4DnRCSQWT0ZQOMC9xsBRxRLVtU9qnqjqnbE9UHUBlb7zp+ISw5jVfWTgH8iE7ChF7akTpXyDP9kPjkRfJlbWr+u3Eb6jgNRcfWQr3xCPC/37cShnDzu/mC2LcXhx61jZpLje10S4oSkcvFUr1jO46hK567zmtO/exNe+3EloyJwtnVxVxBvAIcAfGsvPQW8h6so92YAzz0DaCEizUSkHNAH+KzgDiJS3fcYwCBgsqruEbcYytvAYlV9viQ/kAlclaREHr6sLYs27mH0lDVehxN046alU7NSOXq2q+d1KCVyYu3K/KN3O6au2sGrP6zwOpyI8lHaOuasOzyPICdPmZC2LmrrQYsI/+jdjova1uUfXyzi87kbjn1QGBWXIOJVNb9m5XXAm6r6sao+CDQ/1hOrag6ur+IbXCfzh6q6UEQGi0j+RLvWwEIRWYIb7ZQ/nPV04Hqgh4jM8d3+UOKfzhxTz3b16JFSh+f+t4z1uwIZexAdtuzJ5H+LNnN1l0ZhnRQVLFd1bkjvjg14cdJyKx3rs3TTXv7yybyjtueqMmJS9CbS+DjhpT6d6Na0JkM/nMuUFZEz27rYBCEi+WsSnAd8X+CxgNYqUNWvVLWlqp6kqo/7tr2uqq/7vv9NVVuoaopvIt5O3/ZfVFVU9WRV7ei7fVXyH88ci4jwSK+2ADwUQ3MjPkxbR26e0tfjhflKK38dn4bVKzBk/BxWbNn3+6idsmjH/kMMem8G6qdrMztXmbV2pwdRBU9SYjxvDehKs1qVuOXfM1mwPjJmWxf3Rv8+8JOIbMMNa/0ZQESa45qZTIxoXLMi91zQgie+WsI3CzdHXZNMYbl5yvvT13F68xNoVquS1+GUWpWkREb07cTVr03hptHTWbfzICMmreCxy9t5HVpYZefm8aexM9m8J4sJg0+lUxSMUiqNahUTGX1TN656dQo3jJrBxD95X9u6yCsI3yf+ocBo4Aw9/NEyDjfiyMSQG09vRkq9Kjz46QKuem1KVH9SnbxsK+t3HaR/hM6cLomOjasz+OyTSN9xEFWiur29tB7+bCFTV+3g6avax2xyyFe/mqttnZ0bGbOtix3mqqpTVXWiqu4vsG1Z/iQ2EzsS4+N4wjc3YubanVHdpjt22lpqVS7PBW3qeh1KUOw8cOj3hpXsvOhuby+pf/+2hrHT0rn17BO5olOjYx8QA5rXqcI7N3Rl425X23p/lnezrSNnDrrxXKPqFYj3vRN9MCM9Kj+pbth1kO+XbOHaro1IjKAlFkpry55MJszM+H0CUW6eRu3vpqSmrNjGw58vokdKHf58UYrX4YRVl6Y1+Vffzsxfv5vbPJxtHf3/QSZoRkxaTpxvolF2rjL8k/keR1RyH8xYh0LUdk4XNmLScvIKDRzIzlX+8nH0/W5KYu32/fxp3CxOrFWJl/p0jKoJcMFyfpu6PHFFeyYv28oDE+Z5MoCk2AQhIpeLyH0iclG4AjLe2LInk498y1Lk+27xFr5bHD1LPuTk5jF+RjpntajteedesMxK33XE7yTf90u2MCNGh7/uzcxm0LtpAIwc2JUqSWW3gGWf1CYMvaAln8xez1MelAwuchSTiLwKtAWmAI+KSKqqPhq2yExY+fukCnDbmJl8evvptG1QzYOoSub7JVvYvCeLR3vHxtUDwFdDzjxq29a9WVz35m/cNGoGY//YnZMbVQ9/YCGSm6fcPX4Oq7bt5983pdL0hOgdhRYsd/Rozpa9Wbzx0yrqVEni5jOahe3cxV1BnAX0UNW/AOcAl4cjIOONoj6pqsKAt6ezYss+P0dFlrHT0qlXNYkeKaEvJeml2lXKM3ZQd6pVTGTAO9NZuil2akj885ulTFqyhYcua8NpzWt5HU5EEBEe7tWWnm3r8egXi/gsjLOtpah2LRGZpaqdi7ofibp27appaWlehxFTVm/bzzWv/0Z8HHx062k0OSEym27W7TjAWf/8gbt6tOCeC1p6HU5YpG8/wDVvTCE3Dz4afGpUz/kAmDg7g3s+mEu/7k14/PJ2uBV3TL7M7FwGvDOd2ek7GX1jKqcHKYGKyExV7ervseKuIFJ8ld7micj8Avfni8jR891NTGpWqxJjBqWSlZNH/7enRuwKo+9PT0eAPqmNj7lvrGhyQkXGDjoFVaX/W1PJ2BkdZSz9mbNuFw98PJ/uzWrySK+2lhz8yJ9tfVLtytwaptnWxSWI1sBlvtulBe5f6vtqyoiUelV598ZUdu7Ppv/IqWyLsFKJh3Ly+DAtgx4pdahfrcKxD4ghzetU5r2bU9mXlUP/kdPYsicyE3hxNu3O5Jb30qhbtTyv/V+XmBieHCrVKrja1tUqJHLDqBmkbw/th4LiZlKv9XfDLdv955BGZSJOh8bVeXtgV9bvOsiAt6ez+2DkVKL7dtFmtu3LiomZ06XRtkE1Rt+Uyra9WfQfOY0d+w95HVLAMrNzueXfaezPymHkgG7UrBSdy3aHU71qSbx7Uyo5eXkMeGcaSzbuCdk6XQGlahHpKCLPiMgaXE0Iq4lYBnU/8QTeuL4ry7fs5YZR0z2d4VnQuOlraVi9Ame1LLsFozo3qcHIgd1I33GA69+eFlEJvCiqyp8nzGP++t282KcTrepFR1GnSNC8TmXeHtiNTXsy6TdyGjPW7AjJDPviCga1FJG/i8hi4F+48qGiqueq6stBj8REhbNb1ublvp2Zl7GbP76XRmZ2rqfxrN62n19XbKdvauMyOZmqoFNPOoHXr+/Css17PV+iIRCv/riSz+Zu4L4LW8XMsijh1KVpDR6/vB079h9C1dXKCPZVRHFXEEtwy3xfpqpn+JKCt+8GJiL0bFePZ685md9Wbed2j4uuvz89nYQ44dquZadzujjntqrDy307MWfdrohI4EX5dtFmnv3fUnp1aMCfzjnJ63Ci1uz0XeR32eSFoC5GcQniKmAT8IOIvCUi5+G/zrQpg67o1IjHLm/HpCVbuPuDOZ6UxszKyeWjtHVc0KYudaomhf38kapnu/q/J/A/jZ3FoZzIKie7dNNe7h4/m/YNq/HM1SfbiKVSyl/9IP/zWXZu8KvrFddJPVFVrwNSgB+Be4C6IvKaiFwYtAhM1OrfvSnD/5DCl/M2MuzjeeSFOUl8vWATOw9k0y+Kak6HS34C/37JFu7xKIH7k1/4p1L5BN68vitJidFX7S9S+Fv9INjV9Y5ZGc631PdYYKyI1ASuAYYB/wtaFCZq3XLWSezLymXEpOVUKp/AQ5e1CdsnwrHT0mlSsyKnn2Qzbv3p370pBw/l8tiXi6lQLp5nrjr598UYvVCw8M+Ht55KvWp21Xc8/K1+EOzqegGVDs3nq1H9hu92TCLSE3gJiAdGqupThR6vAbwDnARkAjep6gLfY+/g5lxsUdWyVUIrytxzfgv2Z+Xw9i+rqVQ+nvvDsDTz8s17mb56B8MuTvH0TS/SDTrzRPZn5fLCd8uokBjPP3p7Nwktv/DPi9d1pGPj6p7EEEv8rdMVbCVKECUhIvHAK8AFQAYwQ0Q+U9VFBXYbDsxR1StEJMW3/3m+x0bjRk+9F6oYTXCICH+7pDUHDuXyyg8rqVQ+gT+d0zyk5xw3PZ3EeOHqLmWjiMzxuOu85hw4lMMbk1dRsVw8wy5OCXuSyC/8M/jsk7i8U8OwntuUXsgSBJAKrFDVVQAiMh7oDRRMEG2AJwFUdYmIJItIXVXdrKqTRSQ5hPGZIBIRHru8HQcO5fDM10upVC6Bgaclh+Rcmdm5fDwzg57t6lOrcvmQnCOWiAjDLk5hvy9JVCqfwF3ntQjb+QsW/rn/olZhO685fqFMEA1xcyfyZQDdC+0zF7gS+EVEUoGmuJna0VOEwPwuPk549poOHDiUy0OfLaRiuXiuCcHw0y/mbWRPZg79YqQoUDiICP/o1Y4Dh3J5/ttlVCwXz6AzTwz5ea3wT3QL5aIn/v4SCg+leAqoISJzgDuB2UCJZveIyC0ikiYiaVu3bi1VoCZ4EuPj+Fe/TpzZohYPfDyPL+dtDPo5xk1by4m1K3HKiTWD/tyxLC5OeOaqk7mkfX0e+3Ix46alh/R8Vvgn+oUyQWQABT8+NgKOWMhcVfeo6o2q2hEYANQGVpfkJKr6pqp2VdWutWuX3aUWIkn5hHjeuL4LXZrWYMj42Xy/JHgXhIs37mFW+i76pTax8fOlkBAfxwvXdaRHSh3++ul8Js7OCMl5Chb+ebVfZyv8E6VCmSBmAC1EpJmIlAP6AJ8V3EFEqvseAxgETFbVPSGMyYRJxXIJvH1DN1rXr8rgMbOYsnJbUJ533LR0yiXEWef0cSiXEMer/Ttz6okncN9H8/h6QfCv8vIL/zxshX+iWsgShKrmAHcA3wCLgQ9VdaGIDBaRwb7dWgMLRWQJcDEwJP94EXkf+A1oJSIZInJzqGI1oVE1KZF3b0qlac2KDHo3jVnpxzc+e39WDhNnr+fS9vWpXtFW/Twe+bUFOjSqxp3vz+bHpVuC9twTZ2fw+k8r6d+9Cdefmhy05zXhV2RFuWhkFeUi05Y9mVzzxm/s3H+I9285pdT1rcdPT2fYJ/OZMPhUuiZb/0Mw7D6YTb+3prJiyz7evSmVU0484bieb3b6Tq57cyqdGldnzKDuVtshCpS2opwxQVGnahJjB3WncvmE46pvPW56Oq3qVqFL0xpBjrDsqlYhkX/f3J0mNSty8+gZzD6Oq7xNuzO59d8zrfBPDLHfoAmLRjUqMmZQd0SE/xs5jXU7SlYJa37GbuZl7KZfd+ucDraalcoxZlB3alUpz8B3prNoQ8m7Aa3wT2yyBGHC5sTalRkzKJWD2bn0HzmtRPWtx01fS4XEeK7obLNwQ6Fugau869+eVqKrPCv8E7ssQZiwSqlXlXdvSmX7viz6j5zK9gDqW+/NzOY/czZwWYf6VLWx9CFT8Cqv/8ipAdc7tsI/scsShAm7jo2r8/YN3cjYeZDrA6hv/emcDRw4lEu/MlpzOpzyr/KycvLo//ZUNu4+WOz+VvgntlmCMJ445cQTeOP6Lizfspcbi6lvraqMnbqWtg2q0qFR6UY/mZJJqVeV925KZef+bPqPnMa2Iq7ylmzaY4V/YpwlCOOZc1rVYUSf4stjzl63iyWb9lrndJid3Kg6o27sxoZdB/m/kdPYdeDQEY/v2H+IQe+mWeGfGGcJwnjq4vb1efaaDkxZuZ07xh1d33rctHQqlYund0frnA63bsk1eWtAV1Zt3c/AUTPY57vKy87N47YxM9myN4s3B3S1wj8xzBKE8dyVnRvx6OXt+G7xkeUxdx/I5vO5G+jdqSGVy4dy4WFTlDNb1OaV/p1ZsH43N42ewdrt+zn9qe+ZtnoHz1x1shX+iXH2X2ciwvWnNOVAVg5P/ncJFcvFc+/5Lbnuzalk5eTZst4eu6BNXV64riNDxs/m0hG/sDcrh7YNqlrhnzLAEoSJGLeefRL7s3IY8f0K5q7bxdodBzihUjnaNbTOaa/16tCALbszeeyrxQCs3LKPLXszqVPFmpdimTUxmYhyzwUt6dOtMUs3u4laezKz2bI38Al1JnTWbN9PQrwbKJCryohJKzyOyISaJQgTUUSEhDihYOExeyPy3pY9mXw0M4OcXNc/lJ2rTEhbZ8k7xlmCMBEl/43I109tb0QRYsSk5eQVWvnZriJinyUIE1HsjSgyzUrfRXbukb+X7Fxl1trjq/FhIpt1UpuIYm9EkemrIWd6HYLxgCUIE1HsjciYyBFTFeVEZCuwtpSH1wKCUzg5+tlrcSR7PY5kr8dhsfBaNFXV2v4eiKkEcTxEJK2osntljb0WR7LX40j2ehwW66+FdVIbY4zxyxKEMcYYvyxBHPam1wFEEHstjmSvx5Hs9Tgspl8L64Mwxhjjl11BGGOM8csShDHGGL/KfIIQkZ4islREVojIMK/j8ZKINBaRH0RksYgsFJEhXsfkNRGJF5HZIvKF17F4TUSqi8gEEVni+xs51euYvCQi9/j+TxaIyPsiEnNrn5fpBCEi8cArwMVAG6CviLTxNipP5QBDVbU1cApwexl/PQCGAIu9DiJCvAR8raopQAfK8OsiIg2Bu4CuqtoOiAf6eBtV8JXpBAGkAitUdZWqHgLGA709jskzqrpRVWf5vt+LewMos2XDRKQRcAkw0utYvCYiVYGzgLcBVPWQqu7yNCjvJQAVRCQBqAhs8DieoCvrCaIhsK7A/QzK8BtiQSKSDHQCpnkcipdeBP4M5HkcRyQ4EdgKjPI1uY0UkUpeB+UVVV0PPAukAxuB3ar6P2+jCr6yniDEz7YyP+5XRCoDHwN3q+oer+PxgohcCmxR1ZlexxIhEoDOwGuq2gnYD5TZPjsRqYFrbWgGNAAqicj/eRtV8JX1BJEBNC5wvxExeJlYEiKSiEsOY1X1E6/j8dDpQC8RWYNreuwhImO8DclTGUCGquZfUU7AJYyy6nxgtapuVdVs4BPgNI9jCrqyniBmAC1EpJmIlMN1Mn3mcUyeERHBtTEvVtXnvY7HS6r6F1VtpKrJuL+L71U15j4hBkpVNwHrRKSVb9N5wCIPQ/JaOnCKiFT0/d+cRwx22pfpehCqmiMidwDf4EYhvKOqCz0Oy0unA9cD80Vkjm/bcFX9yruQTAS5Exjr+zC1CrjR43g8o6rTRGQCMAs3+m82Mbjshi21YYwxxq+y3sRkjDGmCJYgjDHG+GUJwhhjjF+WIIwxxvhlCcIYY4xfliBMmSQiJ4jIHN9tk4isL3C/3DGO7SoiI0p4vptEZL6IzPOt/tnbt/0GEWlwPD+LMaFiw1xNmSciDwP7VPXZAtsSVDUnSM/fCPgJ6Kyqu31LmdRW1dUi8iNwn6qmBeNcxgSTXUEY4yMio0XkeRH5AXhaRFJFZIpvcbop+bOIReSc/PoQIvKwiLwjIj+KyCoRucvPU9cB9gL7AFR1ny85XA10xU0+myMiFUSki4j8JCIzReQbEanvO8+PIvKiL44FIpLq2352gSuf2SJSJfSvlCkryvRMamP8aAmcr6q5+Utc+2bcnw88AVzl55gU4FygCrBURF7zrc+Tby6wGVgtIpOAT1T1c1Wd4JvJf5+qpvnWwXoZ6K2qW0XkOuBx4Cbf81RS1dNE5CzgHaAdcB9wu6r+6rsyyQzy62HKMEsQxhzpI1XN9X1fDXhXRFrgVvlNLOKYL1U1C8gSkS1AXdzidgD4kk1PoBtuzZ4XRKSLqj5c6Hla4d70v3XL+xCPW0o63/u+55ssIlVFpDrwK/C8iIzFJZ4MjAkSa2Iy5kj7C3z/KPCDr2LYZUBRJSWzCnyfi58PXupMV9UncYv/+bsSEWChqnb03dqr6oUFn8bP0z4FDAIqAFNFJKW4H86YkrAEYUzRqgHrfd/fUNonEZEGIlJwaeyOwFrf93txTVMAS4Ha+bWeRSRRRNoWOO463/YzcAVqdovISao6X1WfBtJwzV3GBIU1MRlTtGdwTUz3At8fx/MkAs/6hrNm4iqzDfY9Nhp4XUQOAqcCVwMjRKQa7v/zRSB/heGdIjIFqMrhfom7ReRc3JXLIuC/xxGnMUewYa7GRAEbDmu8YE1Mxhhj/LIrCGOMMX7ZFYQxxhi/LEEYY4zxyxKEMcYYvyxBGGOM8csShDHGGL/+H5BTw9wLqMlOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始VAE结果\n",
    "# 自动训练10次\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f1adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "训练次数: 1 Epoch: 0001 log_lik= 1.8334504 train_kl= -0.00006 train_loss= 1.83351 train_acc= 0.49599 val_roc= 0.66252 val_ap= 0.69121 time= 1.88023\n",
      "训练次数: 1 Epoch: 0002 log_lik= 1.604034 train_kl= -0.00010 train_loss= 1.60414 train_acc= 0.48598 val_roc= 0.63888 val_ap= 0.67884 time= 0.08459\n",
      "训练次数: 1 Epoch: 0003 log_lik= 1.4941248 train_kl= -0.00027 train_loss= 1.49440 train_acc= 0.45819 val_roc= 0.63485 val_ap= 0.67759 time= 0.07568\n",
      "训练次数: 1 Epoch: 0004 log_lik= 1.4466003 train_kl= -0.00044 train_loss= 1.44704 train_acc= 0.43426 val_roc= 0.63365 val_ap= 0.67804 time= 0.08962\n",
      "训练次数: 1 Epoch: 0005 log_lik= 1.3396677 train_kl= -0.00061 train_loss= 1.34027 train_acc= 0.40317 val_roc= 0.63773 val_ap= 0.68207 time= 0.07468\n",
      "训练次数: 1 Epoch: 0006 log_lik= 1.227683 train_kl= -0.00079 train_loss= 1.22848 train_acc= 0.39665 val_roc= 0.64775 val_ap= 0.68963 time= 0.07369\n",
      "训练次数: 1 Epoch: 0007 log_lik= 1.1353141 train_kl= -0.00105 train_loss= 1.13637 train_acc= 0.39520 val_roc= 0.66297 val_ap= 0.70104 time= 0.07369\n",
      "训练次数: 1 Epoch: 0008 log_lik= 1.0245692 train_kl= -0.00141 train_loss= 1.02598 train_acc= 0.39222 val_roc= 0.68606 val_ap= 0.71815 time= 0.07667\n",
      "训练次数: 1 Epoch: 0009 log_lik= 0.92002136 train_kl= -0.00189 train_loss= 0.92191 train_acc= 0.38746 val_roc= 0.70896 val_ap= 0.73383 time= 0.08066\n",
      "训练次数: 1 Epoch: 0010 log_lik= 0.8388646 train_kl= -0.00247 train_loss= 0.84134 train_acc= 0.37331 val_roc= 0.73381 val_ap= 0.75156 time= 0.07368\n",
      "训练次数: 1 Epoch: 0011 log_lik= 0.76964813 train_kl= -0.00315 train_loss= 0.77280 train_acc= 0.38104 val_roc= 0.75570 val_ap= 0.76673 time= 0.07468\n",
      "训练次数: 1 Epoch: 0012 log_lik= 0.7191615 train_kl= -0.00391 train_loss= 0.72307 train_acc= 0.38533 val_roc= 0.74354 val_ap= 0.74464 time= 0.07568\n",
      "训练次数: 1 Epoch: 0013 log_lik= 0.68541664 train_kl= -0.00474 train_loss= 0.69016 train_acc= 0.35191 val_roc= 0.72755 val_ap= 0.73032 time= 0.07668\n",
      "训练次数: 1 Epoch: 0014 log_lik= 0.6764367 train_kl= -0.00561 train_loss= 0.68205 train_acc= 0.29117 val_roc= 0.74681 val_ap= 0.74697 time= 0.07269\n",
      "训练次数: 1 Epoch: 0015 log_lik= 0.662306 train_kl= -0.00644 train_loss= 0.66875 train_acc= 0.28624 val_roc= 0.77314 val_ap= 0.76997 time= 0.07369\n",
      "训练次数: 1 Epoch: 0016 log_lik= 0.64659745 train_kl= -0.00724 train_loss= 0.65384 train_acc= 0.31201 val_roc= 0.77986 val_ap= 0.77304 time= 0.07667\n",
      "训练次数: 1 Epoch: 0017 log_lik= 0.6372777 train_kl= -0.00799 train_loss= 0.64527 train_acc= 0.35747 val_roc= 0.77921 val_ap= 0.77245 time= 0.08066\n",
      "训练次数: 1 Epoch: 0018 log_lik= 0.6271411 train_kl= -0.00868 train_loss= 0.63582 train_acc= 0.36586 val_roc= 0.77233 val_ap= 0.76749 time= 0.08117\n",
      "训练次数: 1 Epoch: 0019 log_lik= 0.62064636 train_kl= -0.00931 train_loss= 0.62996 train_acc= 0.36263 val_roc= 0.77115 val_ap= 0.76755 time= 0.07955\n",
      "训练次数: 1 Epoch: 0020 log_lik= 0.6165453 train_kl= -0.00987 train_loss= 0.62642 train_acc= 0.37540 val_roc= 0.78239 val_ap= 0.77811 time= 0.07667\n",
      "训练次数: 1 Epoch: 0021 log_lik= 0.60691583 train_kl= -0.01037 train_loss= 0.61729 train_acc= 0.41048 val_roc= 0.78882 val_ap= 0.78146 time= 0.07468\n",
      "训练次数: 1 Epoch: 0022 log_lik= 0.60159457 train_kl= -0.01081 train_loss= 0.61240 train_acc= 0.43651 val_roc= 0.79073 val_ap= 0.78510 time= 0.07568\n",
      "训练次数: 1 Epoch: 0023 log_lik= 0.5917767 train_kl= -0.01118 train_loss= 0.60296 train_acc= 0.45555 val_roc= 0.79199 val_ap= 0.78808 time= 0.07469\n",
      "训练次数: 1 Epoch: 0024 log_lik= 0.584723 train_kl= -0.01150 train_loss= 0.59622 train_acc= 0.45530 val_roc= 0.79890 val_ap= 0.79560 time= 0.07468\n",
      "训练次数: 1 Epoch: 0025 log_lik= 0.5751885 train_kl= -0.01178 train_loss= 0.58696 train_acc= 0.45945 val_roc= 0.81165 val_ap= 0.80763 time= 0.07568\n",
      "训练次数: 1 Epoch: 0026 log_lik= 0.5646802 train_kl= -0.01200 train_loss= 0.57668 train_acc= 0.46756 val_roc= 0.82186 val_ap= 0.81732 time= 0.07468\n",
      "训练次数: 1 Epoch: 0027 log_lik= 0.5569399 train_kl= -0.01217 train_loss= 0.56911 train_acc= 0.47513 val_roc= 0.82909 val_ap= 0.82458 time= 0.07369\n",
      "训练次数: 1 Epoch: 0028 log_lik= 0.545897 train_kl= -0.01229 train_loss= 0.55819 train_acc= 0.47610 val_roc= 0.83404 val_ap= 0.82962 time= 0.07966\n",
      "训练次数: 1 Epoch: 0029 log_lik= 0.5394846 train_kl= -0.01238 train_loss= 0.55187 train_acc= 0.47261 val_roc= 0.84022 val_ap= 0.83404 time= 0.07468\n",
      "训练次数: 1 Epoch: 0030 log_lik= 0.53602505 train_kl= -0.01247 train_loss= 0.54849 train_acc= 0.47119 val_roc= 0.84756 val_ap= 0.83857 time= 0.07568\n",
      "训练次数: 1 Epoch: 0031 log_lik= 0.5321998 train_kl= -0.01255 train_loss= 0.54475 train_acc= 0.47210 val_roc= 0.85147 val_ap= 0.84087 time= 0.07667\n",
      "训练次数: 1 Epoch: 0032 log_lik= 0.5271446 train_kl= -0.01262 train_loss= 0.53976 train_acc= 0.48089 val_roc= 0.85466 val_ap= 0.84462 time= 0.08165\n",
      "训练次数: 1 Epoch: 0033 log_lik= 0.51914674 train_kl= -0.01267 train_loss= 0.53181 train_acc= 0.49284 val_roc= 0.85651 val_ap= 0.84715 time= 0.07468\n",
      "训练次数: 1 Epoch: 0034 log_lik= 0.51321787 train_kl= -0.01271 train_loss= 0.52593 train_acc= 0.49899 val_roc= 0.85916 val_ap= 0.85089 time= 0.07468\n",
      "训练次数: 1 Epoch: 0035 log_lik= 0.5111528 train_kl= -0.01276 train_loss= 0.52391 train_acc= 0.49832 val_roc= 0.86202 val_ap= 0.85327 time= 0.07568\n",
      "训练次数: 1 Epoch: 0036 log_lik= 0.51042396 train_kl= -0.01281 train_loss= 0.52324 train_acc= 0.49920 val_roc= 0.86491 val_ap= 0.85555 time= 0.07468\n",
      "训练次数: 1 Epoch: 0037 log_lik= 0.5095966 train_kl= -0.01283 train_loss= 0.52243 train_acc= 0.50080 val_roc= 0.86805 val_ap= 0.85954 time= 0.08763\n",
      "训练次数: 1 Epoch: 0038 log_lik= 0.5066065 train_kl= -0.01282 train_loss= 0.51942 train_acc= 0.50434 val_roc= 0.87156 val_ap= 0.86354 time= 0.07468\n",
      "训练次数: 1 Epoch: 0039 log_lik= 0.5003991 train_kl= -0.01278 train_loss= 0.51318 train_acc= 0.51016 val_roc= 0.87561 val_ap= 0.86809 time= 0.07369\n",
      "训练次数: 1 Epoch: 0040 log_lik= 0.49472103 train_kl= -0.01273 train_loss= 0.50745 train_acc= 0.51522 val_roc= 0.87964 val_ap= 0.87355 time= 0.07369\n",
      "训练次数: 1 Epoch: 0041 log_lik= 0.4917068 train_kl= -0.01267 train_loss= 0.50438 train_acc= 0.51689 val_roc= 0.88469 val_ap= 0.88041 time= 0.07468\n",
      "训练次数: 1 Epoch: 0042 log_lik= 0.49000645 train_kl= -0.01262 train_loss= 0.50263 train_acc= 0.51646 val_roc= 0.88937 val_ap= 0.88807 time= 0.07767\n",
      "训练次数: 1 Epoch: 0043 log_lik= 0.4874071 train_kl= -0.01257 train_loss= 0.49997 train_acc= 0.51574 val_roc= 0.89290 val_ap= 0.89359 time= 0.07667\n",
      "训练次数: 1 Epoch: 0044 log_lik= 0.4850228 train_kl= -0.01252 train_loss= 0.49754 train_acc= 0.51712 val_roc= 0.89659 val_ap= 0.89905 time= 0.08364\n",
      "训练次数: 1 Epoch: 0045 log_lik= 0.48311988 train_kl= -0.01248 train_loss= 0.49560 train_acc= 0.51873 val_roc= 0.89910 val_ap= 0.90373 time= 0.07468\n",
      "训练次数: 1 Epoch: 0046 log_lik= 0.48116818 train_kl= -0.01245 train_loss= 0.49362 train_acc= 0.51744 val_roc= 0.90061 val_ap= 0.90639 time= 0.07369\n",
      "训练次数: 1 Epoch: 0047 log_lik= 0.4811695 train_kl= -0.01242 train_loss= 0.49358 train_acc= 0.51642 val_roc= 0.90322 val_ap= 0.91007 time= 0.07372\n",
      "训练次数: 1 Epoch: 0048 log_lik= 0.47976196 train_kl= -0.01238 train_loss= 0.49214 train_acc= 0.51717 val_roc= 0.90468 val_ap= 0.91182 time= 0.07568\n",
      "训练次数: 1 Epoch: 0049 log_lik= 0.4782455 train_kl= -0.01233 train_loss= 0.49058 train_acc= 0.51986 val_roc= 0.90565 val_ap= 0.91293 time= 0.07368\n",
      "训练次数: 1 Epoch: 0050 log_lik= 0.47752568 train_kl= -0.01228 train_loss= 0.48981 train_acc= 0.51889 val_roc= 0.90646 val_ap= 0.91373 time= 0.07655\n",
      "训练次数: 1 Epoch: 0051 log_lik= 0.47629118 train_kl= -0.01223 train_loss= 0.48852 train_acc= 0.51906 val_roc= 0.90792 val_ap= 0.91512 time= 0.07667\n",
      "训练次数: 1 Epoch: 0052 log_lik= 0.47501683 train_kl= -0.01218 train_loss= 0.48719 train_acc= 0.51902 val_roc= 0.90909 val_ap= 0.91644 time= 0.07369\n",
      "训练次数: 1 Epoch: 0053 log_lik= 0.472937 train_kl= -0.01212 train_loss= 0.48506 train_acc= 0.52029 val_roc= 0.91026 val_ap= 0.91780 time= 0.07767\n",
      "训练次数: 1 Epoch: 0054 log_lik= 0.47108272 train_kl= -0.01207 train_loss= 0.48315 train_acc= 0.52285 val_roc= 0.91087 val_ap= 0.91806 time= 0.08265\n",
      "训练次数: 1 Epoch: 0055 log_lik= 0.46995434 train_kl= -0.01201 train_loss= 0.48197 train_acc= 0.52395 val_roc= 0.91141 val_ap= 0.91820 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 1 Epoch: 0056 log_lik= 0.46907163 train_kl= -0.01196 train_loss= 0.48103 train_acc= 0.52416 val_roc= 0.91148 val_ap= 0.91796 time= 0.09261\n",
      "训练次数: 1 Epoch: 0057 log_lik= 0.46762866 train_kl= -0.01190 train_loss= 0.47953 train_acc= 0.52395 val_roc= 0.91122 val_ap= 0.91749 time= 0.09062\n",
      "训练次数: 1 Epoch: 0058 log_lik= 0.46730542 train_kl= -0.01185 train_loss= 0.47916 train_acc= 0.52372 val_roc= 0.91097 val_ap= 0.91721 time= 0.07568\n",
      "训练次数: 1 Epoch: 0059 log_lik= 0.46603486 train_kl= -0.01180 train_loss= 0.47784 train_acc= 0.52417 val_roc= 0.91116 val_ap= 0.91734 time= 0.07667\n",
      "训练次数: 1 Epoch: 0060 log_lik= 0.46452236 train_kl= -0.01176 train_loss= 0.47628 train_acc= 0.52484 val_roc= 0.91106 val_ap= 0.91694 time= 0.07568\n",
      "训练次数: 1 Epoch: 0061 log_lik= 0.46369553 train_kl= -0.01171 train_loss= 0.47541 train_acc= 0.52594 val_roc= 0.91090 val_ap= 0.91687 time= 0.08066\n",
      "训练次数: 1 Epoch: 0062 log_lik= 0.4631422 train_kl= -0.01167 train_loss= 0.47481 train_acc= 0.52448 val_roc= 0.91139 val_ap= 0.91798 time= 0.08862\n",
      "训练次数: 1 Epoch: 0063 log_lik= 0.4623009 train_kl= -0.01163 train_loss= 0.47393 train_acc= 0.52469 val_roc= 0.91222 val_ap= 0.91877 time= 0.08165\n",
      "训练次数: 1 Epoch: 0064 log_lik= 0.46144345 train_kl= -0.01159 train_loss= 0.47303 train_acc= 0.52581 val_roc= 0.91272 val_ap= 0.91957 time= 0.07468\n",
      "训练次数: 1 Epoch: 0065 log_lik= 0.46103522 train_kl= -0.01155 train_loss= 0.47258 train_acc= 0.52577 val_roc= 0.91323 val_ap= 0.92017 time= 0.07568\n",
      "训练次数: 1 Epoch: 0066 log_lik= 0.4606484 train_kl= -0.01151 train_loss= 0.47216 train_acc= 0.52431 val_roc= 0.91339 val_ap= 0.92070 time= 0.08265\n",
      "训练次数: 1 Epoch: 0067 log_lik= 0.4597517 train_kl= -0.01148 train_loss= 0.47123 train_acc= 0.52592 val_roc= 0.91315 val_ap= 0.92144 time= 0.07369\n",
      "训练次数: 1 Epoch: 0068 log_lik= 0.45951635 train_kl= -0.01146 train_loss= 0.47097 train_acc= 0.52567 val_roc= 0.91321 val_ap= 0.92185 time= 0.07468\n",
      "训练次数: 1 Epoch: 0069 log_lik= 0.45859557 train_kl= -0.01143 train_loss= 0.47003 train_acc= 0.52434 val_roc= 0.91399 val_ap= 0.92283 time= 0.07369\n",
      "训练次数: 1 Epoch: 0070 log_lik= 0.45766467 train_kl= -0.01142 train_loss= 0.46908 train_acc= 0.52498 val_roc= 0.91427 val_ap= 0.92285 time= 0.07369\n",
      "训练次数: 1 Epoch: 0071 log_lik= 0.45771953 train_kl= -0.01140 train_loss= 0.46912 train_acc= 0.52569 val_roc= 0.91466 val_ap= 0.92329 time= 0.07527\n",
      "训练次数: 1 Epoch: 0072 log_lik= 0.45687792 train_kl= -0.01138 train_loss= 0.46826 train_acc= 0.52567 val_roc= 0.91435 val_ap= 0.92320 time= 0.07568\n",
      "训练次数: 1 Epoch: 0073 log_lik= 0.45628536 train_kl= -0.01137 train_loss= 0.46765 train_acc= 0.52592 val_roc= 0.91430 val_ap= 0.92325 time= 0.07966\n",
      "训练次数: 1 Epoch: 0074 log_lik= 0.45594022 train_kl= -0.01135 train_loss= 0.46729 train_acc= 0.52582 val_roc= 0.91417 val_ap= 0.92274 time= 0.07378\n",
      "训练次数: 1 Epoch: 0075 log_lik= 0.45560244 train_kl= -0.01133 train_loss= 0.46693 train_acc= 0.52660 val_roc= 0.91415 val_ap= 0.92259 time= 0.07468\n",
      "训练次数: 1 Epoch: 0076 log_lik= 0.45525876 train_kl= -0.01131 train_loss= 0.46657 train_acc= 0.52685 val_roc= 0.91404 val_ap= 0.92226 time= 0.07369\n",
      "训练次数: 1 Epoch: 0077 log_lik= 0.45469022 train_kl= -0.01129 train_loss= 0.46598 train_acc= 0.52645 val_roc= 0.91363 val_ap= 0.92163 time= 0.07667\n",
      "训练次数: 1 Epoch: 0078 log_lik= 0.45449188 train_kl= -0.01127 train_loss= 0.46577 train_acc= 0.52676 val_roc= 0.91314 val_ap= 0.92107 time= 0.07468\n",
      "训练次数: 1 Epoch: 0079 log_lik= 0.45364872 train_kl= -0.01126 train_loss= 0.46491 train_acc= 0.52613 val_roc= 0.91250 val_ap= 0.92016 time= 0.07568\n",
      "训练次数: 1 Epoch: 0080 log_lik= 0.45408106 train_kl= -0.01123 train_loss= 0.46531 train_acc= 0.52698 val_roc= 0.91214 val_ap= 0.91944 time= 0.07369\n",
      "训练次数: 1 Epoch: 0081 log_lik= 0.45328972 train_kl= -0.01120 train_loss= 0.46449 train_acc= 0.52695 val_roc= 0.91180 val_ap= 0.91878 time= 0.07568\n",
      "训练次数: 1 Epoch: 0082 log_lik= 0.4524592 train_kl= -0.01118 train_loss= 0.46364 train_acc= 0.52691 val_roc= 0.91064 val_ap= 0.91784 time= 0.07568\n",
      "训练次数: 1 Epoch: 0083 log_lik= 0.45218965 train_kl= -0.01116 train_loss= 0.46335 train_acc= 0.52663 val_roc= 0.90990 val_ap= 0.91720 time= 0.07468\n",
      "训练次数: 1 Epoch: 0084 log_lik= 0.45227543 train_kl= -0.01114 train_loss= 0.46342 train_acc= 0.52706 val_roc= 0.90984 val_ap= 0.91683 time= 0.07568\n",
      "训练次数: 1 Epoch: 0085 log_lik= 0.4519315 train_kl= -0.01111 train_loss= 0.46305 train_acc= 0.52697 val_roc= 0.90929 val_ap= 0.91562 time= 0.07468\n",
      "训练次数: 1 Epoch: 0086 log_lik= 0.4518364 train_kl= -0.01109 train_loss= 0.46293 train_acc= 0.52619 val_roc= 0.90879 val_ap= 0.91522 time= 0.07767\n",
      "训练次数: 1 Epoch: 0087 log_lik= 0.4512147 train_kl= -0.01108 train_loss= 0.46229 train_acc= 0.52732 val_roc= 0.90815 val_ap= 0.91511 time= 0.07369\n",
      "训练次数: 1 Epoch: 0088 log_lik= 0.45117047 train_kl= -0.01107 train_loss= 0.46224 train_acc= 0.52746 val_roc= 0.90782 val_ap= 0.91556 time= 0.07568\n",
      "训练次数: 1 Epoch: 0089 log_lik= 0.45098007 train_kl= -0.01106 train_loss= 0.46204 train_acc= 0.52599 val_roc= 0.90820 val_ap= 0.91556 time= 0.08274\n",
      "训练次数: 1 Epoch: 0090 log_lik= 0.45039037 train_kl= -0.01105 train_loss= 0.46144 train_acc= 0.52727 val_roc= 0.90799 val_ap= 0.91502 time= 0.07568\n",
      "训练次数: 1 Epoch: 0091 log_lik= 0.45010856 train_kl= -0.01103 train_loss= 0.46114 train_acc= 0.52740 val_roc= 0.90811 val_ap= 0.91525 time= 0.08166\n",
      "训练次数: 1 Epoch: 0092 log_lik= 0.44996753 train_kl= -0.01102 train_loss= 0.46099 train_acc= 0.52736 val_roc= 0.90765 val_ap= 0.91539 time= 0.08176\n",
      "训练次数: 1 Epoch: 0093 log_lik= 0.44960248 train_kl= -0.01102 train_loss= 0.46062 train_acc= 0.52712 val_roc= 0.90768 val_ap= 0.91547 time= 0.08165\n",
      "训练次数: 1 Epoch: 0094 log_lik= 0.44934756 train_kl= -0.01102 train_loss= 0.46037 train_acc= 0.52634 val_roc= 0.90805 val_ap= 0.91570 time= 0.08166\n",
      "训练次数: 1 Epoch: 0095 log_lik= 0.44853663 train_kl= -0.01102 train_loss= 0.45956 train_acc= 0.52766 val_roc= 0.90838 val_ap= 0.91594 time= 0.08962\n",
      "训练次数: 1 Epoch: 0096 log_lik= 0.4484203 train_kl= -0.01102 train_loss= 0.45944 train_acc= 0.52754 val_roc= 0.90799 val_ap= 0.91587 time= 0.08165\n",
      "训练次数: 1 Epoch: 0097 log_lik= 0.44807938 train_kl= -0.01103 train_loss= 0.45911 train_acc= 0.52632 val_roc= 0.90786 val_ap= 0.91570 time= 0.07369\n",
      "训练次数: 1 Epoch: 0098 log_lik= 0.44761044 train_kl= -0.01103 train_loss= 0.45864 train_acc= 0.52798 val_roc= 0.90795 val_ap= 0.91588 time= 0.07867\n",
      "训练次数: 1 Epoch: 0099 log_lik= 0.44727945 train_kl= -0.01103 train_loss= 0.45831 train_acc= 0.52825 val_roc= 0.90768 val_ap= 0.91566 time= 0.07469\n",
      "训练次数: 1 Epoch: 0100 log_lik= 0.44760022 train_kl= -0.01104 train_loss= 0.45864 train_acc= 0.52780 val_roc= 0.90785 val_ap= 0.91607 time= 0.07369\n",
      "Optimization Finished!\n",
      "训练次数: 1 ROC score: 0.9032330077161549\n",
      "训练次数: 1 AP score: 0.8985234841541487\n",
      "训练次数: 2 Epoch: 0001 log_lik= 1.8133383 train_kl= -0.00006 train_loss= 1.81340 train_acc= 0.49645 val_roc= 0.68411 val_ap= 0.72509 time= 1.43277\n",
      "训练次数: 2 Epoch: 0002 log_lik= 1.4213353 train_kl= -0.00026 train_loss= 1.42160 train_acc= 0.46802 val_roc= 0.67856 val_ap= 0.72239 time= 0.08360\n",
      "训练次数: 2 Epoch: 0003 log_lik= 1.2838366 train_kl= -0.00076 train_loss= 1.28460 train_acc= 0.37338 val_roc= 0.68347 val_ap= 0.72453 time= 0.08464\n",
      "训练次数: 2 Epoch: 0004 log_lik= 1.1417518 train_kl= -0.00120 train_loss= 1.14295 train_acc= 0.32129 val_roc= 0.68606 val_ap= 0.72773 time= 0.07667\n",
      "训练次数: 2 Epoch: 0005 log_lik= 1.0001281 train_kl= -0.00161 train_loss= 1.00174 train_acc= 0.29786 val_roc= 0.69345 val_ap= 0.73408 time= 0.07482\n",
      "训练次数: 2 Epoch: 0006 log_lik= 0.8977668 train_kl= -0.00208 train_loss= 0.89984 train_acc= 0.29096 val_roc= 0.71896 val_ap= 0.75232 time= 0.07966\n",
      "训练次数: 2 Epoch: 0007 log_lik= 0.7975715 train_kl= -0.00262 train_loss= 0.80019 train_acc= 0.33224 val_roc= 0.77176 val_ap= 0.79260 time= 0.07269\n",
      "训练次数: 2 Epoch: 0008 log_lik= 0.7429628 train_kl= -0.00331 train_loss= 0.74627 train_acc= 0.38399 val_roc= 0.78972 val_ap= 0.80635 time= 0.07369\n",
      "训练次数: 2 Epoch: 0009 log_lik= 0.7175649 train_kl= -0.00413 train_loss= 0.72169 train_acc= 0.38813 val_roc= 0.76114 val_ap= 0.78197 time= 0.07966\n",
      "训练次数: 2 Epoch: 0010 log_lik= 0.7075668 train_kl= -0.00503 train_loss= 0.71259 train_acc= 0.30002 val_roc= 0.74914 val_ap= 0.77314 time= 0.07767\n",
      "训练次数: 2 Epoch: 0011 log_lik= 0.70663166 train_kl= -0.00594 train_loss= 0.71257 train_acc= 0.18177 val_roc= 0.76899 val_ap= 0.79020 time= 0.07369\n",
      "训练次数: 2 Epoch: 0012 log_lik= 0.699795 train_kl= -0.00674 train_loss= 0.70654 train_acc= 0.14824 val_roc= 0.80837 val_ap= 0.82049 time= 0.07368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0013 log_lik= 0.67174166 train_kl= -0.00744 train_loss= 0.67918 train_acc= 0.17292 val_roc= 0.83208 val_ap= 0.83191 time= 0.07568\n",
      "训练次数: 2 Epoch: 0014 log_lik= 0.65413326 train_kl= -0.00808 train_loss= 0.66221 train_acc= 0.21191 val_roc= 0.83357 val_ap= 0.82815 time= 0.07568\n",
      "训练次数: 2 Epoch: 0015 log_lik= 0.6387449 train_kl= -0.00869 train_loss= 0.64743 train_acc= 0.24660 val_roc= 0.83915 val_ap= 0.83208 time= 0.07369\n",
      "训练次数: 2 Epoch: 0016 log_lik= 0.62193966 train_kl= -0.00926 train_loss= 0.63120 train_acc= 0.27877 val_roc= 0.84941 val_ap= 0.84076 time= 0.07671\n",
      "训练次数: 2 Epoch: 0017 log_lik= 0.5973854 train_kl= -0.00979 train_loss= 0.60717 train_acc= 0.33212 val_roc= 0.85093 val_ap= 0.83757 time= 0.08066\n",
      "训练次数: 2 Epoch: 0018 log_lik= 0.5760336 train_kl= -0.01027 train_loss= 0.58631 train_acc= 0.39309 val_roc= 0.84460 val_ap= 0.82936 time= 0.07468\n",
      "训练次数: 2 Epoch: 0019 log_lik= 0.56175977 train_kl= -0.01072 train_loss= 0.57248 train_acc= 0.43931 val_roc= 0.84573 val_ap= 0.82911 time= 0.07376\n",
      "训练次数: 2 Epoch: 0020 log_lik= 0.5532874 train_kl= -0.01109 train_loss= 0.56438 train_acc= 0.46244 val_roc= 0.85379 val_ap= 0.83822 time= 0.07567\n",
      "训练次数: 2 Epoch: 0021 log_lik= 0.5460109 train_kl= -0.01138 train_loss= 0.55739 train_acc= 0.47537 val_roc= 0.85904 val_ap= 0.84466 time= 0.07469\n",
      "训练次数: 2 Epoch: 0022 log_lik= 0.5492008 train_kl= -0.01161 train_loss= 0.56081 train_acc= 0.47510 val_roc= 0.86406 val_ap= 0.85219 time= 0.08264\n",
      "训练次数: 2 Epoch: 0023 log_lik= 0.54895574 train_kl= -0.01178 train_loss= 0.56074 train_acc= 0.48005 val_roc= 0.87023 val_ap= 0.85771 time= 0.07568\n",
      "训练次数: 2 Epoch: 0024 log_lik= 0.5420237 train_kl= -0.01192 train_loss= 0.55394 train_acc= 0.49219 val_roc= 0.87465 val_ap= 0.86240 time= 0.07468\n",
      "训练次数: 2 Epoch: 0025 log_lik= 0.5342319 train_kl= -0.01202 train_loss= 0.54625 train_acc= 0.50086 val_roc= 0.87960 val_ap= 0.86822 time= 0.07568\n",
      "训练次数: 2 Epoch: 0026 log_lik= 0.52805233 train_kl= -0.01207 train_loss= 0.54013 train_acc= 0.50599 val_roc= 0.88371 val_ap= 0.87378 time= 0.07269\n",
      "训练次数: 2 Epoch: 0027 log_lik= 0.5214539 train_kl= -0.01207 train_loss= 0.53353 train_acc= 0.50954 val_roc= 0.88603 val_ap= 0.87743 time= 0.07369\n",
      "训练次数: 2 Epoch: 0028 log_lik= 0.51441425 train_kl= -0.01203 train_loss= 0.52645 train_acc= 0.51150 val_roc= 0.88840 val_ap= 0.88179 time= 0.07767\n",
      "训练次数: 2 Epoch: 0029 log_lik= 0.50915813 train_kl= -0.01197 train_loss= 0.52112 train_acc= 0.51041 val_roc= 0.89075 val_ap= 0.88618 time= 0.07468\n",
      "训练次数: 2 Epoch: 0030 log_lik= 0.50469065 train_kl= -0.01189 train_loss= 0.51658 train_acc= 0.50916 val_roc= 0.89008 val_ap= 0.88642 time= 0.07369\n",
      "训练次数: 2 Epoch: 0031 log_lik= 0.50055546 train_kl= -0.01181 train_loss= 0.51237 train_acc= 0.51078 val_roc= 0.89001 val_ap= 0.88440 time= 0.07667\n",
      "训练次数: 2 Epoch: 0032 log_lik= 0.4991442 train_kl= -0.01171 train_loss= 0.51086 train_acc= 0.51039 val_roc= 0.88994 val_ap= 0.88361 time= 0.07269\n",
      "训练次数: 2 Epoch: 0033 log_lik= 0.4962455 train_kl= -0.01160 train_loss= 0.50784 train_acc= 0.51400 val_roc= 0.89101 val_ap= 0.88521 time= 0.07269\n",
      "训练次数: 2 Epoch: 0034 log_lik= 0.4932927 train_kl= -0.01147 train_loss= 0.50476 train_acc= 0.51496 val_roc= 0.89210 val_ap= 0.88737 time= 0.07369\n",
      "训练次数: 2 Epoch: 0035 log_lik= 0.49106818 train_kl= -0.01133 train_loss= 0.50240 train_acc= 0.51399 val_roc= 0.89309 val_ap= 0.89046 time= 0.08165\n",
      "训练次数: 2 Epoch: 0036 log_lik= 0.48807293 train_kl= -0.01122 train_loss= 0.49929 train_acc= 0.51427 val_roc= 0.89387 val_ap= 0.89130 time= 0.07468\n",
      "训练次数: 2 Epoch: 0037 log_lik= 0.48518276 train_kl= -0.01112 train_loss= 0.49630 train_acc= 0.51601 val_roc= 0.89342 val_ap= 0.89041 time= 0.07668\n",
      "训练次数: 2 Epoch: 0038 log_lik= 0.48199156 train_kl= -0.01104 train_loss= 0.49304 train_acc= 0.51977 val_roc= 0.89355 val_ap= 0.88909 time= 0.07468\n",
      "训练次数: 2 Epoch: 0039 log_lik= 0.48108208 train_kl= -0.01098 train_loss= 0.49206 train_acc= 0.51915 val_roc= 0.89368 val_ap= 0.88879 time= 0.07368\n",
      "训练次数: 2 Epoch: 0040 log_lik= 0.47894496 train_kl= -0.01091 train_loss= 0.48985 train_acc= 0.51954 val_roc= 0.89463 val_ap= 0.88923 time= 0.07770\n",
      "训练次数: 2 Epoch: 0041 log_lik= 0.47624668 train_kl= -0.01084 train_loss= 0.48709 train_acc= 0.52021 val_roc= 0.89634 val_ap= 0.89135 time= 0.08066\n",
      "训练次数: 2 Epoch: 0042 log_lik= 0.4734692 train_kl= -0.01077 train_loss= 0.48424 train_acc= 0.52159 val_roc= 0.89740 val_ap= 0.89228 time= 0.07568\n",
      "训练次数: 2 Epoch: 0043 log_lik= 0.47206736 train_kl= -0.01071 train_loss= 0.48277 train_acc= 0.52335 val_roc= 0.89753 val_ap= 0.89236 time= 0.07667\n",
      "训练次数: 2 Epoch: 0044 log_lik= 0.46986088 train_kl= -0.01066 train_loss= 0.48052 train_acc= 0.52567 val_roc= 0.89741 val_ap= 0.89100 time= 0.07667\n",
      "训练次数: 2 Epoch: 0045 log_lik= 0.46924397 train_kl= -0.01063 train_loss= 0.47987 train_acc= 0.52623 val_roc= 0.89745 val_ap= 0.89059 time= 0.07269\n",
      "训练次数: 2 Epoch: 0046 log_lik= 0.46908253 train_kl= -0.01060 train_loss= 0.47968 train_acc= 0.52644 val_roc= 0.89771 val_ap= 0.89084 time= 0.07568\n",
      "训练次数: 2 Epoch: 0047 log_lik= 0.4682934 train_kl= -0.01056 train_loss= 0.47885 train_acc= 0.52655 val_roc= 0.89887 val_ap= 0.89199 time= 0.07368\n",
      "训练次数: 2 Epoch: 0048 log_lik= 0.46769527 train_kl= -0.01051 train_loss= 0.47821 train_acc= 0.52545 val_roc= 0.89971 val_ap= 0.89309 time= 0.07667\n",
      "训练次数: 2 Epoch: 0049 log_lik= 0.4666792 train_kl= -0.01048 train_loss= 0.47716 train_acc= 0.52584 val_roc= 0.89984 val_ap= 0.89394 time= 0.08464\n",
      "训练次数: 2 Epoch: 0050 log_lik= 0.4648871 train_kl= -0.01045 train_loss= 0.47533 train_acc= 0.52707 val_roc= 0.89938 val_ap= 0.89318 time= 0.07667\n",
      "训练次数: 2 Epoch: 0051 log_lik= 0.46239784 train_kl= -0.01042 train_loss= 0.47282 train_acc= 0.52931 val_roc= 0.89904 val_ap= 0.89281 time= 0.07369\n",
      "训练次数: 2 Epoch: 0052 log_lik= 0.46163532 train_kl= -0.01040 train_loss= 0.47203 train_acc= 0.52993 val_roc= 0.89942 val_ap= 0.89487 time= 0.07767\n",
      "训练次数: 2 Epoch: 0053 log_lik= 0.46116924 train_kl= -0.01036 train_loss= 0.47153 train_acc= 0.53027 val_roc= 0.90016 val_ap= 0.89635 time= 0.08066\n",
      "训练次数: 2 Epoch: 0054 log_lik= 0.45935228 train_kl= -0.01033 train_loss= 0.46968 train_acc= 0.53005 val_roc= 0.90066 val_ap= 0.89799 time= 0.07621\n",
      "训练次数: 2 Epoch: 0055 log_lik= 0.45850742 train_kl= -0.01030 train_loss= 0.46881 train_acc= 0.52880 val_roc= 0.90126 val_ap= 0.89881 time= 0.07568\n",
      "训练次数: 2 Epoch: 0056 log_lik= 0.457755 train_kl= -0.01028 train_loss= 0.46804 train_acc= 0.52954 val_roc= 0.90082 val_ap= 0.89842 time= 0.07269\n",
      "训练次数: 2 Epoch: 0057 log_lik= 0.4563951 train_kl= -0.01028 train_loss= 0.46667 train_acc= 0.53005 val_roc= 0.90042 val_ap= 0.89827 time= 0.07369\n",
      "训练次数: 2 Epoch: 0058 log_lik= 0.45657226 train_kl= -0.01028 train_loss= 0.46686 train_acc= 0.53033 val_roc= 0.90059 val_ap= 0.89909 time= 0.07678\n",
      "训练次数: 2 Epoch: 0059 log_lik= 0.4556352 train_kl= -0.01030 train_loss= 0.46594 train_acc= 0.53062 val_roc= 0.90153 val_ap= 0.90036 time= 0.07269\n",
      "训练次数: 2 Epoch: 0060 log_lik= 0.4546785 train_kl= -0.01033 train_loss= 0.46501 train_acc= 0.53093 val_roc= 0.90176 val_ap= 0.90089 time= 0.07368\n",
      "训练次数: 2 Epoch: 0061 log_lik= 0.45352644 train_kl= -0.01036 train_loss= 0.46389 train_acc= 0.53049 val_roc= 0.90204 val_ap= 0.90112 time= 0.07867\n",
      "训练次数: 2 Epoch: 0062 log_lik= 0.45317915 train_kl= -0.01040 train_loss= 0.46358 train_acc= 0.53089 val_roc= 0.90183 val_ap= 0.90079 time= 0.08165\n",
      "训练次数: 2 Epoch: 0063 log_lik= 0.45298913 train_kl= -0.01045 train_loss= 0.46344 train_acc= 0.53091 val_roc= 0.90153 val_ap= 0.90027 time= 0.07468\n",
      "训练次数: 2 Epoch: 0064 log_lik= 0.45167863 train_kl= -0.01051 train_loss= 0.46219 train_acc= 0.53138 val_roc= 0.90139 val_ap= 0.90011 time= 0.07369\n",
      "训练次数: 2 Epoch: 0065 log_lik= 0.45111006 train_kl= -0.01057 train_loss= 0.46168 train_acc= 0.53084 val_roc= 0.90199 val_ap= 0.90094 time= 0.07170\n",
      "训练次数: 2 Epoch: 0066 log_lik= 0.45043275 train_kl= -0.01063 train_loss= 0.46106 train_acc= 0.53279 val_roc= 0.90251 val_ap= 0.90154 time= 0.07468\n",
      "训练次数: 2 Epoch: 0067 log_lik= 0.44995114 train_kl= -0.01068 train_loss= 0.46063 train_acc= 0.53328 val_roc= 0.90243 val_ap= 0.90092 time= 0.07468\n",
      "训练次数: 2 Epoch: 0068 log_lik= 0.44911534 train_kl= -0.01073 train_loss= 0.45985 train_acc= 0.53322 val_roc= 0.90224 val_ap= 0.90068 time= 0.07568\n",
      "训练次数: 2 Epoch: 0069 log_lik= 0.4488555 train_kl= -0.01079 train_loss= 0.45964 train_acc= 0.53235 val_roc= 0.90191 val_ap= 0.89981 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0070 log_lik= 0.44777644 train_kl= -0.01084 train_loss= 0.45861 train_acc= 0.53413 val_roc= 0.90220 val_ap= 0.89954 time= 0.07468\n",
      "训练次数: 2 Epoch: 0071 log_lik= 0.44742328 train_kl= -0.01089 train_loss= 0.45831 train_acc= 0.53331 val_roc= 0.90224 val_ap= 0.89917 time= 0.07373\n",
      "训练次数: 2 Epoch: 0072 log_lik= 0.4466147 train_kl= -0.01093 train_loss= 0.45755 train_acc= 0.53404 val_roc= 0.90241 val_ap= 0.89936 time= 0.07269\n",
      "训练次数: 2 Epoch: 0073 log_lik= 0.44633228 train_kl= -0.01098 train_loss= 0.45731 train_acc= 0.53364 val_roc= 0.90241 val_ap= 0.89937 time= 0.07369\n",
      "训练次数: 2 Epoch: 0074 log_lik= 0.4457634 train_kl= -0.01102 train_loss= 0.45678 train_acc= 0.53517 val_roc= 0.90222 val_ap= 0.89842 time= 0.07430\n",
      "训练次数: 2 Epoch: 0075 log_lik= 0.44529158 train_kl= -0.01105 train_loss= 0.45634 train_acc= 0.53441 val_roc= 0.90211 val_ap= 0.89794 time= 0.07667\n",
      "训练次数: 2 Epoch: 0076 log_lik= 0.44492048 train_kl= -0.01108 train_loss= 0.45600 train_acc= 0.53436 val_roc= 0.90179 val_ap= 0.89764 time= 0.07867\n",
      "训练次数: 2 Epoch: 0077 log_lik= 0.44439796 train_kl= -0.01110 train_loss= 0.45550 train_acc= 0.53386 val_roc= 0.90183 val_ap= 0.89778 time= 0.07369\n",
      "训练次数: 2 Epoch: 0078 log_lik= 0.44427323 train_kl= -0.01113 train_loss= 0.45540 train_acc= 0.53362 val_roc= 0.90152 val_ap= 0.89755 time= 0.07477\n",
      "训练次数: 2 Epoch: 0079 log_lik= 0.44388485 train_kl= -0.01115 train_loss= 0.45504 train_acc= 0.53275 val_roc= 0.90179 val_ap= 0.89787 time= 0.07767\n",
      "训练次数: 2 Epoch: 0080 log_lik= 0.44308668 train_kl= -0.01118 train_loss= 0.45426 train_acc= 0.53517 val_roc= 0.90186 val_ap= 0.89804 time= 0.08165\n",
      "训练次数: 2 Epoch: 0081 log_lik= 0.4426924 train_kl= -0.01121 train_loss= 0.45390 train_acc= 0.53378 val_roc= 0.90192 val_ap= 0.89778 time= 0.08066\n",
      "训练次数: 2 Epoch: 0082 log_lik= 0.44198593 train_kl= -0.01123 train_loss= 0.45322 train_acc= 0.53455 val_roc= 0.90199 val_ap= 0.89860 time= 0.07568\n",
      "训练次数: 2 Epoch: 0083 log_lik= 0.44209594 train_kl= -0.01125 train_loss= 0.45335 train_acc= 0.53433 val_roc= 0.90202 val_ap= 0.89850 time= 0.07369\n",
      "训练次数: 2 Epoch: 0084 log_lik= 0.441715 train_kl= -0.01127 train_loss= 0.45299 train_acc= 0.53423 val_roc= 0.90209 val_ap= 0.89851 time= 0.07568\n",
      "训练次数: 2 Epoch: 0085 log_lik= 0.4407181 train_kl= -0.01130 train_loss= 0.45202 train_acc= 0.53376 val_roc= 0.90273 val_ap= 0.89873 time= 0.07767\n",
      "训练次数: 2 Epoch: 0086 log_lik= 0.44075033 train_kl= -0.01133 train_loss= 0.45208 train_acc= 0.53462 val_roc= 0.90302 val_ap= 0.89914 time= 0.08265\n",
      "训练次数: 2 Epoch: 0087 log_lik= 0.44035414 train_kl= -0.01136 train_loss= 0.45171 train_acc= 0.53468 val_roc= 0.90292 val_ap= 0.89931 time= 0.08962\n",
      "训练次数: 2 Epoch: 0088 log_lik= 0.44001496 train_kl= -0.01138 train_loss= 0.45140 train_acc= 0.53518 val_roc= 0.90282 val_ap= 0.89937 time= 0.07782\n",
      "训练次数: 2 Epoch: 0089 log_lik= 0.43962684 train_kl= -0.01141 train_loss= 0.45104 train_acc= 0.53398 val_roc= 0.90270 val_ap= 0.89932 time= 0.07369\n",
      "训练次数: 2 Epoch: 0090 log_lik= 0.43875822 train_kl= -0.01143 train_loss= 0.45019 train_acc= 0.53483 val_roc= 0.90302 val_ap= 0.89954 time= 0.07368\n",
      "训练次数: 2 Epoch: 0091 log_lik= 0.4385354 train_kl= -0.01146 train_loss= 0.45000 train_acc= 0.53447 val_roc= 0.90329 val_ap= 0.89965 time= 0.07667\n",
      "训练次数: 2 Epoch: 0092 log_lik= 0.4380739 train_kl= -0.01149 train_loss= 0.44957 train_acc= 0.53559 val_roc= 0.90373 val_ap= 0.89995 time= 0.07767\n",
      "训练次数: 2 Epoch: 0093 log_lik= 0.4375043 train_kl= -0.01152 train_loss= 0.44902 train_acc= 0.53608 val_roc= 0.90406 val_ap= 0.89984 time= 0.07468\n",
      "训练次数: 2 Epoch: 0094 log_lik= 0.43752852 train_kl= -0.01154 train_loss= 0.44907 train_acc= 0.53592 val_roc= 0.90416 val_ap= 0.89955 time= 0.07568\n",
      "训练次数: 2 Epoch: 0095 log_lik= 0.43653667 train_kl= -0.01156 train_loss= 0.44810 train_acc= 0.53689 val_roc= 0.90329 val_ap= 0.89827 time= 0.07369\n",
      "训练次数: 2 Epoch: 0096 log_lik= 0.43625674 train_kl= -0.01159 train_loss= 0.44784 train_acc= 0.53690 val_roc= 0.90299 val_ap= 0.89776 time= 0.07469\n",
      "训练次数: 2 Epoch: 0097 log_lik= 0.43583447 train_kl= -0.01161 train_loss= 0.44744 train_acc= 0.53632 val_roc= 0.90348 val_ap= 0.89814 time= 0.07369\n",
      "训练次数: 2 Epoch: 0098 log_lik= 0.43576166 train_kl= -0.01162 train_loss= 0.44738 train_acc= 0.53579 val_roc= 0.90377 val_ap= 0.89793 time= 0.07866\n",
      "训练次数: 2 Epoch: 0099 log_lik= 0.43551758 train_kl= -0.01163 train_loss= 0.44715 train_acc= 0.53673 val_roc= 0.90241 val_ap= 0.89732 time= 0.07568\n",
      "训练次数: 2 Epoch: 0100 log_lik= 0.4348501 train_kl= -0.01165 train_loss= 0.44650 train_acc= 0.53813 val_roc= 0.90116 val_ap= 0.89600 time= 0.07468\n",
      "Optimization Finished!\n",
      "训练次数: 2 ROC score: 0.921913087938242\n",
      "训练次数: 2 AP score: 0.9217383067099449\n",
      "训练次数: 3 Epoch: 0001 log_lik= 1.8039792 train_kl= -0.00004 train_loss= 1.80402 train_acc= 0.49680 val_roc= 0.68364 val_ap= 0.70323 time= 1.32855\n",
      "训练次数: 3 Epoch: 0002 log_lik= 1.503316 train_kl= -0.00022 train_loss= 1.50354 train_acc= 0.47590 val_roc= 0.67672 val_ap= 0.70566 time= 0.08659\n",
      "训练次数: 3 Epoch: 0003 log_lik= 1.2850872 train_kl= -0.00056 train_loss= 1.28565 train_acc= 0.41235 val_roc= 0.68051 val_ap= 0.70915 time= 0.07767\n",
      "训练次数: 3 Epoch: 0004 log_lik= 1.1467268 train_kl= -0.00098 train_loss= 1.14770 train_acc= 0.35396 val_roc= 0.68535 val_ap= 0.71460 time= 0.09957\n",
      "训练次数: 3 Epoch: 0005 log_lik= 1.0414618 train_kl= -0.00139 train_loss= 1.04286 train_acc= 0.32595 val_roc= 0.68827 val_ap= 0.71660 time= 0.07568\n",
      "训练次数: 3 Epoch: 0006 log_lik= 0.93581045 train_kl= -0.00192 train_loss= 0.93773 train_acc= 0.31522 val_roc= 0.69424 val_ap= 0.72158 time= 0.07667\n",
      "训练次数: 3 Epoch: 0007 log_lik= 0.85116935 train_kl= -0.00255 train_loss= 0.85372 train_acc= 0.29577 val_roc= 0.70540 val_ap= 0.73218 time= 0.07468\n",
      "训练次数: 3 Epoch: 0008 log_lik= 0.7821519 train_kl= -0.00330 train_loss= 0.78546 train_acc= 0.29219 val_roc= 0.73012 val_ap= 0.75050 time= 0.07468\n",
      "训练次数: 3 Epoch: 0009 log_lik= 0.7360198 train_kl= -0.00413 train_loss= 0.74015 train_acc= 0.30999 val_roc= 0.76534 val_ap= 0.77896 time= 0.07667\n",
      "训练次数: 3 Epoch: 0010 log_lik= 0.7090847 train_kl= -0.00503 train_loss= 0.71412 train_acc= 0.31695 val_roc= 0.77312 val_ap= 0.78513 time= 0.07767\n",
      "训练次数: 3 Epoch: 0011 log_lik= 0.6984407 train_kl= -0.00598 train_loss= 0.70442 train_acc= 0.27590 val_roc= 0.76855 val_ap= 0.78261 time= 0.07369\n",
      "训练次数: 3 Epoch: 0012 log_lik= 0.69118184 train_kl= -0.00693 train_loss= 0.69812 train_acc= 0.19054 val_roc= 0.78112 val_ap= 0.79436 time= 0.07489\n",
      "训练次数: 3 Epoch: 0013 log_lik= 0.6874785 train_kl= -0.00778 train_loss= 0.69525 train_acc= 0.15272 val_roc= 0.80298 val_ap= 0.81497 time= 0.07468\n",
      "训练次数: 3 Epoch: 0014 log_lik= 0.673365 train_kl= -0.00849 train_loss= 0.68186 train_acc= 0.15615 val_roc= 0.81545 val_ap= 0.81877 time= 0.08416\n",
      "训练次数: 3 Epoch: 0015 log_lik= 0.66298366 train_kl= -0.00911 train_loss= 0.67210 train_acc= 0.16675 val_roc= 0.81286 val_ap= 0.80680 time= 0.07269\n",
      "训练次数: 3 Epoch: 0016 log_lik= 0.6545785 train_kl= -0.00968 train_loss= 0.66425 train_acc= 0.19364 val_roc= 0.81396 val_ap= 0.80509 time= 0.07767\n",
      "训练次数: 3 Epoch: 0017 log_lik= 0.6354145 train_kl= -0.01019 train_loss= 0.64560 train_acc= 0.25629 val_roc= 0.82543 val_ap= 0.81924 time= 0.07867\n",
      "训练次数: 3 Epoch: 0018 log_lik= 0.6103624 train_kl= -0.01067 train_loss= 0.62103 train_acc= 0.31706 val_roc= 0.83712 val_ap= 0.83419 time= 0.07468\n",
      "训练次数: 3 Epoch: 0019 log_lik= 0.5916707 train_kl= -0.01111 train_loss= 0.60278 train_acc= 0.34541 val_roc= 0.84567 val_ap= 0.84318 time= 0.08066\n",
      "训练次数: 3 Epoch: 0020 log_lik= 0.5774454 train_kl= -0.01150 train_loss= 0.58895 train_acc= 0.37761 val_roc= 0.85105 val_ap= 0.84682 time= 0.07468\n",
      "训练次数: 3 Epoch: 0021 log_lik= 0.5615968 train_kl= -0.01182 train_loss= 0.57342 train_acc= 0.41972 val_roc= 0.85154 val_ap= 0.84572 time= 0.07369\n",
      "训练次数: 3 Epoch: 0022 log_lik= 0.5546477 train_kl= -0.01211 train_loss= 0.56675 train_acc= 0.44798 val_roc= 0.85372 val_ap= 0.84719 time= 0.07369\n",
      "训练次数: 3 Epoch: 0023 log_lik= 0.5519249 train_kl= -0.01236 train_loss= 0.56428 train_acc= 0.45677 val_roc= 0.86093 val_ap= 0.85559 time= 0.07369\n",
      "训练次数: 3 Epoch: 0024 log_lik= 0.5476232 train_kl= -0.01257 train_loss= 0.56019 train_acc= 0.46078 val_roc= 0.86854 val_ap= 0.86534 time= 0.07568\n",
      "训练次数: 3 Epoch: 0025 log_lik= 0.54496324 train_kl= -0.01273 train_loss= 0.55769 train_acc= 0.46017 val_roc= 0.87356 val_ap= 0.87013 time= 0.07667\n",
      "训练次数: 3 Epoch: 0026 log_lik= 0.5374592 train_kl= -0.01283 train_loss= 0.55029 train_acc= 0.46901 val_roc= 0.87500 val_ap= 0.86942 time= 0.08165\n",
      "训练次数: 3 Epoch: 0027 log_lik= 0.52807075 train_kl= -0.01290 train_loss= 0.54097 train_acc= 0.48264 val_roc= 0.87679 val_ap= 0.87020 time= 0.07369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0028 log_lik= 0.519756 train_kl= -0.01295 train_loss= 0.53270 train_acc= 0.49540 val_roc= 0.88164 val_ap= 0.87592 time= 0.07468\n",
      "训练次数: 3 Epoch: 0029 log_lik= 0.51291716 train_kl= -0.01297 train_loss= 0.52588 train_acc= 0.50196 val_roc= 0.88715 val_ap= 0.88115 time= 0.07824\n",
      "训练次数: 3 Epoch: 0030 log_lik= 0.5091868 train_kl= -0.01295 train_loss= 0.52214 train_acc= 0.50228 val_roc= 0.89104 val_ap= 0.88367 time= 0.08464\n",
      "训练次数: 3 Epoch: 0031 log_lik= 0.5082279 train_kl= -0.01289 train_loss= 0.52112 train_acc= 0.49727 val_roc= 0.89128 val_ap= 0.88241 time= 0.07667\n",
      "训练次数: 3 Epoch: 0032 log_lik= 0.5057286 train_kl= -0.01279 train_loss= 0.51852 train_acc= 0.49629 val_roc= 0.89196 val_ap= 0.88192 time= 0.07376\n",
      "训练次数: 3 Epoch: 0033 log_lik= 0.5003857 train_kl= -0.01267 train_loss= 0.51306 train_acc= 0.50443 val_roc= 0.89504 val_ap= 0.88558 time= 0.07568\n",
      "训练次数: 3 Epoch: 0034 log_lik= 0.4961685 train_kl= -0.01255 train_loss= 0.50872 train_acc= 0.50924 val_roc= 0.90043 val_ap= 0.89196 time= 0.08265\n",
      "训练次数: 3 Epoch: 0035 log_lik= 0.4940097 train_kl= -0.01244 train_loss= 0.50645 train_acc= 0.50928 val_roc= 0.90546 val_ap= 0.89766 time= 0.08265\n",
      "训练次数: 3 Epoch: 0036 log_lik= 0.49079016 train_kl= -0.01234 train_loss= 0.50313 train_acc= 0.50776 val_roc= 0.90844 val_ap= 0.90175 time= 0.07269\n",
      "训练次数: 3 Epoch: 0037 log_lik= 0.49019286 train_kl= -0.01226 train_loss= 0.50246 train_acc= 0.50542 val_roc= 0.90999 val_ap= 0.90442 time= 0.07369\n",
      "训练次数: 3 Epoch: 0038 log_lik= 0.48749807 train_kl= -0.01220 train_loss= 0.49970 train_acc= 0.50799 val_roc= 0.91162 val_ap= 0.90708 time= 0.07368\n",
      "训练次数: 3 Epoch: 0039 log_lik= 0.48500767 train_kl= -0.01214 train_loss= 0.49715 train_acc= 0.51165 val_roc= 0.91320 val_ap= 0.90958 time= 0.08066\n",
      "训练次数: 3 Epoch: 0040 log_lik= 0.48255846 train_kl= -0.01209 train_loss= 0.49465 train_acc= 0.51490 val_roc= 0.91534 val_ap= 0.91228 time= 0.07568\n",
      "训练次数: 3 Epoch: 0041 log_lik= 0.47996962 train_kl= -0.01204 train_loss= 0.49201 train_acc= 0.51697 val_roc= 0.91784 val_ap= 0.91534 time= 0.08265\n",
      "训练次数: 3 Epoch: 0042 log_lik= 0.47709632 train_kl= -0.01201 train_loss= 0.48910 train_acc= 0.51790 val_roc= 0.92061 val_ap= 0.91849 time= 0.07667\n",
      "训练次数: 3 Epoch: 0043 log_lik= 0.47425088 train_kl= -0.01198 train_loss= 0.48623 train_acc= 0.51908 val_roc= 0.92207 val_ap= 0.91975 time= 0.07966\n",
      "训练次数: 3 Epoch: 0044 log_lik= 0.47447985 train_kl= -0.01195 train_loss= 0.48643 train_acc= 0.51967 val_roc= 0.92212 val_ap= 0.91951 time= 0.07369\n",
      "训练次数: 3 Epoch: 0045 log_lik= 0.47223002 train_kl= -0.01193 train_loss= 0.48416 train_acc= 0.52001 val_roc= 0.92254 val_ap= 0.91952 time= 0.07369\n",
      "训练次数: 3 Epoch: 0046 log_lik= 0.47186726 train_kl= -0.01189 train_loss= 0.48376 train_acc= 0.52190 val_roc= 0.92401 val_ap= 0.92104 time= 0.07867\n",
      "训练次数: 3 Epoch: 0047 log_lik= 0.4701433 train_kl= -0.01185 train_loss= 0.48199 train_acc= 0.51990 val_roc= 0.92563 val_ap= 0.92194 time= 0.07535\n",
      "训练次数: 3 Epoch: 0048 log_lik= 0.4690502 train_kl= -0.01182 train_loss= 0.48087 train_acc= 0.52150 val_roc= 0.92721 val_ap= 0.92282 time= 0.08166\n",
      "训练次数: 3 Epoch: 0049 log_lik= 0.4668511 train_kl= -0.01179 train_loss= 0.47864 train_acc= 0.52125 val_roc= 0.92783 val_ap= 0.92326 time= 0.08464\n",
      "训练次数: 3 Epoch: 0050 log_lik= 0.46566302 train_kl= -0.01176 train_loss= 0.47742 train_acc= 0.52270 val_roc= 0.92777 val_ap= 0.92288 time= 0.07568\n",
      "训练次数: 3 Epoch: 0051 log_lik= 0.46434474 train_kl= -0.01173 train_loss= 0.47607 train_acc= 0.52270 val_roc= 0.92796 val_ap= 0.92302 time= 0.07468\n",
      "训练次数: 3 Epoch: 0052 log_lik= 0.46300286 train_kl= -0.01169 train_loss= 0.47469 train_acc= 0.52302 val_roc= 0.92803 val_ap= 0.92342 time= 0.07568\n",
      "训练次数: 3 Epoch: 0053 log_lik= 0.46180698 train_kl= -0.01165 train_loss= 0.47346 train_acc= 0.52319 val_roc= 0.92794 val_ap= 0.92334 time= 0.07468\n",
      "训练次数: 3 Epoch: 0054 log_lik= 0.46075392 train_kl= -0.01161 train_loss= 0.47237 train_acc= 0.52346 val_roc= 0.92741 val_ap= 0.92236 time= 0.07767\n",
      "训练次数: 3 Epoch: 0055 log_lik= 0.45994428 train_kl= -0.01158 train_loss= 0.47152 train_acc= 0.52394 val_roc= 0.92784 val_ap= 0.92288 time= 0.08166\n",
      "训练次数: 3 Epoch: 0056 log_lik= 0.4588299 train_kl= -0.01155 train_loss= 0.47038 train_acc= 0.52396 val_roc= 0.92794 val_ap= 0.92324 time= 0.07767\n",
      "训练次数: 3 Epoch: 0057 log_lik= 0.45810968 train_kl= -0.01153 train_loss= 0.46964 train_acc= 0.52387 val_roc= 0.92819 val_ap= 0.92378 time= 0.07369\n",
      "训练次数: 3 Epoch: 0058 log_lik= 0.4573869 train_kl= -0.01151 train_loss= 0.46890 train_acc= 0.52577 val_roc= 0.92886 val_ap= 0.92441 time= 0.07568\n",
      "训练次数: 3 Epoch: 0059 log_lik= 0.45654967 train_kl= -0.01149 train_loss= 0.46804 train_acc= 0.52599 val_roc= 0.92909 val_ap= 0.92475 time= 0.07667\n",
      "训练次数: 3 Epoch: 0060 log_lik= 0.4551928 train_kl= -0.01148 train_loss= 0.46667 train_acc= 0.52647 val_roc= 0.92953 val_ap= 0.92519 time= 0.07468\n",
      "训练次数: 3 Epoch: 0061 log_lik= 0.45442867 train_kl= -0.01147 train_loss= 0.46590 train_acc= 0.52738 val_roc= 0.92995 val_ap= 0.92562 time= 0.07468\n",
      "训练次数: 3 Epoch: 0062 log_lik= 0.45405778 train_kl= -0.01147 train_loss= 0.46553 train_acc= 0.52829 val_roc= 0.93124 val_ap= 0.92746 time= 0.07468\n",
      "训练次数: 3 Epoch: 0063 log_lik= 0.45294 train_kl= -0.01146 train_loss= 0.46440 train_acc= 0.52931 val_roc= 0.93292 val_ap= 0.92973 time= 0.07667\n",
      "训练次数: 3 Epoch: 0064 log_lik= 0.45243207 train_kl= -0.01146 train_loss= 0.46389 train_acc= 0.52984 val_roc= 0.93357 val_ap= 0.93063 time= 0.07369\n",
      "训练次数: 3 Epoch: 0065 log_lik= 0.45185834 train_kl= -0.01146 train_loss= 0.46332 train_acc= 0.53030 val_roc= 0.93425 val_ap= 0.93106 time= 0.07573\n",
      "训练次数: 3 Epoch: 0066 log_lik= 0.45055884 train_kl= -0.01146 train_loss= 0.46202 train_acc= 0.53201 val_roc= 0.93467 val_ap= 0.93112 time= 0.07269\n",
      "训练次数: 3 Epoch: 0067 log_lik= 0.44997668 train_kl= -0.01146 train_loss= 0.46144 train_acc= 0.53221 val_roc= 0.93569 val_ap= 0.93258 time= 0.07468\n",
      "训练次数: 3 Epoch: 0068 log_lik= 0.44995937 train_kl= -0.01146 train_loss= 0.46142 train_acc= 0.53277 val_roc= 0.93685 val_ap= 0.93427 time= 0.09062\n",
      "训练次数: 3 Epoch: 0069 log_lik= 0.44900194 train_kl= -0.01145 train_loss= 0.46045 train_acc= 0.53435 val_roc= 0.93746 val_ap= 0.93513 time= 0.07667\n",
      "训练次数: 3 Epoch: 0070 log_lik= 0.44853204 train_kl= -0.01144 train_loss= 0.45997 train_acc= 0.53371 val_roc= 0.93814 val_ap= 0.93546 time= 0.07468\n",
      "训练次数: 3 Epoch: 0071 log_lik= 0.44838557 train_kl= -0.01143 train_loss= 0.45981 train_acc= 0.53423 val_roc= 0.93830 val_ap= 0.93563 time= 0.07767\n",
      "训练次数: 3 Epoch: 0072 log_lik= 0.44745454 train_kl= -0.01142 train_loss= 0.45887 train_acc= 0.53404 val_roc= 0.93869 val_ap= 0.93608 time= 0.08165\n",
      "训练次数: 3 Epoch: 0073 log_lik= 0.44734973 train_kl= -0.01141 train_loss= 0.45876 train_acc= 0.53356 val_roc= 0.93916 val_ap= 0.93716 time= 0.07668\n",
      "训练次数: 3 Epoch: 0074 log_lik= 0.4470326 train_kl= -0.01139 train_loss= 0.45843 train_acc= 0.53316 val_roc= 0.93973 val_ap= 0.93757 time= 0.07966\n",
      "训练次数: 3 Epoch: 0075 log_lik= 0.44664213 train_kl= -0.01138 train_loss= 0.45802 train_acc= 0.53318 val_roc= 0.94032 val_ap= 0.93787 time= 0.07583\n",
      "训练次数: 3 Epoch: 0076 log_lik= 0.4456619 train_kl= -0.01137 train_loss= 0.45704 train_acc= 0.53454 val_roc= 0.94033 val_ap= 0.93770 time= 0.08066\n",
      "训练次数: 3 Epoch: 0077 log_lik= 0.44559094 train_kl= -0.01138 train_loss= 0.45697 train_acc= 0.53386 val_roc= 0.94031 val_ap= 0.93781 time= 0.07568\n",
      "训练次数: 3 Epoch: 0078 log_lik= 0.44470382 train_kl= -0.01137 train_loss= 0.45607 train_acc= 0.53357 val_roc= 0.94002 val_ap= 0.93780 time= 0.07569\n",
      "训练次数: 3 Epoch: 0079 log_lik= 0.4447986 train_kl= -0.01135 train_loss= 0.45615 train_acc= 0.53405 val_roc= 0.93996 val_ap= 0.93828 time= 0.07568\n",
      "训练次数: 3 Epoch: 0080 log_lik= 0.44442242 train_kl= -0.01135 train_loss= 0.45577 train_acc= 0.53397 val_roc= 0.94084 val_ap= 0.93920 time= 0.07767\n",
      "训练次数: 3 Epoch: 0081 log_lik= 0.44372553 train_kl= -0.01136 train_loss= 0.45508 train_acc= 0.53524 val_roc= 0.94139 val_ap= 0.93956 time= 0.07468\n",
      "训练次数: 3 Epoch: 0082 log_lik= 0.44296634 train_kl= -0.01137 train_loss= 0.45434 train_acc= 0.53477 val_roc= 0.94143 val_ap= 0.93929 time= 0.08284\n",
      "训练次数: 3 Epoch: 0083 log_lik= 0.4431599 train_kl= -0.01138 train_loss= 0.45454 train_acc= 0.53487 val_roc= 0.94090 val_ap= 0.93880 time= 0.07667\n",
      "训练次数: 3 Epoch: 0084 log_lik= 0.44263625 train_kl= -0.01138 train_loss= 0.45401 train_acc= 0.53570 val_roc= 0.94057 val_ap= 0.93888 time= 0.08265\n",
      "训练次数: 3 Epoch: 0085 log_lik= 0.4423357 train_kl= -0.01138 train_loss= 0.45372 train_acc= 0.53527 val_roc= 0.94065 val_ap= 0.93936 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0086 log_lik= 0.44208744 train_kl= -0.01138 train_loss= 0.45347 train_acc= 0.53660 val_roc= 0.94130 val_ap= 0.94004 time= 0.08165\n",
      "训练次数: 3 Epoch: 0087 log_lik= 0.4421671 train_kl= -0.01138 train_loss= 0.45355 train_acc= 0.53667 val_roc= 0.94165 val_ap= 0.94046 time= 0.07667\n",
      "训练次数: 3 Epoch: 0088 log_lik= 0.44157776 train_kl= -0.01139 train_loss= 0.45296 train_acc= 0.53663 val_roc= 0.94184 val_ap= 0.94057 time= 0.07468\n",
      "训练次数: 3 Epoch: 0089 log_lik= 0.44133154 train_kl= -0.01140 train_loss= 0.45273 train_acc= 0.53697 val_roc= 0.94127 val_ap= 0.93995 time= 0.07468\n",
      "训练次数: 3 Epoch: 0090 log_lik= 0.44045588 train_kl= -0.01140 train_loss= 0.45186 train_acc= 0.53635 val_roc= 0.94120 val_ap= 0.93991 time= 0.07369\n",
      "训练次数: 3 Epoch: 0091 log_lik= 0.44023734 train_kl= -0.01140 train_loss= 0.45164 train_acc= 0.53666 val_roc= 0.94135 val_ap= 0.93991 time= 0.07667\n",
      "训练次数: 3 Epoch: 0092 log_lik= 0.43984935 train_kl= -0.01140 train_loss= 0.45125 train_acc= 0.53712 val_roc= 0.94148 val_ap= 0.94039 time= 0.07568\n",
      "训练次数: 3 Epoch: 0093 log_lik= 0.4399679 train_kl= -0.01141 train_loss= 0.45138 train_acc= 0.53750 val_roc= 0.94200 val_ap= 0.94081 time= 0.07468\n",
      "训练次数: 3 Epoch: 0094 log_lik= 0.43959996 train_kl= -0.01142 train_loss= 0.45102 train_acc= 0.53707 val_roc= 0.94200 val_ap= 0.94053 time= 0.07568\n",
      "训练次数: 3 Epoch: 0095 log_lik= 0.43940464 train_kl= -0.01143 train_loss= 0.45083 train_acc= 0.53739 val_roc= 0.94191 val_ap= 0.94047 time= 0.07966\n",
      "训练次数: 3 Epoch: 0096 log_lik= 0.43902257 train_kl= -0.01143 train_loss= 0.45045 train_acc= 0.53652 val_roc= 0.94220 val_ap= 0.94092 time= 0.07468\n",
      "训练次数: 3 Epoch: 0097 log_lik= 0.43875876 train_kl= -0.01143 train_loss= 0.45019 train_acc= 0.53767 val_roc= 0.94220 val_ap= 0.94092 time= 0.07269\n",
      "训练次数: 3 Epoch: 0098 log_lik= 0.43835554 train_kl= -0.01145 train_loss= 0.44980 train_acc= 0.53743 val_roc= 0.94253 val_ap= 0.94130 time= 0.07369\n",
      "训练次数: 3 Epoch: 0099 log_lik= 0.43795526 train_kl= -0.01147 train_loss= 0.44942 train_acc= 0.53698 val_roc= 0.94237 val_ap= 0.94116 time= 0.07470\n",
      "训练次数: 3 Epoch: 0100 log_lik= 0.43772233 train_kl= -0.01148 train_loss= 0.44920 train_acc= 0.53785 val_roc= 0.94221 val_ap= 0.94101 time= 0.07369\n",
      "Optimization Finished!\n",
      "训练次数: 3 ROC score: 0.9229500700322976\n",
      "训练次数: 3 AP score: 0.9301399721188317\n",
      "训练次数: 4 Epoch: 0001 log_lik= 1.744036 train_kl= -0.00005 train_loss= 1.74409 train_acc= 0.49393 val_roc= 0.70918 val_ap= 0.71341 time= 1.43413\n",
      "训练次数: 4 Epoch: 0002 log_lik= 1.4970164 train_kl= -0.00031 train_loss= 1.49732 train_acc= 0.47224 val_roc= 0.68798 val_ap= 0.69843 time= 0.07862\n",
      "训练次数: 4 Epoch: 0003 log_lik= 1.3064805 train_kl= -0.00071 train_loss= 1.30719 train_acc= 0.42191 val_roc= 0.67828 val_ap= 0.69799 time= 0.07568\n",
      "训练次数: 4 Epoch: 0004 log_lik= 1.2075918 train_kl= -0.00121 train_loss= 1.20880 train_acc= 0.35305 val_roc= 0.67737 val_ap= 0.70065 time= 0.08265\n",
      "训练次数: 4 Epoch: 0005 log_lik= 1.0964425 train_kl= -0.00166 train_loss= 1.09810 train_acc= 0.32897 val_roc= 0.68340 val_ap= 0.70709 time= 0.07368\n",
      "训练次数: 4 Epoch: 0006 log_lik= 0.97692186 train_kl= -0.00211 train_loss= 0.97904 train_acc= 0.34566 val_roc= 0.69686 val_ap= 0.72066 time= 0.07369\n",
      "训练次数: 4 Epoch: 0007 log_lik= 0.8798392 train_kl= -0.00268 train_loss= 0.88252 train_acc= 0.36057 val_roc= 0.71117 val_ap= 0.73400 time= 0.08166\n",
      "训练次数: 4 Epoch: 0008 log_lik= 0.81430346 train_kl= -0.00339 train_loss= 0.81769 train_acc= 0.35547 val_roc= 0.72012 val_ap= 0.74053 time= 0.07468\n",
      "训练次数: 4 Epoch: 0009 log_lik= 0.77130044 train_kl= -0.00419 train_loss= 0.77549 train_acc= 0.32762 val_roc= 0.73028 val_ap= 0.74784 time= 0.08763\n",
      "训练次数: 4 Epoch: 0010 log_lik= 0.7481772 train_kl= -0.00504 train_loss= 0.75321 train_acc= 0.29699 val_roc= 0.74930 val_ap= 0.76350 time= 0.07767\n",
      "训练次数: 4 Epoch: 0011 log_lik= 0.7209438 train_kl= -0.00586 train_loss= 0.72681 train_acc= 0.26610 val_roc= 0.77152 val_ap= 0.78329 time= 0.07568\n",
      "训练次数: 4 Epoch: 0012 log_lik= 0.7074905 train_kl= -0.00667 train_loss= 0.71416 train_acc= 0.24185 val_roc= 0.79353 val_ap= 0.80300 time= 0.07469\n",
      "训练次数: 4 Epoch: 0013 log_lik= 0.6956393 train_kl= -0.00744 train_loss= 0.70308 train_acc= 0.21197 val_roc= 0.81156 val_ap= 0.81937 time= 0.07667\n",
      "训练次数: 4 Epoch: 0014 log_lik= 0.68564093 train_kl= -0.00816 train_loss= 0.69380 train_acc= 0.18927 val_roc= 0.83322 val_ap= 0.83840 time= 0.07468\n",
      "训练次数: 4 Epoch: 0015 log_lik= 0.6707376 train_kl= -0.00881 train_loss= 0.67955 train_acc= 0.19510 val_roc= 0.85778 val_ap= 0.86218 time= 0.07369\n",
      "训练次数: 4 Epoch: 0016 log_lik= 0.650835 train_kl= -0.00938 train_loss= 0.66022 train_acc= 0.23349 val_roc= 0.87431 val_ap= 0.87781 time= 0.07369\n",
      "训练次数: 4 Epoch: 0017 log_lik= 0.6278119 train_kl= -0.00990 train_loss= 0.63771 train_acc= 0.30514 val_roc= 0.88065 val_ap= 0.88363 time= 0.07568\n",
      "训练次数: 4 Epoch: 0018 log_lik= 0.6048611 train_kl= -0.01037 train_loss= 0.61523 train_acc= 0.35822 val_roc= 0.88290 val_ap= 0.88491 time= 0.07767\n",
      "训练次数: 4 Epoch: 0019 log_lik= 0.5839647 train_kl= -0.01080 train_loss= 0.59476 train_acc= 0.39305 val_roc= 0.88493 val_ap= 0.88705 time= 0.07468\n",
      "训练次数: 4 Epoch: 0020 log_lik= 0.56730264 train_kl= -0.01119 train_loss= 0.57849 train_acc= 0.41798 val_roc= 0.88463 val_ap= 0.88557 time= 0.07568\n",
      "训练次数: 4 Epoch: 0021 log_lik= 0.55662245 train_kl= -0.01153 train_loss= 0.56815 train_acc= 0.43976 val_roc= 0.88514 val_ap= 0.88630 time= 0.07369\n",
      "训练次数: 4 Epoch: 0022 log_lik= 0.5530822 train_kl= -0.01183 train_loss= 0.56491 train_acc= 0.45798 val_roc= 0.88754 val_ap= 0.88905 time= 0.08216\n",
      "训练次数: 4 Epoch: 0023 log_lik= 0.54696167 train_kl= -0.01207 train_loss= 0.55903 train_acc= 0.47982 val_roc= 0.88989 val_ap= 0.89079 time= 0.08066\n",
      "训练次数: 4 Epoch: 0024 log_lik= 0.54193807 train_kl= -0.01225 train_loss= 0.55418 train_acc= 0.49965 val_roc= 0.89277 val_ap= 0.89332 time= 0.07269\n",
      "训练次数: 4 Epoch: 0025 log_lik= 0.5400672 train_kl= -0.01239 train_loss= 0.55246 train_acc= 0.50289 val_roc= 0.89689 val_ap= 0.89815 time= 0.07369\n",
      "训练次数: 4 Epoch: 0026 log_lik= 0.53876954 train_kl= -0.01250 train_loss= 0.55127 train_acc= 0.49636 val_roc= 0.90137 val_ap= 0.90195 time= 0.07368\n",
      "训练次数: 4 Epoch: 0027 log_lik= 0.53683096 train_kl= -0.01256 train_loss= 0.54939 train_acc= 0.49073 val_roc= 0.90559 val_ap= 0.90633 time= 0.07468\n",
      "训练次数: 4 Epoch: 0028 log_lik= 0.5301438 train_kl= -0.01257 train_loss= 0.54271 train_acc= 0.48960 val_roc= 0.90912 val_ap= 0.90971 time= 0.07469\n",
      "训练次数: 4 Epoch: 0029 log_lik= 0.519161 train_kl= -0.01255 train_loss= 0.53171 train_acc= 0.49684 val_roc= 0.91112 val_ap= 0.91238 time= 0.07568\n",
      "训练次数: 4 Epoch: 0030 log_lik= 0.5089527 train_kl= -0.01252 train_loss= 0.52148 train_acc= 0.50462 val_roc= 0.91180 val_ap= 0.91414 time= 0.07269\n",
      "训练次数: 4 Epoch: 0031 log_lik= 0.50272065 train_kl= -0.01250 train_loss= 0.51522 train_acc= 0.50828 val_roc= 0.91217 val_ap= 0.91470 time= 0.07468\n",
      "训练次数: 4 Epoch: 0032 log_lik= 0.4994987 train_kl= -0.01246 train_loss= 0.51196 train_acc= 0.50784 val_roc= 0.91099 val_ap= 0.91302 time= 0.07866\n",
      "训练次数: 4 Epoch: 0033 log_lik= 0.4993905 train_kl= -0.01240 train_loss= 0.51179 train_acc= 0.50431 val_roc= 0.91009 val_ap= 0.91205 time= 0.07632\n",
      "训练次数: 4 Epoch: 0034 log_lik= 0.4982613 train_kl= -0.01234 train_loss= 0.51060 train_acc= 0.50434 val_roc= 0.90880 val_ap= 0.91092 time= 0.07269\n",
      "训练次数: 4 Epoch: 0035 log_lik= 0.49753883 train_kl= -0.01227 train_loss= 0.50981 train_acc= 0.50529 val_roc= 0.90733 val_ap= 0.91041 time= 0.08265\n",
      "训练次数: 4 Epoch: 0036 log_lik= 0.49487236 train_kl= -0.01222 train_loss= 0.50709 train_acc= 0.50776 val_roc= 0.90558 val_ap= 0.90886 time= 0.07369\n",
      "训练次数: 4 Epoch: 0037 log_lik= 0.49307618 train_kl= -0.01218 train_loss= 0.50526 train_acc= 0.50889 val_roc= 0.90421 val_ap= 0.90775 time= 0.07468\n",
      "训练次数: 4 Epoch: 0038 log_lik= 0.49168238 train_kl= -0.01214 train_loss= 0.50382 train_acc= 0.50716 val_roc= 0.90435 val_ap= 0.90790 time= 0.07568\n",
      "训练次数: 4 Epoch: 0039 log_lik= 0.48763347 train_kl= -0.01209 train_loss= 0.49972 train_acc= 0.50845 val_roc= 0.90481 val_ap= 0.90818 time= 0.07369\n",
      "训练次数: 4 Epoch: 0040 log_lik= 0.4852076 train_kl= -0.01203 train_loss= 0.49724 train_acc= 0.50964 val_roc= 0.90509 val_ap= 0.90835 time= 0.07369\n",
      "训练次数: 4 Epoch: 0041 log_lik= 0.48073417 train_kl= -0.01198 train_loss= 0.49271 train_acc= 0.51369 val_roc= 0.90500 val_ap= 0.90838 time= 0.08166\n",
      "训练次数: 4 Epoch: 0042 log_lik= 0.47801498 train_kl= -0.01194 train_loss= 0.48996 train_acc= 0.51612 val_roc= 0.90467 val_ap= 0.90780 time= 0.07374\n",
      "训练次数: 4 Epoch: 0043 log_lik= 0.47680256 train_kl= -0.01191 train_loss= 0.48871 train_acc= 0.51573 val_roc= 0.90341 val_ap= 0.90640 time= 0.07469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 Epoch: 0044 log_lik= 0.47589293 train_kl= -0.01189 train_loss= 0.48778 train_acc= 0.51442 val_roc= 0.90298 val_ap= 0.90591 time= 0.07369\n",
      "训练次数: 4 Epoch: 0045 log_lik= 0.47544852 train_kl= -0.01185 train_loss= 0.48730 train_acc= 0.51337 val_roc= 0.90237 val_ap= 0.90474 time= 0.07368\n",
      "训练次数: 4 Epoch: 0046 log_lik= 0.47436506 train_kl= -0.01180 train_loss= 0.48616 train_acc= 0.51492 val_roc= 0.90220 val_ap= 0.90436 time= 0.07369\n",
      "训练次数: 4 Epoch: 0047 log_lik= 0.47302672 train_kl= -0.01173 train_loss= 0.48476 train_acc= 0.51655 val_roc= 0.90156 val_ap= 0.90321 time= 0.08265\n",
      "训练次数: 4 Epoch: 0048 log_lik= 0.4731149 train_kl= -0.01165 train_loss= 0.48477 train_acc= 0.51630 val_roc= 0.90140 val_ap= 0.90252 time= 0.08265\n",
      "训练次数: 4 Epoch: 0049 log_lik= 0.47210702 train_kl= -0.01157 train_loss= 0.48367 train_acc= 0.51768 val_roc= 0.90166 val_ap= 0.90291 time= 0.07369\n",
      "训练次数: 4 Epoch: 0050 log_lik= 0.47192138 train_kl= -0.01148 train_loss= 0.48340 train_acc= 0.51614 val_roc= 0.90142 val_ap= 0.90196 time= 0.07568\n",
      "训练次数: 4 Epoch: 0051 log_lik= 0.47041836 train_kl= -0.01138 train_loss= 0.48179 train_acc= 0.51666 val_roc= 0.90113 val_ap= 0.90079 time= 0.07369\n",
      "训练次数: 4 Epoch: 0052 log_lik= 0.4698868 train_kl= -0.01127 train_loss= 0.48116 train_acc= 0.51627 val_roc= 0.90110 val_ap= 0.90054 time= 0.07421\n",
      "训练次数: 4 Epoch: 0053 log_lik= 0.46822858 train_kl= -0.01116 train_loss= 0.47939 train_acc= 0.51648 val_roc= 0.90120 val_ap= 0.90040 time= 0.07867\n",
      "训练次数: 4 Epoch: 0054 log_lik= 0.46715605 train_kl= -0.01105 train_loss= 0.47820 train_acc= 0.51688 val_roc= 0.90150 val_ap= 0.90007 time= 0.07767\n",
      "训练次数: 4 Epoch: 0055 log_lik= 0.46647754 train_kl= -0.01093 train_loss= 0.47741 train_acc= 0.51715 val_roc= 0.90250 val_ap= 0.90050 time= 0.07568\n",
      "训练次数: 4 Epoch: 0056 log_lik= 0.46596935 train_kl= -0.01083 train_loss= 0.47680 train_acc= 0.51735 val_roc= 0.90324 val_ap= 0.90112 time= 0.07767\n",
      "训练次数: 4 Epoch: 0057 log_lik= 0.46519378 train_kl= -0.01074 train_loss= 0.47594 train_acc= 0.51682 val_roc= 0.90374 val_ap= 0.90162 time= 0.07568\n",
      "训练次数: 4 Epoch: 0058 log_lik= 0.4656079 train_kl= -0.01067 train_loss= 0.47628 train_acc= 0.51647 val_roc= 0.90447 val_ap= 0.90198 time= 0.07568\n",
      "训练次数: 4 Epoch: 0059 log_lik= 0.4647615 train_kl= -0.01061 train_loss= 0.47537 train_acc= 0.51817 val_roc= 0.90559 val_ap= 0.90341 time= 0.07767\n",
      "训练次数: 4 Epoch: 0060 log_lik= 0.46374118 train_kl= -0.01057 train_loss= 0.47431 train_acc= 0.51803 val_roc= 0.90653 val_ap= 0.90431 time= 0.07469\n",
      "训练次数: 4 Epoch: 0061 log_lik= 0.4633988 train_kl= -0.01055 train_loss= 0.47395 train_acc= 0.51829 val_roc= 0.90681 val_ap= 0.90444 time= 0.08265\n",
      "训练次数: 4 Epoch: 0062 log_lik= 0.46263286 train_kl= -0.01054 train_loss= 0.47317 train_acc= 0.51765 val_roc= 0.90740 val_ap= 0.90553 time= 0.07468\n",
      "训练次数: 4 Epoch: 0063 log_lik= 0.461842 train_kl= -0.01054 train_loss= 0.47238 train_acc= 0.51859 val_roc= 0.90885 val_ap= 0.90765 time= 0.08166\n",
      "训练次数: 4 Epoch: 0064 log_lik= 0.46129745 train_kl= -0.01055 train_loss= 0.47184 train_acc= 0.51793 val_roc= 0.91029 val_ap= 0.90980 time= 0.07369\n",
      "训练次数: 4 Epoch: 0065 log_lik= 0.46093163 train_kl= -0.01055 train_loss= 0.47148 train_acc= 0.51829 val_roc= 0.91091 val_ap= 0.91087 time= 0.08066\n",
      "训练次数: 4 Epoch: 0066 log_lik= 0.4605111 train_kl= -0.01055 train_loss= 0.47106 train_acc= 0.51907 val_roc= 0.91162 val_ap= 0.91223 time= 0.07468\n",
      "训练次数: 4 Epoch: 0067 log_lik= 0.4595936 train_kl= -0.01056 train_loss= 0.47015 train_acc= 0.51848 val_roc= 0.91261 val_ap= 0.91412 time= 0.08265\n",
      "训练次数: 4 Epoch: 0068 log_lik= 0.4591771 train_kl= -0.01057 train_loss= 0.46975 train_acc= 0.51876 val_roc= 0.91318 val_ap= 0.91549 time= 0.07468\n",
      "训练次数: 4 Epoch: 0069 log_lik= 0.45824465 train_kl= -0.01058 train_loss= 0.46882 train_acc= 0.51904 val_roc= 0.91385 val_ap= 0.91613 time= 0.07369\n",
      "训练次数: 4 Epoch: 0070 log_lik= 0.457936 train_kl= -0.01058 train_loss= 0.46851 train_acc= 0.51919 val_roc= 0.91472 val_ap= 0.91768 time= 0.07269\n",
      "训练次数: 4 Epoch: 0071 log_lik= 0.4574996 train_kl= -0.01057 train_loss= 0.46807 train_acc= 0.51883 val_roc= 0.91469 val_ap= 0.91781 time= 0.07568\n",
      "训练次数: 4 Epoch: 0072 log_lik= 0.4577533 train_kl= -0.01056 train_loss= 0.46831 train_acc= 0.51986 val_roc= 0.91473 val_ap= 0.91842 time= 0.07767\n",
      "训练次数: 4 Epoch: 0073 log_lik= 0.45616195 train_kl= -0.01056 train_loss= 0.46672 train_acc= 0.51960 val_roc= 0.91440 val_ap= 0.91869 time= 0.07369\n",
      "训练次数: 4 Epoch: 0074 log_lik= 0.4557212 train_kl= -0.01056 train_loss= 0.46628 train_acc= 0.52059 val_roc= 0.91431 val_ap= 0.91854 time= 0.08066\n",
      "训练次数: 4 Epoch: 0075 log_lik= 0.45496643 train_kl= -0.01056 train_loss= 0.46552 train_acc= 0.51970 val_roc= 0.91433 val_ap= 0.91878 time= 0.07568\n",
      "训练次数: 4 Epoch: 0076 log_lik= 0.45454785 train_kl= -0.01055 train_loss= 0.46510 train_acc= 0.52026 val_roc= 0.91469 val_ap= 0.91924 time= 0.07369\n",
      "训练次数: 4 Epoch: 0077 log_lik= 0.45448983 train_kl= -0.01055 train_loss= 0.46504 train_acc= 0.52016 val_roc= 0.91461 val_ap= 0.91941 time= 0.07368\n",
      "训练次数: 4 Epoch: 0078 log_lik= 0.4538485 train_kl= -0.01054 train_loss= 0.46439 train_acc= 0.52029 val_roc= 0.91422 val_ap= 0.91935 time= 0.07568\n",
      "训练次数: 4 Epoch: 0079 log_lik= 0.45332786 train_kl= -0.01056 train_loss= 0.46389 train_acc= 0.52098 val_roc= 0.91427 val_ap= 0.91996 time= 0.08265\n",
      "训练次数: 4 Epoch: 0080 log_lik= 0.45296168 train_kl= -0.01058 train_loss= 0.46354 train_acc= 0.52166 val_roc= 0.91428 val_ap= 0.92035 time= 0.07667\n",
      "训练次数: 4 Epoch: 0081 log_lik= 0.45265943 train_kl= -0.01059 train_loss= 0.46325 train_acc= 0.52115 val_roc= 0.91456 val_ap= 0.92090 time= 0.07269\n",
      "训练次数: 4 Epoch: 0082 log_lik= 0.45211658 train_kl= -0.01060 train_loss= 0.46271 train_acc= 0.52217 val_roc= 0.91418 val_ap= 0.92066 time= 0.08165\n",
      "训练次数: 4 Epoch: 0083 log_lik= 0.4522522 train_kl= -0.01061 train_loss= 0.46286 train_acc= 0.52178 val_roc= 0.91473 val_ap= 0.92157 time= 0.07468\n",
      "训练次数: 4 Epoch: 0084 log_lik= 0.45136282 train_kl= -0.01063 train_loss= 0.46199 train_acc= 0.52248 val_roc= 0.91422 val_ap= 0.92136 time= 0.07468\n",
      "训练次数: 4 Epoch: 0085 log_lik= 0.4510422 train_kl= -0.01065 train_loss= 0.46169 train_acc= 0.52292 val_roc= 0.91355 val_ap= 0.92106 time= 0.07568\n",
      "训练次数: 4 Epoch: 0086 log_lik= 0.4507895 train_kl= -0.01067 train_loss= 0.46146 train_acc= 0.52403 val_roc= 0.91327 val_ap= 0.92086 time= 0.07468\n",
      "训练次数: 4 Epoch: 0087 log_lik= 0.45085165 train_kl= -0.01069 train_loss= 0.46154 train_acc= 0.52279 val_roc= 0.91370 val_ap= 0.92142 time= 0.08165\n",
      "训练次数: 4 Epoch: 0088 log_lik= 0.4503245 train_kl= -0.01069 train_loss= 0.46102 train_acc= 0.52402 val_roc= 0.91398 val_ap= 0.92178 time= 0.08222\n",
      "训练次数: 4 Epoch: 0089 log_lik= 0.4496668 train_kl= -0.01070 train_loss= 0.46037 train_acc= 0.52435 val_roc= 0.91386 val_ap= 0.92169 time= 0.07350\n",
      "训练次数: 4 Epoch: 0090 log_lik= 0.44972706 train_kl= -0.01072 train_loss= 0.46045 train_acc= 0.52339 val_roc= 0.91344 val_ap= 0.92144 time= 0.07468\n",
      "训练次数: 4 Epoch: 0091 log_lik= 0.44947022 train_kl= -0.01074 train_loss= 0.46021 train_acc= 0.52536 val_roc= 0.91353 val_ap= 0.92180 time= 0.07269\n",
      "训练次数: 4 Epoch: 0092 log_lik= 0.44843522 train_kl= -0.01076 train_loss= 0.45920 train_acc= 0.52528 val_roc= 0.91311 val_ap= 0.92121 time= 0.07568\n",
      "训练次数: 4 Epoch: 0093 log_lik= 0.44813803 train_kl= -0.01077 train_loss= 0.45891 train_acc= 0.52542 val_roc= 0.91314 val_ap= 0.92132 time= 0.07966\n",
      "训练次数: 4 Epoch: 0094 log_lik= 0.44747362 train_kl= -0.01077 train_loss= 0.45825 train_acc= 0.52705 val_roc= 0.91385 val_ap= 0.92224 time= 0.07468\n",
      "训练次数: 4 Epoch: 0095 log_lik= 0.44679463 train_kl= -0.01079 train_loss= 0.45758 train_acc= 0.52701 val_roc= 0.91401 val_ap= 0.92255 time= 0.07668\n",
      "训练次数: 4 Epoch: 0096 log_lik= 0.44624665 train_kl= -0.01080 train_loss= 0.45704 train_acc= 0.52734 val_roc= 0.91457 val_ap= 0.92299 time= 0.07269\n",
      "训练次数: 4 Epoch: 0097 log_lik= 0.4459343 train_kl= -0.01081 train_loss= 0.45675 train_acc= 0.52760 val_roc= 0.91479 val_ap= 0.92314 time= 0.07369\n",
      "训练次数: 4 Epoch: 0098 log_lik= 0.44545838 train_kl= -0.01081 train_loss= 0.45627 train_acc= 0.52842 val_roc= 0.91489 val_ap= 0.92339 time= 0.07568\n",
      "训练次数: 4 Epoch: 0099 log_lik= 0.44519043 train_kl= -0.01082 train_loss= 0.45601 train_acc= 0.52921 val_roc= 0.91521 val_ap= 0.92383 time= 0.07568\n",
      "训练次数: 4 Epoch: 0100 log_lik= 0.44465706 train_kl= -0.01083 train_loss= 0.45549 train_acc= 0.52926 val_roc= 0.91499 val_ap= 0.92371 time= 0.07866\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 ROC score: 0.9210597380900086\n",
      "训练次数: 4 AP score: 0.9230886454537566\n",
      "训练次数: 5 Epoch: 0001 log_lik= 1.7505344 train_kl= -0.00005 train_loss= 1.75059 train_acc= 0.49532 val_roc= 0.66881 val_ap= 0.69970 time= 1.35744\n",
      "训练次数: 5 Epoch: 0002 log_lik= 1.5285958 train_kl= -0.00015 train_loss= 1.52875 train_acc= 0.47853 val_roc= 0.66531 val_ap= 0.70289 time= 0.09259\n",
      "训练次数: 5 Epoch: 0003 log_lik= 1.3710485 train_kl= -0.00043 train_loss= 1.37148 train_acc= 0.44748 val_roc= 0.66909 val_ap= 0.70493 time= 0.08165\n",
      "训练次数: 5 Epoch: 0004 log_lik= 1.1983352 train_kl= -0.00080 train_loss= 1.19913 train_acc= 0.40725 val_roc= 0.67673 val_ap= 0.71174 time= 0.07667\n",
      "训练次数: 5 Epoch: 0005 log_lik= 1.0483097 train_kl= -0.00125 train_loss= 1.04955 train_acc= 0.37506 val_roc= 0.68645 val_ap= 0.72069 time= 0.07767\n",
      "训练次数: 5 Epoch: 0006 log_lik= 0.90971476 train_kl= -0.00180 train_loss= 0.91152 train_acc= 0.37272 val_roc= 0.68988 val_ap= 0.72468 time= 0.07767\n",
      "训练次数: 5 Epoch: 0007 log_lik= 0.8187719 train_kl= -0.00255 train_loss= 0.82132 train_acc= 0.36809 val_roc= 0.67498 val_ap= 0.70874 time= 0.07767\n",
      "训练次数: 5 Epoch: 0008 log_lik= 0.75408345 train_kl= -0.00349 train_loss= 0.75757 train_acc= 0.34417 val_roc= 0.67555 val_ap= 0.70480 time= 0.07568\n",
      "训练次数: 5 Epoch: 0009 log_lik= 0.7276711 train_kl= -0.00454 train_loss= 0.73221 train_acc= 0.31962 val_roc= 0.70164 val_ap= 0.72649 time= 0.07369\n",
      "训练次数: 5 Epoch: 0010 log_lik= 0.7117027 train_kl= -0.00564 train_loss= 0.71735 train_acc= 0.27031 val_roc= 0.74938 val_ap= 0.77737 time= 0.07468\n",
      "训练次数: 5 Epoch: 0011 log_lik= 0.69698524 train_kl= -0.00674 train_loss= 0.70373 train_acc= 0.21623 val_roc= 0.78073 val_ap= 0.80930 time= 0.07966\n",
      "训练次数: 5 Epoch: 0012 log_lik= 0.6970569 train_kl= -0.00781 train_loss= 0.70486 train_acc= 0.13860 val_roc= 0.80737 val_ap= 0.82551 time= 0.07568\n",
      "训练次数: 5 Epoch: 0013 log_lik= 0.68285066 train_kl= -0.00876 train_loss= 0.69161 train_acc= 0.13227 val_roc= 0.83021 val_ap= 0.83785 time= 0.07568\n",
      "训练次数: 5 Epoch: 0014 log_lik= 0.6590566 train_kl= -0.00960 train_loss= 0.66865 train_acc= 0.19241 val_roc= 0.82719 val_ap= 0.82455 time= 0.07767\n",
      "训练次数: 5 Epoch: 0015 log_lik= 0.6343319 train_kl= -0.01037 train_loss= 0.64471 train_acc= 0.27497 val_roc= 0.82559 val_ap= 0.81776 time= 0.07468\n",
      "训练次数: 5 Epoch: 0016 log_lik= 0.6158873 train_kl= -0.01110 train_loss= 0.62699 train_acc= 0.33768 val_roc= 0.84302 val_ap= 0.83748 time= 0.07369\n",
      "训练次数: 5 Epoch: 0017 log_lik= 0.596546 train_kl= -0.01176 train_loss= 0.60831 train_acc= 0.38487 val_roc= 0.85479 val_ap= 0.84827 time= 0.07369\n",
      "训练次数: 5 Epoch: 0018 log_lik= 0.58859575 train_kl= -0.01237 train_loss= 0.60096 train_acc= 0.40203 val_roc= 0.85436 val_ap= 0.84728 time= 0.07568\n",
      "训练次数: 5 Epoch: 0019 log_lik= 0.5826197 train_kl= -0.01291 train_loss= 0.59553 train_acc= 0.41671 val_roc= 0.85540 val_ap= 0.84801 time= 0.07570\n",
      "训练次数: 5 Epoch: 0020 log_lik= 0.57881653 train_kl= -0.01335 train_loss= 0.59217 train_acc= 0.43310 val_roc= 0.86495 val_ap= 0.85994 time= 0.07568\n",
      "训练次数: 5 Epoch: 0021 log_lik= 0.5683906 train_kl= -0.01370 train_loss= 0.58209 train_acc= 0.44949 val_roc= 0.87447 val_ap= 0.87305 time= 0.07468\n",
      "训练次数: 5 Epoch: 0022 log_lik= 0.5558133 train_kl= -0.01397 train_loss= 0.56979 train_acc= 0.46596 val_roc= 0.88324 val_ap= 0.88236 time= 0.07568\n",
      "训练次数: 5 Epoch: 0023 log_lik= 0.5414426 train_kl= -0.01420 train_loss= 0.55564 train_acc= 0.47904 val_roc= 0.89037 val_ap= 0.88867 time= 0.07568\n",
      "训练次数: 5 Epoch: 0024 log_lik= 0.533448 train_kl= -0.01439 train_loss= 0.54784 train_acc= 0.47912 val_roc= 0.89602 val_ap= 0.89337 time= 0.07468\n",
      "训练次数: 5 Epoch: 0025 log_lik= 0.5282636 train_kl= -0.01454 train_loss= 0.54281 train_acc= 0.47746 val_roc= 0.89773 val_ap= 0.89358 time= 0.07767\n",
      "训练次数: 5 Epoch: 0026 log_lik= 0.5216323 train_kl= -0.01465 train_loss= 0.53628 train_acc= 0.48730 val_roc= 0.89550 val_ap= 0.88920 time= 0.07468\n",
      "训练次数: 5 Epoch: 0027 log_lik= 0.51528007 train_kl= -0.01473 train_loss= 0.53001 train_acc= 0.49876 val_roc= 0.89079 val_ap= 0.88420 time= 0.07369\n",
      "训练次数: 5 Epoch: 0028 log_lik= 0.51250505 train_kl= -0.01480 train_loss= 0.52730 train_acc= 0.50403 val_roc= 0.88738 val_ap= 0.88182 time= 0.07866\n",
      "训练次数: 5 Epoch: 0029 log_lik= 0.5131976 train_kl= -0.01485 train_loss= 0.52805 train_acc= 0.49921 val_roc= 0.88697 val_ap= 0.88298 time= 0.07469\n",
      "训练次数: 5 Epoch: 0030 log_lik= 0.51297694 train_kl= -0.01489 train_loss= 0.52787 train_acc= 0.49354 val_roc= 0.88829 val_ap= 0.88495 time= 0.07468\n",
      "训练次数: 5 Epoch: 0031 log_lik= 0.5098192 train_kl= -0.01489 train_loss= 0.52471 train_acc= 0.49635 val_roc= 0.89028 val_ap= 0.88654 time= 0.07767\n",
      "训练次数: 5 Epoch: 0032 log_lik= 0.5055279 train_kl= -0.01487 train_loss= 0.52040 train_acc= 0.50301 val_roc= 0.89273 val_ap= 0.88848 time= 0.07568\n",
      "训练次数: 5 Epoch: 0033 log_lik= 0.5005823 train_kl= -0.01484 train_loss= 0.51542 train_acc= 0.50949 val_roc= 0.89544 val_ap= 0.89223 time= 0.07469\n",
      "训练次数: 5 Epoch: 0034 log_lik= 0.49688524 train_kl= -0.01480 train_loss= 0.51168 train_acc= 0.51087 val_roc= 0.89938 val_ap= 0.89798 time= 0.07667\n",
      "训练次数: 5 Epoch: 0035 log_lik= 0.49314427 train_kl= -0.01476 train_loss= 0.50790 train_acc= 0.51026 val_roc= 0.90208 val_ap= 0.90239 time= 0.07667\n",
      "训练次数: 5 Epoch: 0036 log_lik= 0.48972297 train_kl= -0.01470 train_loss= 0.50443 train_acc= 0.50869 val_roc= 0.90412 val_ap= 0.90490 time= 0.07369\n",
      "训练次数: 5 Epoch: 0037 log_lik= 0.48742577 train_kl= -0.01463 train_loss= 0.50206 train_acc= 0.51066 val_roc= 0.90578 val_ap= 0.90654 time= 0.07568\n",
      "训练次数: 5 Epoch: 0038 log_lik= 0.48338708 train_kl= -0.01454 train_loss= 0.49793 train_acc= 0.51403 val_roc= 0.90739 val_ap= 0.90872 time= 0.07663\n",
      "训练次数: 5 Epoch: 0039 log_lik= 0.48082486 train_kl= -0.01445 train_loss= 0.49528 train_acc= 0.51668 val_roc= 0.90889 val_ap= 0.91113 time= 0.07468\n",
      "训练次数: 5 Epoch: 0040 log_lik= 0.47830564 train_kl= -0.01436 train_loss= 0.49266 train_acc= 0.51598 val_roc= 0.90937 val_ap= 0.91306 time= 0.08066\n",
      "训练次数: 5 Epoch: 0041 log_lik= 0.47688383 train_kl= -0.01426 train_loss= 0.49114 train_acc= 0.51379 val_roc= 0.90953 val_ap= 0.91354 time= 0.08165\n",
      "训练次数: 5 Epoch: 0042 log_lik= 0.4775658 train_kl= -0.01416 train_loss= 0.49173 train_acc= 0.51307 val_roc= 0.90968 val_ap= 0.91382 time= 0.07568\n",
      "训练次数: 5 Epoch: 0043 log_lik= 0.47506568 train_kl= -0.01406 train_loss= 0.48913 train_acc= 0.51446 val_roc= 0.91019 val_ap= 0.91447 time= 0.07668\n",
      "训练次数: 5 Epoch: 0044 log_lik= 0.47212934 train_kl= -0.01396 train_loss= 0.48609 train_acc= 0.51796 val_roc= 0.91028 val_ap= 0.91475 time= 0.07269\n",
      "训练次数: 5 Epoch: 0045 log_lik= 0.4706536 train_kl= -0.01386 train_loss= 0.48451 train_acc= 0.51799 val_roc= 0.90980 val_ap= 0.91538 time= 0.07269\n",
      "训练次数: 5 Epoch: 0046 log_lik= 0.46991405 train_kl= -0.01376 train_loss= 0.48367 train_acc= 0.51739 val_roc= 0.91013 val_ap= 0.91621 time= 0.07470\n",
      "训练次数: 5 Epoch: 0047 log_lik= 0.46915472 train_kl= -0.01366 train_loss= 0.48281 train_acc= 0.51757 val_roc= 0.91083 val_ap= 0.91736 time= 0.07469\n",
      "训练次数: 5 Epoch: 0048 log_lik= 0.46725142 train_kl= -0.01355 train_loss= 0.48080 train_acc= 0.51800 val_roc= 0.91159 val_ap= 0.91816 time= 0.07468\n",
      "训练次数: 5 Epoch: 0049 log_lik= 0.46550316 train_kl= -0.01344 train_loss= 0.47894 train_acc= 0.51902 val_roc= 0.91200 val_ap= 0.91890 time= 0.07568\n",
      "训练次数: 5 Epoch: 0050 log_lik= 0.46413854 train_kl= -0.01334 train_loss= 0.47747 train_acc= 0.51939 val_roc= 0.91235 val_ap= 0.92005 time= 0.07468\n",
      "训练次数: 5 Epoch: 0051 log_lik= 0.4629976 train_kl= -0.01323 train_loss= 0.47623 train_acc= 0.51929 val_roc= 0.91259 val_ap= 0.92097 time= 0.07369\n",
      "训练次数: 5 Epoch: 0052 log_lik= 0.46197024 train_kl= -0.01314 train_loss= 0.47511 train_acc= 0.51922 val_roc= 0.91253 val_ap= 0.92169 time= 0.07667\n",
      "训练次数: 5 Epoch: 0053 log_lik= 0.4611444 train_kl= -0.01306 train_loss= 0.47420 train_acc= 0.51883 val_roc= 0.91291 val_ap= 0.92271 time= 0.08265\n",
      "训练次数: 5 Epoch: 0054 log_lik= 0.45958063 train_kl= -0.01297 train_loss= 0.47255 train_acc= 0.52006 val_roc= 0.91346 val_ap= 0.92355 time= 0.08564\n",
      "训练次数: 5 Epoch: 0055 log_lik= 0.45789644 train_kl= -0.01288 train_loss= 0.47078 train_acc= 0.52054 val_roc= 0.91440 val_ap= 0.92466 time= 0.07468\n",
      "训练次数: 5 Epoch: 0056 log_lik= 0.45744073 train_kl= -0.01280 train_loss= 0.47024 train_acc= 0.52046 val_roc= 0.91461 val_ap= 0.92481 time= 0.07567\n",
      "训练次数: 5 Epoch: 0057 log_lik= 0.4561749 train_kl= -0.01272 train_loss= 0.46889 train_acc= 0.52191 val_roc= 0.91418 val_ap= 0.92475 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0058 log_lik= 0.45578074 train_kl= -0.01263 train_loss= 0.46841 train_acc= 0.52210 val_roc= 0.91372 val_ap= 0.92489 time= 0.07867\n",
      "训练次数: 5 Epoch: 0059 log_lik= 0.4551612 train_kl= -0.01254 train_loss= 0.46770 train_acc= 0.52199 val_roc= 0.91407 val_ap= 0.92531 time= 0.07368\n",
      "训练次数: 5 Epoch: 0060 log_lik= 0.45359424 train_kl= -0.01246 train_loss= 0.46605 train_acc= 0.52233 val_roc= 0.91430 val_ap= 0.92554 time= 0.07369\n",
      "训练次数: 5 Epoch: 0061 log_lik= 0.45347798 train_kl= -0.01238 train_loss= 0.46585 train_acc= 0.52189 val_roc= 0.91425 val_ap= 0.92529 time= 0.07667\n",
      "训练次数: 5 Epoch: 0062 log_lik= 0.45255846 train_kl= -0.01230 train_loss= 0.46486 train_acc= 0.52336 val_roc= 0.91438 val_ap= 0.92527 time= 0.07668\n",
      "训练次数: 5 Epoch: 0063 log_lik= 0.4517146 train_kl= -0.01222 train_loss= 0.46394 train_acc= 0.52332 val_roc= 0.91427 val_ap= 0.92525 time= 0.07269\n",
      "训练次数: 5 Epoch: 0064 log_lik= 0.4515198 train_kl= -0.01215 train_loss= 0.46367 train_acc= 0.52324 val_roc= 0.91461 val_ap= 0.92567 time= 0.07668\n",
      "训练次数: 5 Epoch: 0065 log_lik= 0.4504753 train_kl= -0.01208 train_loss= 0.46256 train_acc= 0.52360 val_roc= 0.91505 val_ap= 0.92608 time= 0.07667\n",
      "训练次数: 5 Epoch: 0066 log_lik= 0.44988927 train_kl= -0.01201 train_loss= 0.46190 train_acc= 0.52417 val_roc= 0.91495 val_ap= 0.92592 time= 0.07468\n",
      "训练次数: 5 Epoch: 0067 log_lik= 0.44982427 train_kl= -0.01194 train_loss= 0.46176 train_acc= 0.52484 val_roc= 0.91532 val_ap= 0.92625 time= 0.07667\n",
      "训练次数: 5 Epoch: 0068 log_lik= 0.44860747 train_kl= -0.01187 train_loss= 0.46048 train_acc= 0.52446 val_roc= 0.91566 val_ap= 0.92631 time= 0.07966\n",
      "训练次数: 5 Epoch: 0069 log_lik= 0.4486227 train_kl= -0.01181 train_loss= 0.46043 train_acc= 0.52498 val_roc= 0.91571 val_ap= 0.92625 time= 0.07369\n",
      "训练次数: 5 Epoch: 0070 log_lik= 0.4478976 train_kl= -0.01176 train_loss= 0.45966 train_acc= 0.52545 val_roc= 0.91576 val_ap= 0.92606 time= 0.07469\n",
      "训练次数: 5 Epoch: 0071 log_lik= 0.44702062 train_kl= -0.01171 train_loss= 0.45873 train_acc= 0.52660 val_roc= 0.91649 val_ap= 0.92686 time= 0.07491\n",
      "训练次数: 5 Epoch: 0072 log_lik= 0.44618037 train_kl= -0.01167 train_loss= 0.45785 train_acc= 0.52662 val_roc= 0.91651 val_ap= 0.92670 time= 0.08265\n",
      "训练次数: 5 Epoch: 0073 log_lik= 0.4461867 train_kl= -0.01163 train_loss= 0.45781 train_acc= 0.52710 val_roc= 0.91673 val_ap= 0.92712 time= 0.07767\n",
      "训练次数: 5 Epoch: 0074 log_lik= 0.44521368 train_kl= -0.01160 train_loss= 0.45681 train_acc= 0.52717 val_roc= 0.91680 val_ap= 0.92696 time= 0.07568\n",
      "训练次数: 5 Epoch: 0075 log_lik= 0.4451721 train_kl= -0.01157 train_loss= 0.45675 train_acc= 0.52799 val_roc= 0.91680 val_ap= 0.92704 time= 0.07568\n",
      "训练次数: 5 Epoch: 0076 log_lik= 0.44476697 train_kl= -0.01155 train_loss= 0.45632 train_acc= 0.52841 val_roc= 0.91700 val_ap= 0.92722 time= 0.07769\n",
      "训练次数: 5 Epoch: 0077 log_lik= 0.444586 train_kl= -0.01153 train_loss= 0.45611 train_acc= 0.52857 val_roc= 0.91761 val_ap= 0.92793 time= 0.07468\n",
      "训练次数: 5 Epoch: 0078 log_lik= 0.44404888 train_kl= -0.01151 train_loss= 0.45556 train_acc= 0.52942 val_roc= 0.91749 val_ap= 0.92810 time= 0.07468\n",
      "训练次数: 5 Epoch: 0079 log_lik= 0.44364908 train_kl= -0.01150 train_loss= 0.45515 train_acc= 0.52935 val_roc= 0.91769 val_ap= 0.92827 time= 0.07874\n",
      "训练次数: 5 Epoch: 0080 log_lik= 0.44317907 train_kl= -0.01150 train_loss= 0.45468 train_acc= 0.52969 val_roc= 0.91788 val_ap= 0.92858 time= 0.08265\n",
      "训练次数: 5 Epoch: 0081 log_lik= 0.44251537 train_kl= -0.01150 train_loss= 0.45402 train_acc= 0.53026 val_roc= 0.91804 val_ap= 0.92880 time= 0.07468\n",
      "训练次数: 5 Epoch: 0082 log_lik= 0.44308144 train_kl= -0.01150 train_loss= 0.45458 train_acc= 0.53140 val_roc= 0.91810 val_ap= 0.92897 time= 0.07568\n",
      "训练次数: 5 Epoch: 0083 log_lik= 0.4420364 train_kl= -0.01150 train_loss= 0.45354 train_acc= 0.53057 val_roc= 0.91858 val_ap= 0.92930 time= 0.07368\n",
      "训练次数: 5 Epoch: 0084 log_lik= 0.44130972 train_kl= -0.01150 train_loss= 0.45281 train_acc= 0.53154 val_roc= 0.91846 val_ap= 0.92945 time= 0.07269\n",
      "训练次数: 5 Epoch: 0085 log_lik= 0.44114017 train_kl= -0.01151 train_loss= 0.45265 train_acc= 0.53227 val_roc= 0.91807 val_ap= 0.92929 time= 0.07668\n",
      "训练次数: 5 Epoch: 0086 log_lik= 0.44067112 train_kl= -0.01151 train_loss= 0.45218 train_acc= 0.53188 val_roc= 0.91781 val_ap= 0.92857 time= 0.07369\n",
      "训练次数: 5 Epoch: 0087 log_lik= 0.4404024 train_kl= -0.01152 train_loss= 0.45192 train_acc= 0.53282 val_roc= 0.91829 val_ap= 0.92885 time= 0.07369\n",
      "训练次数: 5 Epoch: 0088 log_lik= 0.43980128 train_kl= -0.01153 train_loss= 0.45133 train_acc= 0.53212 val_roc= 0.91889 val_ap= 0.92960 time= 0.08464\n",
      "训练次数: 5 Epoch: 0089 log_lik= 0.43961263 train_kl= -0.01153 train_loss= 0.45114 train_acc= 0.53290 val_roc= 0.91885 val_ap= 0.92997 time= 0.07468\n",
      "训练次数: 5 Epoch: 0090 log_lik= 0.43939212 train_kl= -0.01154 train_loss= 0.45093 train_acc= 0.53258 val_roc= 0.91853 val_ap= 0.92943 time= 0.07468\n",
      "训练次数: 5 Epoch: 0091 log_lik= 0.43923035 train_kl= -0.01154 train_loss= 0.45077 train_acc= 0.53298 val_roc= 0.91803 val_ap= 0.92843 time= 0.07572\n",
      "训练次数: 5 Epoch: 0092 log_lik= 0.4383871 train_kl= -0.01155 train_loss= 0.44994 train_acc= 0.53315 val_roc= 0.91824 val_ap= 0.92852 time= 0.07567\n",
      "训练次数: 5 Epoch: 0093 log_lik= 0.43840966 train_kl= -0.01156 train_loss= 0.44997 train_acc= 0.53374 val_roc= 0.91859 val_ap= 0.92892 time= 0.07668\n",
      "训练次数: 5 Epoch: 0094 log_lik= 0.43858477 train_kl= -0.01156 train_loss= 0.45015 train_acc= 0.53307 val_roc= 0.91871 val_ap= 0.92892 time= 0.07269\n",
      "训练次数: 5 Epoch: 0095 log_lik= 0.4378338 train_kl= -0.01157 train_loss= 0.44941 train_acc= 0.53225 val_roc= 0.91849 val_ap= 0.92880 time= 0.07369\n",
      "训练次数: 5 Epoch: 0096 log_lik= 0.4375578 train_kl= -0.01158 train_loss= 0.44914 train_acc= 0.53360 val_roc= 0.91852 val_ap= 0.92824 time= 0.07369\n",
      "训练次数: 5 Epoch: 0097 log_lik= 0.43694723 train_kl= -0.01159 train_loss= 0.44854 train_acc= 0.53365 val_roc= 0.91924 val_ap= 0.92842 time= 0.07468\n",
      "训练次数: 5 Epoch: 0098 log_lik= 0.43700415 train_kl= -0.01161 train_loss= 0.44861 train_acc= 0.53368 val_roc= 0.91868 val_ap= 0.92807 time= 0.07667\n",
      "训练次数: 5 Epoch: 0099 log_lik= 0.43684974 train_kl= -0.01162 train_loss= 0.44847 train_acc= 0.53439 val_roc= 0.91846 val_ap= 0.92776 time= 0.07468\n",
      "训练次数: 5 Epoch: 0100 log_lik= 0.43689728 train_kl= -0.01164 train_loss= 0.44853 train_acc= 0.53338 val_roc= 0.91905 val_ap= 0.92807 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 5 ROC score: 0.9278937381404175\n",
      "训练次数: 5 AP score: 0.9387125923791326\n",
      "训练次数: 6 Epoch: 0001 log_lik= 1.7877698 train_kl= -0.00004 train_loss= 1.78781 train_acc= 0.49689 val_roc= 0.69470 val_ap= 0.71212 time= 1.45681\n",
      "训练次数: 6 Epoch: 0002 log_lik= 1.5409502 train_kl= -0.00019 train_loss= 1.54114 train_acc= 0.47153 val_roc= 0.68619 val_ap= 0.71037 time= 0.08559\n",
      "训练次数: 6 Epoch: 0003 log_lik= 1.4044322 train_kl= -0.00052 train_loss= 1.40495 train_acc= 0.41522 val_roc= 0.68882 val_ap= 0.71618 time= 0.09062\n",
      "训练次数: 6 Epoch: 0004 log_lik= 1.2723489 train_kl= -0.00084 train_loss= 1.27319 train_acc= 0.37946 val_roc= 0.69559 val_ap= 0.72266 time= 0.07667\n",
      "训练次数: 6 Epoch: 0005 log_lik= 1.1852356 train_kl= -0.00116 train_loss= 1.18640 train_acc= 0.36772 val_roc= 0.70806 val_ap= 0.73351 time= 0.07468\n",
      "训练次数: 6 Epoch: 0006 log_lik= 1.0831664 train_kl= -0.00153 train_loss= 1.08470 train_acc= 0.36870 val_roc= 0.72360 val_ap= 0.74645 time= 0.07568\n",
      "训练次数: 6 Epoch: 0007 log_lik= 0.9630296 train_kl= -0.00200 train_loss= 0.96503 train_acc= 0.35888 val_roc= 0.74701 val_ap= 0.76908 time= 0.08663\n",
      "训练次数: 6 Epoch: 0008 log_lik= 0.8785076 train_kl= -0.00260 train_loss= 0.88111 train_acc= 0.35369 val_roc= 0.77017 val_ap= 0.79158 time= 0.08165\n",
      "训练次数: 6 Epoch: 0009 log_lik= 0.79025346 train_kl= -0.00334 train_loss= 0.79359 train_acc= 0.34573 val_roc= 0.78911 val_ap= 0.80804 time= 0.07468\n",
      "训练次数: 6 Epoch: 0010 log_lik= 0.73480016 train_kl= -0.00418 train_loss= 0.73898 train_acc= 0.32685 val_roc= 0.81364 val_ap= 0.82649 time= 0.07668\n",
      "训练次数: 6 Epoch: 0011 log_lik= 0.6957098 train_kl= -0.00507 train_loss= 0.70078 train_acc= 0.32214 val_roc= 0.84558 val_ap= 0.85206 time= 0.07468\n",
      "训练次数: 6 Epoch: 0012 log_lik= 0.6699628 train_kl= -0.00594 train_loss= 0.67590 train_acc= 0.31485 val_roc= 0.86871 val_ap= 0.87142 time= 0.07369\n",
      "训练次数: 6 Epoch: 0013 log_lik= 0.6456964 train_kl= -0.00679 train_loss= 0.65248 train_acc= 0.32366 val_roc= 0.87606 val_ap= 0.87459 time= 0.07368\n",
      "训练次数: 6 Epoch: 0014 log_lik= 0.6263725 train_kl= -0.00760 train_loss= 0.63397 train_acc= 0.31856 val_roc= 0.87674 val_ap= 0.87210 time= 0.07369\n",
      "训练次数: 6 Epoch: 0015 log_lik= 0.6063138 train_kl= -0.00836 train_loss= 0.61467 train_acc= 0.34104 val_roc= 0.87374 val_ap= 0.86995 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0016 log_lik= 0.58647215 train_kl= -0.00908 train_loss= 0.59555 train_acc= 0.37311 val_roc= 0.87409 val_ap= 0.87061 time= 0.07568\n",
      "训练次数: 6 Epoch: 0017 log_lik= 0.5698451 train_kl= -0.00975 train_loss= 0.57960 train_acc= 0.40496 val_roc= 0.88087 val_ap= 0.87721 time= 0.07468\n",
      "训练次数: 6 Epoch: 0018 log_lik= 0.56102365 train_kl= -0.01037 train_loss= 0.57140 train_acc= 0.43237 val_roc= 0.88592 val_ap= 0.88160 time= 0.07468\n",
      "训练次数: 6 Epoch: 0019 log_lik= 0.55959 train_kl= -0.01091 train_loss= 0.57050 train_acc= 0.44649 val_roc= 0.88797 val_ap= 0.88404 time= 0.07767\n",
      "训练次数: 6 Epoch: 0020 log_lik= 0.5526525 train_kl= -0.01134 train_loss= 0.56399 train_acc= 0.46903 val_roc= 0.88819 val_ap= 0.88519 time= 0.07568\n",
      "训练次数: 6 Epoch: 0021 log_lik= 0.54390526 train_kl= -0.01170 train_loss= 0.55560 train_acc= 0.48614 val_roc= 0.89322 val_ap= 0.89038 time= 0.07369\n",
      "训练次数: 6 Epoch: 0022 log_lik= 0.5385062 train_kl= -0.01200 train_loss= 0.55051 train_acc= 0.49247 val_roc= 0.90133 val_ap= 0.89911 time= 0.07667\n",
      "训练次数: 6 Epoch: 0023 log_lik= 0.5357212 train_kl= -0.01226 train_loss= 0.54798 train_acc= 0.49134 val_roc= 0.90679 val_ap= 0.90609 time= 0.07567\n",
      "训练次数: 6 Epoch: 0024 log_lik= 0.53235304 train_kl= -0.01247 train_loss= 0.54482 train_acc= 0.48460 val_roc= 0.91047 val_ap= 0.91156 time= 0.07369\n",
      "训练次数: 6 Epoch: 0025 log_lik= 0.5229443 train_kl= -0.01262 train_loss= 0.53556 train_acc= 0.48443 val_roc= 0.91366 val_ap= 0.91588 time= 0.07568\n",
      "训练次数: 6 Epoch: 0026 log_lik= 0.5140221 train_kl= -0.01270 train_loss= 0.52672 train_acc= 0.48916 val_roc= 0.91800 val_ap= 0.92099 time= 0.07469\n",
      "训练次数: 6 Epoch: 0027 log_lik= 0.50334233 train_kl= -0.01271 train_loss= 0.51605 train_acc= 0.49879 val_roc= 0.92132 val_ap= 0.92548 time= 0.07468\n",
      "训练次数: 6 Epoch: 0028 log_lik= 0.49377957 train_kl= -0.01268 train_loss= 0.50646 train_acc= 0.51211 val_roc= 0.92242 val_ap= 0.92798 time= 0.07668\n",
      "训练次数: 6 Epoch: 0029 log_lik= 0.48894098 train_kl= -0.01266 train_loss= 0.50160 train_acc= 0.51993 val_roc= 0.92200 val_ap= 0.92854 time= 0.07568\n",
      "训练次数: 6 Epoch: 0030 log_lik= 0.4889544 train_kl= -0.01264 train_loss= 0.50160 train_acc= 0.51752 val_roc= 0.92092 val_ap= 0.92812 time= 0.07468\n",
      "训练次数: 6 Epoch: 0031 log_lik= 0.49108592 train_kl= -0.01265 train_loss= 0.50374 train_acc= 0.50700 val_roc= 0.92046 val_ap= 0.92846 time= 0.08165\n",
      "训练次数: 6 Epoch: 0032 log_lik= 0.49203095 train_kl= -0.01267 train_loss= 0.50470 train_acc= 0.50514 val_roc= 0.91969 val_ap= 0.92848 time= 0.07369\n",
      "训练次数: 6 Epoch: 0033 log_lik= 0.48955193 train_kl= -0.01267 train_loss= 0.50222 train_acc= 0.50644 val_roc= 0.91923 val_ap= 0.92822 time= 0.08365\n",
      "训练次数: 6 Epoch: 0034 log_lik= 0.48385787 train_kl= -0.01265 train_loss= 0.49651 train_acc= 0.51537 val_roc= 0.91871 val_ap= 0.92784 time= 0.07468\n",
      "训练次数: 6 Epoch: 0035 log_lik= 0.47828642 train_kl= -0.01265 train_loss= 0.49094 train_acc= 0.52209 val_roc= 0.91814 val_ap= 0.92751 time= 0.08165\n",
      "训练次数: 6 Epoch: 0036 log_lik= 0.47420496 train_kl= -0.01266 train_loss= 0.48687 train_acc= 0.52635 val_roc= 0.91897 val_ap= 0.92828 time= 0.08165\n",
      "训练次数: 6 Epoch: 0037 log_lik= 0.4721744 train_kl= -0.01269 train_loss= 0.48486 train_acc= 0.52748 val_roc= 0.92044 val_ap= 0.92931 time= 0.07667\n",
      "训练次数: 6 Epoch: 0038 log_lik= 0.47258285 train_kl= -0.01271 train_loss= 0.48529 train_acc= 0.52458 val_roc= 0.92199 val_ap= 0.93065 time= 0.07568\n",
      "训练次数: 6 Epoch: 0039 log_lik= 0.4717154 train_kl= -0.01271 train_loss= 0.48442 train_acc= 0.52432 val_roc= 0.92299 val_ap= 0.93142 time= 0.09123\n",
      "训练次数: 6 Epoch: 0040 log_lik= 0.46960506 train_kl= -0.01268 train_loss= 0.48228 train_acc= 0.52509 val_roc= 0.92354 val_ap= 0.93203 time= 0.07867\n",
      "训练次数: 6 Epoch: 0041 log_lik= 0.46884206 train_kl= -0.01263 train_loss= 0.48147 train_acc= 0.52711 val_roc= 0.92417 val_ap= 0.93241 time= 0.07568\n",
      "训练次数: 6 Epoch: 0042 log_lik= 0.46600822 train_kl= -0.01258 train_loss= 0.47858 train_acc= 0.52999 val_roc= 0.92475 val_ap= 0.93264 time= 0.07369\n",
      "训练次数: 6 Epoch: 0043 log_lik= 0.46510574 train_kl= -0.01252 train_loss= 0.47762 train_acc= 0.53183 val_roc= 0.92515 val_ap= 0.93307 time= 0.07468\n",
      "训练次数: 6 Epoch: 0044 log_lik= 0.46347213 train_kl= -0.01245 train_loss= 0.47592 train_acc= 0.53228 val_roc= 0.92569 val_ap= 0.93341 time= 0.07468\n",
      "训练次数: 6 Epoch: 0045 log_lik= 0.4627287 train_kl= -0.01238 train_loss= 0.47511 train_acc= 0.53272 val_roc= 0.92628 val_ap= 0.93429 time= 0.07866\n",
      "训练次数: 6 Epoch: 0046 log_lik= 0.46150362 train_kl= -0.01231 train_loss= 0.47381 train_acc= 0.53204 val_roc= 0.92635 val_ap= 0.93435 time= 0.07667\n",
      "训练次数: 6 Epoch: 0047 log_lik= 0.46005493 train_kl= -0.01224 train_loss= 0.47229 train_acc= 0.53114 val_roc= 0.92654 val_ap= 0.93486 time= 0.07568\n",
      "训练次数: 6 Epoch: 0048 log_lik= 0.45899627 train_kl= -0.01216 train_loss= 0.47116 train_acc= 0.53223 val_roc= 0.92712 val_ap= 0.93545 time= 0.08165\n",
      "训练次数: 6 Epoch: 0049 log_lik= 0.4576063 train_kl= -0.01209 train_loss= 0.46970 train_acc= 0.53317 val_roc= 0.92724 val_ap= 0.93605 time= 0.07468\n",
      "训练次数: 6 Epoch: 0050 log_lik= 0.45688444 train_kl= -0.01201 train_loss= 0.46890 train_acc= 0.53403 val_roc= 0.92698 val_ap= 0.93593 time= 0.07369\n",
      "训练次数: 6 Epoch: 0051 log_lik= 0.45497602 train_kl= -0.01193 train_loss= 0.46691 train_acc= 0.53654 val_roc= 0.92672 val_ap= 0.93568 time= 0.07369\n",
      "训练次数: 6 Epoch: 0052 log_lik= 0.45471504 train_kl= -0.01185 train_loss= 0.46657 train_acc= 0.53807 val_roc= 0.92647 val_ap= 0.93569 time= 0.07668\n",
      "训练次数: 6 Epoch: 0053 log_lik= 0.4542463 train_kl= -0.01179 train_loss= 0.46603 train_acc= 0.53675 val_roc= 0.92643 val_ap= 0.93564 time= 0.07468\n",
      "训练次数: 6 Epoch: 0054 log_lik= 0.45419133 train_kl= -0.01174 train_loss= 0.46593 train_acc= 0.53628 val_roc= 0.92651 val_ap= 0.93574 time= 0.07369\n",
      "训练次数: 6 Epoch: 0055 log_lik= 0.45382783 train_kl= -0.01170 train_loss= 0.46553 train_acc= 0.53715 val_roc= 0.92680 val_ap= 0.93590 time= 0.07667\n",
      "训练次数: 6 Epoch: 0056 log_lik= 0.45236254 train_kl= -0.01167 train_loss= 0.46404 train_acc= 0.53680 val_roc= 0.92702 val_ap= 0.93628 time= 0.07369\n",
      "训练次数: 6 Epoch: 0057 log_lik= 0.45102105 train_kl= -0.01166 train_loss= 0.46268 train_acc= 0.53965 val_roc= 0.92734 val_ap= 0.93668 time= 0.07369\n",
      "训练次数: 6 Epoch: 0058 log_lik= 0.45047858 train_kl= -0.01165 train_loss= 0.46213 train_acc= 0.53951 val_roc= 0.92735 val_ap= 0.93713 time= 0.07925\n",
      "训练次数: 6 Epoch: 0059 log_lik= 0.45015103 train_kl= -0.01164 train_loss= 0.46179 train_acc= 0.53969 val_roc= 0.92747 val_ap= 0.93732 time= 0.07668\n",
      "训练次数: 6 Epoch: 0060 log_lik= 0.44971114 train_kl= -0.01163 train_loss= 0.46134 train_acc= 0.53945 val_roc= 0.92748 val_ap= 0.93756 time= 0.08165\n",
      "训练次数: 6 Epoch: 0061 log_lik= 0.44899178 train_kl= -0.01162 train_loss= 0.46061 train_acc= 0.53881 val_roc= 0.92776 val_ap= 0.93805 time= 0.07568\n",
      "训练次数: 6 Epoch: 0062 log_lik= 0.44904482 train_kl= -0.01160 train_loss= 0.46065 train_acc= 0.53930 val_roc= 0.92825 val_ap= 0.93894 time= 0.07568\n",
      "训练次数: 6 Epoch: 0063 log_lik= 0.4480753 train_kl= -0.01159 train_loss= 0.45966 train_acc= 0.54011 val_roc= 0.92818 val_ap= 0.93949 time= 0.07468\n",
      "训练次数: 6 Epoch: 0064 log_lik= 0.4474262 train_kl= -0.01158 train_loss= 0.45900 train_acc= 0.54152 val_roc= 0.92848 val_ap= 0.93991 time= 0.07468\n",
      "训练次数: 6 Epoch: 0065 log_lik= 0.44669813 train_kl= -0.01157 train_loss= 0.45826 train_acc= 0.54087 val_roc= 0.92867 val_ap= 0.94012 time= 0.07468\n",
      "训练次数: 6 Epoch: 0066 log_lik= 0.44666278 train_kl= -0.01155 train_loss= 0.45821 train_acc= 0.54042 val_roc= 0.92893 val_ap= 0.94073 time= 0.07369\n",
      "训练次数: 6 Epoch: 0067 log_lik= 0.4464778 train_kl= -0.01154 train_loss= 0.45802 train_acc= 0.53987 val_roc= 0.92870 val_ap= 0.94094 time= 0.07468\n",
      "训练次数: 6 Epoch: 0068 log_lik= 0.4456202 train_kl= -0.01153 train_loss= 0.45715 train_acc= 0.54133 val_roc= 0.92858 val_ap= 0.94124 time= 0.07369\n",
      "训练次数: 6 Epoch: 0069 log_lik= 0.44534966 train_kl= -0.01153 train_loss= 0.45688 train_acc= 0.54132 val_roc= 0.92917 val_ap= 0.94167 time= 0.07468\n",
      "训练次数: 6 Epoch: 0070 log_lik= 0.44449458 train_kl= -0.01152 train_loss= 0.45602 train_acc= 0.54087 val_roc= 0.92935 val_ap= 0.94160 time= 0.07468\n",
      "训练次数: 6 Epoch: 0071 log_lik= 0.44457185 train_kl= -0.01152 train_loss= 0.45610 train_acc= 0.54058 val_roc= 0.92936 val_ap= 0.94202 time= 0.07568\n",
      "训练次数: 6 Epoch: 0072 log_lik= 0.44382656 train_kl= -0.01151 train_loss= 0.45534 train_acc= 0.54124 val_roc= 0.92946 val_ap= 0.94184 time= 0.07667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0073 log_lik= 0.44366577 train_kl= -0.01151 train_loss= 0.45518 train_acc= 0.54134 val_roc= 0.92935 val_ap= 0.94172 time= 0.07668\n",
      "训练次数: 6 Epoch: 0074 log_lik= 0.44300324 train_kl= -0.01151 train_loss= 0.45451 train_acc= 0.54136 val_roc= 0.92967 val_ap= 0.94194 time= 0.07668\n",
      "训练次数: 6 Epoch: 0075 log_lik= 0.4426089 train_kl= -0.01151 train_loss= 0.45412 train_acc= 0.54165 val_roc= 0.93014 val_ap= 0.94224 time= 0.07369\n",
      "训练次数: 6 Epoch: 0076 log_lik= 0.4424126 train_kl= -0.01152 train_loss= 0.45393 train_acc= 0.54201 val_roc= 0.93006 val_ap= 0.94209 time= 0.07471\n",
      "训练次数: 6 Epoch: 0077 log_lik= 0.44207573 train_kl= -0.01152 train_loss= 0.45360 train_acc= 0.54174 val_roc= 0.93037 val_ap= 0.94223 time= 0.08364\n",
      "训练次数: 6 Epoch: 0078 log_lik= 0.44187728 train_kl= -0.01153 train_loss= 0.45340 train_acc= 0.54252 val_roc= 0.93092 val_ap= 0.94232 time= 0.07568\n",
      "训练次数: 6 Epoch: 0079 log_lik= 0.4413929 train_kl= -0.01153 train_loss= 0.45292 train_acc= 0.54193 val_roc= 0.93137 val_ap= 0.94273 time= 0.07420\n",
      "训练次数: 6 Epoch: 0080 log_lik= 0.4412168 train_kl= -0.01152 train_loss= 0.45274 train_acc= 0.54148 val_roc= 0.93131 val_ap= 0.94272 time= 0.07468\n",
      "训练次数: 6 Epoch: 0081 log_lik= 0.4408124 train_kl= -0.01153 train_loss= 0.45234 train_acc= 0.54277 val_roc= 0.93140 val_ap= 0.94270 time= 0.08066\n",
      "训练次数: 6 Epoch: 0082 log_lik= 0.4402817 train_kl= -0.01154 train_loss= 0.45182 train_acc= 0.54342 val_roc= 0.93176 val_ap= 0.94316 time= 0.08364\n",
      "训练次数: 6 Epoch: 0083 log_lik= 0.439902 train_kl= -0.01155 train_loss= 0.45145 train_acc= 0.54355 val_roc= 0.93202 val_ap= 0.94337 time= 0.07767\n",
      "训练次数: 6 Epoch: 0084 log_lik= 0.43969014 train_kl= -0.01156 train_loss= 0.45125 train_acc= 0.54405 val_roc= 0.93217 val_ap= 0.94345 time= 0.07966\n",
      "训练次数: 6 Epoch: 0085 log_lik= 0.43978164 train_kl= -0.01155 train_loss= 0.45133 train_acc= 0.54323 val_roc= 0.93240 val_ap= 0.94384 time= 0.07767\n",
      "训练次数: 6 Epoch: 0086 log_lik= 0.43923664 train_kl= -0.01154 train_loss= 0.45078 train_acc= 0.54440 val_roc= 0.93254 val_ap= 0.94411 time= 0.08265\n",
      "训练次数: 6 Epoch: 0087 log_lik= 0.4389253 train_kl= -0.01155 train_loss= 0.45047 train_acc= 0.54380 val_roc= 0.93286 val_ap= 0.94452 time= 0.08663\n",
      "训练次数: 6 Epoch: 0088 log_lik= 0.43840477 train_kl= -0.01156 train_loss= 0.44997 train_acc= 0.54395 val_roc= 0.93289 val_ap= 0.94437 time= 0.07667\n",
      "训练次数: 6 Epoch: 0089 log_lik= 0.4383718 train_kl= -0.01158 train_loss= 0.44995 train_acc= 0.54426 val_roc= 0.93300 val_ap= 0.94437 time= 0.07369\n",
      "训练次数: 6 Epoch: 0090 log_lik= 0.43772635 train_kl= -0.01159 train_loss= 0.44931 train_acc= 0.54332 val_roc= 0.93342 val_ap= 0.94498 time= 0.08165\n",
      "训练次数: 6 Epoch: 0091 log_lik= 0.43693545 train_kl= -0.01159 train_loss= 0.44852 train_acc= 0.54419 val_roc= 0.93355 val_ap= 0.94508 time= 0.07866\n",
      "训练次数: 6 Epoch: 0092 log_lik= 0.4368534 train_kl= -0.01159 train_loss= 0.44845 train_acc= 0.54442 val_roc= 0.93412 val_ap= 0.94590 time= 0.07767\n",
      "训练次数: 6 Epoch: 0093 log_lik= 0.43729132 train_kl= -0.01160 train_loss= 0.44889 train_acc= 0.54486 val_roc= 0.93381 val_ap= 0.94546 time= 0.07591\n",
      "训练次数: 6 Epoch: 0094 log_lik= 0.43653986 train_kl= -0.01161 train_loss= 0.44815 train_acc= 0.54514 val_roc= 0.93390 val_ap= 0.94528 time= 0.07667\n",
      "训练次数: 6 Epoch: 0095 log_lik= 0.4359563 train_kl= -0.01162 train_loss= 0.44757 train_acc= 0.54420 val_roc= 0.93390 val_ap= 0.94511 time= 0.07369\n",
      "训练次数: 6 Epoch: 0096 log_lik= 0.4362709 train_kl= -0.01162 train_loss= 0.44789 train_acc= 0.54377 val_roc= 0.93409 val_ap= 0.94531 time= 0.07368\n",
      "训练次数: 6 Epoch: 0097 log_lik= 0.43581527 train_kl= -0.01163 train_loss= 0.44745 train_acc= 0.54513 val_roc= 0.93428 val_ap= 0.94553 time= 0.08564\n",
      "训练次数: 6 Epoch: 0098 log_lik= 0.43554538 train_kl= -0.01165 train_loss= 0.44720 train_acc= 0.54527 val_roc= 0.93428 val_ap= 0.94553 time= 0.08265\n",
      "训练次数: 6 Epoch: 0099 log_lik= 0.43514743 train_kl= -0.01167 train_loss= 0.44682 train_acc= 0.54570 val_roc= 0.93439 val_ap= 0.94563 time= 0.07568\n",
      "训练次数: 6 Epoch: 0100 log_lik= 0.435005 train_kl= -0.01169 train_loss= 0.44669 train_acc= 0.54393 val_roc= 0.93420 val_ap= 0.94561 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 6 ROC score: 0.9339644041493687\n",
      "训练次数: 6 AP score: 0.9253182149432306\n",
      "训练次数: 7 Epoch: 0001 log_lik= 1.7301579 train_kl= -0.00004 train_loss= 1.73020 train_acc= 0.49859 val_roc= 0.67955 val_ap= 0.72826 time= 1.48641\n",
      "训练次数: 7 Epoch: 0002 log_lik= 1.4953016 train_kl= -0.00020 train_loss= 1.49550 train_acc= 0.47780 val_roc= 0.68207 val_ap= 0.73151 time= 0.08758\n",
      "训练次数: 7 Epoch: 0003 log_lik= 1.3028237 train_kl= -0.00048 train_loss= 1.30330 train_acc= 0.45393 val_roc= 0.69687 val_ap= 0.74501 time= 0.07767\n",
      "训练次数: 7 Epoch: 0004 log_lik= 1.1301607 train_kl= -0.00083 train_loss= 1.13099 train_acc= 0.43702 val_roc= 0.71684 val_ap= 0.76369 time= 0.07468\n",
      "训练次数: 7 Epoch: 0005 log_lik= 0.9686115 train_kl= -0.00136 train_loss= 0.96997 train_acc= 0.43333 val_roc= 0.73795 val_ap= 0.78204 time= 0.07495\n",
      "训练次数: 7 Epoch: 0006 log_lik= 0.83098954 train_kl= -0.00212 train_loss= 0.83311 train_acc= 0.42142 val_roc= 0.77087 val_ap= 0.80703 time= 0.07468\n",
      "训练次数: 7 Epoch: 0007 log_lik= 0.7509679 train_kl= -0.00311 train_loss= 0.75408 train_acc= 0.40757 val_roc= 0.78742 val_ap= 0.82000 time= 0.08165\n",
      "训练次数: 7 Epoch: 0008 log_lik= 0.7157827 train_kl= -0.00428 train_loss= 0.72006 train_acc= 0.36742 val_roc= 0.77390 val_ap= 0.80121 time= 0.08664\n",
      "训练次数: 7 Epoch: 0009 log_lik= 0.69958514 train_kl= -0.00556 train_loss= 0.70514 train_acc= 0.25635 val_roc= 0.80235 val_ap= 0.81645 time= 0.07269\n",
      "训练次数: 7 Epoch: 0010 log_lik= 0.6754697 train_kl= -0.00680 train_loss= 0.68227 train_acc= 0.27595 val_roc= 0.82512 val_ap= 0.82524 time= 0.07369\n",
      "训练次数: 7 Epoch: 0011 log_lik= 0.6576464 train_kl= -0.00799 train_loss= 0.66563 train_acc= 0.33319 val_roc= 0.82640 val_ap= 0.82424 time= 0.08066\n",
      "训练次数: 7 Epoch: 0012 log_lik= 0.6417984 train_kl= -0.00913 train_loss= 0.65093 train_acc= 0.30609 val_roc= 0.82744 val_ap= 0.82623 time= 0.07468\n",
      "训练次数: 7 Epoch: 0013 log_lik= 0.62765205 train_kl= -0.01020 train_loss= 0.63785 train_acc= 0.29936 val_roc= 0.83076 val_ap= 0.82342 time= 0.07667\n",
      "训练次数: 7 Epoch: 0014 log_lik= 0.606327 train_kl= -0.01111 train_loss= 0.61744 train_acc= 0.38757 val_roc= 0.82796 val_ap= 0.81973 time= 0.07369\n",
      "训练次数: 7 Epoch: 0015 log_lik= 0.59573525 train_kl= -0.01195 train_loss= 0.60768 train_acc= 0.45007 val_roc= 0.82936 val_ap= 0.82484 time= 0.07369\n",
      "训练次数: 7 Epoch: 0016 log_lik= 0.5955088 train_kl= -0.01273 train_loss= 0.60823 train_acc= 0.45631 val_roc= 0.83329 val_ap= 0.82669 time= 0.07568\n",
      "训练次数: 7 Epoch: 0017 log_lik= 0.5932403 train_kl= -0.01342 train_loss= 0.60666 train_acc= 0.46076 val_roc= 0.84231 val_ap= 0.83716 time= 0.07369\n",
      "训练次数: 7 Epoch: 0018 log_lik= 0.58169657 train_kl= -0.01403 train_loss= 0.59572 train_acc= 0.47551 val_roc= 0.85320 val_ap= 0.85086 time= 0.07468\n",
      "训练次数: 7 Epoch: 0019 log_lik= 0.56635725 train_kl= -0.01457 train_loss= 0.58092 train_acc= 0.48319 val_roc= 0.86299 val_ap= 0.86051 time= 0.07369\n",
      "训练次数: 7 Epoch: 0020 log_lik= 0.55367637 train_kl= -0.01503 train_loss= 0.56871 train_acc= 0.48461 val_roc= 0.87142 val_ap= 0.87036 time= 0.07468\n",
      "训练次数: 7 Epoch: 0021 log_lik= 0.53872985 train_kl= -0.01542 train_loss= 0.55415 train_acc= 0.49674 val_roc= 0.88021 val_ap= 0.88304 time= 0.07966\n",
      "训练次数: 7 Epoch: 0022 log_lik= 0.52278364 train_kl= -0.01575 train_loss= 0.53853 train_acc= 0.50889 val_roc= 0.88366 val_ap= 0.88969 time= 0.07468\n",
      "训练次数: 7 Epoch: 0023 log_lik= 0.51559675 train_kl= -0.01602 train_loss= 0.53162 train_acc= 0.51801 val_roc= 0.88210 val_ap= 0.89123 time= 0.07677\n",
      "训练次数: 7 Epoch: 0024 log_lik= 0.5134482 train_kl= -0.01626 train_loss= 0.52970 train_acc= 0.51779 val_roc= 0.88209 val_ap= 0.89254 time= 0.07368\n",
      "训练次数: 7 Epoch: 0025 log_lik= 0.51016885 train_kl= -0.01647 train_loss= 0.52663 train_acc= 0.52223 val_roc= 0.88232 val_ap= 0.89211 time= 0.07468\n",
      "训练次数: 7 Epoch: 0026 log_lik= 0.50741094 train_kl= -0.01666 train_loss= 0.52407 train_acc= 0.52838 val_roc= 0.88084 val_ap= 0.89192 time= 0.08465\n",
      "训练次数: 7 Epoch: 0027 log_lik= 0.50413704 train_kl= -0.01683 train_loss= 0.52096 train_acc= 0.52951 val_roc= 0.88149 val_ap= 0.89315 time= 0.08365\n",
      "训练次数: 7 Epoch: 0028 log_lik= 0.5017498 train_kl= -0.01697 train_loss= 0.51872 train_acc= 0.52622 val_roc= 0.88386 val_ap= 0.89446 time= 0.07594\n",
      "训练次数: 7 Epoch: 0029 log_lik= 0.49725077 train_kl= -0.01708 train_loss= 0.51433 train_acc= 0.52758 val_roc= 0.88639 val_ap= 0.89682 time= 0.07767\n",
      "训练次数: 7 Epoch: 0030 log_lik= 0.49364436 train_kl= -0.01716 train_loss= 0.51081 train_acc= 0.52768 val_roc= 0.88754 val_ap= 0.89803 time= 0.07568\n",
      "训练次数: 7 Epoch: 0031 log_lik= 0.4892414 train_kl= -0.01722 train_loss= 0.50646 train_acc= 0.52928 val_roc= 0.88955 val_ap= 0.90030 time= 0.08166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0032 log_lik= 0.48463568 train_kl= -0.01724 train_loss= 0.50188 train_acc= 0.53337 val_roc= 0.89184 val_ap= 0.90151 time= 0.07667\n",
      "训练次数: 7 Epoch: 0033 log_lik= 0.48113 train_kl= -0.01726 train_loss= 0.49839 train_acc= 0.53453 val_roc= 0.89419 val_ap= 0.90383 time= 0.07568\n",
      "训练次数: 7 Epoch: 0034 log_lik= 0.47940752 train_kl= -0.01726 train_loss= 0.49667 train_acc= 0.53228 val_roc= 0.89552 val_ap= 0.90562 time= 0.07866\n",
      "训练次数: 7 Epoch: 0035 log_lik= 0.47650674 train_kl= -0.01725 train_loss= 0.49376 train_acc= 0.53396 val_roc= 0.89767 val_ap= 0.90846 time= 0.07369\n",
      "训练次数: 7 Epoch: 0036 log_lik= 0.47332123 train_kl= -0.01722 train_loss= 0.49055 train_acc= 0.53416 val_roc= 0.90056 val_ap= 0.91143 time= 0.07767\n",
      "训练次数: 7 Epoch: 0037 log_lik= 0.47118458 train_kl= -0.01719 train_loss= 0.48837 train_acc= 0.53080 val_roc= 0.90311 val_ap= 0.91393 time= 0.07667\n",
      "训练次数: 7 Epoch: 0038 log_lik= 0.47024623 train_kl= -0.01714 train_loss= 0.48738 train_acc= 0.52777 val_roc= 0.90413 val_ap= 0.91547 time= 0.07667\n",
      "训练次数: 7 Epoch: 0039 log_lik= 0.46858364 train_kl= -0.01705 train_loss= 0.48563 train_acc= 0.52901 val_roc= 0.90574 val_ap= 0.91779 time= 0.07468\n",
      "训练次数: 7 Epoch: 0040 log_lik= 0.46639156 train_kl= -0.01694 train_loss= 0.48333 train_acc= 0.53060 val_roc= 0.90665 val_ap= 0.91902 time= 0.07468\n",
      "训练次数: 7 Epoch: 0041 log_lik= 0.4653334 train_kl= -0.01683 train_loss= 0.48217 train_acc= 0.52947 val_roc= 0.90702 val_ap= 0.92016 time= 0.07521\n",
      "训练次数: 7 Epoch: 0042 log_lik= 0.46440434 train_kl= -0.01672 train_loss= 0.48113 train_acc= 0.52888 val_roc= 0.90757 val_ap= 0.92152 time= 0.07568\n",
      "训练次数: 7 Epoch: 0043 log_lik= 0.46222487 train_kl= -0.01660 train_loss= 0.47882 train_acc= 0.53218 val_roc= 0.90931 val_ap= 0.92354 time= 0.07468\n",
      "训练次数: 7 Epoch: 0044 log_lik= 0.4607438 train_kl= -0.01647 train_loss= 0.47721 train_acc= 0.53180 val_roc= 0.90989 val_ap= 0.92380 time= 0.07568\n",
      "训练次数: 7 Epoch: 0045 log_lik= 0.45961523 train_kl= -0.01633 train_loss= 0.47595 train_acc= 0.53142 val_roc= 0.90976 val_ap= 0.92403 time= 0.07668\n",
      "训练次数: 7 Epoch: 0046 log_lik= 0.45838076 train_kl= -0.01618 train_loss= 0.47456 train_acc= 0.53295 val_roc= 0.91013 val_ap= 0.92528 time= 0.07568\n",
      "训练次数: 7 Epoch: 0047 log_lik= 0.45737252 train_kl= -0.01602 train_loss= 0.47339 train_acc= 0.53447 val_roc= 0.90941 val_ap= 0.92454 time= 0.07667\n",
      "训练次数: 7 Epoch: 0048 log_lik= 0.45605376 train_kl= -0.01587 train_loss= 0.47192 train_acc= 0.53413 val_roc= 0.90892 val_ap= 0.92378 time= 0.07369\n",
      "训练次数: 7 Epoch: 0049 log_lik= 0.45552856 train_kl= -0.01572 train_loss= 0.47125 train_acc= 0.53527 val_roc= 0.90916 val_ap= 0.92455 time= 0.07369\n",
      "训练次数: 7 Epoch: 0050 log_lik= 0.45455787 train_kl= -0.01557 train_loss= 0.47013 train_acc= 0.53621 val_roc= 0.90901 val_ap= 0.92413 time= 0.07468\n",
      "训练次数: 7 Epoch: 0051 log_lik= 0.4535753 train_kl= -0.01543 train_loss= 0.46901 train_acc= 0.53603 val_roc= 0.90723 val_ap= 0.92176 time= 0.07667\n",
      "训练次数: 7 Epoch: 0052 log_lik= 0.45296472 train_kl= -0.01530 train_loss= 0.46826 train_acc= 0.53686 val_roc= 0.90756 val_ap= 0.92201 time= 0.07468\n",
      "训练次数: 7 Epoch: 0053 log_lik= 0.4523582 train_kl= -0.01515 train_loss= 0.46751 train_acc= 0.53842 val_roc= 0.90750 val_ap= 0.92181 time= 0.07568\n",
      "训练次数: 7 Epoch: 0054 log_lik= 0.4518339 train_kl= -0.01501 train_loss= 0.46684 train_acc= 0.53771 val_roc= 0.90674 val_ap= 0.91985 time= 0.07468\n",
      "训练次数: 7 Epoch: 0055 log_lik= 0.45054942 train_kl= -0.01488 train_loss= 0.46543 train_acc= 0.53856 val_roc= 0.90697 val_ap= 0.91949 time= 0.07369\n",
      "训练次数: 7 Epoch: 0056 log_lik= 0.45083123 train_kl= -0.01475 train_loss= 0.46558 train_acc= 0.53866 val_roc= 0.90752 val_ap= 0.92012 time= 0.07568\n",
      "训练次数: 7 Epoch: 0057 log_lik= 0.44994035 train_kl= -0.01462 train_loss= 0.46456 train_acc= 0.53804 val_roc= 0.90739 val_ap= 0.91948 time= 0.07667\n",
      "训练次数: 7 Epoch: 0058 log_lik= 0.44881377 train_kl= -0.01450 train_loss= 0.46332 train_acc= 0.53909 val_roc= 0.90646 val_ap= 0.91742 time= 0.07568\n",
      "训练次数: 7 Epoch: 0059 log_lik= 0.44836494 train_kl= -0.01439 train_loss= 0.46276 train_acc= 0.53853 val_roc= 0.90781 val_ap= 0.91840 time= 0.07767\n",
      "训练次数: 7 Epoch: 0060 log_lik= 0.44743583 train_kl= -0.01427 train_loss= 0.46171 train_acc= 0.53910 val_roc= 0.90824 val_ap= 0.91904 time= 0.08265\n",
      "训练次数: 7 Epoch: 0061 log_lik= 0.44750315 train_kl= -0.01415 train_loss= 0.46166 train_acc= 0.53741 val_roc= 0.90779 val_ap= 0.91813 time= 0.08166\n",
      "训练次数: 7 Epoch: 0062 log_lik= 0.4468361 train_kl= -0.01405 train_loss= 0.46089 train_acc= 0.53975 val_roc= 0.90849 val_ap= 0.91834 time= 0.07468\n",
      "训练次数: 7 Epoch: 0063 log_lik= 0.44638053 train_kl= -0.01395 train_loss= 0.46034 train_acc= 0.53873 val_roc= 0.90947 val_ap= 0.91948 time= 0.07468\n",
      "训练次数: 7 Epoch: 0064 log_lik= 0.4457237 train_kl= -0.01385 train_loss= 0.45957 train_acc= 0.53852 val_roc= 0.90905 val_ap= 0.91964 time= 0.07369\n",
      "训练次数: 7 Epoch: 0065 log_lik= 0.4452107 train_kl= -0.01376 train_loss= 0.45897 train_acc= 0.53817 val_roc= 0.90957 val_ap= 0.91954 time= 0.07468\n",
      "训练次数: 7 Epoch: 0066 log_lik= 0.44423026 train_kl= -0.01368 train_loss= 0.45791 train_acc= 0.53812 val_roc= 0.91073 val_ap= 0.92002 time= 0.08265\n",
      "训练次数: 7 Epoch: 0067 log_lik= 0.44384477 train_kl= -0.01360 train_loss= 0.45744 train_acc= 0.53726 val_roc= 0.91068 val_ap= 0.92046 time= 0.07568\n",
      "训练次数: 7 Epoch: 0068 log_lik= 0.44307968 train_kl= -0.01351 train_loss= 0.45659 train_acc= 0.53818 val_roc= 0.91035 val_ap= 0.92081 time= 0.07568\n",
      "训练次数: 7 Epoch: 0069 log_lik= 0.44271538 train_kl= -0.01343 train_loss= 0.45615 train_acc= 0.53774 val_roc= 0.91165 val_ap= 0.92188 time= 0.07369\n",
      "训练次数: 7 Epoch: 0070 log_lik= 0.44224486 train_kl= -0.01336 train_loss= 0.45560 train_acc= 0.53796 val_roc= 0.91217 val_ap= 0.92166 time= 0.07369\n",
      "训练次数: 7 Epoch: 0071 log_lik= 0.44207457 train_kl= -0.01328 train_loss= 0.45536 train_acc= 0.53722 val_roc= 0.91148 val_ap= 0.92200 time= 0.08265\n",
      "训练次数: 7 Epoch: 0072 log_lik= 0.44155318 train_kl= -0.01322 train_loss= 0.45477 train_acc= 0.53845 val_roc= 0.91252 val_ap= 0.92283 time= 0.08165\n",
      "训练次数: 7 Epoch: 0073 log_lik= 0.44074258 train_kl= -0.01315 train_loss= 0.45389 train_acc= 0.53772 val_roc= 0.91315 val_ap= 0.92326 time= 0.07966\n",
      "训练次数: 7 Epoch: 0074 log_lik= 0.44068566 train_kl= -0.01308 train_loss= 0.45377 train_acc= 0.53812 val_roc= 0.91265 val_ap= 0.92260 time= 0.07369\n",
      "训练次数: 7 Epoch: 0075 log_lik= 0.4399573 train_kl= -0.01303 train_loss= 0.45298 train_acc= 0.53937 val_roc= 0.91237 val_ap= 0.92257 time= 0.07568\n",
      "训练次数: 7 Epoch: 0076 log_lik= 0.43955436 train_kl= -0.01297 train_loss= 0.45252 train_acc= 0.53948 val_roc= 0.91337 val_ap= 0.92357 time= 0.07599\n",
      "训练次数: 7 Epoch: 0077 log_lik= 0.43900222 train_kl= -0.01291 train_loss= 0.45191 train_acc= 0.53946 val_roc= 0.91311 val_ap= 0.92297 time= 0.07940\n",
      "训练次数: 7 Epoch: 0078 log_lik= 0.43895528 train_kl= -0.01285 train_loss= 0.45181 train_acc= 0.53832 val_roc= 0.91206 val_ap= 0.92235 time= 0.07369\n",
      "训练次数: 7 Epoch: 0079 log_lik= 0.4385665 train_kl= -0.01280 train_loss= 0.45137 train_acc= 0.53890 val_roc= 0.91285 val_ap= 0.92261 time= 0.07568\n",
      "训练次数: 7 Epoch: 0080 log_lik= 0.43822178 train_kl= -0.01275 train_loss= 0.45098 train_acc= 0.53745 val_roc= 0.91250 val_ap= 0.92233 time= 0.07966\n",
      "训练次数: 7 Epoch: 0081 log_lik= 0.43791676 train_kl= -0.01271 train_loss= 0.45063 train_acc= 0.53821 val_roc= 0.91164 val_ap= 0.92191 time= 0.07568\n",
      "训练次数: 7 Epoch: 0082 log_lik= 0.43750533 train_kl= -0.01267 train_loss= 0.45017 train_acc= 0.54039 val_roc= 0.91232 val_ap= 0.92216 time= 0.08165\n",
      "训练次数: 7 Epoch: 0083 log_lik= 0.4373029 train_kl= -0.01262 train_loss= 0.44993 train_acc= 0.53953 val_roc= 0.91239 val_ap= 0.92213 time= 0.08165\n",
      "训练次数: 7 Epoch: 0084 log_lik= 0.43729466 train_kl= -0.01258 train_loss= 0.44988 train_acc= 0.53909 val_roc= 0.91180 val_ap= 0.92205 time= 0.07369\n",
      "训练次数: 7 Epoch: 0085 log_lik= 0.43666992 train_kl= -0.01256 train_loss= 0.44923 train_acc= 0.53988 val_roc= 0.91133 val_ap= 0.92192 time= 0.07568\n",
      "训练次数: 7 Epoch: 0086 log_lik= 0.43606475 train_kl= -0.01254 train_loss= 0.44861 train_acc= 0.54036 val_roc= 0.91172 val_ap= 0.92193 time= 0.08364\n",
      "训练次数: 7 Epoch: 0087 log_lik= 0.43627813 train_kl= -0.01251 train_loss= 0.44879 train_acc= 0.53930 val_roc= 0.91229 val_ap= 0.92207 time= 0.08265\n",
      "训练次数: 7 Epoch: 0088 log_lik= 0.43554395 train_kl= -0.01248 train_loss= 0.44803 train_acc= 0.53891 val_roc= 0.91143 val_ap= 0.92179 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0089 log_lik= 0.43566254 train_kl= -0.01247 train_loss= 0.44813 train_acc= 0.53926 val_roc= 0.91135 val_ap= 0.92193 time= 0.07767\n",
      "训练次数: 7 Epoch: 0090 log_lik= 0.43503943 train_kl= -0.01245 train_loss= 0.44749 train_acc= 0.53854 val_roc= 0.91240 val_ap= 0.92234 time= 0.07468\n",
      "训练次数: 7 Epoch: 0091 log_lik= 0.4351512 train_kl= -0.01243 train_loss= 0.44758 train_acc= 0.53804 val_roc= 0.91216 val_ap= 0.92209 time= 0.08265\n",
      "训练次数: 7 Epoch: 0092 log_lik= 0.43465468 train_kl= -0.01242 train_loss= 0.44708 train_acc= 0.53966 val_roc= 0.91156 val_ap= 0.92202 time= 0.09161\n",
      "训练次数: 7 Epoch: 0093 log_lik= 0.43424192 train_kl= -0.01241 train_loss= 0.44666 train_acc= 0.53899 val_roc= 0.91177 val_ap= 0.92187 time= 0.08479\n",
      "训练次数: 7 Epoch: 0094 log_lik= 0.43418244 train_kl= -0.01240 train_loss= 0.44658 train_acc= 0.53825 val_roc= 0.91271 val_ap= 0.92305 time= 0.08165\n",
      "训练次数: 7 Epoch: 0095 log_lik= 0.43365455 train_kl= -0.01239 train_loss= 0.44604 train_acc= 0.53939 val_roc= 0.91278 val_ap= 0.92305 time= 0.07468\n",
      "训练次数: 7 Epoch: 0096 log_lik= 0.4337374 train_kl= -0.01239 train_loss= 0.44613 train_acc= 0.53947 val_roc= 0.91249 val_ap= 0.92239 time= 0.07568\n",
      "训练次数: 7 Epoch: 0097 log_lik= 0.43302405 train_kl= -0.01238 train_loss= 0.44540 train_acc= 0.53835 val_roc= 0.91165 val_ap= 0.92223 time= 0.07468\n",
      "训练次数: 7 Epoch: 0098 log_lik= 0.43294862 train_kl= -0.01237 train_loss= 0.44532 train_acc= 0.53878 val_roc= 0.91232 val_ap= 0.92286 time= 0.07568\n",
      "训练次数: 7 Epoch: 0099 log_lik= 0.43251958 train_kl= -0.01238 train_loss= 0.44489 train_acc= 0.54001 val_roc= 0.91334 val_ap= 0.92361 time= 0.07866\n",
      "训练次数: 7 Epoch: 0100 log_lik= 0.43244025 train_kl= -0.01237 train_loss= 0.44481 train_acc= 0.54039 val_roc= 0.91239 val_ap= 0.92264 time= 0.07767\n",
      "Optimization Finished!\n",
      "训练次数: 7 ROC score: 0.9310838983325471\n",
      "训练次数: 7 AP score: 0.9384233198279291\n",
      "训练次数: 8 Epoch: 0001 log_lik= 1.7327377 train_kl= -0.00003 train_loss= 1.73277 train_acc= 0.49667 val_roc= 0.69239 val_ap= 0.70338 time= 1.48361\n",
      "训练次数: 8 Epoch: 0002 log_lik= 1.5180463 train_kl= -0.00011 train_loss= 1.51815 train_acc= 0.48966 val_roc= 0.67209 val_ap= 0.68893 time= 0.07861\n",
      "训练次数: 8 Epoch: 0003 log_lik= 1.2744086 train_kl= -0.00038 train_loss= 1.27479 train_acc= 0.45617 val_roc= 0.67026 val_ap= 0.68996 time= 0.07568\n",
      "训练次数: 8 Epoch: 0004 log_lik= 1.0621805 train_kl= -0.00092 train_loss= 1.06310 train_acc= 0.40088 val_roc= 0.68161 val_ap= 0.70140 time= 0.07468\n",
      "训练次数: 8 Epoch: 0005 log_lik= 0.9066399 train_kl= -0.00171 train_loss= 0.90835 train_acc= 0.34968 val_roc= 0.70517 val_ap= 0.72314 time= 0.07471\n",
      "训练次数: 8 Epoch: 0006 log_lik= 0.78324634 train_kl= -0.00270 train_loss= 0.78595 train_acc= 0.32476 val_roc= 0.73821 val_ap= 0.75113 time= 0.07667\n",
      "训练次数: 8 Epoch: 0007 log_lik= 0.71715814 train_kl= -0.00389 train_loss= 0.72105 train_acc= 0.31685 val_roc= 0.76657 val_ap= 0.77494 time= 0.07667\n",
      "训练次数: 8 Epoch: 0008 log_lik= 0.6957312 train_kl= -0.00520 train_loss= 0.70093 train_acc= 0.29973 val_roc= 0.77756 val_ap= 0.78260 time= 0.07667\n",
      "训练次数: 8 Epoch: 0009 log_lik= 0.6863219 train_kl= -0.00657 train_loss= 0.69289 train_acc= 0.20917 val_roc= 0.80799 val_ap= 0.81832 time= 0.07468\n",
      "训练次数: 8 Epoch: 0010 log_lik= 0.6684311 train_kl= -0.00788 train_loss= 0.67631 train_acc= 0.18986 val_roc= 0.83468 val_ap= 0.84207 time= 0.07568\n",
      "训练次数: 8 Epoch: 0011 log_lik= 0.6464725 train_kl= -0.00912 train_loss= 0.65559 train_acc= 0.21918 val_roc= 0.84597 val_ap= 0.84834 time= 0.07966\n",
      "训练次数: 8 Epoch: 0012 log_lik= 0.6186357 train_kl= -0.01030 train_loss= 0.62893 train_acc= 0.26974 val_roc= 0.85076 val_ap= 0.85312 time= 0.07369\n",
      "训练次数: 8 Epoch: 0013 log_lik= 0.59626186 train_kl= -0.01138 train_loss= 0.60764 train_acc= 0.31564 val_roc= 0.85086 val_ap= 0.85247 time= 0.07668\n",
      "训练次数: 8 Epoch: 0014 log_lik= 0.5715188 train_kl= -0.01235 train_loss= 0.58387 train_acc= 0.38637 val_roc= 0.85443 val_ap= 0.85295 time= 0.07368\n",
      "训练次数: 8 Epoch: 0015 log_lik= 0.5524976 train_kl= -0.01324 train_loss= 0.56573 train_acc= 0.44544 val_roc= 0.85622 val_ap= 0.85622 time= 0.07568\n",
      "训练次数: 8 Epoch: 0016 log_lik= 0.5371339 train_kl= -0.01401 train_loss= 0.55115 train_acc= 0.49232 val_roc= 0.85848 val_ap= 0.86068 time= 0.07867\n",
      "训练次数: 8 Epoch: 0017 log_lik= 0.5428965 train_kl= -0.01471 train_loss= 0.55760 train_acc= 0.48648 val_roc= 0.86532 val_ap= 0.86838 time= 0.07468\n",
      "训练次数: 8 Epoch: 0018 log_lik= 0.55275244 train_kl= -0.01528 train_loss= 0.56804 train_acc= 0.47005 val_roc= 0.87107 val_ap= 0.87523 time= 0.07369\n",
      "训练次数: 8 Epoch: 0019 log_lik= 0.5382716 train_kl= -0.01570 train_loss= 0.55397 train_acc= 0.48184 val_roc= 0.87454 val_ap= 0.87881 time= 0.07570\n",
      "训练次数: 8 Epoch: 0020 log_lik= 0.5201916 train_kl= -0.01602 train_loss= 0.53622 train_acc= 0.50055 val_roc= 0.87700 val_ap= 0.88094 time= 0.07568\n",
      "训练次数: 8 Epoch: 0021 log_lik= 0.5117315 train_kl= -0.01632 train_loss= 0.52805 train_acc= 0.50290 val_roc= 0.87974 val_ap= 0.88367 time= 0.07468\n",
      "训练次数: 8 Epoch: 0022 log_lik= 0.51029813 train_kl= -0.01656 train_loss= 0.52686 train_acc= 0.49652 val_roc= 0.88213 val_ap= 0.88525 time= 0.08216\n",
      "训练次数: 8 Epoch: 0023 log_lik= 0.5110058 train_kl= -0.01676 train_loss= 0.52777 train_acc= 0.48883 val_roc= 0.88438 val_ap= 0.88680 time= 0.07568\n",
      "训练次数: 8 Epoch: 0024 log_lik= 0.5053255 train_kl= -0.01689 train_loss= 0.52222 train_acc= 0.49185 val_roc= 0.88676 val_ap= 0.88884 time= 0.08066\n",
      "训练次数: 8 Epoch: 0025 log_lik= 0.49836522 train_kl= -0.01697 train_loss= 0.51534 train_acc= 0.49928 val_roc= 0.89017 val_ap= 0.89258 time= 0.07675\n",
      "训练次数: 8 Epoch: 0026 log_lik= 0.49216202 train_kl= -0.01701 train_loss= 0.50918 train_acc= 0.50611 val_roc= 0.89488 val_ap= 0.89945 time= 0.07469\n",
      "训练次数: 8 Epoch: 0027 log_lik= 0.48767483 train_kl= -0.01705 train_loss= 0.50473 train_acc= 0.50909 val_roc= 0.89839 val_ap= 0.90443 time= 0.07568\n",
      "训练次数: 8 Epoch: 0028 log_lik= 0.48712006 train_kl= -0.01710 train_loss= 0.50422 train_acc= 0.50445 val_roc= 0.90262 val_ap= 0.90890 time= 0.07369\n",
      "训练次数: 8 Epoch: 0029 log_lik= 0.4854159 train_kl= -0.01714 train_loss= 0.50255 train_acc= 0.49819 val_roc= 0.90643 val_ap= 0.91271 time= 0.07468\n",
      "训练次数: 8 Epoch: 0030 log_lik= 0.47940403 train_kl= -0.01717 train_loss= 0.49658 train_acc= 0.50356 val_roc= 0.90872 val_ap= 0.91485 time= 0.07468\n",
      "训练次数: 8 Epoch: 0031 log_lik= 0.47192377 train_kl= -0.01722 train_loss= 0.48914 train_acc= 0.51418 val_roc= 0.91039 val_ap= 0.91644 time= 0.07767\n",
      "训练次数: 8 Epoch: 0032 log_lik= 0.46756667 train_kl= -0.01727 train_loss= 0.48484 train_acc= 0.51970 val_roc= 0.91116 val_ap= 0.91659 time= 0.07468\n",
      "训练次数: 8 Epoch: 0033 log_lik= 0.46752548 train_kl= -0.01733 train_loss= 0.48486 train_acc= 0.51512 val_roc= 0.91226 val_ap= 0.91712 time= 0.08265\n",
      "训练次数: 8 Epoch: 0034 log_lik= 0.46769783 train_kl= -0.01736 train_loss= 0.48506 train_acc= 0.51251 val_roc= 0.91287 val_ap= 0.91710 time= 0.07369\n",
      "训练次数: 8 Epoch: 0035 log_lik= 0.4662604 train_kl= -0.01734 train_loss= 0.48360 train_acc= 0.51608 val_roc= 0.91411 val_ap= 0.91791 time= 0.07469\n",
      "训练次数: 8 Epoch: 0036 log_lik= 0.46350598 train_kl= -0.01730 train_loss= 0.48081 train_acc= 0.52163 val_roc= 0.91518 val_ap= 0.91876 time= 0.07369\n",
      "训练次数: 8 Epoch: 0037 log_lik= 0.4620185 train_kl= -0.01724 train_loss= 0.47926 train_acc= 0.52253 val_roc= 0.91616 val_ap= 0.91998 time= 0.08066\n",
      "训练次数: 8 Epoch: 0038 log_lik= 0.46194097 train_kl= -0.01716 train_loss= 0.47910 train_acc= 0.52116 val_roc= 0.91686 val_ap= 0.92065 time= 0.07522\n",
      "训练次数: 8 Epoch: 0039 log_lik= 0.4608132 train_kl= -0.01706 train_loss= 0.47787 train_acc= 0.52010 val_roc= 0.91726 val_ap= 0.92144 time= 0.07369\n",
      "训练次数: 8 Epoch: 0040 log_lik= 0.45863834 train_kl= -0.01694 train_loss= 0.47558 train_acc= 0.52192 val_roc= 0.91746 val_ap= 0.92136 time= 0.07369\n",
      "训练次数: 8 Epoch: 0041 log_lik= 0.45548078 train_kl= -0.01682 train_loss= 0.47230 train_acc= 0.52576 val_roc= 0.91846 val_ap= 0.92311 time= 0.07468\n",
      "训练次数: 8 Epoch: 0042 log_lik= 0.4533453 train_kl= -0.01669 train_loss= 0.47004 train_acc= 0.52737 val_roc= 0.91898 val_ap= 0.92417 time= 0.07468\n",
      "训练次数: 8 Epoch: 0043 log_lik= 0.45219976 train_kl= -0.01655 train_loss= 0.46875 train_acc= 0.52626 val_roc= 0.91931 val_ap= 0.92523 time= 0.07368\n",
      "训练次数: 8 Epoch: 0044 log_lik= 0.45185736 train_kl= -0.01640 train_loss= 0.46825 train_acc= 0.52581 val_roc= 0.91940 val_ap= 0.92589 time= 0.07468\n",
      "训练次数: 8 Epoch: 0045 log_lik= 0.45109913 train_kl= -0.01623 train_loss= 0.46733 train_acc= 0.52607 val_roc= 0.91885 val_ap= 0.92571 time= 0.07469\n",
      "训练次数: 8 Epoch: 0046 log_lik= 0.44995847 train_kl= -0.01607 train_loss= 0.46603 train_acc= 0.52757 val_roc= 0.91859 val_ap= 0.92572 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 8 Epoch: 0047 log_lik= 0.44829348 train_kl= -0.01592 train_loss= 0.46421 train_acc= 0.52777 val_roc= 0.91823 val_ap= 0.92551 time= 0.07667\n",
      "训练次数: 8 Epoch: 0048 log_lik= 0.44785684 train_kl= -0.01578 train_loss= 0.46364 train_acc= 0.52804 val_roc= 0.91832 val_ap= 0.92588 time= 0.07468\n",
      "训练次数: 8 Epoch: 0049 log_lik= 0.44759363 train_kl= -0.01566 train_loss= 0.46325 train_acc= 0.52613 val_roc= 0.91821 val_ap= 0.92593 time= 0.07369\n",
      "训练次数: 8 Epoch: 0050 log_lik= 0.44666293 train_kl= -0.01554 train_loss= 0.46220 train_acc= 0.52759 val_roc= 0.91917 val_ap= 0.92657 time= 0.08065\n",
      "训练次数: 8 Epoch: 0051 log_lik= 0.4447911 train_kl= -0.01543 train_loss= 0.46022 train_acc= 0.52898 val_roc= 0.91934 val_ap= 0.92638 time= 0.07468\n",
      "训练次数: 8 Epoch: 0052 log_lik= 0.44404614 train_kl= -0.01532 train_loss= 0.45937 train_acc= 0.53018 val_roc= 0.91967 val_ap= 0.92666 time= 0.07269\n",
      "训练次数: 8 Epoch: 0053 log_lik= 0.44323635 train_kl= -0.01522 train_loss= 0.45845 train_acc= 0.52956 val_roc= 0.91952 val_ap= 0.92623 time= 0.07568\n",
      "训练次数: 8 Epoch: 0054 log_lik= 0.44305158 train_kl= -0.01511 train_loss= 0.45816 train_acc= 0.52986 val_roc= 0.91981 val_ap= 0.92609 time= 0.07667\n",
      "训练次数: 8 Epoch: 0055 log_lik= 0.4419968 train_kl= -0.01499 train_loss= 0.45699 train_acc= 0.53113 val_roc= 0.92024 val_ap= 0.92664 time= 0.07468\n",
      "训练次数: 8 Epoch: 0056 log_lik= 0.4411629 train_kl= -0.01486 train_loss= 0.45603 train_acc= 0.53215 val_roc= 0.92021 val_ap= 0.92619 time= 0.07668\n",
      "训练次数: 8 Epoch: 0057 log_lik= 0.44057578 train_kl= -0.01473 train_loss= 0.45531 train_acc= 0.53187 val_roc= 0.91963 val_ap= 0.92479 time= 0.07469\n",
      "训练次数: 8 Epoch: 0058 log_lik= 0.44037256 train_kl= -0.01461 train_loss= 0.45498 train_acc= 0.53058 val_roc= 0.91988 val_ap= 0.92545 time= 0.07368\n",
      "训练次数: 8 Epoch: 0059 log_lik= 0.4392453 train_kl= -0.01448 train_loss= 0.45372 train_acc= 0.53279 val_roc= 0.91994 val_ap= 0.92638 time= 0.08265\n",
      "训练次数: 8 Epoch: 0060 log_lik= 0.43905124 train_kl= -0.01435 train_loss= 0.45340 train_acc= 0.53182 val_roc= 0.91892 val_ap= 0.92516 time= 0.07568\n",
      "训练次数: 8 Epoch: 0061 log_lik= 0.4381496 train_kl= -0.01423 train_loss= 0.45238 train_acc= 0.53265 val_roc= 0.91777 val_ap= 0.92389 time= 0.07369\n",
      "训练次数: 8 Epoch: 0062 log_lik= 0.43797985 train_kl= -0.01411 train_loss= 0.45209 train_acc= 0.53289 val_roc= 0.91769 val_ap= 0.92456 time= 0.07269\n",
      "训练次数: 8 Epoch: 0063 log_lik= 0.4376393 train_kl= -0.01399 train_loss= 0.45163 train_acc= 0.53301 val_roc= 0.91733 val_ap= 0.92462 time= 0.08066\n",
      "训练次数: 8 Epoch: 0064 log_lik= 0.4368573 train_kl= -0.01387 train_loss= 0.45073 train_acc= 0.53344 val_roc= 0.91688 val_ap= 0.92372 time= 0.07540\n",
      "训练次数: 8 Epoch: 0065 log_lik= 0.4368105 train_kl= -0.01376 train_loss= 0.45057 train_acc= 0.53385 val_roc= 0.91623 val_ap= 0.92308 time= 0.07368\n",
      "训练次数: 8 Epoch: 0066 log_lik= 0.4362486 train_kl= -0.01365 train_loss= 0.44990 train_acc= 0.53397 val_roc= 0.91541 val_ap= 0.92293 time= 0.07767\n",
      "训练次数: 8 Epoch: 0067 log_lik= 0.43619183 train_kl= -0.01355 train_loss= 0.44974 train_acc= 0.53395 val_roc= 0.91519 val_ap= 0.92238 time= 0.07569\n",
      "训练次数: 8 Epoch: 0068 log_lik= 0.43566105 train_kl= -0.01346 train_loss= 0.44912 train_acc= 0.53352 val_roc= 0.91521 val_ap= 0.92188 time= 0.08364\n",
      "训练次数: 8 Epoch: 0069 log_lik= 0.4359086 train_kl= -0.01338 train_loss= 0.44929 train_acc= 0.53388 val_roc= 0.91573 val_ap= 0.92299 time= 0.08564\n",
      "训练次数: 8 Epoch: 0070 log_lik= 0.43483853 train_kl= -0.01331 train_loss= 0.44814 train_acc= 0.53443 val_roc= 0.91473 val_ap= 0.92175 time= 0.07489\n",
      "训练次数: 8 Epoch: 0071 log_lik= 0.4346751 train_kl= -0.01324 train_loss= 0.44791 train_acc= 0.53418 val_roc= 0.91438 val_ap= 0.92087 time= 0.07667\n",
      "训练次数: 8 Epoch: 0072 log_lik= 0.4343858 train_kl= -0.01317 train_loss= 0.44756 train_acc= 0.53500 val_roc= 0.91498 val_ap= 0.92113 time= 0.07667\n",
      "训练次数: 8 Epoch: 0073 log_lik= 0.43428415 train_kl= -0.01310 train_loss= 0.44738 train_acc= 0.53423 val_roc= 0.91501 val_ap= 0.92141 time= 0.07568\n",
      "训练次数: 8 Epoch: 0074 log_lik= 0.4336402 train_kl= -0.01303 train_loss= 0.44667 train_acc= 0.53513 val_roc= 0.91557 val_ap= 0.92183 time= 0.07568\n",
      "训练次数: 8 Epoch: 0075 log_lik= 0.43352595 train_kl= -0.01297 train_loss= 0.44649 train_acc= 0.53456 val_roc= 0.91606 val_ap= 0.92146 time= 0.07568\n",
      "训练次数: 8 Epoch: 0076 log_lik= 0.4336416 train_kl= -0.01291 train_loss= 0.44655 train_acc= 0.53435 val_roc= 0.91622 val_ap= 0.92170 time= 0.09360\n",
      "训练次数: 8 Epoch: 0077 log_lik= 0.43274632 train_kl= -0.01285 train_loss= 0.44560 train_acc= 0.53549 val_roc= 0.91619 val_ap= 0.92220 time= 0.07568\n",
      "训练次数: 8 Epoch: 0078 log_lik= 0.43250838 train_kl= -0.01279 train_loss= 0.44530 train_acc= 0.53462 val_roc= 0.91645 val_ap= 0.92187 time= 0.07468\n",
      "训练次数: 8 Epoch: 0079 log_lik= 0.4324751 train_kl= -0.01275 train_loss= 0.44522 train_acc= 0.53542 val_roc= 0.91642 val_ap= 0.92076 time= 0.07468\n",
      "训练次数: 8 Epoch: 0080 log_lik= 0.43222186 train_kl= -0.01271 train_loss= 0.44493 train_acc= 0.53482 val_roc= 0.91658 val_ap= 0.92165 time= 0.07568\n",
      "训练次数: 8 Epoch: 0081 log_lik= 0.431745 train_kl= -0.01266 train_loss= 0.44441 train_acc= 0.53605 val_roc= 0.91674 val_ap= 0.92245 time= 0.08364\n",
      "训练次数: 8 Epoch: 0082 log_lik= 0.4316693 train_kl= -0.01261 train_loss= 0.44428 train_acc= 0.53493 val_roc= 0.91629 val_ap= 0.92152 time= 0.09062\n",
      "训练次数: 8 Epoch: 0083 log_lik= 0.43115452 train_kl= -0.01257 train_loss= 0.44373 train_acc= 0.53518 val_roc= 0.91628 val_ap= 0.92115 time= 0.07468\n",
      "训练次数: 8 Epoch: 0084 log_lik= 0.43127304 train_kl= -0.01255 train_loss= 0.44382 train_acc= 0.53500 val_roc= 0.91636 val_ap= 0.92140 time= 0.07881\n",
      "训练次数: 8 Epoch: 0085 log_lik= 0.43099022 train_kl= -0.01253 train_loss= 0.44352 train_acc= 0.53610 val_roc= 0.91657 val_ap= 0.92183 time= 0.07368\n",
      "训练次数: 8 Epoch: 0086 log_lik= 0.42994475 train_kl= -0.01250 train_loss= 0.44245 train_acc= 0.53517 val_roc= 0.91622 val_ap= 0.92096 time= 0.07468\n",
      "训练次数: 8 Epoch: 0087 log_lik= 0.43039605 train_kl= -0.01249 train_loss= 0.44289 train_acc= 0.53477 val_roc= 0.91612 val_ap= 0.92085 time= 0.07618\n",
      "训练次数: 8 Epoch: 0088 log_lik= 0.42998272 train_kl= -0.01249 train_loss= 0.44247 train_acc= 0.53610 val_roc= 0.91638 val_ap= 0.92128 time= 0.08265\n",
      "训练次数: 8 Epoch: 0089 log_lik= 0.4298651 train_kl= -0.01249 train_loss= 0.44235 train_acc= 0.53683 val_roc= 0.91603 val_ap= 0.92064 time= 0.07667\n",
      "训练次数: 8 Epoch: 0090 log_lik= 0.42931125 train_kl= -0.01247 train_loss= 0.44178 train_acc= 0.53680 val_roc= 0.91608 val_ap= 0.92078 time= 0.07468\n",
      "训练次数: 8 Epoch: 0091 log_lik= 0.4290793 train_kl= -0.01245 train_loss= 0.44153 train_acc= 0.53701 val_roc= 0.91629 val_ap= 0.92150 time= 0.07767\n",
      "训练次数: 8 Epoch: 0092 log_lik= 0.4287515 train_kl= -0.01245 train_loss= 0.44120 train_acc= 0.53699 val_roc= 0.91631 val_ap= 0.92107 time= 0.07369\n",
      "训练次数: 8 Epoch: 0093 log_lik= 0.42885756 train_kl= -0.01244 train_loss= 0.44130 train_acc= 0.53750 val_roc= 0.91636 val_ap= 0.92116 time= 0.07468\n",
      "训练次数: 8 Epoch: 0094 log_lik= 0.42865503 train_kl= -0.01244 train_loss= 0.44110 train_acc= 0.53678 val_roc= 0.91681 val_ap= 0.92221 time= 0.07767\n",
      "训练次数: 8 Epoch: 0095 log_lik= 0.42821857 train_kl= -0.01244 train_loss= 0.44066 train_acc= 0.53748 val_roc= 0.91671 val_ap= 0.92178 time= 0.08962\n",
      "训练次数: 8 Epoch: 0096 log_lik= 0.42816702 train_kl= -0.01244 train_loss= 0.44061 train_acc= 0.53789 val_roc= 0.91665 val_ap= 0.92119 time= 0.08265\n",
      "训练次数: 8 Epoch: 0097 log_lik= 0.42765722 train_kl= -0.01244 train_loss= 0.44010 train_acc= 0.53711 val_roc= 0.91719 val_ap= 0.92253 time= 0.07568\n",
      "训练次数: 8 Epoch: 0098 log_lik= 0.42712274 train_kl= -0.01243 train_loss= 0.43956 train_acc= 0.53697 val_roc= 0.91728 val_ap= 0.92275 time= 0.07468\n",
      "训练次数: 8 Epoch: 0099 log_lik= 0.4275151 train_kl= -0.01243 train_loss= 0.43995 train_acc= 0.53721 val_roc= 0.91706 val_ap= 0.92213 time= 0.07667\n",
      "训练次数: 8 Epoch: 0100 log_lik= 0.42670828 train_kl= -0.01243 train_loss= 0.43914 train_acc= 0.53766 val_roc= 0.91687 val_ap= 0.92187 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 8 ROC score: 0.9278091232820482\n",
      "训练次数: 8 AP score: 0.9400306637992207\n",
      "训练次数: 9 Epoch: 0001 log_lik= 1.7508315 train_kl= -0.00005 train_loss= 1.75088 train_acc= 0.49428 val_roc= 0.70793 val_ap= 0.73189 time= 1.46994\n",
      "训练次数: 9 Epoch: 0002 log_lik= 1.4282771 train_kl= -0.00019 train_loss= 1.42846 train_acc= 0.48137 val_roc= 0.68791 val_ap= 0.71571 time= 0.08161\n",
      "训练次数: 9 Epoch: 0003 log_lik= 1.2035844 train_kl= -0.00061 train_loss= 1.20419 train_acc= 0.42449 val_roc= 0.68461 val_ap= 0.71370 time= 0.08962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0004 log_lik= 1.0719904 train_kl= -0.00123 train_loss= 1.07322 train_acc= 0.34578 val_roc= 0.68461 val_ap= 0.71285 time= 0.08265\n",
      "训练次数: 9 Epoch: 0005 log_lik= 0.9684319 train_kl= -0.00193 train_loss= 0.97036 train_acc= 0.29863 val_roc= 0.68746 val_ap= 0.71536 time= 0.07369\n",
      "训练次数: 9 Epoch: 0006 log_lik= 0.8758491 train_kl= -0.00265 train_loss= 0.87850 train_acc= 0.26490 val_roc= 0.69715 val_ap= 0.72426 time= 0.08564\n",
      "训练次数: 9 Epoch: 0007 log_lik= 0.7899197 train_kl= -0.00339 train_loss= 0.79331 train_acc= 0.26883 val_roc= 0.72173 val_ap= 0.74628 time= 0.07568\n",
      "训练次数: 9 Epoch: 0008 log_lik= 0.745581 train_kl= -0.00417 train_loss= 0.74975 train_acc= 0.31064 val_roc= 0.76656 val_ap= 0.78307 time= 0.08270\n",
      "训练次数: 9 Epoch: 0009 log_lik= 0.7225053 train_kl= -0.00501 train_loss= 0.72751 train_acc= 0.35404 val_roc= 0.79330 val_ap= 0.79903 time= 0.07667\n",
      "训练次数: 9 Epoch: 0010 log_lik= 0.70940375 train_kl= -0.00588 train_loss= 0.71528 train_acc= 0.33586 val_roc= 0.78571 val_ap= 0.79091 time= 0.07672\n",
      "训练次数: 9 Epoch: 0011 log_lik= 0.7073625 train_kl= -0.00675 train_loss= 0.71411 train_acc= 0.22103 val_roc= 0.78441 val_ap= 0.79048 time= 0.07468\n",
      "训练次数: 9 Epoch: 0012 log_lik= 0.7136547 train_kl= -0.00757 train_loss= 0.72122 train_acc= 0.12847 val_roc= 0.80581 val_ap= 0.80663 time= 0.08165\n",
      "训练次数: 9 Epoch: 0013 log_lik= 0.70357764 train_kl= -0.00823 train_loss= 0.71181 train_acc= 0.11001 val_roc= 0.82933 val_ap= 0.82195 time= 0.07269\n",
      "训练次数: 9 Epoch: 0014 log_lik= 0.682166 train_kl= -0.00877 train_loss= 0.69093 train_acc= 0.13459 val_roc= 0.83866 val_ap= 0.82118 time= 0.07369\n",
      "训练次数: 9 Epoch: 0015 log_lik= 0.6625405 train_kl= -0.00923 train_loss= 0.67177 train_acc= 0.18993 val_roc= 0.83446 val_ap= 0.81281 time= 0.07568\n",
      "训练次数: 9 Epoch: 0016 log_lik= 0.6461429 train_kl= -0.00965 train_loss= 0.65579 train_acc= 0.26415 val_roc= 0.83070 val_ap= 0.81028 time= 0.07568\n",
      "训练次数: 9 Epoch: 0017 log_lik= 0.63328683 train_kl= -0.01005 train_loss= 0.64334 train_acc= 0.29997 val_roc= 0.83326 val_ap= 0.81342 time= 0.07568\n",
      "训练次数: 9 Epoch: 0018 log_lik= 0.62091064 train_kl= -0.01042 train_loss= 0.63133 train_acc= 0.32367 val_roc= 0.84002 val_ap= 0.81767 time= 0.07369\n",
      "训练次数: 9 Epoch: 0019 log_lik= 0.60899997 train_kl= -0.01074 train_loss= 0.61974 train_acc= 0.34929 val_roc= 0.84655 val_ap= 0.82137 time= 0.07867\n",
      "训练次数: 9 Epoch: 0020 log_lik= 0.5965732 train_kl= -0.01101 train_loss= 0.60758 train_acc= 0.38843 val_roc= 0.85044 val_ap= 0.82713 time= 0.07468\n",
      "训练次数: 9 Epoch: 0021 log_lik= 0.583184 train_kl= -0.01120 train_loss= 0.59439 train_acc= 0.42540 val_roc= 0.85492 val_ap= 0.83565 time= 0.07369\n",
      "训练次数: 9 Epoch: 0022 log_lik= 0.56895024 train_kl= -0.01135 train_loss= 0.58030 train_acc= 0.45106 val_roc= 0.86242 val_ap= 0.84445 time= 0.07668\n",
      "训练次数: 9 Epoch: 0023 log_lik= 0.55753136 train_kl= -0.01145 train_loss= 0.56898 train_acc= 0.46204 val_roc= 0.87282 val_ap= 0.85654 time= 0.07634\n",
      "训练次数: 9 Epoch: 0024 log_lik= 0.5458771 train_kl= -0.01151 train_loss= 0.55739 train_acc= 0.47085 val_roc= 0.88337 val_ap= 0.86967 time= 0.08165\n",
      "训练次数: 9 Epoch: 0025 log_lik= 0.53735745 train_kl= -0.01154 train_loss= 0.54890 train_acc= 0.47710 val_roc= 0.89244 val_ap= 0.88370 time= 0.07866\n",
      "训练次数: 9 Epoch: 0026 log_lik= 0.5268826 train_kl= -0.01154 train_loss= 0.53843 train_acc= 0.48317 val_roc= 0.89819 val_ap= 0.89245 time= 0.07668\n",
      "训练次数: 9 Epoch: 0027 log_lik= 0.5183224 train_kl= -0.01152 train_loss= 0.52985 train_acc= 0.49471 val_roc= 0.90192 val_ap= 0.89789 time= 0.07567\n",
      "训练次数: 9 Epoch: 0028 log_lik= 0.5126337 train_kl= -0.01149 train_loss= 0.52412 train_acc= 0.50668 val_roc= 0.90262 val_ap= 0.89893 time= 0.07867\n",
      "训练次数: 9 Epoch: 0029 log_lik= 0.50668 train_kl= -0.01145 train_loss= 0.51813 train_acc= 0.51766 val_roc= 0.90275 val_ap= 0.89966 time= 0.07468\n",
      "训练次数: 9 Epoch: 0030 log_lik= 0.50500983 train_kl= -0.01142 train_loss= 0.51643 train_acc= 0.52098 val_roc= 0.90312 val_ap= 0.89980 time= 0.07568\n",
      "训练次数: 9 Epoch: 0031 log_lik= 0.5057746 train_kl= -0.01139 train_loss= 0.51716 train_acc= 0.52309 val_roc= 0.90316 val_ap= 0.89995 time= 0.07468\n",
      "训练次数: 9 Epoch: 0032 log_lik= 0.5059833 train_kl= -0.01136 train_loss= 0.51735 train_acc= 0.52626 val_roc= 0.90366 val_ap= 0.90077 time= 0.07369\n",
      "训练次数: 9 Epoch: 0033 log_lik= 0.5048061 train_kl= -0.01133 train_loss= 0.51614 train_acc= 0.52867 val_roc= 0.90448 val_ap= 0.90190 time= 0.07469\n",
      "训练次数: 9 Epoch: 0034 log_lik= 0.5044275 train_kl= -0.01129 train_loss= 0.51572 train_acc= 0.53020 val_roc= 0.90522 val_ap= 0.90253 time= 0.07667\n",
      "训练次数: 9 Epoch: 0035 log_lik= 0.50150657 train_kl= -0.01123 train_loss= 0.51274 train_acc= 0.53330 val_roc= 0.90697 val_ap= 0.90384 time= 0.07668\n",
      "训练次数: 9 Epoch: 0036 log_lik= 0.4980588 train_kl= -0.01117 train_loss= 0.50923 train_acc= 0.53563 val_roc= 0.90971 val_ap= 0.90696 time= 0.07468\n",
      "训练次数: 9 Epoch: 0037 log_lik= 0.49421433 train_kl= -0.01110 train_loss= 0.50532 train_acc= 0.53666 val_roc= 0.91279 val_ap= 0.91070 time= 0.08364\n",
      "训练次数: 9 Epoch: 0038 log_lik= 0.49019256 train_kl= -0.01104 train_loss= 0.50123 train_acc= 0.53833 val_roc= 0.91710 val_ap= 0.91570 time= 0.08464\n",
      "训练次数: 9 Epoch: 0039 log_lik= 0.48541737 train_kl= -0.01099 train_loss= 0.49640 train_acc= 0.54046 val_roc= 0.92044 val_ap= 0.91942 time= 0.07369\n",
      "训练次数: 9 Epoch: 0040 log_lik= 0.483507 train_kl= -0.01094 train_loss= 0.49444 train_acc= 0.53769 val_roc= 0.92242 val_ap= 0.92241 time= 0.07468\n",
      "训练次数: 9 Epoch: 0041 log_lik= 0.48099574 train_kl= -0.01089 train_loss= 0.49189 train_acc= 0.53777 val_roc= 0.92397 val_ap= 0.92447 time= 0.07568\n",
      "训练次数: 9 Epoch: 0042 log_lik= 0.47816625 train_kl= -0.01085 train_loss= 0.48902 train_acc= 0.53620 val_roc= 0.92524 val_ap= 0.92685 time= 0.07377\n",
      "训练次数: 9 Epoch: 0043 log_lik= 0.47673205 train_kl= -0.01081 train_loss= 0.48754 train_acc= 0.53731 val_roc= 0.92682 val_ap= 0.92939 time= 0.07368\n",
      "训练次数: 9 Epoch: 0044 log_lik= 0.47559068 train_kl= -0.01078 train_loss= 0.48637 train_acc= 0.53705 val_roc= 0.92776 val_ap= 0.93085 time= 0.07568\n",
      "训练次数: 9 Epoch: 0045 log_lik= 0.47368518 train_kl= -0.01076 train_loss= 0.48445 train_acc= 0.53751 val_roc= 0.92783 val_ap= 0.93082 time= 0.08166\n",
      "训练次数: 9 Epoch: 0046 log_lik= 0.47253862 train_kl= -0.01074 train_loss= 0.48328 train_acc= 0.54015 val_roc= 0.92789 val_ap= 0.93123 time= 0.07468\n",
      "训练次数: 9 Epoch: 0047 log_lik= 0.4714748 train_kl= -0.01071 train_loss= 0.48219 train_acc= 0.53979 val_roc= 0.92816 val_ap= 0.93205 time= 0.08962\n",
      "训练次数: 9 Epoch: 0048 log_lik= 0.46950436 train_kl= -0.01068 train_loss= 0.48018 train_acc= 0.53921 val_roc= 0.92962 val_ap= 0.93379 time= 0.07568\n",
      "训练次数: 9 Epoch: 0049 log_lik= 0.46875256 train_kl= -0.01064 train_loss= 0.47939 train_acc= 0.53827 val_roc= 0.93104 val_ap= 0.93515 time= 0.07568\n",
      "训练次数: 9 Epoch: 0050 log_lik= 0.4677223 train_kl= -0.01061 train_loss= 0.47833 train_acc= 0.53971 val_roc= 0.93221 val_ap= 0.93636 time= 0.07468\n",
      "训练次数: 9 Epoch: 0051 log_lik= 0.46618888 train_kl= -0.01058 train_loss= 0.47677 train_acc= 0.54074 val_roc= 0.93269 val_ap= 0.93672 time= 0.08265\n",
      "训练次数: 9 Epoch: 0052 log_lik= 0.46395984 train_kl= -0.01054 train_loss= 0.47450 train_acc= 0.54206 val_roc= 0.93312 val_ap= 0.93709 time= 0.07368\n",
      "训练次数: 9 Epoch: 0053 log_lik= 0.4617518 train_kl= -0.01050 train_loss= 0.47226 train_acc= 0.54372 val_roc= 0.93341 val_ap= 0.93761 time= 0.07468\n",
      "训练次数: 9 Epoch: 0054 log_lik= 0.46071476 train_kl= -0.01047 train_loss= 0.47119 train_acc= 0.54557 val_roc= 0.93371 val_ap= 0.93811 time= 0.07469\n",
      "训练次数: 9 Epoch: 0055 log_lik= 0.45991552 train_kl= -0.01046 train_loss= 0.47037 train_acc= 0.54367 val_roc= 0.93396 val_ap= 0.93826 time= 0.07468\n",
      "训练次数: 9 Epoch: 0056 log_lik= 0.4588262 train_kl= -0.01045 train_loss= 0.46928 train_acc= 0.54464 val_roc= 0.93376 val_ap= 0.93766 time= 0.07468\n",
      "训练次数: 9 Epoch: 0057 log_lik= 0.4591257 train_kl= -0.01045 train_loss= 0.46957 train_acc= 0.54447 val_roc= 0.93361 val_ap= 0.93698 time= 0.07568\n",
      "训练次数: 9 Epoch: 0058 log_lik= 0.4580837 train_kl= -0.01045 train_loss= 0.46853 train_acc= 0.54568 val_roc= 0.93286 val_ap= 0.93626 time= 0.07369\n",
      "训练次数: 9 Epoch: 0059 log_lik= 0.45779958 train_kl= -0.01045 train_loss= 0.46825 train_acc= 0.54629 val_roc= 0.93259 val_ap= 0.93579 time= 0.07567\n",
      "训练次数: 9 Epoch: 0060 log_lik= 0.45632014 train_kl= -0.01047 train_loss= 0.46679 train_acc= 0.54651 val_roc= 0.93312 val_ap= 0.93590 time= 0.07470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0061 log_lik= 0.456045 train_kl= -0.01049 train_loss= 0.46653 train_acc= 0.54834 val_roc= 0.93377 val_ap= 0.93600 time= 0.07668\n",
      "训练次数: 9 Epoch: 0062 log_lik= 0.455388 train_kl= -0.01049 train_loss= 0.46588 train_acc= 0.54783 val_roc= 0.93355 val_ap= 0.93586 time= 0.07369\n",
      "训练次数: 9 Epoch: 0063 log_lik= 0.45505208 train_kl= -0.01051 train_loss= 0.46556 train_acc= 0.54843 val_roc= 0.93276 val_ap= 0.93527 time= 0.07468\n",
      "训练次数: 9 Epoch: 0064 log_lik= 0.45396775 train_kl= -0.01053 train_loss= 0.46450 train_acc= 0.54836 val_roc= 0.93256 val_ap= 0.93518 time= 0.07867\n",
      "训练次数: 9 Epoch: 0065 log_lik= 0.45356956 train_kl= -0.01053 train_loss= 0.46410 train_acc= 0.54777 val_roc= 0.93246 val_ap= 0.93472 time= 0.07468\n",
      "训练次数: 9 Epoch: 0066 log_lik= 0.45263052 train_kl= -0.01053 train_loss= 0.46316 train_acc= 0.54895 val_roc= 0.93230 val_ap= 0.93466 time= 0.07468\n",
      "训练次数: 9 Epoch: 0067 log_lik= 0.45210966 train_kl= -0.01055 train_loss= 0.46266 train_acc= 0.54863 val_roc= 0.93183 val_ap= 0.93479 time= 0.07568\n",
      "训练次数: 9 Epoch: 0068 log_lik= 0.45169306 train_kl= -0.01057 train_loss= 0.46226 train_acc= 0.54831 val_roc= 0.93209 val_ap= 0.93569 time= 0.07468\n",
      "训练次数: 9 Epoch: 0069 log_lik= 0.45093855 train_kl= -0.01057 train_loss= 0.46151 train_acc= 0.54804 val_roc= 0.93218 val_ap= 0.93589 time= 0.07369\n",
      "训练次数: 9 Epoch: 0070 log_lik= 0.45105785 train_kl= -0.01058 train_loss= 0.46164 train_acc= 0.54734 val_roc= 0.93173 val_ap= 0.93589 time= 0.07767\n",
      "训练次数: 9 Epoch: 0071 log_lik= 0.45056272 train_kl= -0.01060 train_loss= 0.46116 train_acc= 0.54634 val_roc= 0.93111 val_ap= 0.93630 time= 0.07568\n",
      "训练次数: 9 Epoch: 0072 log_lik= 0.45033157 train_kl= -0.01063 train_loss= 0.46096 train_acc= 0.54781 val_roc= 0.93153 val_ap= 0.93679 time= 0.07468\n",
      "训练次数: 9 Epoch: 0073 log_lik= 0.4495403 train_kl= -0.01065 train_loss= 0.46019 train_acc= 0.54753 val_roc= 0.93211 val_ap= 0.93696 time= 0.07468\n",
      "训练次数: 9 Epoch: 0074 log_lik= 0.44897106 train_kl= -0.01066 train_loss= 0.45963 train_acc= 0.54642 val_roc= 0.93196 val_ap= 0.93738 time= 0.07660\n",
      "训练次数: 9 Epoch: 0075 log_lik= 0.44882572 train_kl= -0.01068 train_loss= 0.45951 train_acc= 0.54667 val_roc= 0.93182 val_ap= 0.93821 time= 0.07468\n",
      "训练次数: 9 Epoch: 0076 log_lik= 0.44854754 train_kl= -0.01071 train_loss= 0.45926 train_acc= 0.54677 val_roc= 0.93247 val_ap= 0.93919 time= 0.08317\n",
      "训练次数: 9 Epoch: 0077 log_lik= 0.44814625 train_kl= -0.01073 train_loss= 0.45888 train_acc= 0.54723 val_roc= 0.93321 val_ap= 0.93984 time= 0.07767\n",
      "训练次数: 9 Epoch: 0078 log_lik= 0.44799682 train_kl= -0.01074 train_loss= 0.45874 train_acc= 0.54706 val_roc= 0.93345 val_ap= 0.94030 time= 0.07522\n",
      "训练次数: 9 Epoch: 0079 log_lik= 0.44744536 train_kl= -0.01077 train_loss= 0.45821 train_acc= 0.54696 val_roc= 0.93283 val_ap= 0.94026 time= 0.07767\n",
      "训练次数: 9 Epoch: 0080 log_lik= 0.44714555 train_kl= -0.01080 train_loss= 0.45794 train_acc= 0.54735 val_roc= 0.93313 val_ap= 0.94058 time= 0.07568\n",
      "训练次数: 9 Epoch: 0081 log_lik= 0.44670695 train_kl= -0.01082 train_loss= 0.45753 train_acc= 0.54709 val_roc= 0.93370 val_ap= 0.94090 time= 0.07370\n",
      "训练次数: 9 Epoch: 0082 log_lik= 0.4460843 train_kl= -0.01084 train_loss= 0.45692 train_acc= 0.54730 val_roc= 0.93402 val_ap= 0.94136 time= 0.07568\n",
      "训练次数: 9 Epoch: 0083 log_lik= 0.4462821 train_kl= -0.01086 train_loss= 0.45715 train_acc= 0.54788 val_roc= 0.93351 val_ap= 0.94163 time= 0.07468\n",
      "训练次数: 9 Epoch: 0084 log_lik= 0.4457604 train_kl= -0.01089 train_loss= 0.45665 train_acc= 0.54766 val_roc= 0.93329 val_ap= 0.94161 time= 0.07368\n",
      "训练次数: 9 Epoch: 0085 log_lik= 0.44545436 train_kl= -0.01091 train_loss= 0.45637 train_acc= 0.54599 val_roc= 0.93374 val_ap= 0.94155 time= 0.07369\n",
      "训练次数: 9 Epoch: 0086 log_lik= 0.44486916 train_kl= -0.01092 train_loss= 0.45579 train_acc= 0.54740 val_roc= 0.93386 val_ap= 0.94170 time= 0.07269\n",
      "训练次数: 9 Epoch: 0087 log_lik= 0.44488114 train_kl= -0.01093 train_loss= 0.45581 train_acc= 0.54724 val_roc= 0.93344 val_ap= 0.94169 time= 0.08066\n",
      "训练次数: 9 Epoch: 0088 log_lik= 0.44455707 train_kl= -0.01095 train_loss= 0.45551 train_acc= 0.54703 val_roc= 0.93299 val_ap= 0.94161 time= 0.08165\n",
      "训练次数: 9 Epoch: 0089 log_lik= 0.44440907 train_kl= -0.01096 train_loss= 0.45537 train_acc= 0.54725 val_roc= 0.93318 val_ap= 0.94178 time= 0.07369\n",
      "训练次数: 9 Epoch: 0090 log_lik= 0.44428334 train_kl= -0.01097 train_loss= 0.45525 train_acc= 0.54682 val_roc= 0.93293 val_ap= 0.94149 time= 0.07966\n",
      "训练次数: 9 Epoch: 0091 log_lik= 0.44353977 train_kl= -0.01098 train_loss= 0.45452 train_acc= 0.54629 val_roc= 0.93270 val_ap= 0.94120 time= 0.07668\n",
      "训练次数: 9 Epoch: 0092 log_lik= 0.44352752 train_kl= -0.01099 train_loss= 0.45452 train_acc= 0.54663 val_roc= 0.93250 val_ap= 0.94119 time= 0.07468\n",
      "训练次数: 9 Epoch: 0093 log_lik= 0.44364098 train_kl= -0.01100 train_loss= 0.45464 train_acc= 0.54750 val_roc= 0.93276 val_ap= 0.94145 time= 0.07568\n",
      "训练次数: 9 Epoch: 0094 log_lik= 0.4431066 train_kl= -0.01101 train_loss= 0.45412 train_acc= 0.54688 val_roc= 0.93237 val_ap= 0.94115 time= 0.07667\n",
      "训练次数: 9 Epoch: 0095 log_lik= 0.4431058 train_kl= -0.01102 train_loss= 0.45413 train_acc= 0.54728 val_roc= 0.93201 val_ap= 0.94085 time= 0.07468\n",
      "训练次数: 9 Epoch: 0096 log_lik= 0.44234782 train_kl= -0.01103 train_loss= 0.45338 train_acc= 0.54507 val_roc= 0.93183 val_ap= 0.94087 time= 0.07672\n",
      "训练次数: 9 Epoch: 0097 log_lik= 0.44234097 train_kl= -0.01104 train_loss= 0.45338 train_acc= 0.54616 val_roc= 0.93206 val_ap= 0.94087 time= 0.07468\n",
      "训练次数: 9 Epoch: 0098 log_lik= 0.44218603 train_kl= -0.01105 train_loss= 0.45324 train_acc= 0.54704 val_roc= 0.93186 val_ap= 0.94095 time= 0.07568\n",
      "训练次数: 9 Epoch: 0099 log_lik= 0.44207263 train_kl= -0.01107 train_loss= 0.45314 train_acc= 0.54553 val_roc= 0.93117 val_ap= 0.94053 time= 0.07369\n",
      "训练次数: 9 Epoch: 0100 log_lik= 0.44182968 train_kl= -0.01108 train_loss= 0.45291 train_acc= 0.54574 val_roc= 0.93150 val_ap= 0.94055 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 9 ROC score: 0.9182008360668135\n",
      "训练次数: 9 AP score: 0.9257697025153798\n",
      "训练次数: 10 Epoch: 0001 log_lik= 1.6920568 train_kl= -0.00004 train_loss= 1.69210 train_acc= 0.49795 val_roc= 0.70506 val_ap= 0.73486 time= 1.39665\n",
      "训练次数: 10 Epoch: 0002 log_lik= 1.5021514 train_kl= -0.00020 train_loss= 1.50236 train_acc= 0.47416 val_roc= 0.69945 val_ap= 0.73284 time= 0.08758\n",
      "训练次数: 10 Epoch: 0003 log_lik= 1.3523916 train_kl= -0.00064 train_loss= 1.35303 train_acc= 0.41549 val_roc= 0.70164 val_ap= 0.73563 time= 0.08573\n",
      "训练次数: 10 Epoch: 0004 log_lik= 1.2403437 train_kl= -0.00107 train_loss= 1.24141 train_acc= 0.37927 val_roc= 0.70247 val_ap= 0.73367 time= 0.07667\n",
      "训练次数: 10 Epoch: 0005 log_lik= 1.088852 train_kl= -0.00143 train_loss= 1.09028 train_acc= 0.37125 val_roc= 0.70806 val_ap= 0.73371 time= 0.07568\n",
      "训练次数: 10 Epoch: 0006 log_lik= 0.98804593 train_kl= -0.00184 train_loss= 0.98988 train_acc= 0.36719 val_roc= 0.72138 val_ap= 0.74023 time= 0.08365\n",
      "训练次数: 10 Epoch: 0007 log_lik= 0.8872136 train_kl= -0.00235 train_loss= 0.88957 train_acc= 0.38443 val_roc= 0.74658 val_ap= 0.75650 time= 0.07568\n",
      "训练次数: 10 Epoch: 0008 log_lik= 0.8022563 train_kl= -0.00304 train_loss= 0.80529 train_acc= 0.38582 val_roc= 0.77971 val_ap= 0.77777 time= 0.07468\n",
      "训练次数: 10 Epoch: 0009 log_lik= 0.7494872 train_kl= -0.00388 train_loss= 0.75336 train_acc= 0.39993 val_roc= 0.79401 val_ap= 0.78873 time= 0.07468\n",
      "训练次数: 10 Epoch: 0010 log_lik= 0.7130582 train_kl= -0.00485 train_loss= 0.71791 train_acc= 0.39861 val_roc= 0.80149 val_ap= 0.79932 time= 0.07468\n",
      "训练次数: 10 Epoch: 0011 log_lik= 0.68428427 train_kl= -0.00590 train_loss= 0.69018 train_acc= 0.36906 val_roc= 0.81383 val_ap= 0.80758 time= 0.07369\n",
      "训练次数: 10 Epoch: 0012 log_lik= 0.65529585 train_kl= -0.00695 train_loss= 0.66225 train_acc= 0.33152 val_roc= 0.82849 val_ap= 0.82079 time= 0.07469\n",
      "训练次数: 10 Epoch: 0013 log_lik= 0.63743466 train_kl= -0.00796 train_loss= 0.64540 train_acc= 0.29748 val_roc= 0.83923 val_ap= 0.83321 time= 0.07767\n",
      "训练次数: 10 Epoch: 0014 log_lik= 0.6151311 train_kl= -0.00891 train_loss= 0.62404 train_acc= 0.31408 val_roc= 0.83965 val_ap= 0.82699 time= 0.07468\n",
      "训练次数: 10 Epoch: 0015 log_lik= 0.5991988 train_kl= -0.00978 train_loss= 0.60897 train_acc= 0.34568 val_roc= 0.83731 val_ap= 0.81432 time= 0.07369\n",
      "训练次数: 10 Epoch: 0016 log_lik= 0.58241636 train_kl= -0.01053 train_loss= 0.59295 train_acc= 0.40339 val_roc= 0.83640 val_ap= 0.80929 time= 0.07966\n",
      "训练次数: 10 Epoch: 0017 log_lik= 0.56944805 train_kl= -0.01119 train_loss= 0.58064 train_acc= 0.45467 val_roc= 0.83842 val_ap= 0.81703 time= 0.07568\n",
      "训练次数: 10 Epoch: 0018 log_lik= 0.56075037 train_kl= -0.01176 train_loss= 0.57251 train_acc= 0.48204 val_roc= 0.84596 val_ap= 0.83419 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0019 log_lik= 0.5535337 train_kl= -0.01225 train_loss= 0.56578 train_acc= 0.47993 val_roc= 0.85832 val_ap= 0.85028 time= 0.08364\n",
      "训练次数: 10 Epoch: 0020 log_lik= 0.54773986 train_kl= -0.01267 train_loss= 0.56041 train_acc= 0.47702 val_roc= 0.86968 val_ap= 0.86209 time= 0.07568\n",
      "训练次数: 10 Epoch: 0021 log_lik= 0.53978443 train_kl= -0.01307 train_loss= 0.55286 train_acc= 0.48698 val_roc= 0.88039 val_ap= 0.87576 time= 0.07568\n",
      "训练次数: 10 Epoch: 0022 log_lik= 0.5276763 train_kl= -0.01345 train_loss= 0.54112 train_acc= 0.50176 val_roc= 0.89021 val_ap= 0.88804 time= 0.07667\n",
      "训练次数: 10 Epoch: 0023 log_lik= 0.5146247 train_kl= -0.01376 train_loss= 0.52838 train_acc= 0.51459 val_roc= 0.89673 val_ap= 0.89594 time= 0.07468\n",
      "训练次数: 10 Epoch: 0024 log_lik= 0.50890183 train_kl= -0.01403 train_loss= 0.52293 train_acc= 0.51319 val_roc= 0.90072 val_ap= 0.89878 time= 0.07568\n",
      "训练次数: 10 Epoch: 0025 log_lik= 0.50712746 train_kl= -0.01426 train_loss= 0.52139 train_acc= 0.51248 val_roc= 0.90493 val_ap= 0.90268 time= 0.07667\n",
      "训练次数: 10 Epoch: 0026 log_lik= 0.50406784 train_kl= -0.01444 train_loss= 0.51851 train_acc= 0.51615 val_roc= 0.90846 val_ap= 0.90677 time= 0.07468\n",
      "训练次数: 10 Epoch: 0027 log_lik= 0.50279653 train_kl= -0.01455 train_loss= 0.51735 train_acc= 0.51780 val_roc= 0.91045 val_ap= 0.90959 time= 0.07369\n",
      "训练次数: 10 Epoch: 0028 log_lik= 0.49925765 train_kl= -0.01460 train_loss= 0.51386 train_acc= 0.51595 val_roc= 0.91070 val_ap= 0.91134 time= 0.07667\n",
      "训练次数: 10 Epoch: 0029 log_lik= 0.49618062 train_kl= -0.01462 train_loss= 0.51080 train_acc= 0.51320 val_roc= 0.91064 val_ap= 0.91130 time= 0.07767\n",
      "训练次数: 10 Epoch: 0030 log_lik= 0.49206096 train_kl= -0.01464 train_loss= 0.50670 train_acc= 0.51600 val_roc= 0.91284 val_ap= 0.91308 time= 0.07468\n",
      "训练次数: 10 Epoch: 0031 log_lik= 0.48830098 train_kl= -0.01466 train_loss= 0.50296 train_acc= 0.52077 val_roc= 0.91451 val_ap= 0.91416 time= 0.07468\n",
      "训练次数: 10 Epoch: 0032 log_lik= 0.48413467 train_kl= -0.01468 train_loss= 0.49882 train_acc= 0.52462 val_roc= 0.91460 val_ap= 0.91425 time= 0.07568\n",
      "训练次数: 10 Epoch: 0033 log_lik= 0.481858 train_kl= -0.01469 train_loss= 0.49655 train_acc= 0.52418 val_roc= 0.91366 val_ap= 0.91277 time= 0.07269\n",
      "训练次数: 10 Epoch: 0034 log_lik= 0.47927386 train_kl= -0.01468 train_loss= 0.49395 train_acc= 0.52292 val_roc= 0.91220 val_ap= 0.91127 time= 0.07468\n",
      "训练次数: 10 Epoch: 0035 log_lik= 0.47595283 train_kl= -0.01465 train_loss= 0.49061 train_acc= 0.52651 val_roc= 0.91143 val_ap= 0.91090 time= 0.07568\n",
      "训练次数: 10 Epoch: 0036 log_lik= 0.4728706 train_kl= -0.01461 train_loss= 0.48748 train_acc= 0.53071 val_roc= 0.91100 val_ap= 0.91062 time= 0.07369\n",
      "训练次数: 10 Epoch: 0037 log_lik= 0.47132096 train_kl= -0.01455 train_loss= 0.48587 train_acc= 0.53256 val_roc= 0.90947 val_ap= 0.90852 time= 0.07568\n",
      "训练次数: 10 Epoch: 0038 log_lik= 0.4702704 train_kl= -0.01448 train_loss= 0.48475 train_acc= 0.53116 val_roc= 0.90811 val_ap= 0.90677 time= 0.07568\n",
      "训练次数: 10 Epoch: 0039 log_lik= 0.47006348 train_kl= -0.01439 train_loss= 0.48446 train_acc= 0.53064 val_roc= 0.90801 val_ap= 0.90642 time= 0.07468\n",
      "训练次数: 10 Epoch: 0040 log_lik= 0.46891567 train_kl= -0.01430 train_loss= 0.48322 train_acc= 0.52966 val_roc= 0.90759 val_ap= 0.90631 time= 0.07568\n",
      "训练次数: 10 Epoch: 0041 log_lik= 0.467042 train_kl= -0.01421 train_loss= 0.48125 train_acc= 0.53130 val_roc= 0.90639 val_ap= 0.90537 time= 0.07369\n",
      "训练次数: 10 Epoch: 0042 log_lik= 0.46536845 train_kl= -0.01411 train_loss= 0.47948 train_acc= 0.53238 val_roc= 0.90486 val_ap= 0.90381 time= 0.08265\n",
      "训练次数: 10 Epoch: 0043 log_lik= 0.46435797 train_kl= -0.01401 train_loss= 0.47837 train_acc= 0.53258 val_roc= 0.90494 val_ap= 0.90357 time= 0.07767\n",
      "训练次数: 10 Epoch: 0044 log_lik= 0.4633767 train_kl= -0.01391 train_loss= 0.47728 train_acc= 0.53349 val_roc= 0.90546 val_ap= 0.90371 time= 0.07568\n",
      "训练次数: 10 Epoch: 0045 log_lik= 0.46224746 train_kl= -0.01380 train_loss= 0.47605 train_acc= 0.53280 val_roc= 0.90591 val_ap= 0.90422 time= 0.07468\n",
      "训练次数: 10 Epoch: 0046 log_lik= 0.461064 train_kl= -0.01370 train_loss= 0.47476 train_acc= 0.53379 val_roc= 0.90595 val_ap= 0.90453 time= 0.07667\n",
      "训练次数: 10 Epoch: 0047 log_lik= 0.4596758 train_kl= -0.01359 train_loss= 0.47327 train_acc= 0.53301 val_roc= 0.90655 val_ap= 0.90513 time= 0.07269\n",
      "训练次数: 10 Epoch: 0048 log_lik= 0.4584383 train_kl= -0.01349 train_loss= 0.47193 train_acc= 0.53455 val_roc= 0.90729 val_ap= 0.90649 time= 0.07468\n",
      "训练次数: 10 Epoch: 0049 log_lik= 0.45768404 train_kl= -0.01338 train_loss= 0.47107 train_acc= 0.53468 val_roc= 0.90863 val_ap= 0.90837 time= 0.07668\n",
      "训练次数: 10 Epoch: 0050 log_lik= 0.45697168 train_kl= -0.01327 train_loss= 0.47025 train_acc= 0.53350 val_roc= 0.91010 val_ap= 0.91026 time= 0.07369\n",
      "训练次数: 10 Epoch: 0051 log_lik= 0.45581996 train_kl= -0.01318 train_loss= 0.46900 train_acc= 0.53363 val_roc= 0.91065 val_ap= 0.91116 time= 0.07568\n",
      "训练次数: 10 Epoch: 0052 log_lik= 0.45510358 train_kl= -0.01309 train_loss= 0.46819 train_acc= 0.53574 val_roc= 0.91084 val_ap= 0.91200 time= 0.07468\n",
      "训练次数: 10 Epoch: 0053 log_lik= 0.45410112 train_kl= -0.01301 train_loss= 0.46711 train_acc= 0.53536 val_roc= 0.91181 val_ap= 0.91312 time= 0.07468\n",
      "训练次数: 10 Epoch: 0054 log_lik= 0.4533966 train_kl= -0.01293 train_loss= 0.46632 train_acc= 0.53570 val_roc= 0.91256 val_ap= 0.91402 time= 0.07369\n",
      "训练次数: 10 Epoch: 0055 log_lik= 0.45206138 train_kl= -0.01285 train_loss= 0.46491 train_acc= 0.53599 val_roc= 0.91298 val_ap= 0.91423 time= 0.07568\n",
      "训练次数: 10 Epoch: 0056 log_lik= 0.452027 train_kl= -0.01276 train_loss= 0.46479 train_acc= 0.53714 val_roc= 0.91323 val_ap= 0.91451 time= 0.07369\n",
      "训练次数: 10 Epoch: 0057 log_lik= 0.4513483 train_kl= -0.01268 train_loss= 0.46403 train_acc= 0.53532 val_roc= 0.91337 val_ap= 0.91505 time= 0.07269\n",
      "训练次数: 10 Epoch: 0058 log_lik= 0.45041865 train_kl= -0.01261 train_loss= 0.46302 train_acc= 0.53514 val_roc= 0.91356 val_ap= 0.91562 time= 0.07369\n",
      "训练次数: 10 Epoch: 0059 log_lik= 0.44943956 train_kl= -0.01253 train_loss= 0.46197 train_acc= 0.53578 val_roc= 0.91300 val_ap= 0.91500 time= 0.07469\n",
      "训练次数: 10 Epoch: 0060 log_lik= 0.44869557 train_kl= -0.01245 train_loss= 0.46115 train_acc= 0.53694 val_roc= 0.91240 val_ap= 0.91423 time= 0.07667\n",
      "训练次数: 10 Epoch: 0061 log_lik= 0.44837987 train_kl= -0.01238 train_loss= 0.46076 train_acc= 0.53758 val_roc= 0.91207 val_ap= 0.91392 time= 0.07568\n",
      "训练次数: 10 Epoch: 0062 log_lik= 0.4479818 train_kl= -0.01230 train_loss= 0.46028 train_acc= 0.53655 val_roc= 0.91165 val_ap= 0.91378 time= 0.07767\n",
      "训练次数: 10 Epoch: 0063 log_lik= 0.4473991 train_kl= -0.01223 train_loss= 0.45963 train_acc= 0.53598 val_roc= 0.91093 val_ap= 0.91331 time= 0.07369\n",
      "训练次数: 10 Epoch: 0064 log_lik= 0.44637644 train_kl= -0.01217 train_loss= 0.45855 train_acc= 0.53698 val_roc= 0.91010 val_ap= 0.91236 time= 0.07368\n",
      "训练次数: 10 Epoch: 0065 log_lik= 0.4459292 train_kl= -0.01212 train_loss= 0.45805 train_acc= 0.53768 val_roc= 0.90909 val_ap= 0.91115 time= 0.07767\n",
      "训练次数: 10 Epoch: 0066 log_lik= 0.44548282 train_kl= -0.01206 train_loss= 0.45754 train_acc= 0.53611 val_roc= 0.90880 val_ap= 0.91074 time= 0.07593\n",
      "训练次数: 10 Epoch: 0067 log_lik= 0.44508663 train_kl= -0.01201 train_loss= 0.45710 train_acc= 0.53601 val_roc= 0.90870 val_ap= 0.91081 time= 0.07468\n",
      "训练次数: 10 Epoch: 0068 log_lik= 0.4443496 train_kl= -0.01196 train_loss= 0.45631 train_acc= 0.53646 val_roc= 0.90821 val_ap= 0.91056 time= 0.07667\n",
      "训练次数: 10 Epoch: 0069 log_lik= 0.4439963 train_kl= -0.01192 train_loss= 0.45592 train_acc= 0.53596 val_roc= 0.90791 val_ap= 0.91067 time= 0.07866\n",
      "训练次数: 10 Epoch: 0070 log_lik= 0.44336691 train_kl= -0.01189 train_loss= 0.45525 train_acc= 0.53742 val_roc= 0.90752 val_ap= 0.91011 time= 0.07568\n",
      "训练次数: 10 Epoch: 0071 log_lik= 0.44287488 train_kl= -0.01186 train_loss= 0.45473 train_acc= 0.53621 val_roc= 0.90733 val_ap= 0.90966 time= 0.07867\n",
      "训练次数: 10 Epoch: 0072 log_lik= 0.44298616 train_kl= -0.01184 train_loss= 0.45482 train_acc= 0.53695 val_roc= 0.90726 val_ap= 0.90916 time= 0.08265\n",
      "训练次数: 10 Epoch: 0073 log_lik= 0.4424552 train_kl= -0.01182 train_loss= 0.45427 train_acc= 0.53784 val_roc= 0.90649 val_ap= 0.90853 time= 0.07568\n",
      "训练次数: 10 Epoch: 0074 log_lik= 0.44198668 train_kl= -0.01180 train_loss= 0.45378 train_acc= 0.53773 val_roc= 0.90538 val_ap= 0.90792 time= 0.07468\n",
      "训练次数: 10 Epoch: 0075 log_lik= 0.44125304 train_kl= -0.01179 train_loss= 0.45304 train_acc= 0.53828 val_roc= 0.90497 val_ap= 0.90750 time= 0.07667\n",
      "训练次数: 10 Epoch: 0076 log_lik= 0.441157 train_kl= -0.01178 train_loss= 0.45293 train_acc= 0.53792 val_roc= 0.90419 val_ap= 0.90651 time= 0.07668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0077 log_lik= 0.44080848 train_kl= -0.01176 train_loss= 0.45257 train_acc= 0.53838 val_roc= 0.90353 val_ap= 0.90550 time= 0.07667\n",
      "训练次数: 10 Epoch: 0078 log_lik= 0.44059554 train_kl= -0.01175 train_loss= 0.45235 train_acc= 0.53814 val_roc= 0.90357 val_ap= 0.90535 time= 0.07867\n",
      "训练次数: 10 Epoch: 0079 log_lik= 0.4400585 train_kl= -0.01174 train_loss= 0.45180 train_acc= 0.53882 val_roc= 0.90395 val_ap= 0.90594 time= 0.07468\n",
      "训练次数: 10 Epoch: 0080 log_lik= 0.43963262 train_kl= -0.01173 train_loss= 0.45136 train_acc= 0.53980 val_roc= 0.90299 val_ap= 0.90567 time= 0.07667\n",
      "训练次数: 10 Epoch: 0081 log_lik= 0.4395992 train_kl= -0.01173 train_loss= 0.45133 train_acc= 0.53850 val_roc= 0.90176 val_ap= 0.90498 time= 0.08464\n",
      "训练次数: 10 Epoch: 0082 log_lik= 0.4390214 train_kl= -0.01174 train_loss= 0.45076 train_acc= 0.53901 val_roc= 0.90124 val_ap= 0.90483 time= 0.07369\n",
      "训练次数: 10 Epoch: 0083 log_lik= 0.43850225 train_kl= -0.01173 train_loss= 0.45023 train_acc= 0.53929 val_roc= 0.90221 val_ap= 0.90524 time= 0.07866\n",
      "训练次数: 10 Epoch: 0084 log_lik= 0.43876407 train_kl= -0.01173 train_loss= 0.45049 train_acc= 0.53896 val_roc= 0.90186 val_ap= 0.90492 time= 0.07568\n",
      "训练次数: 10 Epoch: 0085 log_lik= 0.43800205 train_kl= -0.01173 train_loss= 0.44973 train_acc= 0.53968 val_roc= 0.90048 val_ap= 0.90479 time= 0.07767\n",
      "训练次数: 10 Epoch: 0086 log_lik= 0.43744835 train_kl= -0.01172 train_loss= 0.44917 train_acc= 0.53950 val_roc= 0.90014 val_ap= 0.90507 time= 0.07369\n",
      "训练次数: 10 Epoch: 0087 log_lik= 0.43785834 train_kl= -0.01172 train_loss= 0.44958 train_acc= 0.53932 val_roc= 0.90182 val_ap= 0.90582 time= 0.07469\n",
      "训练次数: 10 Epoch: 0088 log_lik= 0.4372919 train_kl= -0.01172 train_loss= 0.44902 train_acc= 0.53929 val_roc= 0.90211 val_ap= 0.90593 time= 0.07468\n",
      "训练次数: 10 Epoch: 0089 log_lik= 0.43730137 train_kl= -0.01173 train_loss= 0.44903 train_acc= 0.53935 val_roc= 0.90123 val_ap= 0.90588 time= 0.07767\n",
      "训练次数: 10 Epoch: 0090 log_lik= 0.43654883 train_kl= -0.01175 train_loss= 0.44830 train_acc= 0.54004 val_roc= 0.90147 val_ap= 0.90646 time= 0.07568\n",
      "训练次数: 10 Epoch: 0091 log_lik= 0.4362031 train_kl= -0.01176 train_loss= 0.44796 train_acc= 0.53969 val_roc= 0.90247 val_ap= 0.90679 time= 0.07667\n",
      "训练次数: 10 Epoch: 0092 log_lik= 0.4358975 train_kl= -0.01177 train_loss= 0.44767 train_acc= 0.54026 val_roc= 0.90273 val_ap= 0.90706 time= 0.07767\n",
      "训练次数: 10 Epoch: 0093 log_lik= 0.43584403 train_kl= -0.01178 train_loss= 0.44762 train_acc= 0.54000 val_roc= 0.90175 val_ap= 0.90718 time= 0.07568\n",
      "训练次数: 10 Epoch: 0094 log_lik= 0.4357319 train_kl= -0.01179 train_loss= 0.44752 train_acc= 0.53975 val_roc= 0.90179 val_ap= 0.90755 time= 0.08065\n",
      "训练次数: 10 Epoch: 0095 log_lik= 0.43529752 train_kl= -0.01181 train_loss= 0.44711 train_acc= 0.54041 val_roc= 0.90295 val_ap= 0.90862 time= 0.08165\n",
      "训练次数: 10 Epoch: 0096 log_lik= 0.4348678 train_kl= -0.01183 train_loss= 0.44670 train_acc= 0.54170 val_roc= 0.90290 val_ap= 0.90811 time= 0.07468\n",
      "训练次数: 10 Epoch: 0097 log_lik= 0.43449104 train_kl= -0.01184 train_loss= 0.44633 train_acc= 0.54183 val_roc= 0.90220 val_ap= 0.90811 time= 0.07668\n",
      "训练次数: 10 Epoch: 0098 log_lik= 0.43463665 train_kl= -0.01185 train_loss= 0.44648 train_acc= 0.54067 val_roc= 0.90179 val_ap= 0.90803 time= 0.07568\n",
      "训练次数: 10 Epoch: 0099 log_lik= 0.43346387 train_kl= -0.01187 train_loss= 0.44533 train_acc= 0.54194 val_roc= 0.90236 val_ap= 0.90852 time= 0.07459\n",
      "训练次数: 10 Epoch: 0100 log_lik= 0.43375552 train_kl= -0.01189 train_loss= 0.44565 train_acc= 0.54117 val_roc= 0.90225 val_ap= 0.90854 time= 0.07667\n",
      "Optimization Finished!\n",
      "训练次数: 10 ROC score: 0.9001076589049035\n",
      "训练次数: 10 AP score: 0.9075136439074628\n",
      "训练次数: 11 Epoch: 0001 log_lik= 1.7342885 train_kl= -0.00004 train_loss= 1.73433 train_acc= 0.49597 val_roc= 0.69100 val_ap= 0.69393 time= 1.50826\n",
      "训练次数: 11 Epoch: 0002 log_lik= 1.4364443 train_kl= -0.00023 train_loss= 1.43667 train_acc= 0.48103 val_roc= 0.66246 val_ap= 0.67216 time= 0.07961\n",
      "训练次数: 11 Epoch: 0003 log_lik= 1.2448199 train_kl= -0.00072 train_loss= 1.24554 train_acc= 0.40695 val_roc= 0.66092 val_ap= 0.67192 time= 0.07468\n",
      "训练次数: 11 Epoch: 0004 log_lik= 1.0881351 train_kl= -0.00131 train_loss= 1.08945 train_acc= 0.34054 val_roc= 0.67887 val_ap= 0.68396 time= 0.07667\n",
      "训练次数: 11 Epoch: 0005 log_lik= 0.962163 train_kl= -0.00185 train_loss= 0.96401 train_acc= 0.35413 val_roc= 0.70036 val_ap= 0.69981 time= 0.08862\n",
      "训练次数: 11 Epoch: 0006 log_lik= 0.868286 train_kl= -0.00252 train_loss= 0.87080 train_acc= 0.35228 val_roc= 0.71097 val_ap= 0.70944 time= 0.08265\n",
      "训练次数: 11 Epoch: 0007 log_lik= 0.79884744 train_kl= -0.00335 train_loss= 0.80220 train_acc= 0.33017 val_roc= 0.72209 val_ap= 0.72132 time= 0.07568\n",
      "训练次数: 11 Epoch: 0008 log_lik= 0.75025403 train_kl= -0.00429 train_loss= 0.75454 train_acc= 0.31587 val_roc= 0.74450 val_ap= 0.74066 time= 0.07369\n",
      "训练次数: 11 Epoch: 0009 log_lik= 0.7154832 train_kl= -0.00529 train_loss= 0.72078 train_acc= 0.32162 val_roc= 0.75100 val_ap= 0.75191 time= 0.07369\n",
      "训练次数: 11 Epoch: 0010 log_lik= 0.70105124 train_kl= -0.00633 train_loss= 0.70738 train_acc= 0.30678 val_roc= 0.73091 val_ap= 0.73812 time= 0.07669\n",
      "训练次数: 11 Epoch: 0011 log_lik= 0.68628 train_kl= -0.00737 train_loss= 0.69365 train_acc= 0.24901 val_roc= 0.72810 val_ap= 0.73710 time= 0.07568\n",
      "训练次数: 11 Epoch: 0012 log_lik= 0.68403006 train_kl= -0.00837 train_loss= 0.69240 train_acc= 0.17310 val_roc= 0.78088 val_ap= 0.77971 time= 0.07369\n",
      "训练次数: 11 Epoch: 0013 log_lik= 0.66368777 train_kl= -0.00926 train_loss= 0.67295 train_acc= 0.18334 val_roc= 0.82465 val_ap= 0.81183 time= 0.07468\n",
      "训练次数: 11 Epoch: 0014 log_lik= 0.6528837 train_kl= -0.01005 train_loss= 0.66294 train_acc= 0.19373 val_roc= 0.83604 val_ap= 0.81676 time= 0.07368\n",
      "训练次数: 11 Epoch: 0015 log_lik= 0.63740134 train_kl= -0.01073 train_loss= 0.64813 train_acc= 0.25103 val_roc= 0.84058 val_ap= 0.81947 time= 0.07468\n",
      "训练次数: 11 Epoch: 0016 log_lik= 0.6125163 train_kl= -0.01129 train_loss= 0.62381 train_acc= 0.35230 val_roc= 0.83656 val_ap= 0.81700 time= 0.07369\n",
      "训练次数: 11 Epoch: 0017 log_lik= 0.58620036 train_kl= -0.01180 train_loss= 0.59800 train_acc= 0.44232 val_roc= 0.83682 val_ap= 0.81936 time= 0.07468\n",
      "训练次数: 11 Epoch: 0018 log_lik= 0.56561005 train_kl= -0.01228 train_loss= 0.57789 train_acc= 0.47694 val_roc= 0.84260 val_ap= 0.82679 time= 0.07767\n",
      "训练次数: 11 Epoch: 0019 log_lik= 0.56297606 train_kl= -0.01272 train_loss= 0.57570 train_acc= 0.47260 val_roc= 0.85410 val_ap= 0.83982 time= 0.07568\n",
      "训练次数: 11 Epoch: 0020 log_lik= 0.56416065 train_kl= -0.01309 train_loss= 0.57725 train_acc= 0.46854 val_roc= 0.85940 val_ap= 0.84933 time= 0.07568\n",
      "训练次数: 11 Epoch: 0021 log_lik= 0.5639585 train_kl= -0.01338 train_loss= 0.57733 train_acc= 0.47048 val_roc= 0.85989 val_ap= 0.85345 time= 0.07468\n",
      "训练次数: 11 Epoch: 0022 log_lik= 0.56464297 train_kl= -0.01361 train_loss= 0.57825 train_acc= 0.48152 val_roc= 0.86316 val_ap= 0.85692 time= 0.07966\n",
      "训练次数: 11 Epoch: 0023 log_lik= 0.55556357 train_kl= -0.01380 train_loss= 0.56936 train_acc= 0.49205 val_roc= 0.86576 val_ap= 0.85720 time= 0.07667\n",
      "训练次数: 11 Epoch: 0024 log_lik= 0.5516035 train_kl= -0.01395 train_loss= 0.56556 train_acc= 0.49683 val_roc= 0.86828 val_ap= 0.86088 time= 0.07369\n",
      "训练次数: 11 Epoch: 0025 log_lik= 0.545595 train_kl= -0.01403 train_loss= 0.55962 train_acc= 0.49821 val_roc= 0.87411 val_ap= 0.87128 time= 0.07866\n",
      "训练次数: 11 Epoch: 0026 log_lik= 0.5348173 train_kl= -0.01403 train_loss= 0.54884 train_acc= 0.49844 val_roc= 0.87687 val_ap= 0.87818 time= 0.07568\n",
      "训练次数: 11 Epoch: 0027 log_lik= 0.5298051 train_kl= -0.01398 train_loss= 0.54378 train_acc= 0.49526 val_roc= 0.88131 val_ap= 0.88225 time= 0.07468\n",
      "训练次数: 11 Epoch: 0028 log_lik= 0.5208181 train_kl= -0.01391 train_loss= 0.53473 train_acc= 0.49721 val_roc= 0.88440 val_ap= 0.88155 time= 0.07778\n",
      "训练次数: 11 Epoch: 0029 log_lik= 0.5129474 train_kl= -0.01384 train_loss= 0.52679 train_acc= 0.50127 val_roc= 0.88518 val_ap= 0.87849 time= 0.07469\n",
      "训练次数: 11 Epoch: 0030 log_lik= 0.5094861 train_kl= -0.01377 train_loss= 0.52325 train_acc= 0.50443 val_roc= 0.88820 val_ap= 0.88377 time= 0.07468\n",
      "训练次数: 11 Epoch: 0031 log_lik= 0.50528276 train_kl= -0.01368 train_loss= 0.51896 train_acc= 0.50515 val_roc= 0.89252 val_ap= 0.89189 time= 0.08265\n",
      "训练次数: 11 Epoch: 0032 log_lik= 0.50264835 train_kl= -0.01359 train_loss= 0.51624 train_acc= 0.50823 val_roc= 0.89520 val_ap= 0.89636 time= 0.07468\n",
      "训练次数: 11 Epoch: 0033 log_lik= 0.50218445 train_kl= -0.01353 train_loss= 0.51572 train_acc= 0.51096 val_roc= 0.89725 val_ap= 0.89913 time= 0.07269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 11 Epoch: 0034 log_lik= 0.49922627 train_kl= -0.01349 train_loss= 0.51271 train_acc= 0.51302 val_roc= 0.89756 val_ap= 0.89898 time= 0.08365\n",
      "训练次数: 11 Epoch: 0035 log_lik= 0.4948858 train_kl= -0.01345 train_loss= 0.50833 train_acc= 0.51676 val_roc= 0.89709 val_ap= 0.89790 time= 0.07767\n",
      "训练次数: 11 Epoch: 0036 log_lik= 0.49053067 train_kl= -0.01340 train_loss= 0.50393 train_acc= 0.51817 val_roc= 0.89836 val_ap= 0.89787 time= 0.07468\n",
      "训练次数: 11 Epoch: 0037 log_lik= 0.48621136 train_kl= -0.01335 train_loss= 0.49956 train_acc= 0.52026 val_roc= 0.90068 val_ap= 0.89961 time= 0.08364\n",
      "训练次数: 11 Epoch: 0038 log_lik= 0.483077 train_kl= -0.01329 train_loss= 0.49637 train_acc= 0.52022 val_roc= 0.90254 val_ap= 0.90150 time= 0.08166\n",
      "训练次数: 11 Epoch: 0039 log_lik= 0.48226443 train_kl= -0.01325 train_loss= 0.49551 train_acc= 0.51862 val_roc= 0.90390 val_ap= 0.90407 time= 0.07568\n",
      "训练次数: 11 Epoch: 0040 log_lik= 0.4793903 train_kl= -0.01323 train_loss= 0.49262 train_acc= 0.51958 val_roc= 0.90526 val_ap= 0.90652 time= 0.07667\n",
      "训练次数: 11 Epoch: 0041 log_lik= 0.47742793 train_kl= -0.01321 train_loss= 0.49064 train_acc= 0.52298 val_roc= 0.90542 val_ap= 0.90726 time= 0.08962\n",
      "训练次数: 11 Epoch: 0042 log_lik= 0.4758521 train_kl= -0.01318 train_loss= 0.48903 train_acc= 0.52500 val_roc= 0.90604 val_ap= 0.90839 time= 0.07468\n",
      "训练次数: 11 Epoch: 0043 log_lik= 0.47393587 train_kl= -0.01312 train_loss= 0.48705 train_acc= 0.52676 val_roc= 0.90840 val_ap= 0.91143 time= 0.08066\n",
      "训练次数: 11 Epoch: 0044 log_lik= 0.47218525 train_kl= -0.01303 train_loss= 0.48521 train_acc= 0.52744 val_roc= 0.91155 val_ap= 0.91491 time= 0.08365\n",
      "训练次数: 11 Epoch: 0045 log_lik= 0.47132412 train_kl= -0.01293 train_loss= 0.48425 train_acc= 0.52735 val_roc= 0.91420 val_ap= 0.91874 time= 0.07484\n",
      "训练次数: 11 Epoch: 0046 log_lik= 0.4706909 train_kl= -0.01283 train_loss= 0.48352 train_acc= 0.52848 val_roc= 0.91470 val_ap= 0.92012 time= 0.07667\n",
      "训练次数: 11 Epoch: 0047 log_lik= 0.46845612 train_kl= -0.01275 train_loss= 0.48120 train_acc= 0.52831 val_roc= 0.91479 val_ap= 0.92052 time= 0.07468\n",
      "训练次数: 11 Epoch: 0048 log_lik= 0.46646175 train_kl= -0.01266 train_loss= 0.47912 train_acc= 0.52953 val_roc= 0.91515 val_ap= 0.92157 time= 0.07668\n",
      "训练次数: 11 Epoch: 0049 log_lik= 0.46386677 train_kl= -0.01255 train_loss= 0.47642 train_acc= 0.53026 val_roc= 0.91583 val_ap= 0.92297 time= 0.07767\n",
      "训练次数: 11 Epoch: 0050 log_lik= 0.4632722 train_kl= -0.01244 train_loss= 0.47571 train_acc= 0.53064 val_roc= 0.91639 val_ap= 0.92377 time= 0.07568\n",
      "训练次数: 11 Epoch: 0051 log_lik= 0.46244502 train_kl= -0.01232 train_loss= 0.47477 train_acc= 0.52988 val_roc= 0.91706 val_ap= 0.92467 time= 0.08962\n",
      "训练次数: 11 Epoch: 0052 log_lik= 0.46098414 train_kl= -0.01221 train_loss= 0.47319 train_acc= 0.53080 val_roc= 0.91730 val_ap= 0.92476 time= 0.07468\n",
      "训练次数: 11 Epoch: 0053 log_lik= 0.4600043 train_kl= -0.01210 train_loss= 0.47210 train_acc= 0.53142 val_roc= 0.91733 val_ap= 0.92455 time= 0.07767\n",
      "训练次数: 11 Epoch: 0054 log_lik= 0.45874143 train_kl= -0.01199 train_loss= 0.47074 train_acc= 0.53302 val_roc= 0.91706 val_ap= 0.92466 time= 0.07468\n",
      "训练次数: 11 Epoch: 0055 log_lik= 0.4575054 train_kl= -0.01190 train_loss= 0.46940 train_acc= 0.53201 val_roc= 0.91739 val_ap= 0.92567 time= 0.07620\n",
      "训练次数: 11 Epoch: 0056 log_lik= 0.45709136 train_kl= -0.01181 train_loss= 0.46891 train_acc= 0.53296 val_roc= 0.91688 val_ap= 0.92539 time= 0.08165\n",
      "训练次数: 11 Epoch: 0057 log_lik= 0.45716918 train_kl= -0.01174 train_loss= 0.46891 train_acc= 0.53212 val_roc= 0.91662 val_ap= 0.92477 time= 0.07369\n",
      "训练次数: 11 Epoch: 0058 log_lik= 0.4559482 train_kl= -0.01168 train_loss= 0.46763 train_acc= 0.53278 val_roc= 0.91668 val_ap= 0.92430 time= 0.08165\n",
      "训练次数: 11 Epoch: 0059 log_lik= 0.4548475 train_kl= -0.01163 train_loss= 0.46648 train_acc= 0.53496 val_roc= 0.91631 val_ap= 0.92351 time= 0.07468\n",
      "训练次数: 11 Epoch: 0060 log_lik= 0.4547487 train_kl= -0.01158 train_loss= 0.46633 train_acc= 0.53466 val_roc= 0.91665 val_ap= 0.92390 time= 0.07468\n",
      "训练次数: 11 Epoch: 0061 log_lik= 0.45383367 train_kl= -0.01154 train_loss= 0.46538 train_acc= 0.53488 val_roc= 0.91665 val_ap= 0.92379 time= 0.07568\n",
      "训练次数: 11 Epoch: 0062 log_lik= 0.45359775 train_kl= -0.01151 train_loss= 0.46511 train_acc= 0.53470 val_roc= 0.91626 val_ap= 0.92375 time= 0.07668\n",
      "训练次数: 11 Epoch: 0063 log_lik= 0.4523152 train_kl= -0.01149 train_loss= 0.46381 train_acc= 0.53486 val_roc= 0.91560 val_ap= 0.92336 time= 0.08165\n",
      "训练次数: 11 Epoch: 0064 log_lik= 0.4517363 train_kl= -0.01148 train_loss= 0.46321 train_acc= 0.53661 val_roc= 0.91493 val_ap= 0.92259 time= 0.08265\n",
      "训练次数: 11 Epoch: 0065 log_lik= 0.45154667 train_kl= -0.01146 train_loss= 0.46300 train_acc= 0.53499 val_roc= 0.91472 val_ap= 0.92229 time= 0.07870\n",
      "训练次数: 11 Epoch: 0066 log_lik= 0.45103654 train_kl= -0.01143 train_loss= 0.46247 train_acc= 0.53598 val_roc= 0.91477 val_ap= 0.92228 time= 0.07667\n",
      "训练次数: 11 Epoch: 0067 log_lik= 0.45045787 train_kl= -0.01141 train_loss= 0.46187 train_acc= 0.53672 val_roc= 0.91489 val_ap= 0.92250 time= 0.07667\n",
      "训练次数: 11 Epoch: 0068 log_lik= 0.44985437 train_kl= -0.01139 train_loss= 0.46124 train_acc= 0.53685 val_roc= 0.91435 val_ap= 0.92238 time= 0.07568\n",
      "训练次数: 11 Epoch: 0069 log_lik= 0.44929975 train_kl= -0.01137 train_loss= 0.46067 train_acc= 0.53690 val_roc= 0.91297 val_ap= 0.92142 time= 0.08066\n",
      "训练次数: 11 Epoch: 0070 log_lik= 0.44894996 train_kl= -0.01136 train_loss= 0.46031 train_acc= 0.53666 val_roc= 0.91207 val_ap= 0.92070 time= 0.07534\n",
      "训练次数: 11 Epoch: 0071 log_lik= 0.44853812 train_kl= -0.01135 train_loss= 0.45988 train_acc= 0.53646 val_roc= 0.91148 val_ap= 0.92017 time= 0.07520\n",
      "训练次数: 11 Epoch: 0072 log_lik= 0.4479456 train_kl= -0.01132 train_loss= 0.45926 train_acc= 0.53619 val_roc= 0.91169 val_ap= 0.92038 time= 0.07468\n",
      "训练次数: 11 Epoch: 0073 log_lik= 0.44741306 train_kl= -0.01129 train_loss= 0.45870 train_acc= 0.53641 val_roc= 0.91191 val_ap= 0.92079 time= 0.07269\n",
      "训练次数: 11 Epoch: 0074 log_lik= 0.4469876 train_kl= -0.01125 train_loss= 0.45824 train_acc= 0.53682 val_roc= 0.91148 val_ap= 0.92088 time= 0.08165\n",
      "训练次数: 11 Epoch: 0075 log_lik= 0.44664744 train_kl= -0.01122 train_loss= 0.45787 train_acc= 0.53752 val_roc= 0.91054 val_ap= 0.92034 time= 0.07568\n",
      "训练次数: 11 Epoch: 0076 log_lik= 0.44611308 train_kl= -0.01120 train_loss= 0.45732 train_acc= 0.53601 val_roc= 0.90970 val_ap= 0.92003 time= 0.07767\n",
      "训练次数: 11 Epoch: 0077 log_lik= 0.44606853 train_kl= -0.01118 train_loss= 0.45725 train_acc= 0.53576 val_roc= 0.90983 val_ap= 0.92025 time= 0.07767\n",
      "训练次数: 11 Epoch: 0078 log_lik= 0.44568065 train_kl= -0.01116 train_loss= 0.45684 train_acc= 0.53602 val_roc= 0.91009 val_ap= 0.92061 time= 0.07369\n",
      "训练次数: 11 Epoch: 0079 log_lik= 0.4456703 train_kl= -0.01112 train_loss= 0.45679 train_acc= 0.53670 val_roc= 0.91041 val_ap= 0.92092 time= 0.07368\n",
      "训练次数: 11 Epoch: 0080 log_lik= 0.4445821 train_kl= -0.01110 train_loss= 0.45568 train_acc= 0.53623 val_roc= 0.90974 val_ap= 0.92065 time= 0.07369\n",
      "训练次数: 11 Epoch: 0081 log_lik= 0.44451165 train_kl= -0.01108 train_loss= 0.45559 train_acc= 0.53628 val_roc= 0.90886 val_ap= 0.92031 time= 0.07468\n",
      "训练次数: 11 Epoch: 0082 log_lik= 0.44420567 train_kl= -0.01107 train_loss= 0.45528 train_acc= 0.53607 val_roc= 0.90822 val_ap= 0.92008 time= 0.07966\n",
      "训练次数: 11 Epoch: 0083 log_lik= 0.44405246 train_kl= -0.01108 train_loss= 0.45513 train_acc= 0.53540 val_roc= 0.90850 val_ap= 0.92054 time= 0.08066\n",
      "训练次数: 11 Epoch: 0084 log_lik= 0.4436309 train_kl= -0.01108 train_loss= 0.45471 train_acc= 0.53569 val_roc= 0.90902 val_ap= 0.92111 time= 0.07369\n",
      "训练次数: 11 Epoch: 0085 log_lik= 0.44363475 train_kl= -0.01107 train_loss= 0.45470 train_acc= 0.53558 val_roc= 0.90970 val_ap= 0.92168 time= 0.07369\n",
      "训练次数: 11 Epoch: 0086 log_lik= 0.4429402 train_kl= -0.01106 train_loss= 0.45400 train_acc= 0.53638 val_roc= 0.90957 val_ap= 0.92165 time= 0.07568\n",
      "训练次数: 11 Epoch: 0087 log_lik= 0.44261745 train_kl= -0.01106 train_loss= 0.45368 train_acc= 0.53658 val_roc= 0.90877 val_ap= 0.92118 time= 0.08166\n",
      "训练次数: 11 Epoch: 0088 log_lik= 0.4419459 train_kl= -0.01108 train_loss= 0.45302 train_acc= 0.53579 val_roc= 0.90844 val_ap= 0.92112 time= 0.07767\n",
      "训练次数: 11 Epoch: 0089 log_lik= 0.44168353 train_kl= -0.01110 train_loss= 0.45278 train_acc= 0.53654 val_roc= 0.90833 val_ap= 0.92090 time= 0.07468\n",
      "训练次数: 11 Epoch: 0090 log_lik= 0.44165686 train_kl= -0.01112 train_loss= 0.45278 train_acc= 0.53613 val_roc= 0.90857 val_ap= 0.92128 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 11 Epoch: 0091 log_lik= 0.44098723 train_kl= -0.01113 train_loss= 0.45212 train_acc= 0.53713 val_roc= 0.90912 val_ap= 0.92181 time= 0.07667\n",
      "训练次数: 11 Epoch: 0092 log_lik= 0.44068256 train_kl= -0.01115 train_loss= 0.45183 train_acc= 0.53599 val_roc= 0.90927 val_ap= 0.92190 time= 0.07469\n",
      "训练次数: 11 Epoch: 0093 log_lik= 0.4411523 train_kl= -0.01115 train_loss= 0.45231 train_acc= 0.53709 val_roc= 0.90903 val_ap= 0.92165 time= 0.07468\n",
      "训练次数: 11 Epoch: 0094 log_lik= 0.4404794 train_kl= -0.01117 train_loss= 0.45165 train_acc= 0.53736 val_roc= 0.90864 val_ap= 0.92121 time= 0.07269\n",
      "训练次数: 11 Epoch: 0095 log_lik= 0.4402161 train_kl= -0.01119 train_loss= 0.45141 train_acc= 0.53753 val_roc= 0.90853 val_ap= 0.92144 time= 0.07767\n",
      "训练次数: 11 Epoch: 0096 log_lik= 0.43988448 train_kl= -0.01121 train_loss= 0.45110 train_acc= 0.53630 val_roc= 0.90888 val_ap= 0.92195 time= 0.07468\n",
      "训练次数: 11 Epoch: 0097 log_lik= 0.43956438 train_kl= -0.01123 train_loss= 0.45079 train_acc= 0.53705 val_roc= 0.90873 val_ap= 0.92194 time= 0.08863\n",
      "训练次数: 11 Epoch: 0098 log_lik= 0.43944234 train_kl= -0.01123 train_loss= 0.45067 train_acc= 0.53769 val_roc= 0.90856 val_ap= 0.92166 time= 0.07667\n",
      "训练次数: 11 Epoch: 0099 log_lik= 0.4391525 train_kl= -0.01123 train_loss= 0.45038 train_acc= 0.53819 val_roc= 0.90841 val_ap= 0.92143 time= 0.07468\n",
      "训练次数: 11 Epoch: 0100 log_lik= 0.43865207 train_kl= -0.01124 train_loss= 0.44990 train_acc= 0.53834 val_roc= 0.90815 val_ap= 0.92119 time= 0.07369\n",
      "Optimization Finished!\n",
      "训练次数: 11 ROC score: 0.9174591058189817\n",
      "训练次数: 11 AP score: 0.9212910448716176\n",
      "训练次数: 12 Epoch: 0001 log_lik= 1.7623535 train_kl= -0.00004 train_loss= 1.76239 train_acc= 0.49823 val_roc= 0.70221 val_ap= 0.71792 time= 1.45178\n",
      "训练次数: 12 Epoch: 0002 log_lik= 1.4580585 train_kl= -0.00024 train_loss= 1.45830 train_acc= 0.46377 val_roc= 0.68842 val_ap= 0.71066 time= 0.08265\n",
      "训练次数: 12 Epoch: 0003 log_lik= 1.2273021 train_kl= -0.00061 train_loss= 1.22791 train_acc= 0.39950 val_roc= 0.68470 val_ap= 0.70910 time= 0.07767\n",
      "训练次数: 12 Epoch: 0004 log_lik= 1.0872017 train_kl= -0.00114 train_loss= 1.08834 train_acc= 0.32427 val_roc= 0.69189 val_ap= 0.71554 time= 0.08862\n",
      "训练次数: 12 Epoch: 0005 log_lik= 0.9703771 train_kl= -0.00175 train_loss= 0.97212 train_acc= 0.29007 val_roc= 0.71032 val_ap= 0.73140 time= 0.07269\n",
      "训练次数: 12 Epoch: 0006 log_lik= 0.83919144 train_kl= -0.00244 train_loss= 0.84163 train_acc= 0.30827 val_roc= 0.73983 val_ap= 0.75330 time= 0.07468\n",
      "训练次数: 12 Epoch: 0007 log_lik= 0.7617096 train_kl= -0.00327 train_loss= 0.76498 train_acc= 0.34418 val_roc= 0.74603 val_ap= 0.75412 time= 0.08365\n",
      "训练次数: 12 Epoch: 0008 log_lik= 0.7313292 train_kl= -0.00423 train_loss= 0.73556 train_acc= 0.35413 val_roc= 0.72652 val_ap= 0.73602 time= 0.07392\n",
      "训练次数: 12 Epoch: 0009 log_lik= 0.7170487 train_kl= -0.00528 train_loss= 0.72233 train_acc= 0.27869 val_roc= 0.72358 val_ap= 0.73512 time= 0.07468\n",
      "训练次数: 12 Epoch: 0010 log_lik= 0.7120391 train_kl= -0.00633 train_loss= 0.71837 train_acc= 0.19429 val_roc= 0.74285 val_ap= 0.75100 time= 0.08265\n",
      "训练次数: 12 Epoch: 0011 log_lik= 0.70282483 train_kl= -0.00729 train_loss= 0.71012 train_acc= 0.15494 val_roc= 0.77214 val_ap= 0.77517 time= 0.07668\n",
      "训练次数: 12 Epoch: 0012 log_lik= 0.6837501 train_kl= -0.00815 train_loss= 0.69190 train_acc= 0.16708 val_roc= 0.79828 val_ap= 0.79749 time= 0.07269\n",
      "训练次数: 12 Epoch: 0013 log_lik= 0.6622426 train_kl= -0.00893 train_loss= 0.67117 train_acc= 0.20606 val_roc= 0.80950 val_ap= 0.80744 time= 0.07767\n",
      "训练次数: 12 Epoch: 0014 log_lik= 0.6471909 train_kl= -0.00966 train_loss= 0.65685 train_acc= 0.21045 val_roc= 0.81863 val_ap= 0.81335 time= 0.08265\n",
      "训练次数: 12 Epoch: 0015 log_lik= 0.63594764 train_kl= -0.01031 train_loss= 0.64626 train_acc= 0.20847 val_roc= 0.82682 val_ap= 0.81648 time= 0.07468\n",
      "训练次数: 12 Epoch: 0016 log_lik= 0.6165512 train_kl= -0.01088 train_loss= 0.62743 train_acc= 0.25918 val_roc= 0.83225 val_ap= 0.81865 time= 0.07369\n",
      "训练次数: 12 Epoch: 0017 log_lik= 0.5957033 train_kl= -0.01135 train_loss= 0.60706 train_acc= 0.33181 val_roc= 0.83704 val_ap= 0.82365 time= 0.07468\n",
      "训练次数: 12 Epoch: 0018 log_lik= 0.57697266 train_kl= -0.01176 train_loss= 0.58873 train_acc= 0.39426 val_roc= 0.83949 val_ap= 0.82727 time= 0.07369\n",
      "训练次数: 12 Epoch: 0019 log_lik= 0.561053 train_kl= -0.01211 train_loss= 0.57317 train_acc= 0.43985 val_roc= 0.84168 val_ap= 0.83183 time= 0.07667\n",
      "训练次数: 12 Epoch: 0020 log_lik= 0.55295783 train_kl= -0.01242 train_loss= 0.56538 train_acc= 0.46116 val_roc= 0.84486 val_ap= 0.83769 time= 0.07568\n",
      "训练次数: 12 Epoch: 0021 log_lik= 0.5467946 train_kl= -0.01269 train_loss= 0.55949 train_acc= 0.47249 val_roc= 0.84857 val_ap= 0.84277 time= 0.07369\n",
      "训练次数: 12 Epoch: 0022 log_lik= 0.54728067 train_kl= -0.01292 train_loss= 0.56020 train_acc= 0.47530 val_roc= 0.85219 val_ap= 0.84693 time= 0.07269\n",
      "训练次数: 12 Epoch: 0023 log_lik= 0.5492796 train_kl= -0.01308 train_loss= 0.56236 train_acc= 0.47385 val_roc= 0.85689 val_ap= 0.85320 time= 0.08365\n",
      "训练次数: 12 Epoch: 0024 log_lik= 0.54610634 train_kl= -0.01318 train_loss= 0.55929 train_acc= 0.47471 val_roc= 0.86333 val_ap= 0.86156 time= 0.07568\n",
      "训练次数: 12 Epoch: 0025 log_lik= 0.53554624 train_kl= -0.01323 train_loss= 0.54877 train_acc= 0.48116 val_roc= 0.86978 val_ap= 0.86878 time= 0.07966\n",
      "训练次数: 12 Epoch: 0026 log_lik= 0.5189763 train_kl= -0.01323 train_loss= 0.53220 train_acc= 0.49801 val_roc= 0.87588 val_ap= 0.87681 time= 0.07468\n",
      "训练次数: 12 Epoch: 0027 log_lik= 0.505179 train_kl= -0.01320 train_loss= 0.51837 train_acc= 0.50925 val_roc= 0.88045 val_ap= 0.88306 time= 0.07568\n",
      "训练次数: 12 Epoch: 0028 log_lik= 0.4974986 train_kl= -0.01315 train_loss= 0.51064 train_acc= 0.51511 val_roc= 0.88469 val_ap= 0.88808 time= 0.07568\n",
      "训练次数: 12 Epoch: 0029 log_lik= 0.4942778 train_kl= -0.01309 train_loss= 0.50737 train_acc= 0.51450 val_roc= 0.88832 val_ap= 0.89135 time= 0.07468\n",
      "训练次数: 12 Epoch: 0030 log_lik= 0.4937926 train_kl= -0.01303 train_loss= 0.50682 train_acc= 0.51094 val_roc= 0.89137 val_ap= 0.89384 time= 0.07767\n",
      "训练次数: 12 Epoch: 0031 log_lik= 0.4925856 train_kl= -0.01298 train_loss= 0.50556 train_acc= 0.51042 val_roc= 0.89469 val_ap= 0.89693 time= 0.07767\n",
      "训练次数: 12 Epoch: 0032 log_lik= 0.48996875 train_kl= -0.01294 train_loss= 0.50290 train_acc= 0.51283 val_roc= 0.89604 val_ap= 0.89779 time= 0.07568\n",
      "训练次数: 12 Epoch: 0033 log_lik= 0.48669487 train_kl= -0.01290 train_loss= 0.49959 train_acc= 0.51973 val_roc= 0.89732 val_ap= 0.89937 time= 0.07369\n",
      "训练次数: 12 Epoch: 0034 log_lik= 0.48201573 train_kl= -0.01287 train_loss= 0.49489 train_acc= 0.52520 val_roc= 0.89871 val_ap= 0.90126 time= 0.07468\n",
      "训练次数: 12 Epoch: 0035 log_lik= 0.47826135 train_kl= -0.01284 train_loss= 0.49110 train_acc= 0.53144 val_roc= 0.90075 val_ap= 0.90383 time= 0.08165\n",
      "训练次数: 12 Epoch: 0036 log_lik= 0.4764384 train_kl= -0.01280 train_loss= 0.48924 train_acc= 0.53277 val_roc= 0.90260 val_ap= 0.90632 time= 0.08265\n",
      "训练次数: 12 Epoch: 0037 log_lik= 0.47513646 train_kl= -0.01276 train_loss= 0.48790 train_acc= 0.53265 val_roc= 0.90373 val_ap= 0.90790 time= 0.07468\n",
      "训练次数: 12 Epoch: 0038 log_lik= 0.47304928 train_kl= -0.01272 train_loss= 0.48577 train_acc= 0.53293 val_roc= 0.90429 val_ap= 0.90890 time= 0.07468\n",
      "训练次数: 12 Epoch: 0039 log_lik= 0.47239828 train_kl= -0.01268 train_loss= 0.48508 train_acc= 0.53212 val_roc= 0.90491 val_ap= 0.91012 time= 0.07368\n",
      "训练次数: 12 Epoch: 0040 log_lik= 0.47097713 train_kl= -0.01263 train_loss= 0.48361 train_acc= 0.53297 val_roc= 0.90626 val_ap= 0.91191 time= 0.07468\n",
      "训练次数: 12 Epoch: 0041 log_lik= 0.4689915 train_kl= -0.01258 train_loss= 0.48158 train_acc= 0.53639 val_roc= 0.90824 val_ap= 0.91434 time= 0.08265\n",
      "训练次数: 12 Epoch: 0042 log_lik= 0.4659245 train_kl= -0.01253 train_loss= 0.47846 train_acc= 0.53756 val_roc= 0.90977 val_ap= 0.91671 time= 0.07332\n",
      "训练次数: 12 Epoch: 0043 log_lik= 0.46406797 train_kl= -0.01247 train_loss= 0.47654 train_acc= 0.53791 val_roc= 0.91019 val_ap= 0.91770 time= 0.08066\n",
      "训练次数: 12 Epoch: 0044 log_lik= 0.4626312 train_kl= -0.01239 train_loss= 0.47503 train_acc= 0.53872 val_roc= 0.90977 val_ap= 0.91755 time= 0.07369\n",
      "训练次数: 12 Epoch: 0045 log_lik= 0.46048525 train_kl= -0.01231 train_loss= 0.47279 train_acc= 0.53947 val_roc= 0.90957 val_ap= 0.91770 time= 0.07369\n",
      "训练次数: 12 Epoch: 0046 log_lik= 0.45964113 train_kl= -0.01222 train_loss= 0.47186 train_acc= 0.53920 val_roc= 0.91022 val_ap= 0.91827 time= 0.07369\n",
      "训练次数: 12 Epoch: 0047 log_lik= 0.4581899 train_kl= -0.01212 train_loss= 0.47031 train_acc= 0.53885 val_roc= 0.91041 val_ap= 0.91862 time= 0.07369\n",
      "训练次数: 12 Epoch: 0048 log_lik= 0.4570972 train_kl= -0.01201 train_loss= 0.46911 train_acc= 0.53718 val_roc= 0.91048 val_ap= 0.91886 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 12 Epoch: 0049 log_lik= 0.4558355 train_kl= -0.01190 train_loss= 0.46774 train_acc= 0.53861 val_roc= 0.90968 val_ap= 0.91848 time= 0.08364\n",
      "训练次数: 12 Epoch: 0050 log_lik= 0.4549071 train_kl= -0.01180 train_loss= 0.46671 train_acc= 0.53884 val_roc= 0.90915 val_ap= 0.91791 time= 0.07966\n",
      "训练次数: 12 Epoch: 0051 log_lik= 0.45440367 train_kl= -0.01170 train_loss= 0.46611 train_acc= 0.53833 val_roc= 0.90876 val_ap= 0.91762 time= 0.08066\n",
      "训练次数: 12 Epoch: 0052 log_lik= 0.45457116 train_kl= -0.01162 train_loss= 0.46619 train_acc= 0.53676 val_roc= 0.90968 val_ap= 0.91853 time= 0.08166\n",
      "训练次数: 12 Epoch: 0053 log_lik= 0.4537353 train_kl= -0.01155 train_loss= 0.46529 train_acc= 0.53712 val_roc= 0.90989 val_ap= 0.91843 time= 0.07568\n",
      "训练次数: 12 Epoch: 0054 log_lik= 0.4533121 train_kl= -0.01149 train_loss= 0.46481 train_acc= 0.53699 val_roc= 0.91041 val_ap= 0.91878 time= 0.07568\n",
      "训练次数: 12 Epoch: 0055 log_lik= 0.4525633 train_kl= -0.01146 train_loss= 0.46402 train_acc= 0.53779 val_roc= 0.91048 val_ap= 0.91887 time= 0.07668\n",
      "训练次数: 12 Epoch: 0056 log_lik= 0.45205754 train_kl= -0.01144 train_loss= 0.46350 train_acc= 0.53942 val_roc= 0.91022 val_ap= 0.91868 time= 0.08564\n",
      "训练次数: 12 Epoch: 0057 log_lik= 0.45040885 train_kl= -0.01144 train_loss= 0.46185 train_acc= 0.53900 val_roc= 0.90960 val_ap= 0.91785 time= 0.08962\n",
      "训练次数: 12 Epoch: 0058 log_lik= 0.44975868 train_kl= -0.01143 train_loss= 0.46119 train_acc= 0.54044 val_roc= 0.91028 val_ap= 0.91837 time= 0.07966\n",
      "训练次数: 12 Epoch: 0059 log_lik= 0.4494543 train_kl= -0.01143 train_loss= 0.46089 train_acc= 0.54212 val_roc= 0.91174 val_ap= 0.91978 time= 0.07468\n",
      "训练次数: 12 Epoch: 0060 log_lik= 0.4475189 train_kl= -0.01142 train_loss= 0.45894 train_acc= 0.54190 val_roc= 0.91275 val_ap= 0.92046 time= 0.07568\n",
      "训练次数: 12 Epoch: 0061 log_lik= 0.4477609 train_kl= -0.01141 train_loss= 0.45918 train_acc= 0.54151 val_roc= 0.91318 val_ap= 0.92053 time= 0.07468\n",
      "训练次数: 12 Epoch: 0062 log_lik= 0.4468209 train_kl= -0.01141 train_loss= 0.45823 train_acc= 0.54217 val_roc= 0.91271 val_ap= 0.92035 time= 0.07369\n",
      "训练次数: 12 Epoch: 0063 log_lik= 0.44572046 train_kl= -0.01140 train_loss= 0.45712 train_acc= 0.54281 val_roc= 0.91355 val_ap= 0.92111 time= 0.07468\n",
      "训练次数: 12 Epoch: 0064 log_lik= 0.44569507 train_kl= -0.01139 train_loss= 0.45708 train_acc= 0.54383 val_roc= 0.91389 val_ap= 0.92136 time= 0.07767\n",
      "训练次数: 12 Epoch: 0065 log_lik= 0.44448358 train_kl= -0.01138 train_loss= 0.45586 train_acc= 0.54465 val_roc= 0.91404 val_ap= 0.92162 time= 0.07369\n",
      "训练次数: 12 Epoch: 0066 log_lik= 0.44410273 train_kl= -0.01136 train_loss= 0.45547 train_acc= 0.54349 val_roc= 0.91408 val_ap= 0.92196 time= 0.07568\n",
      "训练次数: 12 Epoch: 0067 log_lik= 0.4438368 train_kl= -0.01135 train_loss= 0.45519 train_acc= 0.54434 val_roc= 0.91368 val_ap= 0.92186 time= 0.07667\n",
      "训练次数: 12 Epoch: 0068 log_lik= 0.4430561 train_kl= -0.01134 train_loss= 0.45440 train_acc= 0.54283 val_roc= 0.91337 val_ap= 0.92194 time= 0.08165\n",
      "训练次数: 12 Epoch: 0069 log_lik= 0.4420555 train_kl= -0.01133 train_loss= 0.45339 train_acc= 0.54506 val_roc= 0.91279 val_ap= 0.92186 time= 0.07369\n",
      "训练次数: 12 Epoch: 0070 log_lik= 0.44192 train_kl= -0.01133 train_loss= 0.45325 train_acc= 0.54528 val_roc= 0.91320 val_ap= 0.92288 time= 0.07690\n",
      "训练次数: 12 Epoch: 0071 log_lik= 0.44179517 train_kl= -0.01133 train_loss= 0.45312 train_acc= 0.54390 val_roc= 0.91261 val_ap= 0.92280 time= 0.07608\n",
      "训练次数: 12 Epoch: 0072 log_lik= 0.44090942 train_kl= -0.01132 train_loss= 0.45222 train_acc= 0.54440 val_roc= 0.91177 val_ap= 0.92247 time= 0.07468\n",
      "训练次数: 12 Epoch: 0073 log_lik= 0.44069037 train_kl= -0.01131 train_loss= 0.45200 train_acc= 0.54416 val_roc= 0.91146 val_ap= 0.92210 time= 0.07966\n",
      "训练次数: 12 Epoch: 0074 log_lik= 0.4401369 train_kl= -0.01133 train_loss= 0.45147 train_acc= 0.54506 val_roc= 0.90980 val_ap= 0.92068 time= 0.07568\n",
      "训练次数: 12 Epoch: 0075 log_lik= 0.43964013 train_kl= -0.01134 train_loss= 0.45098 train_acc= 0.54460 val_roc= 0.90944 val_ap= 0.92128 time= 0.07667\n",
      "训练次数: 12 Epoch: 0076 log_lik= 0.43908918 train_kl= -0.01136 train_loss= 0.45045 train_acc= 0.54526 val_roc= 0.90958 val_ap= 0.92143 time= 0.07568\n",
      "训练次数: 12 Epoch: 0077 log_lik= 0.43896306 train_kl= -0.01139 train_loss= 0.45035 train_acc= 0.54518 val_roc= 0.90906 val_ap= 0.92097 time= 0.07667\n",
      "训练次数: 12 Epoch: 0078 log_lik= 0.4385383 train_kl= -0.01141 train_loss= 0.44995 train_acc= 0.54523 val_roc= 0.90772 val_ap= 0.91981 time= 0.07468\n",
      "训练次数: 12 Epoch: 0079 log_lik= 0.43793243 train_kl= -0.01143 train_loss= 0.44936 train_acc= 0.54637 val_roc= 0.90698 val_ap= 0.91929 time= 0.07521\n",
      "训练次数: 12 Epoch: 0080 log_lik= 0.43753323 train_kl= -0.01146 train_loss= 0.44899 train_acc= 0.54569 val_roc= 0.90710 val_ap= 0.91944 time= 0.07568\n",
      "训练次数: 12 Epoch: 0081 log_lik= 0.43685028 train_kl= -0.01149 train_loss= 0.44834 train_acc= 0.54714 val_roc= 0.90574 val_ap= 0.91868 time= 0.07667\n",
      "训练次数: 12 Epoch: 0082 log_lik= 0.4366763 train_kl= -0.01151 train_loss= 0.44819 train_acc= 0.54616 val_roc= 0.90441 val_ap= 0.91762 time= 0.07767\n",
      "训练次数: 12 Epoch: 0083 log_lik= 0.4363477 train_kl= -0.01153 train_loss= 0.44788 train_acc= 0.54708 val_roc= 0.90487 val_ap= 0.91785 time= 0.07568\n",
      "训练次数: 12 Epoch: 0084 log_lik= 0.43635315 train_kl= -0.01157 train_loss= 0.44792 train_acc= 0.54716 val_roc= 0.90460 val_ap= 0.91746 time= 0.07470\n",
      "训练次数: 12 Epoch: 0085 log_lik= 0.43551287 train_kl= -0.01159 train_loss= 0.44710 train_acc= 0.54737 val_roc= 0.90434 val_ap= 0.91733 time= 0.08365\n",
      "训练次数: 12 Epoch: 0086 log_lik= 0.43567413 train_kl= -0.01161 train_loss= 0.44728 train_acc= 0.54639 val_roc= 0.90337 val_ap= 0.91642 time= 0.08265\n",
      "训练次数: 12 Epoch: 0087 log_lik= 0.43523133 train_kl= -0.01162 train_loss= 0.44685 train_acc= 0.54637 val_roc= 0.90371 val_ap= 0.91612 time= 0.07568\n",
      "训练次数: 12 Epoch: 0088 log_lik= 0.43524003 train_kl= -0.01165 train_loss= 0.44689 train_acc= 0.54734 val_roc= 0.90299 val_ap= 0.91586 time= 0.07568\n",
      "训练次数: 12 Epoch: 0089 log_lik= 0.4345233 train_kl= -0.01167 train_loss= 0.44619 train_acc= 0.54708 val_roc= 0.90209 val_ap= 0.91536 time= 0.08035\n",
      "训练次数: 12 Epoch: 0090 log_lik= 0.43438694 train_kl= -0.01167 train_loss= 0.44606 train_acc= 0.54730 val_roc= 0.90318 val_ap= 0.91662 time= 0.07568\n",
      "训练次数: 12 Epoch: 0091 log_lik= 0.43397695 train_kl= -0.01168 train_loss= 0.44566 train_acc= 0.54763 val_roc= 0.90293 val_ap= 0.91633 time= 0.07369\n",
      "训练次数: 12 Epoch: 0092 log_lik= 0.4338293 train_kl= -0.01169 train_loss= 0.44552 train_acc= 0.54612 val_roc= 0.90133 val_ap= 0.91482 time= 0.07468\n",
      "训练次数: 12 Epoch: 0093 log_lik= 0.43406898 train_kl= -0.01171 train_loss= 0.44578 train_acc= 0.54703 val_roc= 0.90144 val_ap= 0.91518 time= 0.07368\n",
      "训练次数: 12 Epoch: 0094 log_lik= 0.4335801 train_kl= -0.01174 train_loss= 0.44532 train_acc= 0.54728 val_roc= 0.90211 val_ap= 0.91580 time= 0.08066\n",
      "训练次数: 12 Epoch: 0095 log_lik= 0.43290764 train_kl= -0.01176 train_loss= 0.44467 train_acc= 0.54674 val_roc= 0.90140 val_ap= 0.91524 time= 0.07468\n",
      "训练次数: 12 Epoch: 0096 log_lik= 0.43289453 train_kl= -0.01177 train_loss= 0.44467 train_acc= 0.54731 val_roc= 0.90079 val_ap= 0.91466 time= 0.07473\n",
      "训练次数: 12 Epoch: 0097 log_lik= 0.43226972 train_kl= -0.01179 train_loss= 0.44406 train_acc= 0.54798 val_roc= 0.90131 val_ap= 0.91518 time= 0.07767\n",
      "训练次数: 12 Epoch: 0098 log_lik= 0.43249997 train_kl= -0.01181 train_loss= 0.44431 train_acc= 0.54712 val_roc= 0.90139 val_ap= 0.91564 time= 0.07668\n",
      "训练次数: 12 Epoch: 0099 log_lik= 0.43204716 train_kl= -0.01183 train_loss= 0.44388 train_acc= 0.54742 val_roc= 0.90120 val_ap= 0.91565 time= 0.07620\n",
      "训练次数: 12 Epoch: 0100 log_lik= 0.43183374 train_kl= -0.01184 train_loss= 0.44367 train_acc= 0.54721 val_roc= 0.90117 val_ap= 0.91510 time= 0.07667\n",
      "Optimization Finished!\n",
      "训练次数: 12 ROC score: 0.9174483039221687\n",
      "训练次数: 12 AP score: 0.9259998583874995\n",
      "训练次数: 13 Epoch: 0001 log_lik= 1.7794499 train_kl= -0.00006 train_loss= 1.77951 train_acc= 0.49660 val_roc= 0.65299 val_ap= 0.66420 time= 1.43564\n",
      "训练次数: 13 Epoch: 0002 log_lik= 1.5338749 train_kl= -0.00011 train_loss= 1.53398 train_acc= 0.48342 val_roc= 0.66394 val_ap= 0.67320 time= 0.08260\n",
      "训练次数: 13 Epoch: 0003 log_lik= 1.3817205 train_kl= -0.00032 train_loss= 1.38205 train_acc= 0.44105 val_roc= 0.66878 val_ap= 0.67763 time= 0.09360\n",
      "训练次数: 13 Epoch: 0004 log_lik= 1.2556139 train_kl= -0.00061 train_loss= 1.25622 train_acc= 0.38474 val_roc= 0.67403 val_ap= 0.68195 time= 0.09858\n",
      "训练次数: 13 Epoch: 0005 log_lik= 1.1616899 train_kl= -0.00091 train_loss= 1.16260 train_acc= 0.34262 val_roc= 0.68325 val_ap= 0.68891 time= 0.07668\n",
      "训练次数: 13 Epoch: 0006 log_lik= 1.0468639 train_kl= -0.00123 train_loss= 1.04809 train_acc= 0.34090 val_roc= 0.69979 val_ap= 0.70321 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 13 Epoch: 0007 log_lik= 0.9530765 train_kl= -0.00161 train_loss= 0.95468 train_acc= 0.35456 val_roc= 0.73258 val_ap= 0.73012 time= 0.08265\n",
      "训练次数: 13 Epoch: 0008 log_lik= 0.86369675 train_kl= -0.00212 train_loss= 0.86582 train_acc= 0.37872 val_roc= 0.77600 val_ap= 0.76656 time= 0.07568\n",
      "训练次数: 13 Epoch: 0009 log_lik= 0.7948988 train_kl= -0.00281 train_loss= 0.79771 train_acc= 0.39306 val_roc= 0.80522 val_ap= 0.78720 time= 0.08065\n",
      "训练次数: 13 Epoch: 0010 log_lik= 0.7392219 train_kl= -0.00363 train_loss= 0.74285 train_acc= 0.38562 val_roc= 0.80882 val_ap= 0.79146 time= 0.07966\n",
      "训练次数: 13 Epoch: 0011 log_lik= 0.7011326 train_kl= -0.00455 train_loss= 0.70568 train_acc= 0.34669 val_roc= 0.80211 val_ap= 0.79197 time= 0.07966\n",
      "训练次数: 13 Epoch: 0012 log_lik= 0.68442696 train_kl= -0.00551 train_loss= 0.68993 train_acc= 0.25742 val_roc= 0.81080 val_ap= 0.80270 time= 0.08862\n",
      "训练次数: 13 Epoch: 0013 log_lik= 0.67406934 train_kl= -0.00638 train_loss= 0.68045 train_acc= 0.22745 val_roc= 0.82763 val_ap= 0.81622 time= 0.07369\n",
      "训练次数: 13 Epoch: 0014 log_lik= 0.6520043 train_kl= -0.00715 train_loss= 0.65916 train_acc= 0.24278 val_roc= 0.83721 val_ap= 0.82256 time= 0.07568\n",
      "训练次数: 13 Epoch: 0015 log_lik= 0.6338986 train_kl= -0.00784 train_loss= 0.64174 train_acc= 0.28399 val_roc= 0.84434 val_ap= 0.82701 time= 0.07369\n",
      "训练次数: 13 Epoch: 0016 log_lik= 0.61663574 train_kl= -0.00848 train_loss= 0.62511 train_acc= 0.32853 val_roc= 0.85219 val_ap= 0.83284 time= 0.07667\n",
      "训练次数: 13 Epoch: 0017 log_lik= 0.59727776 train_kl= -0.00907 train_loss= 0.60635 train_acc= 0.38420 val_roc= 0.85908 val_ap= 0.83966 time= 0.09061\n",
      "训练次数: 13 Epoch: 0018 log_lik= 0.5851597 train_kl= -0.00965 train_loss= 0.59481 train_acc= 0.40515 val_roc= 0.86481 val_ap= 0.84709 time= 0.07369\n",
      "训练次数: 13 Epoch: 0019 log_lik= 0.57678586 train_kl= -0.01019 train_loss= 0.58697 train_acc= 0.41956 val_roc= 0.87042 val_ap= 0.85476 time= 0.07369\n",
      "训练次数: 13 Epoch: 0020 log_lik= 0.57071525 train_kl= -0.01064 train_loss= 0.58136 train_acc= 0.43455 val_roc= 0.87635 val_ap= 0.86146 time= 0.07568\n",
      "训练次数: 13 Epoch: 0021 log_lik= 0.5605542 train_kl= -0.01103 train_loss= 0.57159 train_acc= 0.44985 val_roc= 0.88213 val_ap= 0.86786 time= 0.07468\n",
      "训练次数: 13 Epoch: 0022 log_lik= 0.5547715 train_kl= -0.01138 train_loss= 0.56615 train_acc= 0.45591 val_roc= 0.88856 val_ap= 0.87414 time= 0.07468\n",
      "训练次数: 13 Epoch: 0023 log_lik= 0.54695666 train_kl= -0.01168 train_loss= 0.55864 train_acc= 0.46796 val_roc= 0.89637 val_ap= 0.88275 time= 0.07468\n",
      "训练次数: 13 Epoch: 0024 log_lik= 0.5377434 train_kl= -0.01195 train_loss= 0.54969 train_acc= 0.48003 val_roc= 0.90181 val_ap= 0.88959 time= 0.07369\n",
      "训练次数: 13 Epoch: 0025 log_lik= 0.52908695 train_kl= -0.01218 train_loss= 0.54127 train_acc= 0.48971 val_roc= 0.90639 val_ap= 0.89467 time= 0.07468\n",
      "训练次数: 13 Epoch: 0026 log_lik= 0.5230627 train_kl= -0.01236 train_loss= 0.53543 train_acc= 0.49667 val_roc= 0.91045 val_ap= 0.89956 time= 0.07468\n",
      "训练次数: 13 Epoch: 0027 log_lik= 0.51721245 train_kl= -0.01249 train_loss= 0.52970 train_acc= 0.49685 val_roc= 0.91441 val_ap= 0.90449 time= 0.07369\n",
      "训练次数: 13 Epoch: 0028 log_lik= 0.5143363 train_kl= -0.01256 train_loss= 0.52690 train_acc= 0.49148 val_roc= 0.91667 val_ap= 0.90963 time= 0.07568\n",
      "训练次数: 13 Epoch: 0029 log_lik= 0.5102755 train_kl= -0.01260 train_loss= 0.52287 train_acc= 0.49342 val_roc= 0.91743 val_ap= 0.91109 time= 0.07468\n",
      "训练次数: 13 Epoch: 0030 log_lik= 0.5059402 train_kl= -0.01261 train_loss= 0.51855 train_acc= 0.50008 val_roc= 0.91741 val_ap= 0.91154 time= 0.08622\n",
      "训练次数: 13 Epoch: 0031 log_lik= 0.50232255 train_kl= -0.01260 train_loss= 0.51492 train_acc= 0.50953 val_roc= 0.91670 val_ap= 0.91079 time= 0.07568\n",
      "训练次数: 13 Epoch: 0032 log_lik= 0.4998965 train_kl= -0.01259 train_loss= 0.51248 train_acc= 0.51612 val_roc= 0.91675 val_ap= 0.91140 time= 0.07369\n",
      "训练次数: 13 Epoch: 0033 log_lik= 0.49722654 train_kl= -0.01258 train_loss= 0.50981 train_acc= 0.51810 val_roc= 0.91717 val_ap= 0.91256 time= 0.07468\n",
      "训练次数: 13 Epoch: 0034 log_lik= 0.4944688 train_kl= -0.01259 train_loss= 0.50705 train_acc= 0.51810 val_roc= 0.91771 val_ap= 0.91466 time= 0.07468\n",
      "训练次数: 13 Epoch: 0035 log_lik= 0.49077484 train_kl= -0.01260 train_loss= 0.50338 train_acc= 0.51875 val_roc= 0.91800 val_ap= 0.91524 time= 0.08165\n",
      "训练次数: 13 Epoch: 0036 log_lik= 0.48852932 train_kl= -0.01261 train_loss= 0.50114 train_acc= 0.52101 val_roc= 0.91904 val_ap= 0.91652 time= 0.07668\n",
      "训练次数: 13 Epoch: 0037 log_lik= 0.48537856 train_kl= -0.01261 train_loss= 0.49799 train_acc= 0.52552 val_roc= 0.92008 val_ap= 0.91799 time= 0.09161\n",
      "训练次数: 13 Epoch: 0038 log_lik= 0.4822149 train_kl= -0.01259 train_loss= 0.49481 train_acc= 0.53007 val_roc= 0.92219 val_ap= 0.92093 time= 0.07568\n",
      "训练次数: 13 Epoch: 0039 log_lik= 0.47978035 train_kl= -0.01257 train_loss= 0.49235 train_acc= 0.53201 val_roc= 0.92429 val_ap= 0.92330 time= 0.07468\n",
      "训练次数: 13 Epoch: 0040 log_lik= 0.4787368 train_kl= -0.01254 train_loss= 0.49128 train_acc= 0.53246 val_roc= 0.92622 val_ap= 0.92514 time= 0.07767\n",
      "训练次数: 13 Epoch: 0041 log_lik= 0.47827363 train_kl= -0.01251 train_loss= 0.49078 train_acc= 0.52910 val_roc= 0.92758 val_ap= 0.92643 time= 0.07468\n",
      "训练次数: 13 Epoch: 0042 log_lik= 0.47585598 train_kl= -0.01246 train_loss= 0.48832 train_acc= 0.53078 val_roc= 0.92874 val_ap= 0.92740 time= 0.07568\n",
      "训练次数: 13 Epoch: 0043 log_lik= 0.47368935 train_kl= -0.01241 train_loss= 0.48610 train_acc= 0.53243 val_roc= 0.92964 val_ap= 0.92811 time= 0.08265\n",
      "训练次数: 13 Epoch: 0044 log_lik= 0.47241363 train_kl= -0.01235 train_loss= 0.48476 train_acc= 0.53394 val_roc= 0.92953 val_ap= 0.92854 time= 0.07369\n",
      "训练次数: 13 Epoch: 0045 log_lik= 0.47216398 train_kl= -0.01229 train_loss= 0.48445 train_acc= 0.53362 val_roc= 0.93008 val_ap= 0.92982 time= 0.07369\n",
      "训练次数: 13 Epoch: 0046 log_lik= 0.4711624 train_kl= -0.01222 train_loss= 0.48339 train_acc= 0.53469 val_roc= 0.93075 val_ap= 0.93094 time= 0.07368\n",
      "训练次数: 13 Epoch: 0047 log_lik= 0.47035044 train_kl= -0.01216 train_loss= 0.48252 train_acc= 0.53426 val_roc= 0.93167 val_ap= 0.93205 time= 0.07468\n",
      "训练次数: 13 Epoch: 0048 log_lik= 0.46874046 train_kl= -0.01211 train_loss= 0.48085 train_acc= 0.53487 val_roc= 0.93222 val_ap= 0.93257 time= 0.07269\n",
      "训练次数: 13 Epoch: 0049 log_lik= 0.4664888 train_kl= -0.01205 train_loss= 0.47853 train_acc= 0.53731 val_roc= 0.93285 val_ap= 0.93349 time= 0.07867\n",
      "训练次数: 13 Epoch: 0050 log_lik= 0.46544042 train_kl= -0.01198 train_loss= 0.47742 train_acc= 0.53895 val_roc= 0.93295 val_ap= 0.93373 time= 0.07468\n",
      "训练次数: 13 Epoch: 0051 log_lik= 0.46437705 train_kl= -0.01192 train_loss= 0.47629 train_acc= 0.54019 val_roc= 0.93328 val_ap= 0.93419 time= 0.07369\n",
      "训练次数: 13 Epoch: 0052 log_lik= 0.46431297 train_kl= -0.01185 train_loss= 0.47616 train_acc= 0.53914 val_roc= 0.93332 val_ap= 0.93432 time= 0.07368\n",
      "训练次数: 13 Epoch: 0053 log_lik= 0.46292996 train_kl= -0.01179 train_loss= 0.47472 train_acc= 0.53881 val_roc= 0.93338 val_ap= 0.93451 time= 0.07867\n",
      "训练次数: 13 Epoch: 0054 log_lik= 0.46224552 train_kl= -0.01173 train_loss= 0.47398 train_acc= 0.54021 val_roc= 0.93332 val_ap= 0.93444 time= 0.07369\n",
      "训练次数: 13 Epoch: 0055 log_lik= 0.4617681 train_kl= -0.01167 train_loss= 0.47344 train_acc= 0.54002 val_roc= 0.93311 val_ap= 0.93432 time= 0.07368\n",
      "训练次数: 13 Epoch: 0056 log_lik= 0.4610016 train_kl= -0.01162 train_loss= 0.47262 train_acc= 0.54052 val_roc= 0.93295 val_ap= 0.93414 time= 0.07867\n",
      "训练次数: 13 Epoch: 0057 log_lik= 0.4600813 train_kl= -0.01157 train_loss= 0.47165 train_acc= 0.53908 val_roc= 0.93299 val_ap= 0.93418 time= 0.07568\n",
      "训练次数: 13 Epoch: 0058 log_lik= 0.4596148 train_kl= -0.01152 train_loss= 0.47114 train_acc= 0.53921 val_roc= 0.93306 val_ap= 0.93440 time= 0.07468\n",
      "训练次数: 13 Epoch: 0059 log_lik= 0.45852482 train_kl= -0.01148 train_loss= 0.47001 train_acc= 0.53790 val_roc= 0.93338 val_ap= 0.93458 time= 0.07667\n",
      "训练次数: 13 Epoch: 0060 log_lik= 0.457966 train_kl= -0.01145 train_loss= 0.46941 train_acc= 0.53984 val_roc= 0.93392 val_ap= 0.93524 time= 0.07390\n",
      "训练次数: 13 Epoch: 0061 log_lik= 0.4568334 train_kl= -0.01142 train_loss= 0.46825 train_acc= 0.53941 val_roc= 0.93431 val_ap= 0.93578 time= 0.07468\n",
      "训练次数: 13 Epoch: 0062 log_lik= 0.4561854 train_kl= -0.01139 train_loss= 0.46757 train_acc= 0.53940 val_roc= 0.93436 val_ap= 0.93629 time= 0.07568\n",
      "训练次数: 13 Epoch: 0063 log_lik= 0.45554218 train_kl= -0.01136 train_loss= 0.46691 train_acc= 0.53858 val_roc= 0.93416 val_ap= 0.93657 time= 0.07457\n",
      "训练次数: 13 Epoch: 0064 log_lik= 0.45558867 train_kl= -0.01134 train_loss= 0.46693 train_acc= 0.53846 val_roc= 0.93470 val_ap= 0.93746 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 13 Epoch: 0065 log_lik= 0.45453644 train_kl= -0.01131 train_loss= 0.46585 train_acc= 0.53871 val_roc= 0.93484 val_ap= 0.93790 time= 0.07668\n",
      "训练次数: 13 Epoch: 0066 log_lik= 0.45390466 train_kl= -0.01129 train_loss= 0.46520 train_acc= 0.53889 val_roc= 0.93523 val_ap= 0.93842 time= 0.07468\n",
      "训练次数: 13 Epoch: 0067 log_lik= 0.4531926 train_kl= -0.01128 train_loss= 0.46447 train_acc= 0.53981 val_roc= 0.93549 val_ap= 0.93901 time= 0.07269\n",
      "训练次数: 13 Epoch: 0068 log_lik= 0.4524452 train_kl= -0.01126 train_loss= 0.46370 train_acc= 0.53753 val_roc= 0.93561 val_ap= 0.93945 time= 0.07966\n",
      "训练次数: 13 Epoch: 0069 log_lik= 0.45193607 train_kl= -0.01124 train_loss= 0.46318 train_acc= 0.53841 val_roc= 0.93549 val_ap= 0.93963 time= 0.08464\n",
      "训练次数: 13 Epoch: 0070 log_lik= 0.45122862 train_kl= -0.01122 train_loss= 0.46245 train_acc= 0.53881 val_roc= 0.93496 val_ap= 0.93934 time= 0.07468\n",
      "训练次数: 13 Epoch: 0071 log_lik= 0.45086703 train_kl= -0.01121 train_loss= 0.46208 train_acc= 0.53798 val_roc= 0.93519 val_ap= 0.93970 time= 0.07667\n",
      "训练次数: 13 Epoch: 0072 log_lik= 0.45012093 train_kl= -0.01120 train_loss= 0.46132 train_acc= 0.53935 val_roc= 0.93530 val_ap= 0.93993 time= 0.07368\n",
      "训练次数: 13 Epoch: 0073 log_lik= 0.44984344 train_kl= -0.01120 train_loss= 0.46104 train_acc= 0.53938 val_roc= 0.93474 val_ap= 0.93928 time= 0.07369\n",
      "训练次数: 13 Epoch: 0074 log_lik= 0.44875446 train_kl= -0.01119 train_loss= 0.45995 train_acc= 0.54043 val_roc= 0.93415 val_ap= 0.93915 time= 0.07369\n",
      "训练次数: 13 Epoch: 0075 log_lik= 0.44863328 train_kl= -0.01118 train_loss= 0.45982 train_acc= 0.54103 val_roc= 0.93347 val_ap= 0.93868 time= 0.07468\n",
      "训练次数: 13 Epoch: 0076 log_lik= 0.44818738 train_kl= -0.01117 train_loss= 0.45936 train_acc= 0.54090 val_roc= 0.93305 val_ap= 0.93846 time= 0.07369\n",
      "训练次数: 13 Epoch: 0077 log_lik= 0.4474228 train_kl= -0.01116 train_loss= 0.45859 train_acc= 0.54133 val_roc= 0.93260 val_ap= 0.93802 time= 0.07434\n",
      "训练次数: 13 Epoch: 0078 log_lik= 0.44727787 train_kl= -0.01116 train_loss= 0.45844 train_acc= 0.54139 val_roc= 0.93248 val_ap= 0.93811 time= 0.07365\n",
      "训练次数: 13 Epoch: 0079 log_lik= 0.44624195 train_kl= -0.01116 train_loss= 0.45740 train_acc= 0.54183 val_roc= 0.93231 val_ap= 0.93827 time= 0.07269\n",
      "训练次数: 13 Epoch: 0080 log_lik= 0.4465302 train_kl= -0.01115 train_loss= 0.45768 train_acc= 0.54229 val_roc= 0.93198 val_ap= 0.93855 time= 0.07671\n",
      "训练次数: 13 Epoch: 0081 log_lik= 0.44585368 train_kl= -0.01115 train_loss= 0.45701 train_acc= 0.54157 val_roc= 0.93153 val_ap= 0.93836 time= 0.07568\n",
      "训练次数: 13 Epoch: 0082 log_lik= 0.44498295 train_kl= -0.01116 train_loss= 0.45615 train_acc= 0.54240 val_roc= 0.93092 val_ap= 0.93797 time= 0.07767\n",
      "训练次数: 13 Epoch: 0083 log_lik= 0.44499817 train_kl= -0.01118 train_loss= 0.45618 train_acc= 0.54189 val_roc= 0.93059 val_ap= 0.93776 time= 0.07393\n",
      "训练次数: 13 Epoch: 0084 log_lik= 0.44447652 train_kl= -0.01119 train_loss= 0.45567 train_acc= 0.54255 val_roc= 0.93024 val_ap= 0.93787 time= 0.08165\n",
      "训练次数: 13 Epoch: 0085 log_lik= 0.44379106 train_kl= -0.01121 train_loss= 0.45500 train_acc= 0.54205 val_roc= 0.93026 val_ap= 0.93810 time= 0.07767\n",
      "训练次数: 13 Epoch: 0086 log_lik= 0.4431542 train_kl= -0.01121 train_loss= 0.45437 train_acc= 0.54381 val_roc= 0.93062 val_ap= 0.93875 time= 0.07667\n",
      "训练次数: 13 Epoch: 0087 log_lik= 0.4433488 train_kl= -0.01121 train_loss= 0.45456 train_acc= 0.54248 val_roc= 0.93085 val_ap= 0.93900 time= 0.08962\n",
      "训练次数: 13 Epoch: 0088 log_lik= 0.44265896 train_kl= -0.01122 train_loss= 0.45388 train_acc= 0.54271 val_roc= 0.93104 val_ap= 0.93903 time= 0.07568\n",
      "训练次数: 13 Epoch: 0089 log_lik= 0.4423761 train_kl= -0.01124 train_loss= 0.45362 train_acc= 0.54324 val_roc= 0.93092 val_ap= 0.93890 time= 0.08066\n",
      "训练次数: 13 Epoch: 0090 log_lik= 0.44183594 train_kl= -0.01127 train_loss= 0.45310 train_acc= 0.54384 val_roc= 0.93110 val_ap= 0.93939 time= 0.07369\n",
      "训练次数: 13 Epoch: 0091 log_lik= 0.44096142 train_kl= -0.01129 train_loss= 0.45225 train_acc= 0.54423 val_roc= 0.93110 val_ap= 0.93958 time= 0.07468\n",
      "训练次数: 13 Epoch: 0092 log_lik= 0.44098508 train_kl= -0.01130 train_loss= 0.45229 train_acc= 0.54331 val_roc= 0.93094 val_ap= 0.93959 time= 0.07667\n",
      "训练次数: 13 Epoch: 0093 log_lik= 0.44047466 train_kl= -0.01131 train_loss= 0.45179 train_acc= 0.54471 val_roc= 0.93140 val_ap= 0.93989 time= 0.07568\n",
      "训练次数: 13 Epoch: 0094 log_lik= 0.44027954 train_kl= -0.01133 train_loss= 0.45160 train_acc= 0.54402 val_roc= 0.93166 val_ap= 0.93989 time= 0.07468\n",
      "训练次数: 13 Epoch: 0095 log_lik= 0.43937635 train_kl= -0.01134 train_loss= 0.45072 train_acc= 0.54395 val_roc= 0.93238 val_ap= 0.94056 time= 0.07866\n",
      "训练次数: 13 Epoch: 0096 log_lik= 0.43977904 train_kl= -0.01136 train_loss= 0.45114 train_acc= 0.54471 val_roc= 0.93217 val_ap= 0.94052 time= 0.07620\n",
      "训练次数: 13 Epoch: 0097 log_lik= 0.43856445 train_kl= -0.01137 train_loss= 0.44993 train_acc= 0.54543 val_roc= 0.93209 val_ap= 0.94067 time= 0.07468\n",
      "训练次数: 13 Epoch: 0098 log_lik= 0.4388694 train_kl= -0.01138 train_loss= 0.45025 train_acc= 0.54624 val_roc= 0.93192 val_ap= 0.94072 time= 0.07468\n",
      "训练次数: 13 Epoch: 0099 log_lik= 0.4384796 train_kl= -0.01139 train_loss= 0.44987 train_acc= 0.54672 val_roc= 0.93198 val_ap= 0.94073 time= 0.07867\n",
      "训练次数: 13 Epoch: 0100 log_lik= 0.4377456 train_kl= -0.01141 train_loss= 0.44916 train_acc= 0.54672 val_roc= 0.93195 val_ap= 0.94080 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 13 ROC score: 0.9171926590309258\n",
      "训练次数: 13 AP score: 0.9238636422359128\n",
      "训练次数: 14 Epoch: 0001 log_lik= 1.778219 train_kl= -0.00006 train_loss= 1.77827 train_acc= 0.49532 val_roc= 0.65907 val_ap= 0.67329 time= 1.45667\n",
      "训练次数: 14 Epoch: 0002 log_lik= 1.3741965 train_kl= -0.00022 train_loss= 1.37442 train_acc= 0.46192 val_roc= 0.64248 val_ap= 0.66330 time= 0.09952\n",
      "训练次数: 14 Epoch: 0003 log_lik= 1.1705865 train_kl= -0.00072 train_loss= 1.17130 train_acc= 0.38923 val_roc= 0.64097 val_ap= 0.66273 time= 0.08165\n",
      "训练次数: 14 Epoch: 0004 log_lik= 1.0290607 train_kl= -0.00136 train_loss= 1.03042 train_acc= 0.32979 val_roc= 0.64556 val_ap= 0.66679 time= 0.09560\n",
      "训练次数: 14 Epoch: 0005 log_lik= 0.92539406 train_kl= -0.00208 train_loss= 0.92748 train_acc= 0.29089 val_roc= 0.65774 val_ap= 0.67752 time= 0.07468\n",
      "训练次数: 14 Epoch: 0006 log_lik= 0.8060573 train_kl= -0.00287 train_loss= 0.80893 train_acc= 0.28551 val_roc= 0.68944 val_ap= 0.70904 time= 0.07619\n",
      "训练次数: 14 Epoch: 0007 log_lik= 0.7454468 train_kl= -0.00372 train_loss= 0.74916 train_acc= 0.30513 val_roc= 0.74413 val_ap= 0.76116 time= 0.07667\n",
      "训练次数: 14 Epoch: 0008 log_lik= 0.7088885 train_kl= -0.00463 train_loss= 0.71352 train_acc= 0.33504 val_roc= 0.77350 val_ap= 0.78127 time= 0.07269\n",
      "训练次数: 14 Epoch: 0009 log_lik= 0.6929011 train_kl= -0.00561 train_loss= 0.69851 train_acc= 0.32358 val_roc= 0.75516 val_ap= 0.76046 time= 0.07369\n",
      "训练次数: 14 Epoch: 0010 log_lik= 0.68025035 train_kl= -0.00660 train_loss= 0.68685 train_acc= 0.23855 val_roc= 0.76920 val_ap= 0.77346 time= 0.07767\n",
      "训练次数: 14 Epoch: 0011 log_lik= 0.6723073 train_kl= -0.00753 train_loss= 0.67984 train_acc= 0.20271 val_roc= 0.79754 val_ap= 0.79892 time= 0.07568\n",
      "训练次数: 14 Epoch: 0012 log_lik= 0.6520865 train_kl= -0.00837 train_loss= 0.66045 train_acc= 0.22054 val_roc= 0.81246 val_ap= 0.80309 time= 0.07767\n",
      "训练次数: 14 Epoch: 0013 log_lik= 0.6327703 train_kl= -0.00912 train_loss= 0.64189 train_acc= 0.26772 val_roc= 0.82014 val_ap= 0.80498 time= 0.07767\n",
      "训练次数: 14 Epoch: 0014 log_lik= 0.61211264 train_kl= -0.00979 train_loss= 0.62190 train_acc= 0.32326 val_roc= 0.82559 val_ap= 0.81141 time= 0.07369\n",
      "训练次数: 14 Epoch: 0015 log_lik= 0.59270555 train_kl= -0.01039 train_loss= 0.60310 train_acc= 0.38586 val_roc= 0.82848 val_ap= 0.81398 time= 0.07369\n",
      "训练次数: 14 Epoch: 0016 log_lik= 0.58470327 train_kl= -0.01093 train_loss= 0.59564 train_acc= 0.42960 val_roc= 0.83449 val_ap= 0.81761 time= 0.07667\n",
      "训练次数: 14 Epoch: 0017 log_lik= 0.5734929 train_kl= -0.01139 train_loss= 0.58488 train_acc= 0.45893 val_roc= 0.83895 val_ap= 0.82099 time= 0.08427\n",
      "训练次数: 14 Epoch: 0018 log_lik= 0.56531036 train_kl= -0.01177 train_loss= 0.57708 train_acc= 0.48155 val_roc= 0.84726 val_ap= 0.83073 time= 0.07468\n",
      "训练次数: 14 Epoch: 0019 log_lik= 0.55473727 train_kl= -0.01210 train_loss= 0.56683 train_acc= 0.49452 val_roc= 0.85677 val_ap= 0.84156 time= 0.07568\n",
      "训练次数: 14 Epoch: 0020 log_lik= 0.5499954 train_kl= -0.01236 train_loss= 0.56236 train_acc= 0.49461 val_roc= 0.86409 val_ap= 0.84905 time= 0.07468\n",
      "训练次数: 14 Epoch: 0021 log_lik= 0.547108 train_kl= -0.01256 train_loss= 0.55967 train_acc= 0.48606 val_roc= 0.87004 val_ap= 0.85567 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 14 Epoch: 0022 log_lik= 0.54538155 train_kl= -0.01270 train_loss= 0.55808 train_acc= 0.47731 val_roc= 0.87363 val_ap= 0.85919 time= 0.07867\n",
      "训练次数: 14 Epoch: 0023 log_lik= 0.54016465 train_kl= -0.01279 train_loss= 0.55296 train_acc= 0.47949 val_roc= 0.87675 val_ap= 0.86226 time= 0.08165\n",
      "训练次数: 14 Epoch: 0024 log_lik= 0.53118503 train_kl= -0.01284 train_loss= 0.54403 train_acc= 0.48544 val_roc= 0.87789 val_ap= 0.86353 time= 0.07474\n",
      "训练次数: 14 Epoch: 0025 log_lik= 0.52350205 train_kl= -0.01286 train_loss= 0.53637 train_acc= 0.49348 val_roc= 0.87911 val_ap= 0.86502 time= 0.07667\n",
      "训练次数: 14 Epoch: 0026 log_lik= 0.51866186 train_kl= -0.01287 train_loss= 0.53153 train_acc= 0.49728 val_roc= 0.88133 val_ap= 0.86727 time= 0.07667\n",
      "训练次数: 14 Epoch: 0027 log_lik= 0.51484364 train_kl= -0.01286 train_loss= 0.52771 train_acc= 0.49750 val_roc= 0.88311 val_ap= 0.86926 time= 0.07468\n",
      "训练次数: 14 Epoch: 0028 log_lik= 0.5110232 train_kl= -0.01285 train_loss= 0.52388 train_acc= 0.49442 val_roc= 0.88414 val_ap= 0.87145 time= 0.07767\n",
      "训练次数: 14 Epoch: 0029 log_lik= 0.50938255 train_kl= -0.01284 train_loss= 0.52222 train_acc= 0.49127 val_roc= 0.88405 val_ap= 0.87239 time= 0.07567\n",
      "训练次数: 14 Epoch: 0030 log_lik= 0.5059554 train_kl= -0.01282 train_loss= 0.51877 train_acc= 0.49175 val_roc= 0.88395 val_ap= 0.87352 time= 0.07667\n",
      "训练次数: 14 Epoch: 0031 log_lik= 0.50226676 train_kl= -0.01279 train_loss= 0.51505 train_acc= 0.49272 val_roc= 0.88534 val_ap= 0.87715 time= 0.07568\n",
      "训练次数: 14 Epoch: 0032 log_lik= 0.4962606 train_kl= -0.01274 train_loss= 0.50900 train_acc= 0.49783 val_roc= 0.88725 val_ap= 0.88142 time= 0.08265\n",
      "训练次数: 14 Epoch: 0033 log_lik= 0.48896304 train_kl= -0.01268 train_loss= 0.50165 train_acc= 0.50556 val_roc= 0.88835 val_ap= 0.88441 time= 0.07468\n",
      "训练次数: 14 Epoch: 0034 log_lik= 0.48497054 train_kl= -0.01263 train_loss= 0.49760 train_acc= 0.51082 val_roc= 0.88973 val_ap= 0.88748 time= 0.07269\n",
      "训练次数: 14 Epoch: 0035 log_lik= 0.48316646 train_kl= -0.01258 train_loss= 0.49574 train_acc= 0.51004 val_roc= 0.89095 val_ap= 0.89039 time= 0.08265\n",
      "训练次数: 14 Epoch: 0036 log_lik= 0.4820545 train_kl= -0.01253 train_loss= 0.49459 train_acc= 0.50890 val_roc= 0.89154 val_ap= 0.89262 time= 0.07468\n",
      "训练次数: 14 Epoch: 0037 log_lik= 0.4814714 train_kl= -0.01249 train_loss= 0.49396 train_acc= 0.50761 val_roc= 0.89287 val_ap= 0.89545 time= 0.07966\n",
      "训练次数: 14 Epoch: 0038 log_lik= 0.4796167 train_kl= -0.01245 train_loss= 0.49206 train_acc= 0.50723 val_roc= 0.89371 val_ap= 0.89752 time= 0.07668\n",
      "训练次数: 14 Epoch: 0039 log_lik= 0.47822568 train_kl= -0.01239 train_loss= 0.49062 train_acc= 0.50851 val_roc= 0.89536 val_ap= 0.89982 time= 0.07369\n",
      "训练次数: 14 Epoch: 0040 log_lik= 0.47560552 train_kl= -0.01232 train_loss= 0.48793 train_acc= 0.51106 val_roc= 0.89675 val_ap= 0.90178 time= 0.08862\n",
      "训练次数: 14 Epoch: 0041 log_lik= 0.47282466 train_kl= -0.01224 train_loss= 0.48507 train_acc= 0.51321 val_roc= 0.89915 val_ap= 0.90490 time= 0.07468\n",
      "训练次数: 14 Epoch: 0042 log_lik= 0.47052974 train_kl= -0.01215 train_loss= 0.48268 train_acc= 0.51437 val_roc= 0.90076 val_ap= 0.90741 time= 0.07368\n",
      "训练次数: 14 Epoch: 0043 log_lik= 0.4683641 train_kl= -0.01205 train_loss= 0.48042 train_acc= 0.51561 val_roc= 0.90247 val_ap= 0.91044 time= 0.07668\n",
      "训练次数: 14 Epoch: 0044 log_lik= 0.46664605 train_kl= -0.01195 train_loss= 0.47860 train_acc= 0.51584 val_roc= 0.90361 val_ap= 0.91195 time= 0.07568\n",
      "训练次数: 14 Epoch: 0045 log_lik= 0.4653432 train_kl= -0.01185 train_loss= 0.47719 train_acc= 0.51567 val_roc= 0.90532 val_ap= 0.91381 time= 0.07468\n",
      "训练次数: 14 Epoch: 0046 log_lik= 0.46417937 train_kl= -0.01175 train_loss= 0.47593 train_acc= 0.51530 val_roc= 0.90713 val_ap= 0.91601 time= 0.07568\n",
      "训练次数: 14 Epoch: 0047 log_lik= 0.46346676 train_kl= -0.01166 train_loss= 0.47513 train_acc= 0.51498 val_roc= 0.90876 val_ap= 0.91788 time= 0.07568\n",
      "训练次数: 14 Epoch: 0048 log_lik= 0.46260634 train_kl= -0.01158 train_loss= 0.47419 train_acc= 0.51594 val_roc= 0.91035 val_ap= 0.91984 time= 0.07568\n",
      "训练次数: 14 Epoch: 0049 log_lik= 0.46117496 train_kl= -0.01152 train_loss= 0.47269 train_acc= 0.51716 val_roc= 0.91207 val_ap= 0.92140 time= 0.07468\n",
      "训练次数: 14 Epoch: 0050 log_lik= 0.45972753 train_kl= -0.01146 train_loss= 0.47118 train_acc= 0.51765 val_roc= 0.91407 val_ap= 0.92340 time= 0.07667\n",
      "训练次数: 14 Epoch: 0051 log_lik= 0.45883876 train_kl= -0.01140 train_loss= 0.47024 train_acc= 0.51800 val_roc= 0.91558 val_ap= 0.92472 time= 0.07369\n",
      "训练次数: 14 Epoch: 0052 log_lik= 0.45820022 train_kl= -0.01136 train_loss= 0.46956 train_acc= 0.51769 val_roc= 0.91662 val_ap= 0.92557 time= 0.07667\n",
      "训练次数: 14 Epoch: 0053 log_lik= 0.45780602 train_kl= -0.01132 train_loss= 0.46912 train_acc= 0.51720 val_roc= 0.91827 val_ap= 0.92717 time= 0.07369\n",
      "训练次数: 14 Epoch: 0054 log_lik= 0.45722687 train_kl= -0.01128 train_loss= 0.46851 train_acc= 0.51760 val_roc= 0.92012 val_ap= 0.92865 time= 0.07369\n",
      "训练次数: 14 Epoch: 0055 log_lik= 0.45609844 train_kl= -0.01125 train_loss= 0.46735 train_acc= 0.51689 val_roc= 0.92197 val_ap= 0.93024 time= 0.07568\n",
      "训练次数: 14 Epoch: 0056 log_lik= 0.45519787 train_kl= -0.01123 train_loss= 0.46642 train_acc= 0.51767 val_roc= 0.92322 val_ap= 0.93119 time= 0.08862\n",
      "训练次数: 14 Epoch: 0057 log_lik= 0.45348585 train_kl= -0.01120 train_loss= 0.46469 train_acc= 0.51887 val_roc= 0.92420 val_ap= 0.93214 time= 0.07369\n",
      "训练次数: 14 Epoch: 0058 log_lik= 0.45304605 train_kl= -0.01119 train_loss= 0.46423 train_acc= 0.51926 val_roc= 0.92528 val_ap= 0.93330 time= 0.07369\n",
      "训练次数: 14 Epoch: 0059 log_lik= 0.45271772 train_kl= -0.01117 train_loss= 0.46389 train_acc= 0.51863 val_roc= 0.92591 val_ap= 0.93381 time= 0.07568\n",
      "训练次数: 14 Epoch: 0060 log_lik= 0.451837 train_kl= -0.01117 train_loss= 0.46300 train_acc= 0.51899 val_roc= 0.92672 val_ap= 0.93450 time= 0.07468\n",
      "训练次数: 14 Epoch: 0061 log_lik= 0.45139024 train_kl= -0.01116 train_loss= 0.46255 train_acc= 0.51959 val_roc= 0.92792 val_ap= 0.93579 time= 0.07667\n",
      "训练次数: 14 Epoch: 0062 log_lik= 0.45100355 train_kl= -0.01115 train_loss= 0.46216 train_acc= 0.51947 val_roc= 0.92805 val_ap= 0.93608 time= 0.07369\n",
      "训练次数: 14 Epoch: 0063 log_lik= 0.4502077 train_kl= -0.01115 train_loss= 0.46136 train_acc= 0.51973 val_roc= 0.92810 val_ap= 0.93600 time= 0.07369\n",
      "训练次数: 14 Epoch: 0064 log_lik= 0.44945776 train_kl= -0.01115 train_loss= 0.46061 train_acc= 0.51905 val_roc= 0.92858 val_ap= 0.93666 time= 0.07550\n",
      "训练次数: 14 Epoch: 0065 log_lik= 0.44940948 train_kl= -0.01115 train_loss= 0.46056 train_acc= 0.51995 val_roc= 0.92899 val_ap= 0.93721 time= 0.07321\n",
      "训练次数: 14 Epoch: 0066 log_lik= 0.44880542 train_kl= -0.01116 train_loss= 0.45997 train_acc= 0.52015 val_roc= 0.92909 val_ap= 0.93720 time= 0.07468\n",
      "训练次数: 14 Epoch: 0067 log_lik= 0.44783765 train_kl= -0.01117 train_loss= 0.45900 train_acc= 0.52028 val_roc= 0.92893 val_ap= 0.93671 time= 0.07468\n",
      "训练次数: 14 Epoch: 0068 log_lik= 0.44724426 train_kl= -0.01117 train_loss= 0.45842 train_acc= 0.52021 val_roc= 0.92874 val_ap= 0.93628 time= 0.07468\n",
      "训练次数: 14 Epoch: 0069 log_lik= 0.44697806 train_kl= -0.01118 train_loss= 0.45816 train_acc= 0.52078 val_roc= 0.92890 val_ap= 0.93658 time= 0.07867\n",
      "训练次数: 14 Epoch: 0070 log_lik= 0.44671953 train_kl= -0.01120 train_loss= 0.45792 train_acc= 0.51988 val_roc= 0.92877 val_ap= 0.93654 time= 0.08265\n",
      "训练次数: 14 Epoch: 0071 log_lik= 0.4459936 train_kl= -0.01121 train_loss= 0.45720 train_acc= 0.52026 val_roc= 0.92862 val_ap= 0.93665 time= 0.07767\n",
      "训练次数: 14 Epoch: 0072 log_lik= 0.44532374 train_kl= -0.01123 train_loss= 0.45655 train_acc= 0.52065 val_roc= 0.92826 val_ap= 0.93613 time= 0.08265\n",
      "训练次数: 14 Epoch: 0073 log_lik= 0.4451111 train_kl= -0.01125 train_loss= 0.45636 train_acc= 0.52009 val_roc= 0.92799 val_ap= 0.93604 time= 0.07468\n",
      "训练次数: 14 Epoch: 0074 log_lik= 0.44437975 train_kl= -0.01127 train_loss= 0.45565 train_acc= 0.52005 val_roc= 0.92767 val_ap= 0.93543 time= 0.07369\n",
      "训练次数: 14 Epoch: 0075 log_lik= 0.4444815 train_kl= -0.01130 train_loss= 0.45578 train_acc= 0.52105 val_roc= 0.92773 val_ap= 0.93528 time= 0.07568\n",
      "训练次数: 14 Epoch: 0076 log_lik= 0.44380397 train_kl= -0.01132 train_loss= 0.45512 train_acc= 0.52093 val_roc= 0.92755 val_ap= 0.93550 time= 0.07667\n",
      "训练次数: 14 Epoch: 0077 log_lik= 0.44320425 train_kl= -0.01134 train_loss= 0.45455 train_acc= 0.52186 val_roc= 0.92776 val_ap= 0.93594 time= 0.07369\n",
      "训练次数: 14 Epoch: 0078 log_lik= 0.4429327 train_kl= -0.01136 train_loss= 0.45430 train_acc= 0.52194 val_roc= 0.92742 val_ap= 0.93581 time= 0.08265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 14 Epoch: 0079 log_lik= 0.44285962 train_kl= -0.01139 train_loss= 0.45425 train_acc= 0.52080 val_roc= 0.92719 val_ap= 0.93499 time= 0.07767\n",
      "训练次数: 14 Epoch: 0080 log_lik= 0.44215778 train_kl= -0.01141 train_loss= 0.45357 train_acc= 0.52169 val_roc= 0.92689 val_ap= 0.93441 time= 0.07568\n",
      "训练次数: 14 Epoch: 0081 log_lik= 0.44183698 train_kl= -0.01143 train_loss= 0.45327 train_acc= 0.52204 val_roc= 0.92667 val_ap= 0.93415 time= 0.07668\n",
      "训练次数: 14 Epoch: 0082 log_lik= 0.4415786 train_kl= -0.01146 train_loss= 0.45304 train_acc= 0.52189 val_roc= 0.92692 val_ap= 0.93469 time= 0.08369\n",
      "训练次数: 14 Epoch: 0083 log_lik= 0.4411225 train_kl= -0.01148 train_loss= 0.45260 train_acc= 0.52207 val_roc= 0.92738 val_ap= 0.93534 time= 0.07368\n",
      "训练次数: 14 Epoch: 0084 log_lik= 0.44068092 train_kl= -0.01150 train_loss= 0.45218 train_acc= 0.52170 val_roc= 0.92724 val_ap= 0.93519 time= 0.07966\n",
      "训练次数: 14 Epoch: 0085 log_lik= 0.4404124 train_kl= -0.01153 train_loss= 0.45194 train_acc= 0.52235 val_roc= 0.92702 val_ap= 0.93443 time= 0.07468\n",
      "训练次数: 14 Epoch: 0086 log_lik= 0.43957132 train_kl= -0.01155 train_loss= 0.45112 train_acc= 0.52277 val_roc= 0.92685 val_ap= 0.93353 time= 0.07468\n",
      "训练次数: 14 Epoch: 0087 log_lik= 0.43942332 train_kl= -0.01157 train_loss= 0.45100 train_acc= 0.52257 val_roc= 0.92699 val_ap= 0.93373 time= 0.07369\n",
      "训练次数: 14 Epoch: 0088 log_lik= 0.4388473 train_kl= -0.01159 train_loss= 0.45044 train_acc= 0.52348 val_roc= 0.92735 val_ap= 0.93399 time= 0.07684\n",
      "训练次数: 14 Epoch: 0089 log_lik= 0.4386558 train_kl= -0.01162 train_loss= 0.45027 train_acc= 0.52403 val_roc= 0.92748 val_ap= 0.93453 time= 0.08265\n",
      "训练次数: 14 Epoch: 0090 log_lik= 0.43828514 train_kl= -0.01163 train_loss= 0.44992 train_acc= 0.52339 val_roc= 0.92754 val_ap= 0.93430 time= 0.07468\n",
      "训练次数: 14 Epoch: 0091 log_lik= 0.43799087 train_kl= -0.01165 train_loss= 0.44964 train_acc= 0.52373 val_roc= 0.92734 val_ap= 0.93358 time= 0.07520\n",
      "训练次数: 14 Epoch: 0092 log_lik= 0.43820637 train_kl= -0.01166 train_loss= 0.44987 train_acc= 0.52448 val_roc= 0.92718 val_ap= 0.93274 time= 0.07667\n",
      "训练次数: 14 Epoch: 0093 log_lik= 0.4378337 train_kl= -0.01168 train_loss= 0.44951 train_acc= 0.52435 val_roc= 0.92732 val_ap= 0.93338 time= 0.08962\n",
      "训练次数: 14 Epoch: 0094 log_lik= 0.43678367 train_kl= -0.01170 train_loss= 0.44849 train_acc= 0.52536 val_roc= 0.92751 val_ap= 0.93398 time= 0.07368\n",
      "训练次数: 14 Epoch: 0095 log_lik= 0.43699074 train_kl= -0.01172 train_loss= 0.44871 train_acc= 0.52509 val_roc= 0.92745 val_ap= 0.93378 time= 0.07767\n",
      "训练次数: 14 Epoch: 0096 log_lik= 0.43636274 train_kl= -0.01174 train_loss= 0.44810 train_acc= 0.52560 val_roc= 0.92700 val_ap= 0.93265 time= 0.07369\n",
      "训练次数: 14 Epoch: 0097 log_lik= 0.43591982 train_kl= -0.01175 train_loss= 0.44767 train_acc= 0.52576 val_roc= 0.92700 val_ap= 0.93216 time= 0.07369\n",
      "训练次数: 14 Epoch: 0098 log_lik= 0.43610576 train_kl= -0.01176 train_loss= 0.44786 train_acc= 0.52607 val_roc= 0.92711 val_ap= 0.93266 time= 0.07369\n",
      "训练次数: 14 Epoch: 0099 log_lik= 0.43575037 train_kl= -0.01177 train_loss= 0.44752 train_acc= 0.52700 val_roc= 0.92740 val_ap= 0.93349 time= 0.07369\n",
      "训练次数: 14 Epoch: 0100 log_lik= 0.43529412 train_kl= -0.01179 train_loss= 0.44709 train_acc= 0.52735 val_roc= 0.92751 val_ap= 0.93348 time= 0.08165\n",
      "Optimization Finished!\n",
      "训练次数: 14 ROC score: 0.9138152659607028\n",
      "训练次数: 14 AP score: 0.9247205785740147\n",
      "训练次数: 15 Epoch: 0001 log_lik= 1.7671572 train_kl= -0.00005 train_loss= 1.76720 train_acc= 0.49347 val_roc= 0.70403 val_ap= 0.72612 time= 1.52362\n",
      "训练次数: 15 Epoch: 0002 log_lik= 1.4097933 train_kl= -0.00025 train_loss= 1.41004 train_acc= 0.45831 val_roc= 0.70128 val_ap= 0.72517 time= 0.08464\n",
      "训练次数: 15 Epoch: 0003 log_lik= 1.2523384 train_kl= -0.00071 train_loss= 1.25305 train_acc= 0.39781 val_roc= 0.71351 val_ap= 0.73369 time= 0.07667\n",
      "训练次数: 15 Epoch: 0004 log_lik= 1.059399 train_kl= -0.00119 train_loss= 1.06059 train_acc= 0.37420 val_roc= 0.73884 val_ap= 0.75125 time= 0.07369\n",
      "训练次数: 15 Epoch: 0005 log_lik= 0.8979285 train_kl= -0.00180 train_loss= 0.89973 train_acc= 0.38828 val_roc= 0.75887 val_ap= 0.76477 time= 0.07369\n",
      "训练次数: 15 Epoch: 0006 log_lik= 0.7924075 train_kl= -0.00263 train_loss= 0.79503 train_acc= 0.39982 val_roc= 0.74457 val_ap= 0.74902 time= 0.07867\n",
      "训练次数: 15 Epoch: 0007 log_lik= 0.7430669 train_kl= -0.00367 train_loss= 0.74674 train_acc= 0.36520 val_roc= 0.74038 val_ap= 0.75499 time= 0.07468\n",
      "训练次数: 15 Epoch: 0008 log_lik= 0.71828526 train_kl= -0.00484 train_loss= 0.72313 train_acc= 0.27900 val_roc= 0.76607 val_ap= 0.77984 time= 0.08863\n",
      "训练次数: 15 Epoch: 0009 log_lik= 0.7091712 train_kl= -0.00601 train_loss= 0.71518 train_acc= 0.19466 val_roc= 0.80902 val_ap= 0.81502 time= 0.07468\n",
      "训练次数: 15 Epoch: 0010 log_lik= 0.68797576 train_kl= -0.00711 train_loss= 0.69509 train_acc= 0.20097 val_roc= 0.83376 val_ap= 0.82754 time= 0.07468\n",
      "训练次数: 15 Epoch: 0011 log_lik= 0.6768415 train_kl= -0.00815 train_loss= 0.68499 train_acc= 0.20521 val_roc= 0.83491 val_ap= 0.82607 time= 0.07469\n",
      "训练次数: 15 Epoch: 0012 log_lik= 0.6628208 train_kl= -0.00912 train_loss= 0.67194 train_acc= 0.20854 val_roc= 0.84701 val_ap= 0.83701 time= 0.07269\n",
      "训练次数: 15 Epoch: 0013 log_lik= 0.64382106 train_kl= -0.01002 train_loss= 0.65384 train_acc= 0.23421 val_roc= 0.85391 val_ap= 0.84373 time= 0.07867\n",
      "训练次数: 15 Epoch: 0014 log_lik= 0.61836874 train_kl= -0.01084 train_loss= 0.62921 train_acc= 0.30385 val_roc= 0.85460 val_ap= 0.84853 time= 0.07667\n",
      "训练次数: 15 Epoch: 0015 log_lik= 0.594779 train_kl= -0.01158 train_loss= 0.60636 train_acc= 0.37652 val_roc= 0.85942 val_ap= 0.85329 time= 0.07369\n",
      "训练次数: 15 Epoch: 0016 log_lik= 0.57469296 train_kl= -0.01226 train_loss= 0.58695 train_acc= 0.42299 val_roc= 0.86412 val_ap= 0.85765 time= 0.07369\n",
      "训练次数: 15 Epoch: 0017 log_lik= 0.5654778 train_kl= -0.01287 train_loss= 0.57835 train_acc= 0.44079 val_roc= 0.86303 val_ap= 0.85795 time= 0.07668\n",
      "训练次数: 15 Epoch: 0018 log_lik= 0.5556365 train_kl= -0.01341 train_loss= 0.56904 train_acc= 0.46000 val_roc= 0.86099 val_ap= 0.85768 time= 0.07369\n",
      "训练次数: 15 Epoch: 0019 log_lik= 0.5501398 train_kl= -0.01386 train_loss= 0.56400 train_acc= 0.48436 val_roc= 0.86325 val_ap= 0.86108 time= 0.07568\n",
      "训练次数: 15 Epoch: 0020 log_lik= 0.5483814 train_kl= -0.01423 train_loss= 0.56261 train_acc= 0.49672 val_roc= 0.86925 val_ap= 0.86787 time= 0.07568\n",
      "训练次数: 15 Epoch: 0021 log_lik= 0.5457782 train_kl= -0.01452 train_loss= 0.56030 train_acc= 0.50280 val_roc= 0.87687 val_ap= 0.87637 time= 0.07369\n",
      "训练次数: 15 Epoch: 0022 log_lik= 0.5391909 train_kl= -0.01475 train_loss= 0.55394 train_acc= 0.50378 val_roc= 0.88077 val_ap= 0.88099 time= 0.07667\n",
      "训练次数: 15 Epoch: 0023 log_lik= 0.53170335 train_kl= -0.01491 train_loss= 0.54661 train_acc= 0.50804 val_roc= 0.88292 val_ap= 0.88312 time= 0.07469\n",
      "训练次数: 15 Epoch: 0024 log_lik= 0.5209717 train_kl= -0.01502 train_loss= 0.53599 train_acc= 0.52112 val_roc= 0.88242 val_ap= 0.88326 time= 0.07369\n",
      "训练次数: 15 Epoch: 0025 log_lik= 0.51700383 train_kl= -0.01510 train_loss= 0.53210 train_acc= 0.52291 val_roc= 0.88301 val_ap= 0.88447 time= 0.07767\n",
      "训练次数: 15 Epoch: 0026 log_lik= 0.51503944 train_kl= -0.01514 train_loss= 0.53018 train_acc= 0.51808 val_roc= 0.88368 val_ap= 0.88475 time= 0.08066\n",
      "训练次数: 15 Epoch: 0027 log_lik= 0.51137745 train_kl= -0.01514 train_loss= 0.52652 train_acc= 0.51970 val_roc= 0.88385 val_ap= 0.88484 time= 0.07369\n",
      "训练次数: 15 Epoch: 0028 log_lik= 0.50711405 train_kl= -0.01512 train_loss= 0.52223 train_acc= 0.52383 val_roc= 0.88375 val_ap= 0.88578 time= 0.07668\n",
      "训练次数: 15 Epoch: 0029 log_lik= 0.50235325 train_kl= -0.01508 train_loss= 0.51743 train_acc= 0.52421 val_roc= 0.88433 val_ap= 0.88858 time= 0.07667\n",
      "训练次数: 15 Epoch: 0030 log_lik= 0.5010022 train_kl= -0.01503 train_loss= 0.51603 train_acc= 0.51904 val_roc= 0.88690 val_ap= 0.89244 time= 0.07668\n",
      "训练次数: 15 Epoch: 0031 log_lik= 0.5003877 train_kl= -0.01498 train_loss= 0.51536 train_acc= 0.51296 val_roc= 0.88995 val_ap= 0.89608 time= 0.07468\n",
      "训练次数: 15 Epoch: 0032 log_lik= 0.49815136 train_kl= -0.01491 train_loss= 0.51306 train_acc= 0.51593 val_roc= 0.89252 val_ap= 0.89900 time= 0.07966\n",
      "训练次数: 15 Epoch: 0033 log_lik= 0.49311972 train_kl= -0.01485 train_loss= 0.50797 train_acc= 0.52089 val_roc= 0.89440 val_ap= 0.90099 time= 0.07468\n",
      "训练次数: 15 Epoch: 0034 log_lik= 0.4886712 train_kl= -0.01479 train_loss= 0.50346 train_acc= 0.52639 val_roc= 0.89631 val_ap= 0.90261 time= 0.07568\n",
      "训练次数: 15 Epoch: 0035 log_lik= 0.48490852 train_kl= -0.01473 train_loss= 0.49964 train_acc= 0.52634 val_roc= 0.89764 val_ap= 0.90303 time= 0.07468\n",
      "训练次数: 15 Epoch: 0036 log_lik= 0.48262966 train_kl= -0.01469 train_loss= 0.49732 train_acc= 0.52401 val_roc= 0.89974 val_ap= 0.90455 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 15 Epoch: 0037 log_lik= 0.47997835 train_kl= -0.01464 train_loss= 0.49462 train_acc= 0.52535 val_roc= 0.90061 val_ap= 0.90519 time= 0.07470\n",
      "训练次数: 15 Epoch: 0038 log_lik= 0.47686458 train_kl= -0.01458 train_loss= 0.49145 train_acc= 0.52917 val_roc= 0.90084 val_ap= 0.90466 time= 0.07468\n",
      "训练次数: 15 Epoch: 0039 log_lik= 0.4732429 train_kl= -0.01452 train_loss= 0.48777 train_acc= 0.53177 val_roc= 0.90156 val_ap= 0.90488 time= 0.08066\n",
      "训练次数: 15 Epoch: 0040 log_lik= 0.4720078 train_kl= -0.01446 train_loss= 0.48647 train_acc= 0.53029 val_roc= 0.90204 val_ap= 0.90486 time= 0.07668\n",
      "训练次数: 15 Epoch: 0041 log_lik= 0.47206035 train_kl= -0.01439 train_loss= 0.48645 train_acc= 0.53004 val_roc= 0.90231 val_ap= 0.90515 time= 0.07369\n",
      "训练次数: 15 Epoch: 0042 log_lik= 0.4708301 train_kl= -0.01430 train_loss= 0.48513 train_acc= 0.53018 val_roc= 0.90231 val_ap= 0.90486 time= 0.07369\n",
      "训练次数: 15 Epoch: 0043 log_lik= 0.46921444 train_kl= -0.01420 train_loss= 0.48342 train_acc= 0.53419 val_roc= 0.90201 val_ap= 0.90514 time= 0.07767\n",
      "训练次数: 15 Epoch: 0044 log_lik= 0.46736538 train_kl= -0.01409 train_loss= 0.48146 train_acc= 0.53488 val_roc= 0.90124 val_ap= 0.90450 time= 0.07668\n",
      "训练次数: 15 Epoch: 0045 log_lik= 0.46659407 train_kl= -0.01398 train_loss= 0.48058 train_acc= 0.53559 val_roc= 0.90030 val_ap= 0.90405 time= 0.07369\n",
      "训练次数: 15 Epoch: 0046 log_lik= 0.46570042 train_kl= -0.01386 train_loss= 0.47956 train_acc= 0.53588 val_roc= 0.89977 val_ap= 0.90357 time= 0.07767\n",
      "训练次数: 15 Epoch: 0047 log_lik= 0.46488068 train_kl= -0.01374 train_loss= 0.47862 train_acc= 0.53728 val_roc= 0.89993 val_ap= 0.90396 time= 0.07468\n",
      "训练次数: 15 Epoch: 0048 log_lik= 0.46272585 train_kl= -0.01362 train_loss= 0.47634 train_acc= 0.53870 val_roc= 0.89943 val_ap= 0.90364 time= 0.07469\n",
      "训练次数: 15 Epoch: 0049 log_lik= 0.46149293 train_kl= -0.01349 train_loss= 0.47498 train_acc= 0.53808 val_roc= 0.89891 val_ap= 0.90241 time= 0.08165\n",
      "训练次数: 15 Epoch: 0050 log_lik= 0.46138582 train_kl= -0.01337 train_loss= 0.47475 train_acc= 0.53599 val_roc= 0.89786 val_ap= 0.90132 time= 0.08165\n",
      "训练次数: 15 Epoch: 0051 log_lik= 0.46079174 train_kl= -0.01325 train_loss= 0.47404 train_acc= 0.53479 val_roc= 0.89638 val_ap= 0.90040 time= 0.07468\n",
      "训练次数: 15 Epoch: 0052 log_lik= 0.4598038 train_kl= -0.01313 train_loss= 0.47293 train_acc= 0.53693 val_roc= 0.89602 val_ap= 0.90063 time= 0.07568\n",
      "训练次数: 15 Epoch: 0053 log_lik= 0.45898902 train_kl= -0.01302 train_loss= 0.47201 train_acc= 0.53692 val_roc= 0.89659 val_ap= 0.90105 time= 0.07667\n",
      "训练次数: 15 Epoch: 0054 log_lik= 0.45852014 train_kl= -0.01292 train_loss= 0.47144 train_acc= 0.53645 val_roc= 0.89764 val_ap= 0.90151 time= 0.07568\n",
      "训练次数: 15 Epoch: 0055 log_lik= 0.4581773 train_kl= -0.01284 train_loss= 0.47101 train_acc= 0.53480 val_roc= 0.89780 val_ap= 0.90163 time= 0.07767\n",
      "训练次数: 15 Epoch: 0056 log_lik= 0.4573761 train_kl= -0.01275 train_loss= 0.47013 train_acc= 0.53483 val_roc= 0.89758 val_ap= 0.90151 time= 0.07568\n",
      "训练次数: 15 Epoch: 0057 log_lik= 0.45617393 train_kl= -0.01267 train_loss= 0.46885 train_acc= 0.53605 val_roc= 0.89891 val_ap= 0.90330 time= 0.07667\n",
      "训练次数: 15 Epoch: 0058 log_lik= 0.45544887 train_kl= -0.01260 train_loss= 0.46805 train_acc= 0.53560 val_roc= 0.90032 val_ap= 0.90410 time= 0.07468\n",
      "训练次数: 15 Epoch: 0059 log_lik= 0.45478162 train_kl= -0.01254 train_loss= 0.46732 train_acc= 0.53521 val_roc= 0.90150 val_ap= 0.90519 time= 0.07568\n",
      "训练次数: 15 Epoch: 0060 log_lik= 0.4543403 train_kl= -0.01248 train_loss= 0.46682 train_acc= 0.53395 val_roc= 0.90212 val_ap= 0.90571 time= 0.07568\n",
      "训练次数: 15 Epoch: 0061 log_lik= 0.4536849 train_kl= -0.01243 train_loss= 0.46611 train_acc= 0.53464 val_roc= 0.90266 val_ap= 0.90656 time= 0.07568\n",
      "训练次数: 15 Epoch: 0062 log_lik= 0.45286605 train_kl= -0.01238 train_loss= 0.46524 train_acc= 0.53544 val_roc= 0.90367 val_ap= 0.90770 time= 0.07468\n",
      "训练次数: 15 Epoch: 0063 log_lik= 0.452654 train_kl= -0.01233 train_loss= 0.46498 train_acc= 0.53467 val_roc= 0.90506 val_ap= 0.90864 time= 0.07369\n",
      "训练次数: 15 Epoch: 0064 log_lik= 0.45156315 train_kl= -0.01228 train_loss= 0.46384 train_acc= 0.53543 val_roc= 0.90569 val_ap= 0.90872 time= 0.07867\n",
      "训练次数: 15 Epoch: 0065 log_lik= 0.4516683 train_kl= -0.01223 train_loss= 0.46389 train_acc= 0.53450 val_roc= 0.90609 val_ap= 0.90945 time= 0.07667\n",
      "训练次数: 15 Epoch: 0066 log_lik= 0.45074213 train_kl= -0.01217 train_loss= 0.46292 train_acc= 0.53475 val_roc= 0.90640 val_ap= 0.91012 time= 0.07568\n",
      "训练次数: 15 Epoch: 0067 log_lik= 0.45068702 train_kl= -0.01213 train_loss= 0.46281 train_acc= 0.53406 val_roc= 0.90676 val_ap= 0.91057 time= 0.07572\n",
      "训练次数: 15 Epoch: 0068 log_lik= 0.4503783 train_kl= -0.01209 train_loss= 0.46247 train_acc= 0.53521 val_roc= 0.90759 val_ap= 0.91135 time= 0.07468\n",
      "训练次数: 15 Epoch: 0069 log_lik= 0.44962507 train_kl= -0.01205 train_loss= 0.46168 train_acc= 0.53567 val_roc= 0.90828 val_ap= 0.91138 time= 0.07568\n",
      "训练次数: 15 Epoch: 0070 log_lik= 0.4492905 train_kl= -0.01202 train_loss= 0.46131 train_acc= 0.53340 val_roc= 0.90828 val_ap= 0.91137 time= 0.08265\n",
      "训练次数: 15 Epoch: 0071 log_lik= 0.4486668 train_kl= -0.01198 train_loss= 0.46065 train_acc= 0.53511 val_roc= 0.90801 val_ap= 0.91216 time= 0.08265\n",
      "训练次数: 15 Epoch: 0072 log_lik= 0.44835055 train_kl= -0.01195 train_loss= 0.46030 train_acc= 0.53382 val_roc= 0.90785 val_ap= 0.91292 time= 0.07468\n",
      "训练次数: 15 Epoch: 0073 log_lik= 0.4475593 train_kl= -0.01192 train_loss= 0.45948 train_acc= 0.53520 val_roc= 0.90834 val_ap= 0.91344 time= 0.07667\n",
      "训练次数: 15 Epoch: 0074 log_lik= 0.44716313 train_kl= -0.01190 train_loss= 0.45906 train_acc= 0.53553 val_roc= 0.90908 val_ap= 0.91355 time= 0.07668\n",
      "训练次数: 15 Epoch: 0075 log_lik= 0.4470398 train_kl= -0.01188 train_loss= 0.45892 train_acc= 0.53390 val_roc= 0.90903 val_ap= 0.91346 time= 0.07468\n",
      "训练次数: 15 Epoch: 0076 log_lik= 0.44612554 train_kl= -0.01187 train_loss= 0.45799 train_acc= 0.53605 val_roc= 0.90880 val_ap= 0.91361 time= 0.07667\n",
      "训练次数: 15 Epoch: 0077 log_lik= 0.44549474 train_kl= -0.01186 train_loss= 0.45736 train_acc= 0.53500 val_roc= 0.90851 val_ap= 0.91406 time= 0.07368\n",
      "训练次数: 15 Epoch: 0078 log_lik= 0.4453268 train_kl= -0.01185 train_loss= 0.45718 train_acc= 0.53507 val_roc= 0.90831 val_ap= 0.91382 time= 0.07867\n",
      "训练次数: 15 Epoch: 0079 log_lik= 0.44503456 train_kl= -0.01184 train_loss= 0.45688 train_acc= 0.53470 val_roc= 0.90883 val_ap= 0.91431 time= 0.07867\n",
      "训练次数: 15 Epoch: 0080 log_lik= 0.44467622 train_kl= -0.01183 train_loss= 0.45651 train_acc= 0.53552 val_roc= 0.90898 val_ap= 0.91458 time= 0.07568\n",
      "训练次数: 15 Epoch: 0081 log_lik= 0.4441963 train_kl= -0.01182 train_loss= 0.45602 train_acc= 0.53539 val_roc= 0.90919 val_ap= 0.91497 time= 0.07369\n",
      "训练次数: 15 Epoch: 0082 log_lik= 0.44438204 train_kl= -0.01182 train_loss= 0.45620 train_acc= 0.53612 val_roc= 0.90941 val_ap= 0.91530 time= 0.07468\n",
      "训练次数: 15 Epoch: 0083 log_lik= 0.44359946 train_kl= -0.01182 train_loss= 0.45542 train_acc= 0.53589 val_roc= 0.90901 val_ap= 0.91473 time= 0.07866\n",
      "训练次数: 15 Epoch: 0084 log_lik= 0.44257236 train_kl= -0.01182 train_loss= 0.45439 train_acc= 0.53665 val_roc= 0.90899 val_ap= 0.91460 time= 0.07369\n",
      "训练次数: 15 Epoch: 0085 log_lik= 0.44260627 train_kl= -0.01182 train_loss= 0.45442 train_acc= 0.53631 val_roc= 0.90970 val_ap= 0.91544 time= 0.08365\n",
      "训练次数: 15 Epoch: 0086 log_lik= 0.44171533 train_kl= -0.01181 train_loss= 0.45352 train_acc= 0.53790 val_roc= 0.91006 val_ap= 0.91606 time= 0.08264\n",
      "训练次数: 15 Epoch: 0087 log_lik= 0.4417795 train_kl= -0.01180 train_loss= 0.45358 train_acc= 0.53654 val_roc= 0.90970 val_ap= 0.91574 time= 0.07568\n",
      "训练次数: 15 Epoch: 0088 log_lik= 0.44179586 train_kl= -0.01179 train_loss= 0.45358 train_acc= 0.53650 val_roc= 0.90957 val_ap= 0.91537 time= 0.07867\n",
      "训练次数: 15 Epoch: 0089 log_lik= 0.44083068 train_kl= -0.01178 train_loss= 0.45261 train_acc= 0.53765 val_roc= 0.91012 val_ap= 0.91643 time= 0.07667\n",
      "训练次数: 15 Epoch: 0090 log_lik= 0.44072527 train_kl= -0.01177 train_loss= 0.45250 train_acc= 0.53870 val_roc= 0.91062 val_ap= 0.91683 time= 0.07369\n",
      "训练次数: 15 Epoch: 0091 log_lik= 0.44088578 train_kl= -0.01177 train_loss= 0.45265 train_acc= 0.53728 val_roc= 0.91028 val_ap= 0.91641 time= 0.08663\n",
      "训练次数: 15 Epoch: 0092 log_lik= 0.44018507 train_kl= -0.01177 train_loss= 0.45195 train_acc= 0.53795 val_roc= 0.90976 val_ap= 0.91564 time= 0.07568\n",
      "训练次数: 15 Epoch: 0093 log_lik= 0.44002625 train_kl= -0.01176 train_loss= 0.45179 train_acc= 0.53758 val_roc= 0.91003 val_ap= 0.91613 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 15 Epoch: 0094 log_lik= 0.43936077 train_kl= -0.01175 train_loss= 0.45111 train_acc= 0.53885 val_roc= 0.91035 val_ap= 0.91678 time= 0.07718\n",
      "训练次数: 15 Epoch: 0095 log_lik= 0.4390825 train_kl= -0.01175 train_loss= 0.45084 train_acc= 0.53817 val_roc= 0.91012 val_ap= 0.91653 time= 0.07574\n",
      "训练次数: 15 Epoch: 0096 log_lik= 0.43863913 train_kl= -0.01176 train_loss= 0.45040 train_acc= 0.53777 val_roc= 0.90937 val_ap= 0.91568 time= 0.07568\n",
      "训练次数: 15 Epoch: 0097 log_lik= 0.438526 train_kl= -0.01177 train_loss= 0.45029 train_acc= 0.53812 val_roc= 0.90945 val_ap= 0.91605 time= 0.07468\n",
      "训练次数: 15 Epoch: 0098 log_lik= 0.4381977 train_kl= -0.01178 train_loss= 0.44997 train_acc= 0.53918 val_roc= 0.91013 val_ap= 0.91727 time= 0.07867\n",
      "训练次数: 15 Epoch: 0099 log_lik= 0.43736967 train_kl= -0.01178 train_loss= 0.44915 train_acc= 0.53904 val_roc= 0.91002 val_ap= 0.91721 time= 0.07269\n",
      "训练次数: 15 Epoch: 0100 log_lik= 0.43762586 train_kl= -0.01179 train_loss= 0.44941 train_acc= 0.53864 val_roc= 0.90989 val_ap= 0.91698 time= 0.08365\n",
      "Optimization Finished!\n",
      "训练次数: 15 ROC score: 0.9192018118381587\n",
      "训练次数: 15 AP score: 0.9204466557349374\n",
      "训练次数: 16 Epoch: 0001 log_lik= 1.7039893 train_kl= -0.00003 train_loss= 1.70402 train_acc= 0.49939 val_roc= 0.70685 val_ap= 0.74166 time= 1.57607\n",
      "训练次数: 16 Epoch: 0002 log_lik= 1.3151406 train_kl= -0.00031 train_loss= 1.31545 train_acc= 0.46126 val_roc= 0.70390 val_ap= 0.73798 time= 0.08557\n",
      "训练次数: 16 Epoch: 0003 log_lik= 1.1159738 train_kl= -0.00108 train_loss= 1.11705 train_acc= 0.33840 val_roc= 0.71163 val_ap= 0.74484 time= 0.07667\n",
      "训练次数: 16 Epoch: 0004 log_lik= 0.9586748 train_kl= -0.00170 train_loss= 0.96037 train_acc= 0.30585 val_roc= 0.72026 val_ap= 0.75282 time= 0.07867\n",
      "训练次数: 16 Epoch: 0005 log_lik= 0.8481889 train_kl= -0.00232 train_loss= 0.85051 train_acc= 0.29853 val_roc= 0.73062 val_ap= 0.76232 time= 0.07568\n",
      "训练次数: 16 Epoch: 0006 log_lik= 0.77804637 train_kl= -0.00306 train_loss= 0.78111 train_acc= 0.29759 val_roc= 0.73881 val_ap= 0.76935 time= 0.07668\n",
      "训练次数: 16 Epoch: 0007 log_lik= 0.73402464 train_kl= -0.00395 train_loss= 0.73797 train_acc= 0.28518 val_roc= 0.74639 val_ap= 0.77573 time= 0.07468\n",
      "训练次数: 16 Epoch: 0008 log_lik= 0.71360993 train_kl= -0.00492 train_loss= 0.71853 train_acc= 0.24910 val_roc= 0.76057 val_ap= 0.78716 time= 0.07369\n",
      "训练次数: 16 Epoch: 0009 log_lik= 0.7054271 train_kl= -0.00590 train_loss= 0.71132 train_acc= 0.20558 val_roc= 0.78790 val_ap= 0.80792 time= 0.07468\n",
      "训练次数: 16 Epoch: 0010 log_lik= 0.69399744 train_kl= -0.00683 train_loss= 0.70082 train_acc= 0.18434 val_roc= 0.82014 val_ap= 0.83533 time= 0.08265\n",
      "训练次数: 16 Epoch: 0011 log_lik= 0.6816147 train_kl= -0.00771 train_loss= 0.68932 train_acc= 0.19464 val_roc= 0.84252 val_ap= 0.85581 time= 0.07469\n",
      "训练次数: 16 Epoch: 0012 log_lik= 0.66430914 train_kl= -0.00854 train_loss= 0.67285 train_acc= 0.20297 val_roc= 0.85648 val_ap= 0.86605 time= 0.07468\n",
      "训练次数: 16 Epoch: 0013 log_lik= 0.64594513 train_kl= -0.00933 train_loss= 0.65528 train_acc= 0.20935 val_roc= 0.86420 val_ap= 0.86879 time= 0.07568\n",
      "训练次数: 16 Epoch: 0014 log_lik= 0.6246888 train_kl= -0.01007 train_loss= 0.63476 train_acc= 0.24264 val_roc= 0.86918 val_ap= 0.87031 time= 0.07472\n",
      "训练次数: 16 Epoch: 0015 log_lik= 0.6036028 train_kl= -0.01072 train_loss= 0.61432 train_acc= 0.28737 val_roc= 0.87124 val_ap= 0.87078 time= 0.07369\n",
      "训练次数: 16 Epoch: 0016 log_lik= 0.58480585 train_kl= -0.01126 train_loss= 0.59606 train_acc= 0.34653 val_roc= 0.87182 val_ap= 0.87061 time= 0.07568\n",
      "训练次数: 16 Epoch: 0017 log_lik= 0.56492114 train_kl= -0.01170 train_loss= 0.57662 train_acc= 0.41600 val_roc= 0.87314 val_ap= 0.87294 time= 0.07269\n",
      "训练次数: 16 Epoch: 0018 log_lik= 0.54547316 train_kl= -0.01208 train_loss= 0.55755 train_acc= 0.46427 val_roc= 0.87846 val_ap= 0.87983 time= 0.07468\n",
      "训练次数: 16 Epoch: 0019 log_lik= 0.5335069 train_kl= -0.01240 train_loss= 0.54591 train_acc= 0.48208 val_roc= 0.88423 val_ap= 0.88738 time= 0.07369\n",
      "训练次数: 16 Epoch: 0020 log_lik= 0.53174984 train_kl= -0.01269 train_loss= 0.54444 train_acc= 0.48414 val_roc= 0.89067 val_ap= 0.89441 time= 0.07468\n",
      "训练次数: 16 Epoch: 0021 log_lik= 0.53036344 train_kl= -0.01295 train_loss= 0.54332 train_acc= 0.48359 val_roc= 0.89656 val_ap= 0.90020 time= 0.07468\n",
      "训练次数: 16 Epoch: 0022 log_lik= 0.52535963 train_kl= -0.01315 train_loss= 0.53851 train_acc= 0.48598 val_roc= 0.90208 val_ap= 0.90551 time= 0.07668\n",
      "训练次数: 16 Epoch: 0023 log_lik= 0.517017 train_kl= -0.01327 train_loss= 0.53028 train_acc= 0.49208 val_roc= 0.90447 val_ap= 0.90717 time= 0.07866\n",
      "训练次数: 16 Epoch: 0024 log_lik= 0.5095583 train_kl= -0.01332 train_loss= 0.52288 train_acc= 0.49868 val_roc= 0.90387 val_ap= 0.90563 time= 0.07568\n",
      "训练次数: 16 Epoch: 0025 log_lik= 0.5044349 train_kl= -0.01334 train_loss= 0.51777 train_acc= 0.50348 val_roc= 0.90172 val_ap= 0.90254 time= 0.08165\n",
      "训练次数: 16 Epoch: 0026 log_lik= 0.4988804 train_kl= -0.01333 train_loss= 0.51222 train_acc= 0.51246 val_roc= 0.89990 val_ap= 0.90051 time= 0.07568\n",
      "训练次数: 16 Epoch: 0027 log_lik= 0.49760368 train_kl= -0.01330 train_loss= 0.51090 train_acc= 0.51267 val_roc= 0.89951 val_ap= 0.90007 time= 0.08265\n",
      "训练次数: 16 Epoch: 0028 log_lik= 0.4936192 train_kl= -0.01322 train_loss= 0.50684 train_acc= 0.51402 val_roc= 0.90074 val_ap= 0.90067 time= 0.07667\n",
      "训练次数: 16 Epoch: 0029 log_lik= 0.49066368 train_kl= -0.01311 train_loss= 0.50377 train_acc= 0.51402 val_roc= 0.90331 val_ap= 0.90341 time= 0.07667\n",
      "训练次数: 16 Epoch: 0030 log_lik= 0.48903838 train_kl= -0.01299 train_loss= 0.50203 train_acc= 0.51137 val_roc= 0.90643 val_ap= 0.90701 time= 0.07430\n",
      "训练次数: 16 Epoch: 0031 log_lik= 0.4882301 train_kl= -0.01290 train_loss= 0.50113 train_acc= 0.50963 val_roc= 0.90872 val_ap= 0.91027 time= 0.07368\n",
      "训练次数: 16 Epoch: 0032 log_lik= 0.48646232 train_kl= -0.01281 train_loss= 0.49928 train_acc= 0.50871 val_roc= 0.91129 val_ap= 0.91393 time= 0.07473\n",
      "训练次数: 16 Epoch: 0033 log_lik= 0.4827225 train_kl= -0.01274 train_loss= 0.49546 train_acc= 0.51191 val_roc= 0.91206 val_ap= 0.91552 time= 0.07369\n",
      "训练次数: 16 Epoch: 0034 log_lik= 0.47803634 train_kl= -0.01269 train_loss= 0.49072 train_acc= 0.51949 val_roc= 0.91262 val_ap= 0.91636 time= 0.07468\n",
      "训练次数: 16 Epoch: 0035 log_lik= 0.47426745 train_kl= -0.01266 train_loss= 0.48693 train_acc= 0.52416 val_roc= 0.91282 val_ap= 0.91730 time= 0.07468\n",
      "训练次数: 16 Epoch: 0036 log_lik= 0.4711879 train_kl= -0.01265 train_loss= 0.48384 train_acc= 0.52510 val_roc= 0.91333 val_ap= 0.91813 time= 0.08365\n",
      "训练次数: 16 Epoch: 0037 log_lik= 0.47012758 train_kl= -0.01266 train_loss= 0.48279 train_acc= 0.52319 val_roc= 0.91450 val_ap= 0.91954 time= 0.07568\n",
      "训练次数: 16 Epoch: 0038 log_lik= 0.4692157 train_kl= -0.01267 train_loss= 0.48188 train_acc= 0.52189 val_roc= 0.91619 val_ap= 0.92099 time= 0.07568\n",
      "训练次数: 16 Epoch: 0039 log_lik= 0.46705434 train_kl= -0.01266 train_loss= 0.47971 train_acc= 0.52171 val_roc= 0.91808 val_ap= 0.92265 time= 0.07369\n",
      "训练次数: 16 Epoch: 0040 log_lik= 0.4651103 train_kl= -0.01264 train_loss= 0.47775 train_acc= 0.52378 val_roc= 0.91943 val_ap= 0.92448 time= 0.07667\n",
      "训练次数: 16 Epoch: 0041 log_lik= 0.46284708 train_kl= -0.01262 train_loss= 0.47547 train_acc= 0.52663 val_roc= 0.92001 val_ap= 0.92521 time= 0.07767\n",
      "训练次数: 16 Epoch: 0042 log_lik= 0.4618404 train_kl= -0.01260 train_loss= 0.47444 train_acc= 0.52696 val_roc= 0.91969 val_ap= 0.92488 time= 0.07369\n",
      "训练次数: 16 Epoch: 0043 log_lik= 0.46090767 train_kl= -0.01256 train_loss= 0.47347 train_acc= 0.52759 val_roc= 0.91949 val_ap= 0.92445 time= 0.08265\n",
      "训练次数: 16 Epoch: 0044 log_lik= 0.45945045 train_kl= -0.01251 train_loss= 0.47196 train_acc= 0.52718 val_roc= 0.92002 val_ap= 0.92464 time= 0.07369\n",
      "训练次数: 16 Epoch: 0045 log_lik= 0.45875114 train_kl= -0.01245 train_loss= 0.47120 train_acc= 0.52819 val_roc= 0.92093 val_ap= 0.92577 time= 0.07568\n",
      "训练次数: 16 Epoch: 0046 log_lik= 0.45707512 train_kl= -0.01239 train_loss= 0.46946 train_acc= 0.52794 val_roc= 0.92173 val_ap= 0.92713 time= 0.07668\n",
      "训练次数: 16 Epoch: 0047 log_lik= 0.4556261 train_kl= -0.01234 train_loss= 0.46796 train_acc= 0.52825 val_roc= 0.92160 val_ap= 0.92721 time= 0.07468\n",
      "训练次数: 16 Epoch: 0048 log_lik= 0.45379797 train_kl= -0.01227 train_loss= 0.46607 train_acc= 0.52837 val_roc= 0.92098 val_ap= 0.92659 time= 0.08165\n",
      "训练次数: 16 Epoch: 0049 log_lik= 0.45315912 train_kl= -0.01219 train_loss= 0.46535 train_acc= 0.52925 val_roc= 0.92031 val_ap= 0.92602 time= 0.09559\n",
      "训练次数: 16 Epoch: 0050 log_lik= 0.45234635 train_kl= -0.01211 train_loss= 0.46445 train_acc= 0.52864 val_roc= 0.91989 val_ap= 0.92558 time= 0.07468\n",
      "训练次数: 16 Epoch: 0051 log_lik= 0.45149586 train_kl= -0.01202 train_loss= 0.46352 train_acc= 0.52986 val_roc= 0.92030 val_ap= 0.92629 time= 0.07472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 16 Epoch: 0052 log_lik= 0.45049962 train_kl= -0.01195 train_loss= 0.46245 train_acc= 0.52895 val_roc= 0.92080 val_ap= 0.92687 time= 0.08364\n",
      "训练次数: 16 Epoch: 0053 log_lik= 0.4501992 train_kl= -0.01189 train_loss= 0.46209 train_acc= 0.52952 val_roc= 0.92102 val_ap= 0.92708 time= 0.07468\n",
      "训练次数: 16 Epoch: 0054 log_lik= 0.44952917 train_kl= -0.01184 train_loss= 0.46137 train_acc= 0.52922 val_roc= 0.92085 val_ap= 0.92706 time= 0.07369\n",
      "训练次数: 16 Epoch: 0055 log_lik= 0.4496471 train_kl= -0.01180 train_loss= 0.46145 train_acc= 0.53002 val_roc= 0.92092 val_ap= 0.92705 time= 0.07667\n",
      "训练次数: 16 Epoch: 0056 log_lik= 0.44837716 train_kl= -0.01178 train_loss= 0.46016 train_acc= 0.52947 val_roc= 0.92155 val_ap= 0.92787 time= 0.07667\n",
      "训练次数: 16 Epoch: 0057 log_lik= 0.44759482 train_kl= -0.01178 train_loss= 0.45937 train_acc= 0.52931 val_roc= 0.92205 val_ap= 0.92821 time= 0.07468\n",
      "训练次数: 16 Epoch: 0058 log_lik= 0.44661337 train_kl= -0.01178 train_loss= 0.45839 train_acc= 0.53085 val_roc= 0.92200 val_ap= 0.92784 time= 0.07867\n",
      "训练次数: 16 Epoch: 0059 log_lik= 0.445967 train_kl= -0.01178 train_loss= 0.45775 train_acc= 0.53128 val_roc= 0.92190 val_ap= 0.92772 time= 0.07468\n",
      "训练次数: 16 Epoch: 0060 log_lik= 0.44550514 train_kl= -0.01179 train_loss= 0.45730 train_acc= 0.53227 val_roc= 0.92183 val_ap= 0.92765 time= 0.07368\n",
      "训练次数: 16 Epoch: 0061 log_lik= 0.4447196 train_kl= -0.01180 train_loss= 0.45652 train_acc= 0.53184 val_roc= 0.92184 val_ap= 0.92797 time= 0.07966\n",
      "训练次数: 16 Epoch: 0062 log_lik= 0.4440899 train_kl= -0.01180 train_loss= 0.45589 train_acc= 0.53228 val_roc= 0.92287 val_ap= 0.92910 time= 0.07966\n",
      "训练次数: 16 Epoch: 0063 log_lik= 0.44311136 train_kl= -0.01179 train_loss= 0.45490 train_acc= 0.53253 val_roc= 0.92278 val_ap= 0.92929 time= 0.07468\n",
      "训练次数: 16 Epoch: 0064 log_lik= 0.4423341 train_kl= -0.01179 train_loss= 0.45413 train_acc= 0.53373 val_roc= 0.92277 val_ap= 0.92926 time= 0.07468\n",
      "训练次数: 16 Epoch: 0065 log_lik= 0.44193426 train_kl= -0.01179 train_loss= 0.45372 train_acc= 0.53288 val_roc= 0.92247 val_ap= 0.92919 time= 0.07468\n",
      "训练次数: 16 Epoch: 0066 log_lik= 0.44139215 train_kl= -0.01179 train_loss= 0.45318 train_acc= 0.53318 val_roc= 0.92249 val_ap= 0.92970 time= 0.07667\n",
      "训练次数: 16 Epoch: 0067 log_lik= 0.44077757 train_kl= -0.01179 train_loss= 0.45257 train_acc= 0.53418 val_roc= 0.92268 val_ap= 0.93017 time= 0.09061\n",
      "训练次数: 16 Epoch: 0068 log_lik= 0.4404833 train_kl= -0.01179 train_loss= 0.45227 train_acc= 0.53369 val_roc= 0.92235 val_ap= 0.93017 time= 0.07568\n",
      "训练次数: 16 Epoch: 0069 log_lik= 0.44009686 train_kl= -0.01178 train_loss= 0.45188 train_acc= 0.53372 val_roc= 0.92205 val_ap= 0.93011 time= 0.07568\n",
      "训练次数: 16 Epoch: 0070 log_lik= 0.43984935 train_kl= -0.01179 train_loss= 0.45164 train_acc= 0.53421 val_roc= 0.92148 val_ap= 0.92983 time= 0.07867\n",
      "训练次数: 16 Epoch: 0071 log_lik= 0.4391071 train_kl= -0.01179 train_loss= 0.45089 train_acc= 0.53436 val_roc= 0.92135 val_ap= 0.92999 time= 0.07568\n",
      "训练次数: 16 Epoch: 0072 log_lik= 0.43892366 train_kl= -0.01179 train_loss= 0.45071 train_acc= 0.53441 val_roc= 0.92213 val_ap= 0.93120 time= 0.07468\n",
      "训练次数: 16 Epoch: 0073 log_lik= 0.43810308 train_kl= -0.01178 train_loss= 0.44988 train_acc= 0.53526 val_roc= 0.92161 val_ap= 0.93086 time= 0.07767\n",
      "训练次数: 16 Epoch: 0074 log_lik= 0.43811157 train_kl= -0.01178 train_loss= 0.44989 train_acc= 0.53448 val_roc= 0.92111 val_ap= 0.93045 time= 0.07568\n",
      "训练次数: 16 Epoch: 0075 log_lik= 0.4374285 train_kl= -0.01179 train_loss= 0.44921 train_acc= 0.53549 val_roc= 0.92093 val_ap= 0.93039 time= 0.07369\n",
      "训练次数: 16 Epoch: 0076 log_lik= 0.43698636 train_kl= -0.01180 train_loss= 0.44879 train_acc= 0.53525 val_roc= 0.92074 val_ap= 0.93039 time= 0.07369\n",
      "训练次数: 16 Epoch: 0077 log_lik= 0.4366425 train_kl= -0.01182 train_loss= 0.44846 train_acc= 0.53565 val_roc= 0.92073 val_ap= 0.93043 time= 0.08464\n",
      "训练次数: 16 Epoch: 0078 log_lik= 0.43616974 train_kl= -0.01183 train_loss= 0.44800 train_acc= 0.53637 val_roc= 0.92089 val_ap= 0.93069 time= 0.07368\n",
      "训练次数: 16 Epoch: 0079 log_lik= 0.43581238 train_kl= -0.01184 train_loss= 0.44766 train_acc= 0.53742 val_roc= 0.92085 val_ap= 0.93057 time= 0.07468\n",
      "训练次数: 16 Epoch: 0080 log_lik= 0.4350567 train_kl= -0.01186 train_loss= 0.44691 train_acc= 0.53698 val_roc= 0.92087 val_ap= 0.93055 time= 0.07369\n",
      "训练次数: 16 Epoch: 0081 log_lik= 0.43524408 train_kl= -0.01187 train_loss= 0.44712 train_acc= 0.53689 val_roc= 0.92073 val_ap= 0.93059 time= 0.07468\n",
      "训练次数: 16 Epoch: 0082 log_lik= 0.43503344 train_kl= -0.01189 train_loss= 0.44692 train_acc= 0.53740 val_roc= 0.92098 val_ap= 0.93115 time= 0.07867\n",
      "训练次数: 16 Epoch: 0083 log_lik= 0.43441042 train_kl= -0.01191 train_loss= 0.44632 train_acc= 0.53696 val_roc= 0.92141 val_ap= 0.93186 time= 0.07468\n",
      "训练次数: 16 Epoch: 0084 log_lik= 0.4342423 train_kl= -0.01192 train_loss= 0.44616 train_acc= 0.53639 val_roc= 0.92151 val_ap= 0.93197 time= 0.07369\n",
      "训练次数: 16 Epoch: 0085 log_lik= 0.43347853 train_kl= -0.01193 train_loss= 0.44541 train_acc= 0.53797 val_roc= 0.92096 val_ap= 0.93136 time= 0.07767\n",
      "训练次数: 16 Epoch: 0086 log_lik= 0.433377 train_kl= -0.01193 train_loss= 0.44531 train_acc= 0.53629 val_roc= 0.92066 val_ap= 0.93120 time= 0.08165\n",
      "训练次数: 16 Epoch: 0087 log_lik= 0.4332691 train_kl= -0.01194 train_loss= 0.44521 train_acc= 0.53657 val_roc= 0.92046 val_ap= 0.93135 time= 0.08464\n",
      "训练次数: 16 Epoch: 0088 log_lik= 0.4328529 train_kl= -0.01195 train_loss= 0.44480 train_acc= 0.53747 val_roc= 0.92074 val_ap= 0.93172 time= 0.07568\n",
      "训练次数: 16 Epoch: 0089 log_lik= 0.43301 train_kl= -0.01195 train_loss= 0.44496 train_acc= 0.53667 val_roc= 0.92089 val_ap= 0.93169 time= 0.07568\n",
      "训练次数: 16 Epoch: 0090 log_lik= 0.43225685 train_kl= -0.01196 train_loss= 0.44421 train_acc= 0.53732 val_roc= 0.92012 val_ap= 0.93117 time= 0.07369\n",
      "训练次数: 16 Epoch: 0091 log_lik= 0.43214193 train_kl= -0.01195 train_loss= 0.44409 train_acc= 0.53700 val_roc= 0.91994 val_ap= 0.93116 time= 0.07667\n",
      "训练次数: 16 Epoch: 0092 log_lik= 0.43177125 train_kl= -0.01195 train_loss= 0.44372 train_acc= 0.53739 val_roc= 0.91988 val_ap= 0.93110 time= 0.07568\n",
      "训练次数: 16 Epoch: 0093 log_lik= 0.43162543 train_kl= -0.01195 train_loss= 0.44357 train_acc= 0.53789 val_roc= 0.91949 val_ap= 0.93084 time= 0.07468\n",
      "训练次数: 16 Epoch: 0094 log_lik= 0.43117484 train_kl= -0.01195 train_loss= 0.44313 train_acc= 0.53772 val_roc= 0.91966 val_ap= 0.93130 time= 0.07568\n",
      "训练次数: 16 Epoch: 0095 log_lik= 0.4312739 train_kl= -0.01196 train_loss= 0.44324 train_acc= 0.53748 val_roc= 0.91998 val_ap= 0.93152 time= 0.07667\n",
      "训练次数: 16 Epoch: 0096 log_lik= 0.43076938 train_kl= -0.01196 train_loss= 0.44273 train_acc= 0.53721 val_roc= 0.91930 val_ap= 0.93088 time= 0.08196\n",
      "训练次数: 16 Epoch: 0097 log_lik= 0.43088397 train_kl= -0.01196 train_loss= 0.44284 train_acc= 0.53772 val_roc= 0.91775 val_ap= 0.92932 time= 0.07468\n",
      "训练次数: 16 Epoch: 0098 log_lik= 0.43099457 train_kl= -0.01197 train_loss= 0.44296 train_acc= 0.53757 val_roc= 0.91876 val_ap= 0.93058 time= 0.07368\n",
      "训练次数: 16 Epoch: 0099 log_lik= 0.43022755 train_kl= -0.01198 train_loss= 0.44221 train_acc= 0.53752 val_roc= 0.91978 val_ap= 0.93156 time= 0.07469\n",
      "训练次数: 16 Epoch: 0100 log_lik= 0.43017933 train_kl= -0.01198 train_loss= 0.44216 train_acc= 0.53774 val_roc= 0.91892 val_ap= 0.93066 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 16 ROC score: 0.9120221510897313\n",
      "训练次数: 16 AP score: 0.9187517886683596\n",
      "训练次数: 17 Epoch: 0001 log_lik= 1.8734605 train_kl= -0.00006 train_loss= 1.87353 train_acc= 0.49647 val_roc= 0.69762 val_ap= 0.72236 time= 1.57351\n",
      "训练次数: 17 Epoch: 0002 log_lik= 1.5631965 train_kl= -0.00008 train_loss= 1.56328 train_acc= 0.49297 val_roc= 0.68691 val_ap= 0.71524 time= 0.08662\n",
      "训练次数: 17 Epoch: 0003 log_lik= 1.3875538 train_kl= -0.00026 train_loss= 1.38782 train_acc= 0.46452 val_roc= 0.68648 val_ap= 0.71593 time= 0.07667\n",
      "训练次数: 17 Epoch: 0004 log_lik= 1.2731018 train_kl= -0.00056 train_loss= 1.27366 train_acc= 0.40795 val_roc= 0.68662 val_ap= 0.71521 time= 0.07667\n",
      "训练次数: 17 Epoch: 0005 log_lik= 1.1558383 train_kl= -0.00095 train_loss= 1.15679 train_acc= 0.35557 val_roc= 0.69001 val_ap= 0.71739 time= 0.08217\n",
      "训练次数: 17 Epoch: 0006 log_lik= 1.0752908 train_kl= -0.00138 train_loss= 1.07667 train_acc= 0.32587 val_roc= 0.69658 val_ap= 0.72211 time= 0.07767\n",
      "训练次数: 17 Epoch: 0007 log_lik= 0.96967375 train_kl= -0.00183 train_loss= 0.97151 train_acc= 0.31500 val_roc= 0.70643 val_ap= 0.73002 time= 0.08265\n",
      "训练次数: 17 Epoch: 0008 log_lik= 0.8663508 train_kl= -0.00234 train_loss= 0.86869 train_acc= 0.32943 val_roc= 0.72326 val_ap= 0.74334 time= 0.08265\n",
      "训练次数: 17 Epoch: 0009 log_lik= 0.7998794 train_kl= -0.00292 train_loss= 0.80280 train_acc= 0.35011 val_roc= 0.74373 val_ap= 0.76011 time= 0.07369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 17 Epoch: 0010 log_lik= 0.7587878 train_kl= -0.00361 train_loss= 0.76240 train_acc= 0.34347 val_roc= 0.74818 val_ap= 0.76550 time= 0.07771\n",
      "训练次数: 17 Epoch: 0011 log_lik= 0.730909 train_kl= -0.00436 train_loss= 0.73527 train_acc= 0.32713 val_roc= 0.74546 val_ap= 0.76471 time= 0.08365\n",
      "训练次数: 17 Epoch: 0012 log_lik= 0.71976286 train_kl= -0.00513 train_loss= 0.72489 train_acc= 0.27181 val_roc= 0.74997 val_ap= 0.76927 time= 0.07667\n",
      "训练次数: 17 Epoch: 0013 log_lik= 0.711543 train_kl= -0.00584 train_loss= 0.71738 train_acc= 0.22369 val_roc= 0.76261 val_ap= 0.77941 time= 0.08464\n",
      "训练次数: 17 Epoch: 0014 log_lik= 0.70221335 train_kl= -0.00647 train_loss= 0.70868 train_acc= 0.19260 val_roc= 0.77961 val_ap= 0.79396 time= 0.07667\n",
      "训练次数: 17 Epoch: 0015 log_lik= 0.69403416 train_kl= -0.00702 train_loss= 0.70105 train_acc= 0.18016 val_roc= 0.79982 val_ap= 0.80908 time= 0.08264\n",
      "训练次数: 17 Epoch: 0016 log_lik= 0.6856696 train_kl= -0.00750 train_loss= 0.69317 train_acc= 0.17700 val_roc= 0.81119 val_ap= 0.81548 time= 0.07668\n",
      "训练次数: 17 Epoch: 0017 log_lik= 0.6712891 train_kl= -0.00793 train_loss= 0.67922 train_acc= 0.19357 val_roc= 0.81451 val_ap= 0.81567 time= 0.09261\n",
      "训练次数: 17 Epoch: 0018 log_lik= 0.6617176 train_kl= -0.00833 train_loss= 0.67004 train_acc= 0.20712 val_roc= 0.81677 val_ap= 0.81692 time= 0.08265\n",
      "训练次数: 17 Epoch: 0019 log_lik= 0.6468036 train_kl= -0.00869 train_loss= 0.65550 train_acc= 0.23509 val_roc= 0.81881 val_ap= 0.81686 time= 0.08464\n",
      "训练次数: 17 Epoch: 0020 log_lik= 0.63551575 train_kl= -0.00904 train_loss= 0.64455 train_acc= 0.26690 val_roc= 0.81899 val_ap= 0.81430 time= 0.07468\n",
      "训练次数: 17 Epoch: 0021 log_lik= 0.62306386 train_kl= -0.00935 train_loss= 0.63241 train_acc= 0.30802 val_roc= 0.81804 val_ap= 0.81013 time= 0.07468\n",
      "训练次数: 17 Epoch: 0022 log_lik= 0.61329436 train_kl= -0.00962 train_loss= 0.62292 train_acc= 0.35458 val_roc= 0.81590 val_ap= 0.80826 time= 0.07369\n",
      "训练次数: 17 Epoch: 0023 log_lik= 0.6015553 train_kl= -0.00986 train_loss= 0.61141 train_acc= 0.40144 val_roc= 0.81522 val_ap= 0.80958 time= 0.07667\n",
      "训练次数: 17 Epoch: 0024 log_lik= 0.59366 train_kl= -0.01005 train_loss= 0.60371 train_acc= 0.42566 val_roc= 0.82006 val_ap= 0.81461 time= 0.07468\n",
      "训练次数: 17 Epoch: 0025 log_lik= 0.5838227 train_kl= -0.01021 train_loss= 0.59403 train_acc= 0.44184 val_roc= 0.82640 val_ap= 0.81926 time= 0.08464\n",
      "训练次数: 17 Epoch: 0026 log_lik= 0.57287276 train_kl= -0.01036 train_loss= 0.58323 train_acc= 0.46275 val_roc= 0.83461 val_ap= 0.82646 time= 0.07667\n",
      "训练次数: 17 Epoch: 0027 log_lik= 0.56173134 train_kl= -0.01050 train_loss= 0.57223 train_acc= 0.47218 val_roc= 0.84427 val_ap= 0.83654 time= 0.07796\n",
      "训练次数: 17 Epoch: 0028 log_lik= 0.5533834 train_kl= -0.01063 train_loss= 0.56402 train_acc= 0.47308 val_roc= 0.85262 val_ap= 0.84590 time= 0.08464\n",
      "训练次数: 17 Epoch: 0029 log_lik= 0.5448991 train_kl= -0.01076 train_loss= 0.55566 train_acc= 0.47331 val_roc= 0.85725 val_ap= 0.85114 time= 0.07568\n",
      "训练次数: 17 Epoch: 0030 log_lik= 0.53762627 train_kl= -0.01085 train_loss= 0.54848 train_acc= 0.47532 val_roc= 0.85932 val_ap= 0.85328 time= 0.07667\n",
      "训练次数: 17 Epoch: 0031 log_lik= 0.5272158 train_kl= -0.01090 train_loss= 0.53811 train_acc= 0.49148 val_roc= 0.86209 val_ap= 0.85546 time= 0.08265\n",
      "训练次数: 17 Epoch: 0032 log_lik= 0.51900285 train_kl= -0.01091 train_loss= 0.52992 train_acc= 0.50162 val_roc= 0.86456 val_ap= 0.85836 time= 0.08564\n",
      "训练次数: 17 Epoch: 0033 log_lik= 0.5121358 train_kl= -0.01092 train_loss= 0.52305 train_acc= 0.50940 val_roc= 0.86860 val_ap= 0.86177 time= 0.07468\n",
      "训练次数: 17 Epoch: 0034 log_lik= 0.5074313 train_kl= -0.01092 train_loss= 0.51835 train_acc= 0.51095 val_roc= 0.87285 val_ap= 0.86634 time= 0.07468\n",
      "训练次数: 17 Epoch: 0035 log_lik= 0.50561804 train_kl= -0.01092 train_loss= 0.51654 train_acc= 0.50901 val_roc= 0.87685 val_ap= 0.87080 time= 0.07771\n",
      "训练次数: 17 Epoch: 0036 log_lik= 0.5046967 train_kl= -0.01093 train_loss= 0.51562 train_acc= 0.50233 val_roc= 0.88144 val_ap= 0.87602 time= 0.08364\n",
      "训练次数: 17 Epoch: 0037 log_lik= 0.50120103 train_kl= -0.01091 train_loss= 0.51211 train_acc= 0.50289 val_roc= 0.88556 val_ap= 0.88085 time= 0.07667\n",
      "训练次数: 17 Epoch: 0038 log_lik= 0.49749103 train_kl= -0.01089 train_loss= 0.50838 train_acc= 0.50500 val_roc= 0.89023 val_ap= 0.88612 time= 0.07568\n",
      "训练次数: 17 Epoch: 0039 log_lik= 0.49071774 train_kl= -0.01088 train_loss= 0.50160 train_acc= 0.51010 val_roc= 0.89562 val_ap= 0.89263 time= 0.07568\n",
      "训练次数: 17 Epoch: 0040 log_lik= 0.4858963 train_kl= -0.01089 train_loss= 0.49679 train_acc= 0.51315 val_roc= 0.90144 val_ap= 0.89894 time= 0.07667\n",
      "训练次数: 17 Epoch: 0041 log_lik= 0.4837111 train_kl= -0.01091 train_loss= 0.49462 train_acc= 0.51553 val_roc= 0.90512 val_ap= 0.90441 time= 0.07468\n",
      "训练次数: 17 Epoch: 0042 log_lik= 0.48150432 train_kl= -0.01091 train_loss= 0.49241 train_acc= 0.51439 val_roc= 0.90702 val_ap= 0.90764 time= 0.07468\n",
      "训练次数: 17 Epoch: 0043 log_lik= 0.48042798 train_kl= -0.01088 train_loss= 0.49131 train_acc= 0.51259 val_roc= 0.90757 val_ap= 0.90928 time= 0.08265\n",
      "训练次数: 17 Epoch: 0044 log_lik= 0.47769195 train_kl= -0.01084 train_loss= 0.48853 train_acc= 0.51365 val_roc= 0.90869 val_ap= 0.91083 time= 0.07767\n",
      "训练次数: 17 Epoch: 0045 log_lik= 0.47582632 train_kl= -0.01079 train_loss= 0.48662 train_acc= 0.51533 val_roc= 0.91128 val_ap= 0.91382 time= 0.07568\n",
      "训练次数: 17 Epoch: 0046 log_lik= 0.47424835 train_kl= -0.01075 train_loss= 0.48500 train_acc= 0.51664 val_roc= 0.91337 val_ap= 0.91626 time= 0.07866\n",
      "训练次数: 17 Epoch: 0047 log_lik= 0.47224572 train_kl= -0.01071 train_loss= 0.48296 train_acc= 0.51687 val_roc= 0.91399 val_ap= 0.91694 time= 0.07667\n",
      "训练次数: 17 Epoch: 0048 log_lik= 0.47175357 train_kl= -0.01066 train_loss= 0.48242 train_acc= 0.51754 val_roc= 0.91392 val_ap= 0.91694 time= 0.07468\n",
      "训练次数: 17 Epoch: 0049 log_lik= 0.46876296 train_kl= -0.01061 train_loss= 0.47937 train_acc= 0.51944 val_roc= 0.91266 val_ap= 0.91576 time= 0.07667\n",
      "训练次数: 17 Epoch: 0050 log_lik= 0.46826077 train_kl= -0.01056 train_loss= 0.47882 train_acc= 0.51930 val_roc= 0.91341 val_ap= 0.91706 time= 0.07966\n",
      "训练次数: 17 Epoch: 0051 log_lik= 0.46662983 train_kl= -0.01053 train_loss= 0.47716 train_acc= 0.51914 val_roc= 0.91492 val_ap= 0.91921 time= 0.07568\n",
      "训练次数: 17 Epoch: 0052 log_lik= 0.46468994 train_kl= -0.01051 train_loss= 0.47520 train_acc= 0.52094 val_roc= 0.91499 val_ap= 0.91968 time= 0.07568\n",
      "训练次数: 17 Epoch: 0053 log_lik= 0.46392414 train_kl= -0.01049 train_loss= 0.47442 train_acc= 0.52120 val_roc= 0.91369 val_ap= 0.91820 time= 0.07767\n",
      "训练次数: 17 Epoch: 0054 log_lik= 0.46335778 train_kl= -0.01047 train_loss= 0.47382 train_acc= 0.52116 val_roc= 0.91261 val_ap= 0.91692 time= 0.07468\n",
      "训练次数: 17 Epoch: 0055 log_lik= 0.46103957 train_kl= -0.01044 train_loss= 0.47148 train_acc= 0.52248 val_roc= 0.91318 val_ap= 0.91759 time= 0.07668\n",
      "训练次数: 17 Epoch: 0056 log_lik= 0.46087578 train_kl= -0.01042 train_loss= 0.47129 train_acc= 0.52512 val_roc= 0.91375 val_ap= 0.91764 time= 0.07468\n",
      "训练次数: 17 Epoch: 0057 log_lik= 0.45873857 train_kl= -0.01041 train_loss= 0.46914 train_acc= 0.52630 val_roc= 0.91392 val_ap= 0.91784 time= 0.07767\n",
      "训练次数: 17 Epoch: 0058 log_lik= 0.4579233 train_kl= -0.01040 train_loss= 0.46832 train_acc= 0.52564 val_roc= 0.91370 val_ap= 0.91769 time= 0.07568\n",
      "训练次数: 17 Epoch: 0059 log_lik= 0.45768502 train_kl= -0.01041 train_loss= 0.46809 train_acc= 0.52494 val_roc= 0.91328 val_ap= 0.91770 time= 0.07568\n",
      "训练次数: 17 Epoch: 0060 log_lik= 0.45661986 train_kl= -0.01044 train_loss= 0.46706 train_acc= 0.52543 val_roc= 0.91353 val_ap= 0.91849 time= 0.07468\n",
      "训练次数: 17 Epoch: 0061 log_lik= 0.45531255 train_kl= -0.01047 train_loss= 0.46578 train_acc= 0.52743 val_roc= 0.91385 val_ap= 0.91896 time= 0.07867\n",
      "训练次数: 17 Epoch: 0062 log_lik= 0.45458472 train_kl= -0.01050 train_loss= 0.46509 train_acc= 0.52782 val_roc= 0.91443 val_ap= 0.91967 time= 0.07770\n",
      "训练次数: 17 Epoch: 0063 log_lik= 0.45378482 train_kl= -0.01052 train_loss= 0.46431 train_acc= 0.52915 val_roc= 0.91448 val_ap= 0.91938 time= 0.07468\n",
      "训练次数: 17 Epoch: 0064 log_lik= 0.45247576 train_kl= -0.01055 train_loss= 0.46303 train_acc= 0.52829 val_roc= 0.91482 val_ap= 0.91987 time= 0.07369\n",
      "训练次数: 17 Epoch: 0065 log_lik= 0.45123607 train_kl= -0.01060 train_loss= 0.46183 train_acc= 0.52949 val_roc= 0.91512 val_ap= 0.92015 time= 0.08165\n",
      "训练次数: 17 Epoch: 0066 log_lik= 0.45057315 train_kl= -0.01064 train_loss= 0.46121 train_acc= 0.52981 val_roc= 0.91527 val_ap= 0.92041 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 17 Epoch: 0067 log_lik= 0.45040327 train_kl= -0.01068 train_loss= 0.46108 train_acc= 0.53164 val_roc= 0.91460 val_ap= 0.91990 time= 0.07568\n",
      "训练次数: 17 Epoch: 0068 log_lik= 0.44905362 train_kl= -0.01071 train_loss= 0.45976 train_acc= 0.53205 val_roc= 0.91453 val_ap= 0.91972 time= 0.07468\n",
      "训练次数: 17 Epoch: 0069 log_lik= 0.44897702 train_kl= -0.01075 train_loss= 0.45972 train_acc= 0.53113 val_roc= 0.91480 val_ap= 0.91973 time= 0.07468\n",
      "训练次数: 17 Epoch: 0070 log_lik= 0.448005 train_kl= -0.01080 train_loss= 0.45881 train_acc= 0.53128 val_roc= 0.91547 val_ap= 0.91974 time= 0.07667\n",
      "训练次数: 17 Epoch: 0071 log_lik= 0.4466965 train_kl= -0.01086 train_loss= 0.45755 train_acc= 0.53283 val_roc= 0.91514 val_ap= 0.91937 time= 0.07966\n",
      "训练次数: 17 Epoch: 0072 log_lik= 0.44711 train_kl= -0.01091 train_loss= 0.45802 train_acc= 0.53262 val_roc= 0.91401 val_ap= 0.91832 time= 0.07567\n",
      "训练次数: 17 Epoch: 0073 log_lik= 0.4456127 train_kl= -0.01095 train_loss= 0.45657 train_acc= 0.53332 val_roc= 0.91307 val_ap= 0.91768 time= 0.07867\n",
      "训练次数: 17 Epoch: 0074 log_lik= 0.44583386 train_kl= -0.01099 train_loss= 0.45683 train_acc= 0.53305 val_roc= 0.91359 val_ap= 0.91811 time= 0.07667\n",
      "训练次数: 17 Epoch: 0075 log_lik= 0.44480085 train_kl= -0.01103 train_loss= 0.45583 train_acc= 0.53403 val_roc= 0.91389 val_ap= 0.91785 time= 0.07568\n",
      "训练次数: 17 Epoch: 0076 log_lik= 0.44409093 train_kl= -0.01108 train_loss= 0.45517 train_acc= 0.53470 val_roc= 0.91386 val_ap= 0.91807 time= 0.07568\n",
      "训练次数: 17 Epoch: 0077 log_lik= 0.44363236 train_kl= -0.01113 train_loss= 0.45476 train_acc= 0.53489 val_roc= 0.91352 val_ap= 0.91802 time= 0.07568\n",
      "训练次数: 17 Epoch: 0078 log_lik= 0.44337735 train_kl= -0.01117 train_loss= 0.45455 train_acc= 0.53480 val_roc= 0.91326 val_ap= 0.91799 time= 0.07468\n",
      "训练次数: 17 Epoch: 0079 log_lik= 0.44240364 train_kl= -0.01121 train_loss= 0.45361 train_acc= 0.53497 val_roc= 0.91337 val_ap= 0.91857 time= 0.07767\n",
      "训练次数: 17 Epoch: 0080 log_lik= 0.4420663 train_kl= -0.01124 train_loss= 0.45330 train_acc= 0.53627 val_roc= 0.91330 val_ap= 0.91846 time= 0.07568\n",
      "训练次数: 17 Epoch: 0081 log_lik= 0.44141534 train_kl= -0.01127 train_loss= 0.45268 train_acc= 0.53715 val_roc= 0.91323 val_ap= 0.91884 time= 0.07468\n",
      "训练次数: 17 Epoch: 0082 log_lik= 0.44138795 train_kl= -0.01130 train_loss= 0.45269 train_acc= 0.53682 val_roc= 0.91326 val_ap= 0.91929 time= 0.07469\n",
      "训练次数: 17 Epoch: 0083 log_lik= 0.44051713 train_kl= -0.01134 train_loss= 0.45186 train_acc= 0.53757 val_roc= 0.91349 val_ap= 0.91942 time= 0.07866\n",
      "训练次数: 17 Epoch: 0084 log_lik= 0.44007692 train_kl= -0.01138 train_loss= 0.45145 train_acc= 0.53800 val_roc= 0.91313 val_ap= 0.91968 time= 0.07767\n",
      "训练次数: 17 Epoch: 0085 log_lik= 0.43945262 train_kl= -0.01141 train_loss= 0.45086 train_acc= 0.53799 val_roc= 0.91284 val_ap= 0.91982 time= 0.07667\n",
      "训练次数: 17 Epoch: 0086 log_lik= 0.43902513 train_kl= -0.01144 train_loss= 0.45046 train_acc= 0.53915 val_roc= 0.91343 val_ap= 0.92056 time= 0.07568\n",
      "训练次数: 17 Epoch: 0087 log_lik= 0.43882182 train_kl= -0.01148 train_loss= 0.45030 train_acc= 0.53835 val_roc= 0.91433 val_ap= 0.92140 time= 0.07468\n",
      "训练次数: 17 Epoch: 0088 log_lik= 0.43819943 train_kl= -0.01151 train_loss= 0.44971 train_acc= 0.53906 val_roc= 0.91448 val_ap= 0.92168 time= 0.07568\n",
      "训练次数: 17 Epoch: 0089 log_lik= 0.43756127 train_kl= -0.01153 train_loss= 0.44909 train_acc= 0.53957 val_roc= 0.91427 val_ap= 0.92135 time= 0.07667\n",
      "训练次数: 17 Epoch: 0090 log_lik= 0.43745998 train_kl= -0.01155 train_loss= 0.44901 train_acc= 0.54090 val_roc= 0.91466 val_ap= 0.92173 time= 0.07568\n",
      "训练次数: 17 Epoch: 0091 log_lik= 0.43751007 train_kl= -0.01157 train_loss= 0.44908 train_acc= 0.53963 val_roc= 0.91482 val_ap= 0.92230 time= 0.08564\n",
      "训练次数: 17 Epoch: 0092 log_lik= 0.4367722 train_kl= -0.01160 train_loss= 0.44837 train_acc= 0.54041 val_roc= 0.91480 val_ap= 0.92237 time= 0.08265\n",
      "训练次数: 17 Epoch: 0093 log_lik= 0.43658307 train_kl= -0.01162 train_loss= 0.44821 train_acc= 0.54124 val_roc= 0.91548 val_ap= 0.92216 time= 0.07468\n",
      "训练次数: 17 Epoch: 0094 log_lik= 0.43634012 train_kl= -0.01164 train_loss= 0.44798 train_acc= 0.54071 val_roc= 0.91514 val_ap= 0.92162 time= 0.07568\n",
      "训练次数: 17 Epoch: 0095 log_lik= 0.43595156 train_kl= -0.01165 train_loss= 0.44760 train_acc= 0.54114 val_roc= 0.91480 val_ap= 0.92208 time= 0.08166\n",
      "训练次数: 17 Epoch: 0096 log_lik= 0.4357457 train_kl= -0.01166 train_loss= 0.44740 train_acc= 0.54079 val_roc= 0.91506 val_ap= 0.92191 time= 0.07966\n",
      "训练次数: 17 Epoch: 0097 log_lik= 0.43550032 train_kl= -0.01168 train_loss= 0.44718 train_acc= 0.54174 val_roc= 0.91668 val_ap= 0.92263 time= 0.07568\n",
      "训练次数: 17 Epoch: 0098 log_lik= 0.43526998 train_kl= -0.01171 train_loss= 0.44698 train_acc= 0.54185 val_roc= 0.91639 val_ap= 0.92251 time= 0.07567\n",
      "训练次数: 17 Epoch: 0099 log_lik= 0.43487957 train_kl= -0.01172 train_loss= 0.44660 train_acc= 0.54270 val_roc= 0.91519 val_ap= 0.92172 time= 0.07667\n",
      "训练次数: 17 Epoch: 0100 log_lik= 0.4345882 train_kl= -0.01173 train_loss= 0.44632 train_acc= 0.54285 val_roc= 0.91586 val_ap= 0.92221 time= 0.07770\n",
      "Optimization Finished!\n",
      "训练次数: 17 ROC score: 0.917621134271178\n",
      "训练次数: 17 AP score: 0.9232483688149711\n",
      "训练次数: 18 Epoch: 0001 log_lik= 1.745357 train_kl= -0.00005 train_loss= 1.74541 train_acc= 0.49505 val_roc= 0.66174 val_ap= 0.69722 time= 1.56859\n",
      "训练次数: 18 Epoch: 0002 log_lik= 1.5102532 train_kl= -0.00024 train_loss= 1.51049 train_acc= 0.47125 val_roc= 0.65018 val_ap= 0.69308 time= 0.08957\n",
      "训练次数: 18 Epoch: 0003 log_lik= 1.2857524 train_kl= -0.00062 train_loss= 1.28638 train_acc= 0.39790 val_roc= 0.65341 val_ap= 0.70009 time= 0.07568\n",
      "训练次数: 18 Epoch: 0004 log_lik= 1.1508758 train_kl= -0.00110 train_loss= 1.15198 train_acc= 0.34047 val_roc= 0.66479 val_ap= 0.71366 time= 0.07468\n",
      "训练次数: 18 Epoch: 0005 log_lik= 0.9755843 train_kl= -0.00157 train_loss= 0.97715 train_acc= 0.34575 val_roc= 0.68454 val_ap= 0.73362 time= 0.07369\n",
      "训练次数: 18 Epoch: 0006 log_lik= 0.8467909 train_kl= -0.00213 train_loss= 0.84892 train_acc= 0.37195 val_roc= 0.70777 val_ap= 0.74570 time= 0.07468\n",
      "训练次数: 18 Epoch: 0007 log_lik= 0.7731142 train_kl= -0.00291 train_loss= 0.77602 train_acc= 0.38619 val_roc= 0.71035 val_ap= 0.74291 time= 0.07767\n",
      "训练次数: 18 Epoch: 0008 log_lik= 0.73135716 train_kl= -0.00391 train_loss= 0.73527 train_acc= 0.33580 val_roc= 0.71265 val_ap= 0.74419 time= 0.07469\n",
      "训练次数: 18 Epoch: 0009 log_lik= 0.71618724 train_kl= -0.00502 train_loss= 0.72121 train_acc= 0.24929 val_roc= 0.73383 val_ap= 0.75413 time= 0.07767\n",
      "训练次数: 18 Epoch: 0010 log_lik= 0.70169723 train_kl= -0.00613 train_loss= 0.70783 train_acc= 0.20187 val_roc= 0.76307 val_ap= 0.77017 time= 0.07667\n",
      "训练次数: 18 Epoch: 0011 log_lik= 0.6824481 train_kl= -0.00719 train_loss= 0.68964 train_acc= 0.19527 val_roc= 0.79977 val_ap= 0.79637 time= 0.07368\n",
      "训练次数: 18 Epoch: 0012 log_lik= 0.6647777 train_kl= -0.00821 train_loss= 0.67299 train_acc= 0.22781 val_roc= 0.82627 val_ap= 0.82256 time= 0.07966\n",
      "训练次数: 18 Epoch: 0013 log_lik= 0.64333934 train_kl= -0.00919 train_loss= 0.65253 train_acc= 0.24523 val_roc= 0.83469 val_ap= 0.83448 time= 0.07568\n",
      "训练次数: 18 Epoch: 0014 log_lik= 0.6273853 train_kl= -0.01012 train_loss= 0.63751 train_acc= 0.21975 val_roc= 0.83769 val_ap= 0.83869 time= 0.07544\n",
      "训练次数: 18 Epoch: 0015 log_lik= 0.6135355 train_kl= -0.01096 train_loss= 0.62450 train_acc= 0.23015 val_roc= 0.85227 val_ap= 0.85311 time= 0.07369\n",
      "训练次数: 18 Epoch: 0016 log_lik= 0.584618 train_kl= -0.01166 train_loss= 0.59628 train_acc= 0.32180 val_roc= 0.86033 val_ap= 0.86019 time= 0.07668\n",
      "训练次数: 18 Epoch: 0017 log_lik= 0.55605865 train_kl= -0.01226 train_loss= 0.56832 train_acc= 0.41924 val_roc= 0.86131 val_ap= 0.86105 time= 0.07568\n",
      "训练次数: 18 Epoch: 0018 log_lik= 0.538162 train_kl= -0.01283 train_loss= 0.55099 train_acc= 0.46366 val_roc= 0.86240 val_ap= 0.86239 time= 0.07567\n",
      "训练次数: 18 Epoch: 0019 log_lik= 0.5308519 train_kl= -0.01336 train_loss= 0.54421 train_acc= 0.47557 val_roc= 0.86250 val_ap= 0.86208 time= 0.07468\n",
      "训练次数: 18 Epoch: 0020 log_lik= 0.5348184 train_kl= -0.01383 train_loss= 0.54864 train_acc= 0.47406 val_roc= 0.86809 val_ap= 0.86952 time= 0.08365\n",
      "训练次数: 18 Epoch: 0021 log_lik= 0.5301491 train_kl= -0.01420 train_loss= 0.54435 train_acc= 0.49050 val_roc= 0.87474 val_ap= 0.87781 time= 0.07468\n",
      "训练次数: 18 Epoch: 0022 log_lik= 0.52286106 train_kl= -0.01451 train_loss= 0.53737 train_acc= 0.50686 val_roc= 0.87996 val_ap= 0.88390 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 18 Epoch: 0023 log_lik= 0.5209015 train_kl= -0.01475 train_loss= 0.53565 train_acc= 0.51226 val_roc= 0.88381 val_ap= 0.88827 time= 0.07568\n",
      "训练次数: 18 Epoch: 0024 log_lik= 0.5194199 train_kl= -0.01492 train_loss= 0.53434 train_acc= 0.51162 val_roc= 0.88600 val_ap= 0.89092 time= 0.07469\n",
      "训练次数: 18 Epoch: 0025 log_lik= 0.51359963 train_kl= -0.01502 train_loss= 0.52862 train_acc= 0.51344 val_roc= 0.88658 val_ap= 0.89226 time= 0.08265\n",
      "训练次数: 18 Epoch: 0026 log_lik= 0.5020855 train_kl= -0.01505 train_loss= 0.51713 train_acc= 0.51871 val_roc= 0.88612 val_ap= 0.89185 time= 0.07667\n",
      "训练次数: 18 Epoch: 0027 log_lik= 0.49450505 train_kl= -0.01504 train_loss= 0.50954 train_acc= 0.52050 val_roc= 0.88637 val_ap= 0.89234 time= 0.07668\n",
      "训练次数: 18 Epoch: 0028 log_lik= 0.48888266 train_kl= -0.01499 train_loss= 0.50388 train_acc= 0.52032 val_roc= 0.88755 val_ap= 0.89394 time= 0.07558\n",
      "训练次数: 18 Epoch: 0029 log_lik= 0.4860325 train_kl= -0.01494 train_loss= 0.50097 train_acc= 0.51864 val_roc= 0.88823 val_ap= 0.89489 time= 0.07767\n",
      "训练次数: 18 Epoch: 0030 log_lik= 0.4830042 train_kl= -0.01487 train_loss= 0.49788 train_acc= 0.51624 val_roc= 0.88836 val_ap= 0.89592 time= 0.07774\n",
      "训练次数: 18 Epoch: 0031 log_lik= 0.48015404 train_kl= -0.01480 train_loss= 0.49495 train_acc= 0.51730 val_roc= 0.88963 val_ap= 0.89753 time= 0.07468\n",
      "训练次数: 18 Epoch: 0032 log_lik= 0.4766514 train_kl= -0.01472 train_loss= 0.49137 train_acc= 0.52170 val_roc= 0.88879 val_ap= 0.89708 time= 0.08165\n",
      "训练次数: 18 Epoch: 0033 log_lik= 0.4760836 train_kl= -0.01464 train_loss= 0.49072 train_acc= 0.52126 val_roc= 0.88835 val_ap= 0.89705 time= 0.07568\n",
      "训练次数: 18 Epoch: 0034 log_lik= 0.4768338 train_kl= -0.01458 train_loss= 0.49141 train_acc= 0.51893 val_roc= 0.88793 val_ap= 0.89713 time= 0.07568\n",
      "训练次数: 18 Epoch: 0035 log_lik= 0.47623688 train_kl= -0.01452 train_loss= 0.49076 train_acc= 0.51503 val_roc= 0.88881 val_ap= 0.89789 time= 0.07568\n",
      "训练次数: 18 Epoch: 0036 log_lik= 0.47463873 train_kl= -0.01446 train_loss= 0.48910 train_acc= 0.51638 val_roc= 0.89046 val_ap= 0.89944 time= 0.08165\n",
      "训练次数: 18 Epoch: 0037 log_lik= 0.47054914 train_kl= -0.01440 train_loss= 0.48495 train_acc= 0.52182 val_roc= 0.89137 val_ap= 0.90010 time= 0.08066\n",
      "训练次数: 18 Epoch: 0038 log_lik= 0.46738705 train_kl= -0.01434 train_loss= 0.48172 train_acc= 0.52562 val_roc= 0.89154 val_ap= 0.90026 time= 0.07667\n",
      "训练次数: 18 Epoch: 0039 log_lik= 0.46570903 train_kl= -0.01429 train_loss= 0.48000 train_acc= 0.52538 val_roc= 0.89177 val_ap= 0.90074 time= 0.07468\n",
      "训练次数: 18 Epoch: 0040 log_lik= 0.46420342 train_kl= -0.01426 train_loss= 0.47846 train_acc= 0.52573 val_roc= 0.89294 val_ap= 0.90131 time= 0.08165\n",
      "训练次数: 18 Epoch: 0041 log_lik= 0.46207398 train_kl= -0.01424 train_loss= 0.47631 train_acc= 0.52644 val_roc= 0.89568 val_ap= 0.90385 time= 0.08962\n",
      "训练次数: 18 Epoch: 0042 log_lik= 0.45947203 train_kl= -0.01421 train_loss= 0.47369 train_acc= 0.52902 val_roc= 0.89744 val_ap= 0.90560 time= 0.07568\n",
      "训练次数: 18 Epoch: 0043 log_lik= 0.45806974 train_kl= -0.01419 train_loss= 0.47225 train_acc= 0.52999 val_roc= 0.89786 val_ap= 0.90630 time= 0.08265\n",
      "训练次数: 18 Epoch: 0044 log_lik= 0.45648387 train_kl= -0.01415 train_loss= 0.47063 train_acc= 0.53121 val_roc= 0.89831 val_ap= 0.90668 time= 0.07568\n",
      "训练次数: 18 Epoch: 0045 log_lik= 0.45551935 train_kl= -0.01409 train_loss= 0.46961 train_acc= 0.53097 val_roc= 0.89935 val_ap= 0.90749 time= 0.07568\n",
      "训练次数: 18 Epoch: 0046 log_lik= 0.4548655 train_kl= -0.01401 train_loss= 0.46887 train_acc= 0.53246 val_roc= 0.90069 val_ap= 0.90856 time= 0.08165\n",
      "训练次数: 18 Epoch: 0047 log_lik= 0.45401433 train_kl= -0.01391 train_loss= 0.46793 train_acc= 0.53242 val_roc= 0.90188 val_ap= 0.91019 time= 0.08265\n",
      "训练次数: 18 Epoch: 0048 log_lik= 0.4526123 train_kl= -0.01382 train_loss= 0.46643 train_acc= 0.53246 val_roc= 0.90168 val_ap= 0.91069 time= 0.08265\n",
      "训练次数: 18 Epoch: 0049 log_lik= 0.45170942 train_kl= -0.01371 train_loss= 0.46542 train_acc= 0.53249 val_roc= 0.90172 val_ap= 0.91110 time= 0.07568\n",
      "训练次数: 18 Epoch: 0050 log_lik= 0.45020583 train_kl= -0.01361 train_loss= 0.46381 train_acc= 0.53270 val_roc= 0.90277 val_ap= 0.91207 time= 0.07568\n",
      "训练次数: 18 Epoch: 0051 log_lik= 0.44913208 train_kl= -0.01350 train_loss= 0.46263 train_acc= 0.53336 val_roc= 0.90264 val_ap= 0.91214 time= 0.07667\n",
      "训练次数: 18 Epoch: 0052 log_lik= 0.4481621 train_kl= -0.01340 train_loss= 0.46156 train_acc= 0.53316 val_roc= 0.90241 val_ap= 0.91220 time= 0.07468\n",
      "训练次数: 18 Epoch: 0053 log_lik= 0.4472581 train_kl= -0.01330 train_loss= 0.46056 train_acc= 0.53425 val_roc= 0.90325 val_ap= 0.91270 time= 0.07470\n",
      "训练次数: 18 Epoch: 0054 log_lik= 0.4462563 train_kl= -0.01321 train_loss= 0.45946 train_acc= 0.53491 val_roc= 0.90434 val_ap= 0.91366 time= 0.07369\n",
      "训练次数: 18 Epoch: 0055 log_lik= 0.44560188 train_kl= -0.01311 train_loss= 0.45871 train_acc= 0.53557 val_roc= 0.90458 val_ap= 0.91383 time= 0.07369\n",
      "训练次数: 18 Epoch: 0056 log_lik= 0.44517648 train_kl= -0.01301 train_loss= 0.45818 train_acc= 0.53560 val_roc= 0.90429 val_ap= 0.91300 time= 0.07468\n",
      "训练次数: 18 Epoch: 0057 log_lik= 0.4442553 train_kl= -0.01291 train_loss= 0.45717 train_acc= 0.53531 val_roc= 0.90448 val_ap= 0.91327 time= 0.07975\n",
      "训练次数: 18 Epoch: 0058 log_lik= 0.4438032 train_kl= -0.01283 train_loss= 0.45663 train_acc= 0.53581 val_roc= 0.90476 val_ap= 0.91361 time= 0.08030\n",
      "训练次数: 18 Epoch: 0059 log_lik= 0.44342035 train_kl= -0.01275 train_loss= 0.45617 train_acc= 0.53678 val_roc= 0.90506 val_ap= 0.91381 time= 0.07468\n",
      "训练次数: 18 Epoch: 0060 log_lik= 0.44245657 train_kl= -0.01269 train_loss= 0.45515 train_acc= 0.53693 val_roc= 0.90564 val_ap= 0.91427 time= 0.07668\n",
      "训练次数: 18 Epoch: 0061 log_lik= 0.4421942 train_kl= -0.01264 train_loss= 0.45483 train_acc= 0.53694 val_roc= 0.90607 val_ap= 0.91432 time= 0.07568\n",
      "训练次数: 18 Epoch: 0062 log_lik= 0.44158 train_kl= -0.01259 train_loss= 0.45417 train_acc= 0.53736 val_roc= 0.90595 val_ap= 0.91403 time= 0.08066\n",
      "训练次数: 18 Epoch: 0063 log_lik= 0.4411581 train_kl= -0.01255 train_loss= 0.45371 train_acc= 0.53740 val_roc= 0.90619 val_ap= 0.91401 time= 0.07568\n",
      "训练次数: 18 Epoch: 0064 log_lik= 0.44051167 train_kl= -0.01250 train_loss= 0.45301 train_acc= 0.53804 val_roc= 0.90702 val_ap= 0.91501 time= 0.07668\n",
      "训练次数: 18 Epoch: 0065 log_lik= 0.4406271 train_kl= -0.01246 train_loss= 0.45308 train_acc= 0.53735 val_roc= 0.90795 val_ap= 0.91551 time= 0.07468\n",
      "训练次数: 18 Epoch: 0066 log_lik= 0.43982556 train_kl= -0.01241 train_loss= 0.45224 train_acc= 0.53780 val_roc= 0.90854 val_ap= 0.91570 time= 0.07369\n",
      "训练次数: 18 Epoch: 0067 log_lik= 0.43946618 train_kl= -0.01237 train_loss= 0.45183 train_acc= 0.53819 val_roc= 0.90882 val_ap= 0.91611 time= 0.07468\n",
      "训练次数: 18 Epoch: 0068 log_lik= 0.43946874 train_kl= -0.01232 train_loss= 0.45178 train_acc= 0.53763 val_roc= 0.90971 val_ap= 0.91714 time= 0.08265\n",
      "训练次数: 18 Epoch: 0069 log_lik= 0.43861684 train_kl= -0.01226 train_loss= 0.45088 train_acc= 0.53763 val_roc= 0.90973 val_ap= 0.91733 time= 0.07468\n",
      "训练次数: 18 Epoch: 0070 log_lik= 0.4383141 train_kl= -0.01221 train_loss= 0.45053 train_acc= 0.53863 val_roc= 0.91028 val_ap= 0.91774 time= 0.07470\n",
      "训练次数: 18 Epoch: 0071 log_lik= 0.43865255 train_kl= -0.01217 train_loss= 0.45082 train_acc= 0.53789 val_roc= 0.91074 val_ap= 0.91837 time= 0.07767\n",
      "训练次数: 18 Epoch: 0072 log_lik= 0.43822566 train_kl= -0.01213 train_loss= 0.45035 train_acc= 0.53749 val_roc= 0.91068 val_ap= 0.91840 time= 0.07468\n",
      "训练次数: 18 Epoch: 0073 log_lik= 0.43760496 train_kl= -0.01208 train_loss= 0.44969 train_acc= 0.53809 val_roc= 0.91090 val_ap= 0.91884 time= 0.07368\n",
      "训练次数: 18 Epoch: 0074 log_lik= 0.43722004 train_kl= -0.01204 train_loss= 0.44926 train_acc= 0.53761 val_roc= 0.91061 val_ap= 0.91843 time= 0.07966\n",
      "训练次数: 18 Epoch: 0075 log_lik= 0.4368853 train_kl= -0.01200 train_loss= 0.44888 train_acc= 0.53819 val_roc= 0.91182 val_ap= 0.91941 time= 0.07667\n",
      "训练次数: 18 Epoch: 0076 log_lik= 0.4369589 train_kl= -0.01196 train_loss= 0.44892 train_acc= 0.53802 val_roc= 0.91216 val_ap= 0.92000 time= 0.07469\n",
      "训练次数: 18 Epoch: 0077 log_lik= 0.43621916 train_kl= -0.01192 train_loss= 0.44814 train_acc= 0.53720 val_roc= 0.91207 val_ap= 0.92027 time= 0.07867\n",
      "训练次数: 18 Epoch: 0078 log_lik= 0.4363398 train_kl= -0.01189 train_loss= 0.44823 train_acc= 0.53713 val_roc= 0.91217 val_ap= 0.92041 time= 0.07468\n",
      "训练次数: 18 Epoch: 0079 log_lik= 0.43606156 train_kl= -0.01187 train_loss= 0.44793 train_acc= 0.53799 val_roc= 0.91275 val_ap= 0.92050 time= 0.07369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 18 Epoch: 0080 log_lik= 0.43569666 train_kl= -0.01185 train_loss= 0.44755 train_acc= 0.53779 val_roc= 0.91289 val_ap= 0.92052 time= 0.07667\n",
      "训练次数: 18 Epoch: 0081 log_lik= 0.4353175 train_kl= -0.01184 train_loss= 0.44715 train_acc= 0.53750 val_roc= 0.91240 val_ap= 0.92040 time= 0.07419\n",
      "训练次数: 18 Epoch: 0082 log_lik= 0.4351578 train_kl= -0.01183 train_loss= 0.44698 train_acc= 0.53736 val_roc= 0.91195 val_ap= 0.92015 time= 0.07568\n",
      "训练次数: 18 Epoch: 0083 log_lik= 0.4351094 train_kl= -0.01182 train_loss= 0.44693 train_acc= 0.53728 val_roc= 0.91294 val_ap= 0.92048 time= 0.07468\n",
      "训练次数: 18 Epoch: 0084 log_lik= 0.4348204 train_kl= -0.01181 train_loss= 0.44663 train_acc= 0.53656 val_roc= 0.91343 val_ap= 0.92104 time= 0.07667\n",
      "训练次数: 18 Epoch: 0085 log_lik= 0.43450496 train_kl= -0.01181 train_loss= 0.44631 train_acc= 0.53829 val_roc= 0.91323 val_ap= 0.92077 time= 0.07469\n",
      "训练次数: 18 Epoch: 0086 log_lik= 0.43404087 train_kl= -0.01180 train_loss= 0.44584 train_acc= 0.53744 val_roc= 0.91279 val_ap= 0.92048 time= 0.07966\n",
      "训练次数: 18 Epoch: 0087 log_lik= 0.43409604 train_kl= -0.01180 train_loss= 0.44589 train_acc= 0.53667 val_roc= 0.91307 val_ap= 0.92085 time= 0.07468\n",
      "训练次数: 18 Epoch: 0088 log_lik= 0.43351528 train_kl= -0.01180 train_loss= 0.44531 train_acc= 0.53760 val_roc= 0.91386 val_ap= 0.92156 time= 0.07667\n",
      "训练次数: 18 Epoch: 0089 log_lik= 0.43306655 train_kl= -0.01180 train_loss= 0.44486 train_acc= 0.53840 val_roc= 0.91459 val_ap= 0.92209 time= 0.07628\n",
      "训练次数: 18 Epoch: 0090 log_lik= 0.43273053 train_kl= -0.01179 train_loss= 0.44452 train_acc= 0.53828 val_roc= 0.91438 val_ap= 0.92161 time= 0.09611\n",
      "训练次数: 18 Epoch: 0091 log_lik= 0.43296188 train_kl= -0.01179 train_loss= 0.44476 train_acc= 0.53796 val_roc= 0.91392 val_ap= 0.92142 time= 0.07767\n",
      "训练次数: 18 Epoch: 0092 log_lik= 0.43261755 train_kl= -0.01180 train_loss= 0.44442 train_acc= 0.53762 val_roc= 0.91409 val_ap= 0.92198 time= 0.07667\n",
      "训练次数: 18 Epoch: 0093 log_lik= 0.4320757 train_kl= -0.01181 train_loss= 0.44388 train_acc= 0.53779 val_roc= 0.91518 val_ap= 0.92292 time= 0.07568\n",
      "训练次数: 18 Epoch: 0094 log_lik= 0.4317993 train_kl= -0.01180 train_loss= 0.44360 train_acc= 0.53753 val_roc= 0.91574 val_ap= 0.92353 time= 0.07469\n",
      "训练次数: 18 Epoch: 0095 log_lik= 0.43167484 train_kl= -0.01178 train_loss= 0.44346 train_acc= 0.53670 val_roc= 0.91490 val_ap= 0.92287 time= 0.07667\n",
      "训练次数: 18 Epoch: 0096 log_lik= 0.43127802 train_kl= -0.01178 train_loss= 0.44306 train_acc= 0.53737 val_roc= 0.91464 val_ap= 0.92279 time= 0.07568\n",
      "训练次数: 18 Epoch: 0097 log_lik= 0.4309597 train_kl= -0.01178 train_loss= 0.44274 train_acc= 0.53785 val_roc= 0.91555 val_ap= 0.92344 time= 0.07468\n",
      "训练次数: 18 Epoch: 0098 log_lik= 0.43060663 train_kl= -0.01179 train_loss= 0.44239 train_acc= 0.53899 val_roc= 0.91576 val_ap= 0.92370 time= 0.07568\n",
      "训练次数: 18 Epoch: 0099 log_lik= 0.43091005 train_kl= -0.01178 train_loss= 0.44269 train_acc= 0.53742 val_roc= 0.91612 val_ap= 0.92412 time= 0.07568\n",
      "训练次数: 18 Epoch: 0100 log_lik= 0.43053198 train_kl= -0.01178 train_loss= 0.44231 train_acc= 0.53668 val_roc= 0.91618 val_ap= 0.92444 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 18 ROC score: 0.9227592365219333\n",
      "训练次数: 18 AP score: 0.9260168675204784\n",
      "训练次数: 19 Epoch: 0001 log_lik= 1.8012273 train_kl= -0.00006 train_loss= 1.80129 train_acc= 0.49410 val_roc= 0.67455 val_ap= 0.70258 time= 1.59987\n",
      "训练次数: 19 Epoch: 0002 log_lik= 1.5796288 train_kl= -0.00011 train_loss= 1.57974 train_acc= 0.47413 val_roc= 0.67993 val_ap= 0.71065 time= 0.08758\n",
      "训练次数: 19 Epoch: 0003 log_lik= 1.4979032 train_kl= -0.00029 train_loss= 1.49819 train_acc= 0.42925 val_roc= 0.68839 val_ap= 0.71826 time= 0.07568\n",
      "训练次数: 19 Epoch: 0004 log_lik= 1.4043058 train_kl= -0.00044 train_loss= 1.40474 train_acc= 0.41133 val_roc= 0.69742 val_ap= 0.72730 time= 0.07468\n",
      "训练次数: 19 Epoch: 0005 log_lik= 1.26329 train_kl= -0.00059 train_loss= 1.26388 train_acc= 0.41062 val_roc= 0.70874 val_ap= 0.73867 time= 0.07667\n",
      "训练次数: 19 Epoch: 0006 log_lik= 1.1392381 train_kl= -0.00080 train_loss= 1.14004 train_acc= 0.40375 val_roc= 0.72184 val_ap= 0.74979 time= 0.07568\n",
      "训练次数: 19 Epoch: 0007 log_lik= 1.0583051 train_kl= -0.00114 train_loss= 1.05944 train_acc= 0.38952 val_roc= 0.73549 val_ap= 0.76167 time= 0.08663\n",
      "训练次数: 19 Epoch: 0008 log_lik= 0.9512648 train_kl= -0.00161 train_loss= 0.95287 train_acc= 0.37880 val_roc= 0.74571 val_ap= 0.77069 time= 0.07720\n",
      "训练次数: 19 Epoch: 0009 log_lik= 0.85495174 train_kl= -0.00220 train_loss= 0.85715 train_acc= 0.37083 val_roc= 0.74903 val_ap= 0.77093 time= 0.07468\n",
      "训练次数: 19 Epoch: 0010 log_lik= 0.787987 train_kl= -0.00293 train_loss= 0.79092 train_acc= 0.33821 val_roc= 0.75528 val_ap= 0.77334 time= 0.07568\n",
      "训练次数: 19 Epoch: 0011 log_lik= 0.7333327 train_kl= -0.00374 train_loss= 0.73707 train_acc= 0.32284 val_roc= 0.77519 val_ap= 0.78682 time= 0.07369\n",
      "训练次数: 19 Epoch: 0012 log_lik= 0.70145845 train_kl= -0.00459 train_loss= 0.70604 train_acc= 0.31122 val_roc= 0.79207 val_ap= 0.79640 time= 0.07568\n",
      "训练次数: 19 Epoch: 0013 log_lik= 0.6801951 train_kl= -0.00545 train_loss= 0.68564 train_acc= 0.29677 val_roc= 0.79955 val_ap= 0.79983 time= 0.07600\n",
      "训练次数: 19 Epoch: 0014 log_lik= 0.6693087 train_kl= -0.00630 train_loss= 0.67561 train_acc= 0.25907 val_roc= 0.81104 val_ap= 0.80872 time= 0.07568\n",
      "训练次数: 19 Epoch: 0015 log_lik= 0.6536056 train_kl= -0.00707 train_loss= 0.66068 train_acc= 0.25575 val_roc= 0.82304 val_ap= 0.81821 time= 0.08465\n",
      "训练次数: 19 Epoch: 0016 log_lik= 0.6401355 train_kl= -0.00776 train_loss= 0.64790 train_acc= 0.26019 val_roc= 0.83234 val_ap= 0.82282 time= 0.07369\n",
      "训练次数: 19 Epoch: 0017 log_lik= 0.6258695 train_kl= -0.00839 train_loss= 0.63426 train_acc= 0.28497 val_roc= 0.83964 val_ap= 0.82606 time= 0.07269\n",
      "训练次数: 19 Epoch: 0018 log_lik= 0.6094603 train_kl= -0.00898 train_loss= 0.61844 train_acc= 0.31876 val_roc= 0.84515 val_ap= 0.83043 time= 0.07368\n",
      "训练次数: 19 Epoch: 0019 log_lik= 0.59827614 train_kl= -0.00955 train_loss= 0.60783 train_acc= 0.33295 val_roc= 0.84837 val_ap= 0.83410 time= 0.07468\n",
      "训练次数: 19 Epoch: 0020 log_lik= 0.58364576 train_kl= -0.01008 train_loss= 0.59373 train_acc= 0.37306 val_roc= 0.84810 val_ap= 0.83536 time= 0.08364\n",
      "训练次数: 19 Epoch: 0021 log_lik= 0.56849694 train_kl= -0.01054 train_loss= 0.57904 train_acc= 0.41284 val_roc= 0.84587 val_ap= 0.83492 time= 0.07568\n",
      "训练次数: 19 Epoch: 0022 log_lik= 0.5553621 train_kl= -0.01095 train_loss= 0.56631 train_acc= 0.45020 val_roc= 0.84719 val_ap= 0.83828 time= 0.07521\n",
      "训练次数: 19 Epoch: 0023 log_lik= 0.54437584 train_kl= -0.01130 train_loss= 0.55568 train_acc= 0.47978 val_roc= 0.85550 val_ap= 0.84771 time= 0.07767\n",
      "训练次数: 19 Epoch: 0024 log_lik= 0.5391594 train_kl= -0.01160 train_loss= 0.55076 train_acc= 0.48810 val_roc= 0.86169 val_ap= 0.85568 time= 0.07667\n",
      "训练次数: 19 Epoch: 0025 log_lik= 0.5369105 train_kl= -0.01184 train_loss= 0.54875 train_acc= 0.48663 val_roc= 0.86702 val_ap= 0.86270 time= 0.07767\n",
      "训练次数: 19 Epoch: 0026 log_lik= 0.5363904 train_kl= -0.01201 train_loss= 0.54840 train_acc= 0.48135 val_roc= 0.87104 val_ap= 0.86796 time= 0.07667\n",
      "训练次数: 19 Epoch: 0027 log_lik= 0.5348544 train_kl= -0.01214 train_loss= 0.54699 train_acc= 0.48133 val_roc= 0.87901 val_ap= 0.87732 time= 0.07668\n",
      "训练次数: 19 Epoch: 0028 log_lik= 0.5283562 train_kl= -0.01223 train_loss= 0.54059 train_acc= 0.48431 val_roc= 0.88783 val_ap= 0.88741 time= 0.07667\n",
      "训练次数: 19 Epoch: 0029 log_lik= 0.5172691 train_kl= -0.01229 train_loss= 0.52956 train_acc= 0.49044 val_roc= 0.89358 val_ap= 0.89435 time= 0.07468\n",
      "训练次数: 19 Epoch: 0030 log_lik= 0.5089699 train_kl= -0.01231 train_loss= 0.52128 train_acc= 0.49559 val_roc= 0.89689 val_ap= 0.89951 time= 0.08066\n",
      "训练次数: 19 Epoch: 0031 log_lik= 0.50268865 train_kl= -0.01229 train_loss= 0.51498 train_acc= 0.50175 val_roc= 0.89818 val_ap= 0.90260 time= 0.07568\n",
      "训练次数: 19 Epoch: 0032 log_lik= 0.49813402 train_kl= -0.01224 train_loss= 0.51038 train_acc= 0.50286 val_roc= 0.90061 val_ap= 0.90589 time= 0.08265\n",
      "训练次数: 19 Epoch: 0033 log_lik= 0.4952209 train_kl= -0.01217 train_loss= 0.50739 train_acc= 0.50451 val_roc= 0.90397 val_ap= 0.90949 time= 0.07468\n",
      "训练次数: 19 Epoch: 0034 log_lik= 0.49209237 train_kl= -0.01209 train_loss= 0.50418 train_acc= 0.50397 val_roc= 0.90700 val_ap= 0.91224 time= 0.07468\n",
      "训练次数: 19 Epoch: 0035 log_lik= 0.49051264 train_kl= -0.01202 train_loss= 0.50253 train_acc= 0.50397 val_roc= 0.90931 val_ap= 0.91442 time= 0.08066\n",
      "训练次数: 19 Epoch: 0036 log_lik= 0.4884031 train_kl= -0.01197 train_loss= 0.50037 train_acc= 0.50328 val_roc= 0.90940 val_ap= 0.91366 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 19 Epoch: 0037 log_lik= 0.48756224 train_kl= -0.01193 train_loss= 0.49949 train_acc= 0.50469 val_roc= 0.90940 val_ap= 0.91256 time= 0.07468\n",
      "训练次数: 19 Epoch: 0038 log_lik= 0.48574588 train_kl= -0.01189 train_loss= 0.49764 train_acc= 0.50755 val_roc= 0.91047 val_ap= 0.91282 time= 0.07481\n",
      "训练次数: 19 Epoch: 0039 log_lik= 0.4820244 train_kl= -0.01185 train_loss= 0.49388 train_acc= 0.51113 val_roc= 0.91240 val_ap= 0.91446 time= 0.07369\n",
      "训练次数: 19 Epoch: 0040 log_lik= 0.47828677 train_kl= -0.01180 train_loss= 0.49009 train_acc= 0.51589 val_roc= 0.91418 val_ap= 0.91604 time= 0.07468\n",
      "训练次数: 19 Epoch: 0041 log_lik= 0.4766841 train_kl= -0.01177 train_loss= 0.48846 train_acc= 0.51829 val_roc= 0.91602 val_ap= 0.91714 time= 0.07469\n",
      "训练次数: 19 Epoch: 0042 log_lik= 0.474421 train_kl= -0.01176 train_loss= 0.48619 train_acc= 0.51877 val_roc= 0.91613 val_ap= 0.91657 time= 0.07568\n",
      "训练次数: 19 Epoch: 0043 log_lik= 0.473039 train_kl= -0.01177 train_loss= 0.48481 train_acc= 0.51961 val_roc= 0.91612 val_ap= 0.91622 time= 0.07767\n",
      "训练次数: 19 Epoch: 0044 log_lik= 0.47225043 train_kl= -0.01178 train_loss= 0.48403 train_acc= 0.51892 val_roc= 0.91616 val_ap= 0.91599 time= 0.07667\n",
      "训练次数: 19 Epoch: 0045 log_lik= 0.47087315 train_kl= -0.01180 train_loss= 0.48267 train_acc= 0.51999 val_roc= 0.91745 val_ap= 0.91680 time= 0.07369\n",
      "训练次数: 19 Epoch: 0046 log_lik= 0.4694726 train_kl= -0.01182 train_loss= 0.48129 train_acc= 0.52182 val_roc= 0.91820 val_ap= 0.91721 time= 0.07767\n",
      "训练次数: 19 Epoch: 0047 log_lik= 0.46725017 train_kl= -0.01183 train_loss= 0.47908 train_acc= 0.52384 val_roc= 0.91884 val_ap= 0.91733 time= 0.07369\n",
      "训练次数: 19 Epoch: 0048 log_lik= 0.46529025 train_kl= -0.01183 train_loss= 0.47712 train_acc= 0.52713 val_roc= 0.91858 val_ap= 0.91740 time= 0.07637\n",
      "训练次数: 19 Epoch: 0049 log_lik= 0.46429178 train_kl= -0.01182 train_loss= 0.47611 train_acc= 0.52854 val_roc= 0.91834 val_ap= 0.91743 time= 0.09161\n",
      "训练次数: 19 Epoch: 0050 log_lik= 0.46375042 train_kl= -0.01180 train_loss= 0.47555 train_acc= 0.52800 val_roc= 0.91894 val_ap= 0.91843 time= 0.07568\n",
      "训练次数: 19 Epoch: 0051 log_lik= 0.46227378 train_kl= -0.01177 train_loss= 0.47404 train_acc= 0.52770 val_roc= 0.91989 val_ap= 0.92021 time= 0.07369\n",
      "训练次数: 19 Epoch: 0052 log_lik= 0.46073952 train_kl= -0.01173 train_loss= 0.47247 train_acc= 0.52906 val_roc= 0.92038 val_ap= 0.92149 time= 0.07767\n",
      "训练次数: 19 Epoch: 0053 log_lik= 0.4601855 train_kl= -0.01168 train_loss= 0.47186 train_acc= 0.52946 val_roc= 0.92131 val_ap= 0.92312 time= 0.07568\n",
      "训练次数: 19 Epoch: 0054 log_lik= 0.45852688 train_kl= -0.01161 train_loss= 0.47014 train_acc= 0.52958 val_roc= 0.92229 val_ap= 0.92495 time= 0.07667\n",
      "训练次数: 19 Epoch: 0055 log_lik= 0.45752072 train_kl= -0.01153 train_loss= 0.46906 train_acc= 0.52935 val_roc= 0.92222 val_ap= 0.92576 time= 0.07767\n",
      "训练次数: 19 Epoch: 0056 log_lik= 0.45673674 train_kl= -0.01146 train_loss= 0.46820 train_acc= 0.53047 val_roc= 0.92261 val_ap= 0.92688 time= 0.07866\n",
      "训练次数: 19 Epoch: 0057 log_lik= 0.45561132 train_kl= -0.01139 train_loss= 0.46700 train_acc= 0.53124 val_roc= 0.92304 val_ap= 0.92786 time= 0.07468\n",
      "训练次数: 19 Epoch: 0058 log_lik= 0.454485 train_kl= -0.01134 train_loss= 0.46583 train_acc= 0.53197 val_roc= 0.92346 val_ap= 0.92855 time= 0.07767\n",
      "训练次数: 19 Epoch: 0059 log_lik= 0.4543101 train_kl= -0.01131 train_loss= 0.46562 train_acc= 0.53072 val_roc= 0.92401 val_ap= 0.92964 time= 0.07867\n",
      "训练次数: 19 Epoch: 0060 log_lik= 0.4531092 train_kl= -0.01129 train_loss= 0.46440 train_acc= 0.53165 val_roc= 0.92474 val_ap= 0.93071 time= 0.07468\n",
      "训练次数: 19 Epoch: 0061 log_lik= 0.4523925 train_kl= -0.01127 train_loss= 0.46367 train_acc= 0.53132 val_roc= 0.92562 val_ap= 0.93179 time= 0.07767\n",
      "训练次数: 19 Epoch: 0062 log_lik= 0.4520367 train_kl= -0.01126 train_loss= 0.46330 train_acc= 0.53207 val_roc= 0.92654 val_ap= 0.93278 time= 0.07568\n",
      "训练次数: 19 Epoch: 0063 log_lik= 0.45053932 train_kl= -0.01124 train_loss= 0.46178 train_acc= 0.53216 val_roc= 0.92700 val_ap= 0.93368 time= 0.07425\n",
      "训练次数: 19 Epoch: 0064 log_lik= 0.450051 train_kl= -0.01124 train_loss= 0.46129 train_acc= 0.53284 val_roc= 0.92709 val_ap= 0.93439 time= 0.07568\n",
      "训练次数: 19 Epoch: 0065 log_lik= 0.44873914 train_kl= -0.01126 train_loss= 0.46000 train_acc= 0.53337 val_roc= 0.92790 val_ap= 0.93511 time= 0.08265\n",
      "训练次数: 19 Epoch: 0066 log_lik= 0.4475393 train_kl= -0.01129 train_loss= 0.45883 train_acc= 0.53502 val_roc= 0.92884 val_ap= 0.93577 time= 0.08365\n",
      "训练次数: 19 Epoch: 0067 log_lik= 0.4475923 train_kl= -0.01131 train_loss= 0.45890 train_acc= 0.53534 val_roc= 0.92926 val_ap= 0.93620 time= 0.07568\n",
      "训练次数: 19 Epoch: 0068 log_lik= 0.44702354 train_kl= -0.01133 train_loss= 0.45836 train_acc= 0.53603 val_roc= 0.92883 val_ap= 0.93591 time= 0.07667\n",
      "训练次数: 19 Epoch: 0069 log_lik= 0.44619155 train_kl= -0.01136 train_loss= 0.45755 train_acc= 0.53497 val_roc= 0.92936 val_ap= 0.93621 time= 0.07348\n",
      "训练次数: 19 Epoch: 0070 log_lik= 0.4458081 train_kl= -0.01137 train_loss= 0.45718 train_acc= 0.53549 val_roc= 0.93007 val_ap= 0.93661 time= 0.08464\n",
      "训练次数: 19 Epoch: 0071 log_lik= 0.44517073 train_kl= -0.01140 train_loss= 0.45657 train_acc= 0.53619 val_roc= 0.92978 val_ap= 0.93663 time= 0.08464\n",
      "训练次数: 19 Epoch: 0072 log_lik= 0.44441873 train_kl= -0.01143 train_loss= 0.45585 train_acc= 0.53630 val_roc= 0.92849 val_ap= 0.93573 time= 0.07568\n",
      "训练次数: 19 Epoch: 0073 log_lik= 0.4441048 train_kl= -0.01144 train_loss= 0.45554 train_acc= 0.53699 val_roc= 0.92909 val_ap= 0.93605 time= 0.07568\n",
      "训练次数: 19 Epoch: 0074 log_lik= 0.4436083 train_kl= -0.01143 train_loss= 0.45504 train_acc= 0.53687 val_roc= 0.92925 val_ap= 0.93575 time= 0.07468\n",
      "训练次数: 19 Epoch: 0075 log_lik= 0.44276235 train_kl= -0.01143 train_loss= 0.45420 train_acc= 0.53635 val_roc= 0.92881 val_ap= 0.93572 time= 0.08265\n",
      "训练次数: 19 Epoch: 0076 log_lik= 0.4425558 train_kl= -0.01144 train_loss= 0.45400 train_acc= 0.53710 val_roc= 0.92888 val_ap= 0.93596 time= 0.07767\n",
      "训练次数: 19 Epoch: 0077 log_lik= 0.4420354 train_kl= -0.01144 train_loss= 0.45347 train_acc= 0.53798 val_roc= 0.92890 val_ap= 0.93622 time= 0.07568\n",
      "训练次数: 19 Epoch: 0078 log_lik= 0.44139555 train_kl= -0.01144 train_loss= 0.45284 train_acc= 0.53772 val_roc= 0.92718 val_ap= 0.93498 time= 0.08265\n",
      "训练次数: 19 Epoch: 0079 log_lik= 0.44118795 train_kl= -0.01146 train_loss= 0.45264 train_acc= 0.53762 val_roc= 0.92637 val_ap= 0.93443 time= 0.08364\n",
      "训练次数: 19 Epoch: 0080 log_lik= 0.44085348 train_kl= -0.01147 train_loss= 0.45233 train_acc= 0.53831 val_roc= 0.92666 val_ap= 0.93455 time= 0.07468\n",
      "训练次数: 19 Epoch: 0081 log_lik= 0.440171 train_kl= -0.01148 train_loss= 0.45165 train_acc= 0.53902 val_roc= 0.92724 val_ap= 0.93508 time= 0.07468\n",
      "训练次数: 19 Epoch: 0082 log_lik= 0.43930253 train_kl= -0.01148 train_loss= 0.45079 train_acc= 0.53954 val_roc= 0.92625 val_ap= 0.93462 time= 0.07568\n",
      "训练次数: 19 Epoch: 0083 log_lik= 0.43948236 train_kl= -0.01149 train_loss= 0.45098 train_acc= 0.53967 val_roc= 0.92563 val_ap= 0.93402 time= 0.07668\n",
      "训练次数: 19 Epoch: 0084 log_lik= 0.43906724 train_kl= -0.01151 train_loss= 0.45057 train_acc= 0.53985 val_roc= 0.92578 val_ap= 0.93375 time= 0.07468\n",
      "训练次数: 19 Epoch: 0085 log_lik= 0.43883216 train_kl= -0.01152 train_loss= 0.45036 train_acc= 0.53852 val_roc= 0.92562 val_ap= 0.93359 time= 0.07468\n",
      "训练次数: 19 Epoch: 0086 log_lik= 0.43811905 train_kl= -0.01157 train_loss= 0.44968 train_acc= 0.54010 val_roc= 0.92531 val_ap= 0.93335 time= 0.07867\n",
      "训练次数: 19 Epoch: 0087 log_lik= 0.43765822 train_kl= -0.01160 train_loss= 0.44926 train_acc= 0.54036 val_roc= 0.92510 val_ap= 0.93316 time= 0.07966\n",
      "训练次数: 19 Epoch: 0088 log_lik= 0.43696785 train_kl= -0.01162 train_loss= 0.44858 train_acc= 0.54084 val_roc= 0.92414 val_ap= 0.93233 time= 0.07468\n",
      "训练次数: 19 Epoch: 0089 log_lik= 0.4368246 train_kl= -0.01164 train_loss= 0.44846 train_acc= 0.54106 val_roc= 0.92436 val_ap= 0.93231 time= 0.08065\n",
      "训练次数: 19 Epoch: 0090 log_lik= 0.43673837 train_kl= -0.01165 train_loss= 0.44839 train_acc= 0.54188 val_roc= 0.92546 val_ap= 0.93297 time= 0.09165\n",
      "训练次数: 19 Epoch: 0091 log_lik= 0.43620318 train_kl= -0.01167 train_loss= 0.44787 train_acc= 0.54119 val_roc= 0.92468 val_ap= 0.93181 time= 0.07767\n",
      "训练次数: 19 Epoch: 0092 log_lik= 0.43593442 train_kl= -0.01170 train_loss= 0.44764 train_acc= 0.54186 val_roc= 0.92330 val_ap= 0.93107 time= 0.07767\n",
      "训练次数: 19 Epoch: 0093 log_lik= 0.43582174 train_kl= -0.01172 train_loss= 0.44754 train_acc= 0.54076 val_roc= 0.92380 val_ap= 0.93095 time= 0.07469\n",
      "训练次数: 19 Epoch: 0094 log_lik= 0.4350579 train_kl= -0.01172 train_loss= 0.44677 train_acc= 0.54178 val_roc= 0.92316 val_ap= 0.93014 time= 0.07269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 19 Epoch: 0095 log_lik= 0.43551436 train_kl= -0.01172 train_loss= 0.44724 train_acc= 0.54100 val_roc= 0.92170 val_ap= 0.92919 time= 0.07667\n",
      "训练次数: 19 Epoch: 0096 log_lik= 0.4346169 train_kl= -0.01175 train_loss= 0.44636 train_acc= 0.54153 val_roc= 0.92109 val_ap= 0.92923 time= 0.08165\n",
      "训练次数: 19 Epoch: 0097 log_lik= 0.43486065 train_kl= -0.01177 train_loss= 0.44663 train_acc= 0.54110 val_roc= 0.92238 val_ap= 0.92919 time= 0.07966\n",
      "训练次数: 19 Epoch: 0098 log_lik= 0.43431512 train_kl= -0.01178 train_loss= 0.44610 train_acc= 0.54235 val_roc= 0.92093 val_ap= 0.92841 time= 0.07468\n",
      "训练次数: 19 Epoch: 0099 log_lik= 0.43371204 train_kl= -0.01179 train_loss= 0.44550 train_acc= 0.54274 val_roc= 0.91915 val_ap= 0.92791 time= 0.07767\n",
      "训练次数: 19 Epoch: 0100 log_lik= 0.43363512 train_kl= -0.01179 train_loss= 0.44542 train_acc= 0.54082 val_roc= 0.91957 val_ap= 0.92821 time= 0.08165\n",
      "Optimization Finished!\n",
      "训练次数: 19 ROC score: 0.9320524684134533\n",
      "训练次数: 19 AP score: 0.9339102451987373\n",
      "训练次数: 20 Epoch: 0001 log_lik= 1.6759671 train_kl= -0.00004 train_loss= 1.67601 train_acc= 0.49626 val_roc= 0.69595 val_ap= 0.72043 time= 1.57526\n",
      "训练次数: 20 Epoch: 0002 log_lik= 1.4637095 train_kl= -0.00031 train_loss= 1.46402 train_acc= 0.45121 val_roc= 0.70234 val_ap= 0.73338 time= 0.08265\n",
      "训练次数: 20 Epoch: 0003 log_lik= 1.2377905 train_kl= -0.00068 train_loss= 1.23847 train_acc= 0.41770 val_roc= 0.70357 val_ap= 0.73961 time= 0.07568\n",
      "训练次数: 20 Epoch: 0004 log_lik= 1.0785127 train_kl= -0.00122 train_loss= 1.07973 train_acc= 0.38009 val_roc= 0.71123 val_ap= 0.75051 time= 0.07567\n",
      "训练次数: 20 Epoch: 0005 log_lik= 0.9408254 train_kl= -0.00191 train_loss= 0.94273 train_acc= 0.34953 val_roc= 0.73216 val_ap= 0.76979 time= 0.07470\n",
      "训练次数: 20 Epoch: 0006 log_lik= 0.81830513 train_kl= -0.00272 train_loss= 0.82103 train_acc= 0.35916 val_roc= 0.77853 val_ap= 0.80141 time= 0.07469\n",
      "训练次数: 20 Epoch: 0007 log_lik= 0.7407284 train_kl= -0.00371 train_loss= 0.74444 train_acc= 0.40375 val_roc= 0.77328 val_ap= 0.79317 time= 0.07767\n",
      "训练次数: 20 Epoch: 0008 log_lik= 0.7146631 train_kl= -0.00488 train_loss= 0.71954 train_acc= 0.39273 val_roc= 0.74046 val_ap= 0.77051 time= 0.07321\n",
      "训练次数: 20 Epoch: 0009 log_lik= 0.70174354 train_kl= -0.00617 train_loss= 0.70791 train_acc= 0.21521 val_roc= 0.76401 val_ap= 0.79187 time= 0.07468\n",
      "训练次数: 20 Epoch: 0010 log_lik= 0.693224 train_kl= -0.00733 train_loss= 0.70056 train_acc= 0.15853 val_roc= 0.81253 val_ap= 0.83233 time= 0.07568\n",
      "训练次数: 20 Epoch: 0011 log_lik= 0.6744686 train_kl= -0.00835 train_loss= 0.68282 train_acc= 0.16400 val_roc= 0.83675 val_ap= 0.84931 time= 0.07468\n",
      "训练次数: 20 Epoch: 0012 log_lik= 0.6595246 train_kl= -0.00931 train_loss= 0.66883 train_acc= 0.16887 val_roc= 0.84370 val_ap= 0.85058 time= 0.07468\n",
      "训练次数: 20 Epoch: 0013 log_lik= 0.63935274 train_kl= -0.01020 train_loss= 0.64955 train_acc= 0.20494 val_roc= 0.85742 val_ap= 0.86026 time= 0.08364\n",
      "训练次数: 20 Epoch: 0014 log_lik= 0.6116325 train_kl= -0.01102 train_loss= 0.62265 train_acc= 0.29230 val_roc= 0.86133 val_ap= 0.86285 time= 0.08265\n",
      "训练次数: 20 Epoch: 0015 log_lik= 0.5807304 train_kl= -0.01178 train_loss= 0.59251 train_acc= 0.39352 val_roc= 0.85816 val_ap= 0.86012 time= 0.07468\n",
      "训练次数: 20 Epoch: 0016 log_lik= 0.5615612 train_kl= -0.01250 train_loss= 0.57406 train_acc= 0.42412 val_roc= 0.86170 val_ap= 0.86589 time= 0.07667\n",
      "训练次数: 20 Epoch: 0017 log_lik= 0.5512607 train_kl= -0.01314 train_loss= 0.56440 train_acc= 0.42908 val_roc= 0.86433 val_ap= 0.87294 time= 0.07667\n",
      "训练次数: 20 Epoch: 0018 log_lik= 0.5462557 train_kl= -0.01368 train_loss= 0.55993 train_acc= 0.44388 val_roc= 0.86240 val_ap= 0.87259 time= 0.07468\n",
      "训练次数: 20 Epoch: 0019 log_lik= 0.53945374 train_kl= -0.01413 train_loss= 0.55358 train_acc= 0.47884 val_roc= 0.86612 val_ap= 0.87559 time= 0.07767\n",
      "训练次数: 20 Epoch: 0020 log_lik= 0.53121954 train_kl= -0.01449 train_loss= 0.54571 train_acc= 0.50781 val_roc= 0.87357 val_ap= 0.88141 time= 0.07568\n",
      "训练次数: 20 Epoch: 0021 log_lik= 0.5298643 train_kl= -0.01478 train_loss= 0.54465 train_acc= 0.51037 val_roc= 0.87895 val_ap= 0.88684 time= 0.08365\n",
      "训练次数: 20 Epoch: 0022 log_lik= 0.53592426 train_kl= -0.01502 train_loss= 0.55094 train_acc= 0.49828 val_roc= 0.88018 val_ap= 0.88901 time= 0.07667\n",
      "训练次数: 20 Epoch: 0023 log_lik= 0.52754676 train_kl= -0.01517 train_loss= 0.54272 train_acc= 0.50678 val_roc= 0.87911 val_ap= 0.88869 time= 0.07667\n",
      "训练次数: 20 Epoch: 0024 log_lik= 0.51667297 train_kl= -0.01527 train_loss= 0.53194 train_acc= 0.51490 val_roc= 0.88110 val_ap= 0.89000 time= 0.07468\n",
      "训练次数: 20 Epoch: 0025 log_lik= 0.50412047 train_kl= -0.01533 train_loss= 0.51945 train_acc= 0.52049 val_roc= 0.88597 val_ap= 0.89342 time= 0.08265\n",
      "训练次数: 20 Epoch: 0026 log_lik= 0.49756157 train_kl= -0.01534 train_loss= 0.51290 train_acc= 0.51839 val_roc= 0.88635 val_ap= 0.89128 time= 0.07568\n",
      "训练次数: 20 Epoch: 0027 log_lik= 0.49447018 train_kl= -0.01530 train_loss= 0.50977 train_acc= 0.51529 val_roc= 0.88645 val_ap= 0.89348 time= 0.07668\n",
      "训练次数: 20 Epoch: 0028 log_lik= 0.48914793 train_kl= -0.01524 train_loss= 0.50439 train_acc= 0.51639 val_roc= 0.88624 val_ap= 0.89593 time= 0.07767\n",
      "训练次数: 20 Epoch: 0029 log_lik= 0.48655054 train_kl= -0.01517 train_loss= 0.50172 train_acc= 0.51295 val_roc= 0.88910 val_ap= 0.89899 time= 0.07568\n",
      "训练次数: 20 Epoch: 0030 log_lik= 0.48335606 train_kl= -0.01511 train_loss= 0.49847 train_acc= 0.51304 val_roc= 0.89361 val_ap= 0.90112 time= 0.07468\n",
      "训练次数: 20 Epoch: 0031 log_lik= 0.47988296 train_kl= -0.01506 train_loss= 0.49494 train_acc= 0.51843 val_roc= 0.89498 val_ap= 0.89988 time= 0.07668\n",
      "训练次数: 20 Epoch: 0032 log_lik= 0.47791383 train_kl= -0.01502 train_loss= 0.49293 train_acc= 0.52096 val_roc= 0.89511 val_ap= 0.90012 time= 0.07468\n",
      "训练次数: 20 Epoch: 0033 log_lik= 0.4759835 train_kl= -0.01497 train_loss= 0.49095 train_acc= 0.52089 val_roc= 0.89516 val_ap= 0.90079 time= 0.07369\n",
      "训练次数: 20 Epoch: 0034 log_lik= 0.47282365 train_kl= -0.01492 train_loss= 0.48774 train_acc= 0.52050 val_roc= 0.89748 val_ap= 0.90291 time= 0.07568\n",
      "训练次数: 20 Epoch: 0035 log_lik= 0.471292 train_kl= -0.01487 train_loss= 0.48616 train_acc= 0.52039 val_roc= 0.90204 val_ap= 0.90665 time= 0.07468\n",
      "训练次数: 20 Epoch: 0036 log_lik= 0.4670824 train_kl= -0.01482 train_loss= 0.48190 train_acc= 0.52567 val_roc= 0.90477 val_ap= 0.90810 time= 0.08066\n",
      "训练次数: 20 Epoch: 0037 log_lik= 0.46424046 train_kl= -0.01478 train_loss= 0.47902 train_acc= 0.52967 val_roc= 0.90679 val_ap= 0.90911 time= 0.07667\n",
      "训练次数: 20 Epoch: 0038 log_lik= 0.46082518 train_kl= -0.01474 train_loss= 0.47557 train_acc= 0.53333 val_roc= 0.90763 val_ap= 0.90931 time= 0.07767\n",
      "训练次数: 20 Epoch: 0039 log_lik= 0.45913586 train_kl= -0.01471 train_loss= 0.47384 train_acc= 0.53282 val_roc= 0.90944 val_ap= 0.91170 time= 0.08066\n",
      "训练次数: 20 Epoch: 0040 log_lik= 0.4587571 train_kl= -0.01466 train_loss= 0.47342 train_acc= 0.53167 val_roc= 0.91169 val_ap= 0.91498 time= 0.08371\n",
      "训练次数: 20 Epoch: 0041 log_lik= 0.45756775 train_kl= -0.01460 train_loss= 0.47217 train_acc= 0.53246 val_roc= 0.91355 val_ap= 0.91716 time= 0.07468\n",
      "训练次数: 20 Epoch: 0042 log_lik= 0.45520505 train_kl= -0.01452 train_loss= 0.46972 train_acc= 0.53673 val_roc= 0.91407 val_ap= 0.91748 time= 0.07667\n",
      "训练次数: 20 Epoch: 0043 log_lik= 0.45295838 train_kl= -0.01442 train_loss= 0.46738 train_acc= 0.53890 val_roc= 0.91435 val_ap= 0.91795 time= 0.07767\n",
      "训练次数: 20 Epoch: 0044 log_lik= 0.45233402 train_kl= -0.01432 train_loss= 0.46666 train_acc= 0.53753 val_roc= 0.91592 val_ap= 0.92011 time= 0.07667\n",
      "训练次数: 20 Epoch: 0045 log_lik= 0.45199698 train_kl= -0.01422 train_loss= 0.46621 train_acc= 0.53759 val_roc= 0.91794 val_ap= 0.92279 time= 0.07369\n",
      "训练次数: 20 Epoch: 0046 log_lik= 0.4503372 train_kl= -0.01410 train_loss= 0.46444 train_acc= 0.53957 val_roc= 0.91872 val_ap= 0.92373 time= 0.07767\n",
      "训练次数: 20 Epoch: 0047 log_lik= 0.44894314 train_kl= -0.01397 train_loss= 0.46292 train_acc= 0.54033 val_roc= 0.91950 val_ap= 0.92414 time= 0.07568\n",
      "训练次数: 20 Epoch: 0048 log_lik= 0.44778612 train_kl= -0.01384 train_loss= 0.46163 train_acc= 0.54114 val_roc= 0.92014 val_ap= 0.92487 time= 0.07369\n",
      "训练次数: 20 Epoch: 0049 log_lik= 0.44636562 train_kl= -0.01370 train_loss= 0.46006 train_acc= 0.54085 val_roc= 0.92160 val_ap= 0.92684 time= 0.07668\n",
      "训练次数: 20 Epoch: 0050 log_lik= 0.4463482 train_kl= -0.01356 train_loss= 0.45991 train_acc= 0.54036 val_roc= 0.92236 val_ap= 0.92811 time= 0.07667\n",
      "训练次数: 20 Epoch: 0051 log_lik= 0.44533703 train_kl= -0.01342 train_loss= 0.45876 train_acc= 0.54031 val_roc= 0.92310 val_ap= 0.92892 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 20 Epoch: 0052 log_lik= 0.44420138 train_kl= -0.01330 train_loss= 0.45750 train_acc= 0.54179 val_roc= 0.92247 val_ap= 0.92772 time= 0.08065\n",
      "训练次数: 20 Epoch: 0053 log_lik= 0.44354478 train_kl= -0.01317 train_loss= 0.45672 train_acc= 0.54093 val_roc= 0.92206 val_ap= 0.92733 time= 0.07598\n",
      "训练次数: 20 Epoch: 0054 log_lik= 0.44306064 train_kl= -0.01306 train_loss= 0.45612 train_acc= 0.54133 val_roc= 0.92245 val_ap= 0.92808 time= 0.07468\n",
      "训练次数: 20 Epoch: 0055 log_lik= 0.44285166 train_kl= -0.01294 train_loss= 0.45579 train_acc= 0.53959 val_roc= 0.92244 val_ap= 0.92826 time= 0.07469\n",
      "训练次数: 20 Epoch: 0056 log_lik= 0.44247407 train_kl= -0.01284 train_loss= 0.45531 train_acc= 0.54042 val_roc= 0.92265 val_ap= 0.92846 time= 0.08265\n",
      "训练次数: 20 Epoch: 0057 log_lik= 0.44155902 train_kl= -0.01274 train_loss= 0.45430 train_acc= 0.54139 val_roc= 0.92186 val_ap= 0.92762 time= 0.07468\n",
      "训练次数: 20 Epoch: 0058 log_lik= 0.44081572 train_kl= -0.01265 train_loss= 0.45347 train_acc= 0.54142 val_roc= 0.92145 val_ap= 0.92739 time= 0.07816\n",
      "训练次数: 20 Epoch: 0059 log_lik= 0.44080883 train_kl= -0.01257 train_loss= 0.45338 train_acc= 0.54077 val_roc= 0.92153 val_ap= 0.92753 time= 0.07468\n",
      "训练次数: 20 Epoch: 0060 log_lik= 0.44034633 train_kl= -0.01250 train_loss= 0.45284 train_acc= 0.54171 val_roc= 0.92173 val_ap= 0.92805 time= 0.07468\n",
      "训练次数: 20 Epoch: 0061 log_lik= 0.4396157 train_kl= -0.01243 train_loss= 0.45204 train_acc= 0.54197 val_roc= 0.92160 val_ap= 0.92805 time= 0.08963\n",
      "训练次数: 20 Epoch: 0062 log_lik= 0.43957686 train_kl= -0.01236 train_loss= 0.45194 train_acc= 0.54138 val_roc= 0.92074 val_ap= 0.92741 time= 0.07568\n",
      "训练次数: 20 Epoch: 0063 log_lik= 0.43887746 train_kl= -0.01231 train_loss= 0.45118 train_acc= 0.54204 val_roc= 0.91998 val_ap= 0.92694 time= 0.07468\n",
      "训练次数: 20 Epoch: 0064 log_lik= 0.43904045 train_kl= -0.01225 train_loss= 0.45129 train_acc= 0.54159 val_roc= 0.92011 val_ap= 0.92708 time= 0.07867\n",
      "训练次数: 20 Epoch: 0065 log_lik= 0.43831265 train_kl= -0.01220 train_loss= 0.45051 train_acc= 0.54161 val_roc= 0.92103 val_ap= 0.92820 time= 0.07667\n",
      "训练次数: 20 Epoch: 0066 log_lik= 0.43795934 train_kl= -0.01214 train_loss= 0.45010 train_acc= 0.54229 val_roc= 0.92112 val_ap= 0.92840 time= 0.07369\n",
      "训练次数: 20 Epoch: 0067 log_lik= 0.43773913 train_kl= -0.01209 train_loss= 0.44983 train_acc= 0.54105 val_roc= 0.92014 val_ap= 0.92764 time= 0.07369\n",
      "训练次数: 20 Epoch: 0068 log_lik= 0.43796062 train_kl= -0.01204 train_loss= 0.45000 train_acc= 0.54021 val_roc= 0.91949 val_ap= 0.92705 time= 0.09062\n",
      "训练次数: 20 Epoch: 0069 log_lik= 0.4371981 train_kl= -0.01200 train_loss= 0.44920 train_acc= 0.54224 val_roc= 0.91934 val_ap= 0.92712 time= 0.07867\n",
      "训练次数: 20 Epoch: 0070 log_lik= 0.4370773 train_kl= -0.01196 train_loss= 0.44904 train_acc= 0.54063 val_roc= 0.91976 val_ap= 0.92772 time= 0.07468\n",
      "训练次数: 20 Epoch: 0071 log_lik= 0.43653077 train_kl= -0.01193 train_loss= 0.44846 train_acc= 0.54155 val_roc= 0.91999 val_ap= 0.92780 time= 0.07468\n",
      "训练次数: 20 Epoch: 0072 log_lik= 0.43683738 train_kl= -0.01189 train_loss= 0.44873 train_acc= 0.54092 val_roc= 0.91917 val_ap= 0.92719 time= 0.07468\n",
      "训练次数: 20 Epoch: 0073 log_lik= 0.43597716 train_kl= -0.01186 train_loss= 0.44784 train_acc= 0.54156 val_roc= 0.91849 val_ap= 0.92683 time= 0.07667\n",
      "训练次数: 20 Epoch: 0074 log_lik= 0.4356819 train_kl= -0.01184 train_loss= 0.44752 train_acc= 0.54198 val_roc= 0.91816 val_ap= 0.92671 time= 0.07672\n",
      "训练次数: 20 Epoch: 0075 log_lik= 0.43526897 train_kl= -0.01182 train_loss= 0.44709 train_acc= 0.54135 val_roc= 0.91787 val_ap= 0.92664 time= 0.07668\n",
      "训练次数: 20 Epoch: 0076 log_lik= 0.4349861 train_kl= -0.01180 train_loss= 0.44678 train_acc= 0.54154 val_roc= 0.91830 val_ap= 0.92686 time= 0.07917\n",
      "训练次数: 20 Epoch: 0077 log_lik= 0.43449217 train_kl= -0.01178 train_loss= 0.44627 train_acc= 0.54151 val_roc= 0.91801 val_ap= 0.92693 time= 0.07468\n",
      "训练次数: 20 Epoch: 0078 log_lik= 0.434107 train_kl= -0.01176 train_loss= 0.44587 train_acc= 0.54311 val_roc= 0.91764 val_ap= 0.92654 time= 0.07668\n",
      "训练次数: 20 Epoch: 0079 log_lik= 0.43375686 train_kl= -0.01176 train_loss= 0.44551 train_acc= 0.54241 val_roc= 0.91762 val_ap= 0.92663 time= 0.08763\n",
      "训练次数: 20 Epoch: 0080 log_lik= 0.4337499 train_kl= -0.01175 train_loss= 0.44550 train_acc= 0.54126 val_roc= 0.91733 val_ap= 0.92645 time= 0.07668\n",
      "训练次数: 20 Epoch: 0081 log_lik= 0.4340516 train_kl= -0.01175 train_loss= 0.44580 train_acc= 0.54257 val_roc= 0.91694 val_ap= 0.92622 time= 0.08265\n",
      "训练次数: 20 Epoch: 0082 log_lik= 0.43320104 train_kl= -0.01175 train_loss= 0.44495 train_acc= 0.54262 val_roc= 0.91714 val_ap= 0.92654 time= 0.08066\n",
      "训练次数: 20 Epoch: 0083 log_lik= 0.4336676 train_kl= -0.01176 train_loss= 0.44542 train_acc= 0.54170 val_roc= 0.91699 val_ap= 0.92651 time= 0.07568\n",
      "训练次数: 20 Epoch: 0084 log_lik= 0.4333342 train_kl= -0.01177 train_loss= 0.44510 train_acc= 0.54308 val_roc= 0.91722 val_ap= 0.92678 time= 0.07468\n",
      "训练次数: 20 Epoch: 0085 log_lik= 0.43289933 train_kl= -0.01177 train_loss= 0.44467 train_acc= 0.54319 val_roc= 0.91729 val_ap= 0.92658 time= 0.07568\n",
      "训练次数: 20 Epoch: 0086 log_lik= 0.43247312 train_kl= -0.01178 train_loss= 0.44425 train_acc= 0.54248 val_roc= 0.91690 val_ap= 0.92634 time= 0.07568\n",
      "训练次数: 20 Epoch: 0087 log_lik= 0.43206185 train_kl= -0.01179 train_loss= 0.44385 train_acc= 0.54270 val_roc= 0.91619 val_ap= 0.92637 time= 0.07767\n",
      "训练次数: 20 Epoch: 0088 log_lik= 0.43246734 train_kl= -0.01180 train_loss= 0.44427 train_acc= 0.54259 val_roc= 0.91555 val_ap= 0.92599 time= 0.07772\n",
      "训练次数: 20 Epoch: 0089 log_lik= 0.43212652 train_kl= -0.01181 train_loss= 0.44394 train_acc= 0.54304 val_roc= 0.91574 val_ap= 0.92568 time= 0.07867\n",
      "训练次数: 20 Epoch: 0090 log_lik= 0.43167004 train_kl= -0.01183 train_loss= 0.44350 train_acc= 0.54352 val_roc= 0.91554 val_ap= 0.92532 time= 0.07468\n",
      "训练次数: 20 Epoch: 0091 log_lik= 0.43158934 train_kl= -0.01184 train_loss= 0.44343 train_acc= 0.54250 val_roc= 0.91479 val_ap= 0.92522 time= 0.08564\n",
      "训练次数: 20 Epoch: 0092 log_lik= 0.43138987 train_kl= -0.01184 train_loss= 0.44323 train_acc= 0.54276 val_roc= 0.91463 val_ap= 0.92555 time= 0.07667\n",
      "训练次数: 20 Epoch: 0093 log_lik= 0.4311909 train_kl= -0.01185 train_loss= 0.44304 train_acc= 0.54355 val_roc= 0.91451 val_ap= 0.92544 time= 0.07668\n",
      "训练次数: 20 Epoch: 0094 log_lik= 0.4308527 train_kl= -0.01186 train_loss= 0.44271 train_acc= 0.54282 val_roc= 0.91428 val_ap= 0.92476 time= 0.07966\n",
      "训练次数: 20 Epoch: 0095 log_lik= 0.4304175 train_kl= -0.01187 train_loss= 0.44229 train_acc= 0.54427 val_roc= 0.91382 val_ap= 0.92456 time= 0.07867\n",
      "训练次数: 20 Epoch: 0096 log_lik= 0.430373 train_kl= -0.01187 train_loss= 0.44224 train_acc= 0.54323 val_roc= 0.91376 val_ap= 0.92506 time= 0.07568\n",
      "训练次数: 20 Epoch: 0097 log_lik= 0.4305789 train_kl= -0.01186 train_loss= 0.44244 train_acc= 0.54231 val_roc= 0.91304 val_ap= 0.92455 time= 0.07767\n",
      "训练次数: 20 Epoch: 0098 log_lik= 0.43039584 train_kl= -0.01186 train_loss= 0.44226 train_acc= 0.54321 val_roc= 0.91287 val_ap= 0.92405 time= 0.07567\n",
      "训练次数: 20 Epoch: 0099 log_lik= 0.43002707 train_kl= -0.01187 train_loss= 0.44190 train_acc= 0.54363 val_roc= 0.91321 val_ap= 0.92435 time= 0.07568\n",
      "训练次数: 20 Epoch: 0100 log_lik= 0.43003514 train_kl= -0.01188 train_loss= 0.44191 train_acc= 0.54318 val_roc= 0.91341 val_ap= 0.92495 time= 0.07468\n",
      "Optimization Finished!\n",
      "训练次数: 20 ROC score: 0.9094909066031995\n",
      "训练次数: 20 AP score: 0.9103904233703116\n",
      "训练次数: 21 Epoch: 0001 log_lik= 1.7085421 train_kl= -0.00005 train_loss= 1.70859 train_acc= 0.49379 val_roc= 0.68230 val_ap= 0.70814 time= 1.62909\n",
      "训练次数: 21 Epoch: 0002 log_lik= 1.4098077 train_kl= -0.00022 train_loss= 1.41002 train_acc= 0.47771 val_roc= 0.66729 val_ap= 0.69129 time= 0.08757\n",
      "训练次数: 21 Epoch: 0003 log_lik= 1.1694413 train_kl= -0.00068 train_loss= 1.17013 train_acc= 0.41354 val_roc= 0.66439 val_ap= 0.68825 time= 0.07767\n",
      "训练次数: 21 Epoch: 0004 log_lik= 1.0431815 train_kl= -0.00136 train_loss= 1.04454 train_acc= 0.32418 val_roc= 0.66664 val_ap= 0.68932 time= 0.08364\n",
      "训练次数: 21 Epoch: 0005 log_lik= 0.9492513 train_kl= -0.00199 train_loss= 0.95124 train_acc= 0.27562 val_roc= 0.67977 val_ap= 0.70122 time= 0.07668\n",
      "训练次数: 21 Epoch: 0006 log_lik= 0.8431895 train_kl= -0.00260 train_loss= 0.84579 train_acc= 0.27536 val_roc= 0.70396 val_ap= 0.72319 time= 0.07667\n",
      "训练次数: 21 Epoch: 0007 log_lik= 0.76767343 train_kl= -0.00325 train_loss= 0.77093 train_acc= 0.28029 val_roc= 0.72864 val_ap= 0.74779 time= 0.08265\n",
      "训练次数: 21 Epoch: 0008 log_lik= 0.7216313 train_kl= -0.00393 train_loss= 0.72556 train_acc= 0.29963 val_roc= 0.74749 val_ap= 0.76462 time= 0.07568\n",
      "训练次数: 21 Epoch: 0009 log_lik= 0.7105131 train_kl= -0.00462 train_loss= 0.71513 train_acc= 0.28617 val_roc= 0.76287 val_ap= 0.77529 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 21 Epoch: 0010 log_lik= 0.69726384 train_kl= -0.00531 train_loss= 0.70257 train_acc= 0.26889 val_roc= 0.77818 val_ap= 0.78463 time= 0.07866\n",
      "训练次数: 21 Epoch: 0011 log_lik= 0.68290734 train_kl= -0.00597 train_loss= 0.68888 train_acc= 0.25654 val_roc= 0.79194 val_ap= 0.79159 time= 0.07468\n",
      "训练次数: 21 Epoch: 0012 log_lik= 0.674084 train_kl= -0.00658 train_loss= 0.68066 train_acc= 0.22493 val_roc= 0.80624 val_ap= 0.79589 time= 0.07369\n",
      "训练次数: 21 Epoch: 0013 log_lik= 0.6613335 train_kl= -0.00714 train_loss= 0.66847 train_acc= 0.23370 val_roc= 0.81227 val_ap= 0.79308 time= 0.07469\n",
      "训练次数: 21 Epoch: 0014 log_lik= 0.64393693 train_kl= -0.00767 train_loss= 0.65161 train_acc= 0.27298 val_roc= 0.81447 val_ap= 0.79231 time= 0.07667\n",
      "训练次数: 21 Epoch: 0015 log_lik= 0.62593347 train_kl= -0.00820 train_loss= 0.63413 train_acc= 0.31410 val_roc= 0.81484 val_ap= 0.79669 time= 0.07966\n",
      "训练次数: 21 Epoch: 0016 log_lik= 0.60590464 train_kl= -0.00874 train_loss= 0.61464 train_acc= 0.36184 val_roc= 0.81561 val_ap= 0.80013 time= 0.08364\n",
      "训练次数: 21 Epoch: 0017 log_lik= 0.5929291 train_kl= -0.00926 train_loss= 0.60219 train_acc= 0.39231 val_roc= 0.81892 val_ap= 0.80480 time= 0.07667\n",
      "训练次数: 21 Epoch: 0018 log_lik= 0.580623 train_kl= -0.00975 train_loss= 0.59037 train_acc= 0.42363 val_roc= 0.82413 val_ap= 0.80730 time= 0.08066\n",
      "训练次数: 21 Epoch: 0019 log_lik= 0.5707209 train_kl= -0.01016 train_loss= 0.58088 train_acc= 0.44961 val_roc= 0.83059 val_ap= 0.81206 time= 0.07668\n",
      "训练次数: 21 Epoch: 0020 log_lik= 0.56518763 train_kl= -0.01050 train_loss= 0.57568 train_acc= 0.46544 val_roc= 0.83634 val_ap= 0.81814 time= 0.08265\n",
      "训练次数: 21 Epoch: 0021 log_lik= 0.55768055 train_kl= -0.01078 train_loss= 0.56846 train_acc= 0.47448 val_roc= 0.84337 val_ap= 0.82641 time= 0.07668\n",
      "训练次数: 21 Epoch: 0022 log_lik= 0.5481388 train_kl= -0.01102 train_loss= 0.55916 train_acc= 0.48561 val_roc= 0.84840 val_ap= 0.83401 time= 0.07468\n",
      "训练次数: 21 Epoch: 0023 log_lik= 0.5410965 train_kl= -0.01122 train_loss= 0.55231 train_acc= 0.49440 val_roc= 0.85384 val_ap= 0.84104 time= 0.07468\n",
      "训练次数: 21 Epoch: 0024 log_lik= 0.5364118 train_kl= -0.01133 train_loss= 0.54774 train_acc= 0.49475 val_roc= 0.85677 val_ap= 0.84361 time= 0.07468\n",
      "训练次数: 21 Epoch: 0025 log_lik= 0.53209376 train_kl= -0.01138 train_loss= 0.54347 train_acc= 0.49202 val_roc= 0.85890 val_ap= 0.84578 time= 0.07932\n",
      "训练次数: 21 Epoch: 0026 log_lik= 0.5290863 train_kl= -0.01140 train_loss= 0.54049 train_acc= 0.48973 val_roc= 0.86076 val_ap= 0.84920 time= 0.07568\n",
      "训练次数: 21 Epoch: 0027 log_lik= 0.523658 train_kl= -0.01142 train_loss= 0.53507 train_acc= 0.49261 val_roc= 0.86213 val_ap= 0.85268 time= 0.07667\n",
      "训练次数: 21 Epoch: 0028 log_lik= 0.518578 train_kl= -0.01141 train_loss= 0.52999 train_acc= 0.49787 val_roc= 0.86302 val_ap= 0.85530 time= 0.07569\n",
      "训练次数: 21 Epoch: 0029 log_lik= 0.5126576 train_kl= -0.01138 train_loss= 0.52404 train_acc= 0.50507 val_roc= 0.86540 val_ap= 0.85628 time= 0.07468\n",
      "训练次数: 21 Epoch: 0030 log_lik= 0.5066423 train_kl= -0.01132 train_loss= 0.51796 train_acc= 0.50773 val_roc= 0.86731 val_ap= 0.85577 time= 0.07568\n",
      "训练次数: 21 Epoch: 0031 log_lik= 0.5023842 train_kl= -0.01127 train_loss= 0.51366 train_acc= 0.51205 val_roc= 0.86860 val_ap= 0.85598 time= 0.07767\n",
      "训练次数: 21 Epoch: 0032 log_lik= 0.49935326 train_kl= -0.01125 train_loss= 0.51061 train_acc= 0.51509 val_roc= 0.87055 val_ap= 0.85878 time= 0.07568\n",
      "训练次数: 21 Epoch: 0033 log_lik= 0.4963555 train_kl= -0.01127 train_loss= 0.50763 train_acc= 0.51562 val_roc= 0.87065 val_ap= 0.85874 time= 0.07369\n",
      "训练次数: 21 Epoch: 0034 log_lik= 0.49546665 train_kl= -0.01132 train_loss= 0.50678 train_acc= 0.51538 val_roc= 0.87210 val_ap= 0.85924 time= 0.07667\n",
      "训练次数: 21 Epoch: 0035 log_lik= 0.4920124 train_kl= -0.01136 train_loss= 0.50338 train_acc= 0.51451 val_roc= 0.87367 val_ap= 0.86006 time= 0.07369\n",
      "训练次数: 21 Epoch: 0036 log_lik= 0.49129 train_kl= -0.01140 train_loss= 0.50269 train_acc= 0.51514 val_roc= 0.87507 val_ap= 0.86152 time= 0.07468\n",
      "训练次数: 21 Epoch: 0037 log_lik= 0.48816055 train_kl= -0.01143 train_loss= 0.49959 train_acc= 0.51589 val_roc= 0.87685 val_ap= 0.86428 time= 0.07469\n",
      "训练次数: 21 Epoch: 0038 log_lik= 0.48606533 train_kl= -0.01145 train_loss= 0.49751 train_acc= 0.51742 val_roc= 0.87869 val_ap= 0.86750 time= 0.07568\n",
      "训练次数: 21 Epoch: 0039 log_lik= 0.4825447 train_kl= -0.01147 train_loss= 0.49401 train_acc= 0.52238 val_roc= 0.88074 val_ap= 0.87072 time= 0.07468\n",
      "训练次数: 21 Epoch: 0040 log_lik= 0.48109096 train_kl= -0.01148 train_loss= 0.49257 train_acc= 0.52186 val_roc= 0.88288 val_ap= 0.87409 time= 0.07568\n",
      "训练次数: 21 Epoch: 0041 log_lik= 0.47963455 train_kl= -0.01149 train_loss= 0.49112 train_acc= 0.52189 val_roc= 0.88379 val_ap= 0.87601 time= 0.07568\n",
      "训练次数: 21 Epoch: 0042 log_lik= 0.47813952 train_kl= -0.01147 train_loss= 0.48961 train_acc= 0.52283 val_roc= 0.88428 val_ap= 0.87764 time= 0.07568\n",
      "训练次数: 21 Epoch: 0043 log_lik= 0.47541443 train_kl= -0.01142 train_loss= 0.48684 train_acc= 0.52392 val_roc= 0.88590 val_ap= 0.88042 time= 0.07767\n",
      "训练次数: 21 Epoch: 0044 log_lik= 0.47316796 train_kl= -0.01137 train_loss= 0.48454 train_acc= 0.52429 val_roc= 0.88749 val_ap= 0.88319 time= 0.08962\n",
      "训练次数: 21 Epoch: 0045 log_lik= 0.47224593 train_kl= -0.01131 train_loss= 0.48356 train_acc= 0.52356 val_roc= 0.88965 val_ap= 0.88690 time= 0.08065\n",
      "训练次数: 21 Epoch: 0046 log_lik= 0.4711832 train_kl= -0.01125 train_loss= 0.48243 train_acc= 0.52299 val_roc= 0.89044 val_ap= 0.88826 time= 0.07668\n",
      "训练次数: 21 Epoch: 0047 log_lik= 0.46966994 train_kl= -0.01118 train_loss= 0.48085 train_acc= 0.52332 val_roc= 0.89054 val_ap= 0.88915 time= 0.07568\n",
      "训练次数: 21 Epoch: 0048 log_lik= 0.46786037 train_kl= -0.01108 train_loss= 0.47895 train_acc= 0.52381 val_roc= 0.89138 val_ap= 0.89059 time= 0.07568\n",
      "训练次数: 21 Epoch: 0049 log_lik= 0.46681362 train_kl= -0.01099 train_loss= 0.47780 train_acc= 0.52317 val_roc= 0.89215 val_ap= 0.89200 time= 0.07468\n",
      "训练次数: 21 Epoch: 0050 log_lik= 0.46562102 train_kl= -0.01092 train_loss= 0.47654 train_acc= 0.52510 val_roc= 0.89313 val_ap= 0.89393 time= 0.07697\n",
      "训练次数: 21 Epoch: 0051 log_lik= 0.46461877 train_kl= -0.01086 train_loss= 0.47548 train_acc= 0.52507 val_roc= 0.89427 val_ap= 0.89551 time= 0.07767\n",
      "训练次数: 21 Epoch: 0052 log_lik= 0.46338212 train_kl= -0.01082 train_loss= 0.47420 train_acc= 0.52442 val_roc= 0.89524 val_ap= 0.89661 time= 0.07475\n",
      "训练次数: 21 Epoch: 0053 log_lik= 0.46312216 train_kl= -0.01080 train_loss= 0.47392 train_acc= 0.52452 val_roc= 0.89575 val_ap= 0.89723 time= 0.07468\n",
      "训练次数: 21 Epoch: 0054 log_lik= 0.46141526 train_kl= -0.01078 train_loss= 0.47220 train_acc= 0.52466 val_roc= 0.89604 val_ap= 0.89779 time= 0.07568\n",
      "训练次数: 21 Epoch: 0055 log_lik= 0.46044758 train_kl= -0.01078 train_loss= 0.47123 train_acc= 0.52612 val_roc= 0.89646 val_ap= 0.89855 time= 0.07368\n",
      "训练次数: 21 Epoch: 0056 log_lik= 0.45881096 train_kl= -0.01081 train_loss= 0.46962 train_acc= 0.52694 val_roc= 0.89666 val_ap= 0.89880 time= 0.07468\n",
      "训练次数: 21 Epoch: 0057 log_lik= 0.4581986 train_kl= -0.01085 train_loss= 0.46905 train_acc= 0.52668 val_roc= 0.89706 val_ap= 0.89850 time= 0.07568\n",
      "训练次数: 21 Epoch: 0058 log_lik= 0.45745865 train_kl= -0.01090 train_loss= 0.46836 train_acc= 0.52707 val_roc= 0.89706 val_ap= 0.89851 time= 0.08066\n",
      "训练次数: 21 Epoch: 0059 log_lik= 0.45629963 train_kl= -0.01094 train_loss= 0.46724 train_acc= 0.52690 val_roc= 0.89683 val_ap= 0.89826 time= 0.08265\n",
      "训练次数: 21 Epoch: 0060 log_lik= 0.4556613 train_kl= -0.01098 train_loss= 0.46664 train_acc= 0.52818 val_roc= 0.89724 val_ap= 0.89832 time= 0.07469\n",
      "训练次数: 21 Epoch: 0061 log_lik= 0.45485038 train_kl= -0.01102 train_loss= 0.46587 train_acc= 0.52875 val_roc= 0.89741 val_ap= 0.89882 time= 0.07568\n",
      "训练次数: 21 Epoch: 0062 log_lik= 0.4542946 train_kl= -0.01106 train_loss= 0.46535 train_acc= 0.52846 val_roc= 0.89795 val_ap= 0.89970 time= 0.07468\n",
      "训练次数: 21 Epoch: 0063 log_lik= 0.45322084 train_kl= -0.01109 train_loss= 0.46431 train_acc= 0.52880 val_roc= 0.89771 val_ap= 0.89938 time= 0.07468\n",
      "训练次数: 21 Epoch: 0064 log_lik= 0.45277297 train_kl= -0.01111 train_loss= 0.46388 train_acc= 0.52859 val_roc= 0.89783 val_ap= 0.90028 time= 0.07667\n",
      "训练次数: 21 Epoch: 0065 log_lik= 0.451967 train_kl= -0.01111 train_loss= 0.46308 train_acc= 0.52976 val_roc= 0.89757 val_ap= 0.90029 time= 0.07667\n",
      "训练次数: 21 Epoch: 0066 log_lik= 0.4513362 train_kl= -0.01112 train_loss= 0.46246 train_acc= 0.52874 val_roc= 0.89719 val_ap= 0.89957 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 21 Epoch: 0067 log_lik= 0.45077673 train_kl= -0.01113 train_loss= 0.46191 train_acc= 0.52915 val_roc= 0.89734 val_ap= 0.89937 time= 0.07568\n",
      "训练次数: 21 Epoch: 0068 log_lik= 0.44959766 train_kl= -0.01115 train_loss= 0.46074 train_acc= 0.53012 val_roc= 0.89771 val_ap= 0.90025 time= 0.07568\n",
      "训练次数: 21 Epoch: 0069 log_lik= 0.44910073 train_kl= -0.01115 train_loss= 0.46025 train_acc= 0.52920 val_roc= 0.89740 val_ap= 0.90018 time= 0.07667\n",
      "训练次数: 21 Epoch: 0070 log_lik= 0.44845927 train_kl= -0.01115 train_loss= 0.45961 train_acc= 0.52870 val_roc= 0.89724 val_ap= 0.90043 time= 0.07767\n",
      "训练次数: 21 Epoch: 0071 log_lik= 0.44802138 train_kl= -0.01116 train_loss= 0.45918 train_acc= 0.52876 val_roc= 0.89744 val_ap= 0.90100 time= 0.08265\n",
      "训练次数: 21 Epoch: 0072 log_lik= 0.44713905 train_kl= -0.01118 train_loss= 0.45832 train_acc= 0.52898 val_roc= 0.89770 val_ap= 0.90142 time= 0.07468\n",
      "训练次数: 21 Epoch: 0073 log_lik= 0.44700065 train_kl= -0.01121 train_loss= 0.45821 train_acc= 0.52895 val_roc= 0.89825 val_ap= 0.90198 time= 0.07468\n",
      "训练次数: 21 Epoch: 0074 log_lik= 0.44644246 train_kl= -0.01123 train_loss= 0.45768 train_acc= 0.52946 val_roc= 0.89754 val_ap= 0.90159 time= 0.07468\n",
      "训练次数: 21 Epoch: 0075 log_lik= 0.44563165 train_kl= -0.01126 train_loss= 0.45689 train_acc= 0.53087 val_roc= 0.89654 val_ap= 0.90041 time= 0.07468\n",
      "训练次数: 21 Epoch: 0076 log_lik= 0.44487685 train_kl= -0.01129 train_loss= 0.45617 train_acc= 0.53108 val_roc= 0.89673 val_ap= 0.90034 time= 0.07369\n",
      "训练次数: 21 Epoch: 0077 log_lik= 0.44428968 train_kl= -0.01133 train_loss= 0.45562 train_acc= 0.53141 val_roc= 0.89756 val_ap= 0.90083 time= 0.08165\n",
      "训练次数: 21 Epoch: 0078 log_lik= 0.44343436 train_kl= -0.01137 train_loss= 0.45480 train_acc= 0.53226 val_roc= 0.89795 val_ap= 0.90082 time= 0.07568\n",
      "训练次数: 21 Epoch: 0079 log_lik= 0.4427227 train_kl= -0.01140 train_loss= 0.45412 train_acc= 0.53261 val_roc= 0.89744 val_ap= 0.90005 time= 0.07867\n",
      "训练次数: 21 Epoch: 0080 log_lik= 0.442517 train_kl= -0.01142 train_loss= 0.45394 train_acc= 0.53312 val_roc= 0.89786 val_ap= 0.90068 time= 0.08265\n",
      "训练次数: 21 Epoch: 0081 log_lik= 0.44192526 train_kl= -0.01145 train_loss= 0.45337 train_acc= 0.53226 val_roc= 0.89861 val_ap= 0.90139 time= 0.07867\n",
      "训练次数: 21 Epoch: 0082 log_lik= 0.44165114 train_kl= -0.01148 train_loss= 0.45314 train_acc= 0.53366 val_roc= 0.89857 val_ap= 0.90173 time= 0.07468\n",
      "训练次数: 21 Epoch: 0083 log_lik= 0.44082585 train_kl= -0.01151 train_loss= 0.45234 train_acc= 0.53302 val_roc= 0.89738 val_ap= 0.90019 time= 0.07568\n",
      "训练次数: 21 Epoch: 0084 log_lik= 0.4405237 train_kl= -0.01153 train_loss= 0.45205 train_acc= 0.53435 val_roc= 0.89773 val_ap= 0.90099 time= 0.08265\n",
      "训练次数: 21 Epoch: 0085 log_lik= 0.44006753 train_kl= -0.01154 train_loss= 0.45161 train_acc= 0.53484 val_roc= 0.89876 val_ap= 0.90308 time= 0.07568\n",
      "训练次数: 21 Epoch: 0086 log_lik= 0.43962467 train_kl= -0.01157 train_loss= 0.45119 train_acc= 0.53366 val_roc= 0.89917 val_ap= 0.90432 time= 0.07468\n",
      "训练次数: 21 Epoch: 0087 log_lik= 0.43888792 train_kl= -0.01159 train_loss= 0.45048 train_acc= 0.53556 val_roc= 0.89864 val_ap= 0.90396 time= 0.07468\n",
      "训练次数: 21 Epoch: 0088 log_lik= 0.4386226 train_kl= -0.01160 train_loss= 0.45022 train_acc= 0.53520 val_roc= 0.89842 val_ap= 0.90403 time= 0.07468\n",
      "训练次数: 21 Epoch: 0089 log_lik= 0.4382819 train_kl= -0.01161 train_loss= 0.44990 train_acc= 0.53459 val_roc= 0.89787 val_ap= 0.90354 time= 0.07468\n",
      "训练次数: 21 Epoch: 0090 log_lik= 0.43771556 train_kl= -0.01164 train_loss= 0.44935 train_acc= 0.53542 val_roc= 0.89832 val_ap= 0.90466 time= 0.07468\n",
      "训练次数: 21 Epoch: 0091 log_lik= 0.43748447 train_kl= -0.01166 train_loss= 0.44915 train_acc= 0.53654 val_roc= 0.89919 val_ap= 0.90583 time= 0.07567\n",
      "训练次数: 21 Epoch: 0092 log_lik= 0.43652317 train_kl= -0.01168 train_loss= 0.44820 train_acc= 0.53734 val_roc= 0.89959 val_ap= 0.90614 time= 0.08664\n",
      "训练次数: 21 Epoch: 0093 log_lik= 0.43622893 train_kl= -0.01170 train_loss= 0.44793 train_acc= 0.53736 val_roc= 0.89984 val_ap= 0.90681 time= 0.07568\n",
      "训练次数: 21 Epoch: 0094 log_lik= 0.43547156 train_kl= -0.01171 train_loss= 0.44718 train_acc= 0.53723 val_roc= 0.90007 val_ap= 0.90683 time= 0.07667\n",
      "训练次数: 21 Epoch: 0095 log_lik= 0.43525377 train_kl= -0.01173 train_loss= 0.44698 train_acc= 0.53682 val_roc= 0.90043 val_ap= 0.90696 time= 0.07369\n",
      "训练次数: 21 Epoch: 0096 log_lik= 0.43452355 train_kl= -0.01176 train_loss= 0.44628 train_acc= 0.53764 val_roc= 0.90136 val_ap= 0.90796 time= 0.07468\n",
      "训练次数: 21 Epoch: 0097 log_lik= 0.4343054 train_kl= -0.01177 train_loss= 0.44608 train_acc= 0.53805 val_roc= 0.90224 val_ap= 0.90893 time= 0.07668\n",
      "训练次数: 21 Epoch: 0098 log_lik= 0.43407217 train_kl= -0.01177 train_loss= 0.44584 train_acc= 0.53763 val_roc= 0.90176 val_ap= 0.90817 time= 0.07667\n",
      "训练次数: 21 Epoch: 0099 log_lik= 0.43357357 train_kl= -0.01180 train_loss= 0.44537 train_acc= 0.53816 val_roc= 0.90172 val_ap= 0.90754 time= 0.07468\n",
      "训练次数: 21 Epoch: 0100 log_lik= 0.43335643 train_kl= -0.01182 train_loss= 0.44518 train_acc= 0.53776 val_roc= 0.90257 val_ap= 0.90871 time= 0.07685\n",
      "Optimization Finished!\n",
      "训练次数: 21 ROC score: 0.9189317644178319\n",
      "训练次数: 21 AP score: 0.9305157642519133\n",
      "训练次数: 22 Epoch: 0001 log_lik= 1.7266082 train_kl= -0.00004 train_loss= 1.72664 train_acc= 0.49843 val_roc= 0.69281 val_ap= 0.71166 time= 1.62342\n",
      "训练次数: 22 Epoch: 0002 log_lik= 1.4860259 train_kl= -0.00019 train_loss= 1.48622 train_acc= 0.48027 val_roc= 0.68416 val_ap= 0.71515 time= 0.08758\n",
      "训练次数: 22 Epoch: 0003 log_lik= 1.3134965 train_kl= -0.00062 train_loss= 1.31411 train_acc= 0.42151 val_roc= 0.68962 val_ap= 0.71997 time= 0.07568\n",
      "训练次数: 22 Epoch: 0004 log_lik= 1.1898499 train_kl= -0.00111 train_loss= 1.19096 train_acc= 0.37219 val_roc= 0.69674 val_ap= 0.72648 time= 0.08265\n",
      "训练次数: 22 Epoch: 0005 log_lik= 1.064316 train_kl= -0.00161 train_loss= 1.06593 train_acc= 0.33681 val_roc= 0.70575 val_ap= 0.73395 time= 0.07627\n",
      "训练次数: 22 Epoch: 0006 log_lik= 0.91911525 train_kl= -0.00215 train_loss= 0.92126 train_acc= 0.34262 val_roc= 0.71684 val_ap= 0.73940 time= 0.07668\n",
      "训练次数: 22 Epoch: 0007 log_lik= 0.8261672 train_kl= -0.00282 train_loss= 0.82898 train_acc= 0.35348 val_roc= 0.73008 val_ap= 0.74167 time= 0.07568\n",
      "训练次数: 22 Epoch: 0008 log_lik= 0.7584416 train_kl= -0.00365 train_loss= 0.76209 train_acc= 0.35373 val_roc= 0.74202 val_ap= 0.74495 time= 0.07667\n",
      "训练次数: 22 Epoch: 0009 log_lik= 0.7245109 train_kl= -0.00462 train_loss= 0.72913 train_acc= 0.32005 val_roc= 0.75259 val_ap= 0.75720 time= 0.08364\n",
      "训练次数: 22 Epoch: 0010 log_lik= 0.7091015 train_kl= -0.00567 train_loss= 0.71477 train_acc= 0.27299 val_roc= 0.76487 val_ap= 0.77033 time= 0.07667\n",
      "训练次数: 22 Epoch: 0011 log_lik= 0.6982879 train_kl= -0.00671 train_loss= 0.70500 train_acc= 0.23732 val_roc= 0.77932 val_ap= 0.78193 time= 0.07767\n",
      "训练次数: 22 Epoch: 0012 log_lik= 0.6829921 train_kl= -0.00770 train_loss= 0.69069 train_acc= 0.24554 val_roc= 0.78996 val_ap= 0.78856 time= 0.07966\n",
      "训练次数: 22 Epoch: 0013 log_lik= 0.66360754 train_kl= -0.00862 train_loss= 0.67223 train_acc= 0.28042 val_roc= 0.79979 val_ap= 0.79561 time= 0.07468\n",
      "训练次数: 22 Epoch: 0014 log_lik= 0.64114684 train_kl= -0.00948 train_loss= 0.65063 train_acc= 0.29364 val_roc= 0.80215 val_ap= 0.79257 time= 0.07369\n",
      "训练次数: 22 Epoch: 0015 log_lik= 0.6189984 train_kl= -0.01027 train_loss= 0.62927 train_acc= 0.31215 val_roc= 0.79670 val_ap= 0.78377 time= 0.07468\n",
      "训练次数: 22 Epoch: 0016 log_lik= 0.6006495 train_kl= -0.01096 train_loss= 0.61161 train_acc= 0.35704 val_roc= 0.78868 val_ap= 0.77409 time= 0.07468\n",
      "训练次数: 22 Epoch: 0017 log_lik= 0.58700395 train_kl= -0.01156 train_loss= 0.59856 train_acc= 0.41507 val_roc= 0.78418 val_ap= 0.76888 time= 0.07468\n",
      "训练次数: 22 Epoch: 0018 log_lik= 0.5790254 train_kl= -0.01206 train_loss= 0.59109 train_acc= 0.45641 val_roc= 0.78587 val_ap= 0.77029 time= 0.07369\n",
      "训练次数: 22 Epoch: 0019 log_lik= 0.5765021 train_kl= -0.01251 train_loss= 0.58901 train_acc= 0.47111 val_roc= 0.79262 val_ap= 0.78109 time= 0.08364\n",
      "训练次数: 22 Epoch: 0020 log_lik= 0.5713657 train_kl= -0.01289 train_loss= 0.58426 train_acc= 0.47352 val_roc= 0.80176 val_ap= 0.79566 time= 0.07568\n",
      "训练次数: 22 Epoch: 0021 log_lik= 0.5622472 train_kl= -0.01322 train_loss= 0.57547 train_acc= 0.47699 val_roc= 0.80863 val_ap= 0.80363 time= 0.07468\n",
      "训练次数: 22 Epoch: 0022 log_lik= 0.5487785 train_kl= -0.01348 train_loss= 0.56226 train_acc= 0.48878 val_roc= 0.81367 val_ap= 0.81062 time= 0.07468\n",
      "训练次数: 22 Epoch: 0023 log_lik= 0.53640383 train_kl= -0.01371 train_loss= 0.55012 train_acc= 0.49461 val_roc= 0.82135 val_ap= 0.82236 time= 0.07568\n",
      "训练次数: 22 Epoch: 0024 log_lik= 0.5283416 train_kl= -0.01393 train_loss= 0.54227 train_acc= 0.49227 val_roc= 0.82858 val_ap= 0.83194 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 22 Epoch: 0025 log_lik= 0.5224296 train_kl= -0.01411 train_loss= 0.53654 train_acc= 0.49137 val_roc= 0.83446 val_ap= 0.83735 time= 0.07568\n",
      "训练次数: 22 Epoch: 0026 log_lik= 0.515275 train_kl= -0.01425 train_loss= 0.52952 train_acc= 0.49970 val_roc= 0.83848 val_ap= 0.84041 time= 0.07568\n",
      "训练次数: 22 Epoch: 0027 log_lik= 0.50940925 train_kl= -0.01433 train_loss= 0.52374 train_acc= 0.50900 val_roc= 0.84486 val_ap= 0.84636 time= 0.07570\n",
      "训练次数: 22 Epoch: 0028 log_lik= 0.5051905 train_kl= -0.01439 train_loss= 0.51958 train_acc= 0.51028 val_roc= 0.85145 val_ap= 0.85273 time= 0.08564\n",
      "训练次数: 22 Epoch: 0029 log_lik= 0.5034261 train_kl= -0.01441 train_loss= 0.51784 train_acc= 0.50767 val_roc= 0.85832 val_ap= 0.85970 time= 0.07966\n",
      "训练次数: 22 Epoch: 0030 log_lik= 0.502903 train_kl= -0.01441 train_loss= 0.51731 train_acc= 0.50178 val_roc= 0.86368 val_ap= 0.86479 time= 0.07767\n",
      "训练次数: 22 Epoch: 0031 log_lik= 0.50003535 train_kl= -0.01438 train_loss= 0.51441 train_acc= 0.50019 val_roc= 0.86819 val_ap= 0.86964 time= 0.07667\n",
      "训练次数: 22 Epoch: 0032 log_lik= 0.49445063 train_kl= -0.01433 train_loss= 0.50878 train_acc= 0.50540 val_roc= 0.87260 val_ap= 0.87359 time= 0.07966\n",
      "训练次数: 22 Epoch: 0033 log_lik= 0.48943847 train_kl= -0.01428 train_loss= 0.50372 train_acc= 0.51258 val_roc= 0.87713 val_ap= 0.87929 time= 0.07568\n",
      "训练次数: 22 Epoch: 0034 log_lik= 0.48568967 train_kl= -0.01423 train_loss= 0.49992 train_acc= 0.51454 val_roc= 0.88180 val_ap= 0.88392 time= 0.07767\n",
      "训练次数: 22 Epoch: 0035 log_lik= 0.48356065 train_kl= -0.01415 train_loss= 0.49771 train_acc= 0.51133 val_roc= 0.88417 val_ap= 0.88592 time= 0.08265\n",
      "训练次数: 22 Epoch: 0036 log_lik= 0.48121497 train_kl= -0.01405 train_loss= 0.49527 train_acc= 0.51125 val_roc= 0.88678 val_ap= 0.88835 time= 0.08365\n",
      "训练次数: 22 Epoch: 0037 log_lik= 0.47880918 train_kl= -0.01394 train_loss= 0.49275 train_acc= 0.51469 val_roc= 0.88884 val_ap= 0.89009 time= 0.07567\n",
      "训练次数: 22 Epoch: 0038 log_lik= 0.4755717 train_kl= -0.01383 train_loss= 0.48940 train_acc= 0.51893 val_roc= 0.89018 val_ap= 0.89124 time= 0.07767\n",
      "训练次数: 22 Epoch: 0039 log_lik= 0.47218665 train_kl= -0.01372 train_loss= 0.48591 train_acc= 0.52204 val_roc= 0.89115 val_ap= 0.89205 time= 0.07820\n",
      "训练次数: 22 Epoch: 0040 log_lik= 0.47148556 train_kl= -0.01361 train_loss= 0.48509 train_acc= 0.52145 val_roc= 0.89132 val_ap= 0.89147 time= 0.07667\n",
      "训练次数: 22 Epoch: 0041 log_lik= 0.47066236 train_kl= -0.01349 train_loss= 0.48415 train_acc= 0.52131 val_roc= 0.89138 val_ap= 0.88982 time= 0.07468\n",
      "训练次数: 22 Epoch: 0042 log_lik= 0.47042763 train_kl= -0.01337 train_loss= 0.48379 train_acc= 0.52284 val_roc= 0.89131 val_ap= 0.88990 time= 0.07626\n",
      "训练次数: 22 Epoch: 0043 log_lik= 0.46909982 train_kl= -0.01325 train_loss= 0.48235 train_acc= 0.52440 val_roc= 0.89193 val_ap= 0.89127 time= 0.07867\n",
      "训练次数: 22 Epoch: 0044 log_lik= 0.46744257 train_kl= -0.01315 train_loss= 0.48059 train_acc= 0.52753 val_roc= 0.89226 val_ap= 0.89218 time= 0.07527\n",
      "训练次数: 22 Epoch: 0045 log_lik= 0.46613038 train_kl= -0.01305 train_loss= 0.47918 train_acc= 0.52819 val_roc= 0.89218 val_ap= 0.89211 time= 0.07667\n",
      "训练次数: 22 Epoch: 0046 log_lik= 0.4649552 train_kl= -0.01294 train_loss= 0.47789 train_acc= 0.53057 val_roc= 0.89196 val_ap= 0.89153 time= 0.08265\n",
      "训练次数: 22 Epoch: 0047 log_lik= 0.46492726 train_kl= -0.01282 train_loss= 0.47775 train_acc= 0.53060 val_roc= 0.89222 val_ap= 0.89187 time= 0.07667\n",
      "训练次数: 22 Epoch: 0048 log_lik= 0.46383676 train_kl= -0.01271 train_loss= 0.47655 train_acc= 0.53072 val_roc= 0.89320 val_ap= 0.89302 time= 0.07867\n",
      "训练次数: 22 Epoch: 0049 log_lik= 0.4623921 train_kl= -0.01262 train_loss= 0.47501 train_acc= 0.53057 val_roc= 0.89420 val_ap= 0.89413 time= 0.07568\n",
      "训练次数: 22 Epoch: 0050 log_lik= 0.46083954 train_kl= -0.01252 train_loss= 0.47336 train_acc= 0.53108 val_roc= 0.89440 val_ap= 0.89499 time= 0.07667\n",
      "训练次数: 22 Epoch: 0051 log_lik= 0.46038404 train_kl= -0.01243 train_loss= 0.47281 train_acc= 0.53150 val_roc= 0.89396 val_ap= 0.89487 time= 0.08166\n",
      "训练次数: 22 Epoch: 0052 log_lik= 0.4589953 train_kl= -0.01234 train_loss= 0.47134 train_acc= 0.53057 val_roc= 0.89387 val_ap= 0.89434 time= 0.07667\n",
      "训练次数: 22 Epoch: 0053 log_lik= 0.45835683 train_kl= -0.01227 train_loss= 0.47063 train_acc= 0.53057 val_roc= 0.89465 val_ap= 0.89558 time= 0.07468\n",
      "训练次数: 22 Epoch: 0054 log_lik= 0.458131 train_kl= -0.01220 train_loss= 0.47033 train_acc= 0.52996 val_roc= 0.89618 val_ap= 0.89744 time= 0.07568\n",
      "训练次数: 22 Epoch: 0055 log_lik= 0.45715418 train_kl= -0.01213 train_loss= 0.46929 train_acc= 0.53037 val_roc= 0.89690 val_ap= 0.89816 time= 0.07697\n",
      "训练次数: 22 Epoch: 0056 log_lik= 0.45586973 train_kl= -0.01207 train_loss= 0.46794 train_acc= 0.53011 val_roc= 0.89685 val_ap= 0.89842 time= 0.07468\n",
      "训练次数: 22 Epoch: 0057 log_lik= 0.45515886 train_kl= -0.01200 train_loss= 0.46716 train_acc= 0.52959 val_roc= 0.89692 val_ap= 0.89868 time= 0.08265\n",
      "训练次数: 22 Epoch: 0058 log_lik= 0.4545591 train_kl= -0.01193 train_loss= 0.46649 train_acc= 0.52894 val_roc= 0.89826 val_ap= 0.89981 time= 0.07468\n",
      "训练次数: 22 Epoch: 0059 log_lik= 0.45427126 train_kl= -0.01187 train_loss= 0.46614 train_acc= 0.52863 val_roc= 0.89938 val_ap= 0.90118 time= 0.07568\n",
      "训练次数: 22 Epoch: 0060 log_lik= 0.45360082 train_kl= -0.01182 train_loss= 0.46542 train_acc= 0.52826 val_roc= 0.89988 val_ap= 0.90199 time= 0.07867\n",
      "训练次数: 22 Epoch: 0061 log_lik= 0.4532735 train_kl= -0.01177 train_loss= 0.46504 train_acc= 0.52938 val_roc= 0.89981 val_ap= 0.90202 time= 0.07468\n",
      "训练次数: 22 Epoch: 0062 log_lik= 0.45239532 train_kl= -0.01172 train_loss= 0.46412 train_acc= 0.52906 val_roc= 0.90013 val_ap= 0.90274 time= 0.07468\n",
      "训练次数: 22 Epoch: 0063 log_lik= 0.45198417 train_kl= -0.01169 train_loss= 0.46367 train_acc= 0.52869 val_roc= 0.90069 val_ap= 0.90399 time= 0.07468\n",
      "训练次数: 22 Epoch: 0064 log_lik= 0.45126867 train_kl= -0.01166 train_loss= 0.46292 train_acc= 0.52831 val_roc= 0.90101 val_ap= 0.90381 time= 0.07867\n",
      "训练次数: 22 Epoch: 0065 log_lik= 0.45065337 train_kl= -0.01163 train_loss= 0.46228 train_acc= 0.52817 val_roc= 0.90081 val_ap= 0.90323 time= 0.08364\n",
      "训练次数: 22 Epoch: 0066 log_lik= 0.4501973 train_kl= -0.01160 train_loss= 0.46179 train_acc= 0.52851 val_roc= 0.90020 val_ap= 0.90305 time= 0.08265\n",
      "训练次数: 22 Epoch: 0067 log_lik= 0.44942454 train_kl= -0.01157 train_loss= 0.46099 train_acc= 0.52895 val_roc= 0.90000 val_ap= 0.90352 time= 0.07667\n",
      "训练次数: 22 Epoch: 0068 log_lik= 0.4492115 train_kl= -0.01155 train_loss= 0.46076 train_acc= 0.53026 val_roc= 0.90016 val_ap= 0.90428 time= 0.07667\n",
      "训练次数: 22 Epoch: 0069 log_lik= 0.44826177 train_kl= -0.01153 train_loss= 0.45979 train_acc= 0.53051 val_roc= 0.90022 val_ap= 0.90448 time= 0.07510\n",
      "训练次数: 22 Epoch: 0070 log_lik= 0.44825602 train_kl= -0.01151 train_loss= 0.45977 train_acc= 0.53030 val_roc= 0.89994 val_ap= 0.90440 time= 0.07722\n",
      "训练次数: 22 Epoch: 0071 log_lik= 0.44750902 train_kl= -0.01149 train_loss= 0.45900 train_acc= 0.53091 val_roc= 0.89902 val_ap= 0.90331 time= 0.07369\n",
      "训练次数: 22 Epoch: 0072 log_lik= 0.44695488 train_kl= -0.01148 train_loss= 0.45844 train_acc= 0.53192 val_roc= 0.89912 val_ap= 0.90437 time= 0.07369\n",
      "训练次数: 22 Epoch: 0073 log_lik= 0.4460513 train_kl= -0.01147 train_loss= 0.45753 train_acc= 0.53149 val_roc= 0.89935 val_ap= 0.90604 time= 0.08066\n",
      "训练次数: 22 Epoch: 0074 log_lik= 0.44572937 train_kl= -0.01147 train_loss= 0.45720 train_acc= 0.53133 val_roc= 0.89810 val_ap= 0.90501 time= 0.07568\n",
      "训练次数: 22 Epoch: 0075 log_lik= 0.4450668 train_kl= -0.01147 train_loss= 0.45654 train_acc= 0.53274 val_roc= 0.89813 val_ap= 0.90529 time= 0.07382\n",
      "训练次数: 22 Epoch: 0076 log_lik= 0.44406235 train_kl= -0.01145 train_loss= 0.45552 train_acc= 0.53202 val_roc= 0.89832 val_ap= 0.90573 time= 0.07667\n",
      "训练次数: 22 Epoch: 0077 log_lik= 0.44344217 train_kl= -0.01145 train_loss= 0.45490 train_acc= 0.53315 val_roc= 0.89787 val_ap= 0.90617 time= 0.07468\n",
      "训练次数: 22 Epoch: 0078 log_lik= 0.44365314 train_kl= -0.01147 train_loss= 0.45512 train_acc= 0.53300 val_roc= 0.89821 val_ap= 0.90661 time= 0.07866\n",
      "训练次数: 22 Epoch: 0079 log_lik= 0.4436607 train_kl= -0.01146 train_loss= 0.45512 train_acc= 0.53197 val_roc= 0.89821 val_ap= 0.90738 time= 0.07568\n",
      "训练次数: 22 Epoch: 0080 log_lik= 0.4422274 train_kl= -0.01148 train_loss= 0.45370 train_acc= 0.53300 val_roc= 0.89823 val_ap= 0.90803 time= 0.08165\n",
      "训练次数: 22 Epoch: 0081 log_lik= 0.44199866 train_kl= -0.01150 train_loss= 0.45350 train_acc= 0.53368 val_roc= 0.89878 val_ap= 0.90822 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 22 Epoch: 0082 log_lik= 0.4417344 train_kl= -0.01149 train_loss= 0.45323 train_acc= 0.53324 val_roc= 0.89880 val_ap= 0.90847 time= 0.07568\n",
      "训练次数: 22 Epoch: 0083 log_lik= 0.44090822 train_kl= -0.01151 train_loss= 0.45241 train_acc= 0.53505 val_roc= 0.89865 val_ap= 0.90894 time= 0.07494\n",
      "训练次数: 22 Epoch: 0084 log_lik= 0.44039345 train_kl= -0.01153 train_loss= 0.45192 train_acc= 0.53476 val_roc= 0.89939 val_ap= 0.91007 time= 0.07667\n",
      "训练次数: 22 Epoch: 0085 log_lik= 0.439505 train_kl= -0.01155 train_loss= 0.45105 train_acc= 0.53580 val_roc= 0.90024 val_ap= 0.91063 time= 0.07568\n",
      "训练次数: 22 Epoch: 0086 log_lik= 0.43913 train_kl= -0.01157 train_loss= 0.45070 train_acc= 0.53514 val_roc= 0.90032 val_ap= 0.91078 time= 0.07469\n",
      "训练次数: 22 Epoch: 0087 log_lik= 0.43857756 train_kl= -0.01159 train_loss= 0.45017 train_acc= 0.53634 val_roc= 0.89994 val_ap= 0.91097 time= 0.07468\n",
      "训练次数: 22 Epoch: 0088 log_lik= 0.43831325 train_kl= -0.01162 train_loss= 0.44993 train_acc= 0.53620 val_roc= 0.90116 val_ap= 0.91215 time= 0.07667\n",
      "训练次数: 22 Epoch: 0089 log_lik= 0.437658 train_kl= -0.01163 train_loss= 0.44929 train_acc= 0.53578 val_roc= 0.90212 val_ap= 0.91334 time= 0.08265\n",
      "训练次数: 22 Epoch: 0090 log_lik= 0.43707716 train_kl= -0.01166 train_loss= 0.44873 train_acc= 0.53624 val_roc= 0.90305 val_ap= 0.91469 time= 0.07468\n",
      "训练次数: 22 Epoch: 0091 log_lik= 0.43652645 train_kl= -0.01168 train_loss= 0.44821 train_acc= 0.53713 val_roc= 0.90237 val_ap= 0.91385 time= 0.07468\n",
      "训练次数: 22 Epoch: 0092 log_lik= 0.4363882 train_kl= -0.01170 train_loss= 0.44809 train_acc= 0.53594 val_roc= 0.90182 val_ap= 0.91262 time= 0.07568\n",
      "训练次数: 22 Epoch: 0093 log_lik= 0.43622482 train_kl= -0.01170 train_loss= 0.44793 train_acc= 0.53555 val_roc= 0.90270 val_ap= 0.91380 time= 0.07468\n",
      "训练次数: 22 Epoch: 0094 log_lik= 0.4353545 train_kl= -0.01172 train_loss= 0.44708 train_acc= 0.53694 val_roc= 0.90432 val_ap= 0.91645 time= 0.07680\n",
      "训练次数: 22 Epoch: 0095 log_lik= 0.4351087 train_kl= -0.01175 train_loss= 0.44686 train_acc= 0.53655 val_roc= 0.90452 val_ap= 0.91659 time= 0.07568\n",
      "训练次数: 22 Epoch: 0096 log_lik= 0.43462098 train_kl= -0.01176 train_loss= 0.44638 train_acc= 0.53636 val_roc= 0.90356 val_ap= 0.91447 time= 0.07369\n",
      "训练次数: 22 Epoch: 0097 log_lik= 0.43475974 train_kl= -0.01176 train_loss= 0.44652 train_acc= 0.53717 val_roc= 0.90347 val_ap= 0.91490 time= 0.09062\n",
      "训练次数: 22 Epoch: 0098 log_lik= 0.43418014 train_kl= -0.01178 train_loss= 0.44596 train_acc= 0.53686 val_roc= 0.90532 val_ap= 0.91806 time= 0.07867\n",
      "训练次数: 22 Epoch: 0099 log_lik= 0.43372613 train_kl= -0.01181 train_loss= 0.44554 train_acc= 0.53736 val_roc= 0.90567 val_ap= 0.91806 time= 0.07468\n",
      "训练次数: 22 Epoch: 0100 log_lik= 0.4334251 train_kl= -0.01182 train_loss= 0.44524 train_acc= 0.53705 val_roc= 0.90473 val_ap= 0.91687 time= 0.07867\n",
      "Optimization Finished!\n",
      "训练次数: 22 ROC score: 0.9121337706901331\n",
      "训练次数: 22 AP score: 0.9228080931910818\n",
      "训练次数: 23 Epoch: 0001 log_lik= 1.7650918 train_kl= -0.00004 train_loss= 1.76513 train_acc= 0.49644 val_roc= 0.71424 val_ap= 0.72722 time= 1.57763\n",
      "训练次数: 23 Epoch: 0002 log_lik= 1.5852655 train_kl= -0.00013 train_loss= 1.58539 train_acc= 0.49080 val_roc= 0.70873 val_ap= 0.72568 time= 0.10551\n",
      "训练次数: 23 Epoch: 0003 log_lik= 1.4592893 train_kl= -0.00037 train_loss= 1.45966 train_acc= 0.46133 val_roc= 0.70523 val_ap= 0.72393 time= 0.07368\n",
      "训练次数: 23 Epoch: 0004 log_lik= 1.2899617 train_kl= -0.00053 train_loss= 1.29049 train_acc= 0.44887 val_roc= 0.70591 val_ap= 0.72946 time= 0.07667\n",
      "训练次数: 23 Epoch: 0005 log_lik= 1.1711773 train_kl= -0.00079 train_loss= 1.17197 train_acc= 0.42725 val_roc= 0.70802 val_ap= 0.73694 time= 0.07468\n",
      "训练次数: 23 Epoch: 0006 log_lik= 1.046975 train_kl= -0.00118 train_loss= 1.04816 train_acc= 0.41460 val_roc= 0.71374 val_ap= 0.74738 time= 0.07468\n",
      "训练次数: 23 Epoch: 0007 log_lik= 0.93377525 train_kl= -0.00174 train_loss= 0.93552 train_acc= 0.39585 val_roc= 0.72679 val_ap= 0.75869 time= 0.07568\n",
      "训练次数: 23 Epoch: 0008 log_lik= 0.8315943 train_kl= -0.00248 train_loss= 0.83407 train_acc= 0.38462 val_roc= 0.75807 val_ap= 0.77859 time= 0.08265\n",
      "训练次数: 23 Epoch: 0009 log_lik= 0.7607251 train_kl= -0.00339 train_loss= 0.76412 train_acc= 0.38984 val_roc= 0.78300 val_ap= 0.78996 time= 0.07468\n",
      "训练次数: 23 Epoch: 0010 log_lik= 0.7215118 train_kl= -0.00444 train_loss= 0.72595 train_acc= 0.37134 val_roc= 0.79391 val_ap= 0.79375 time= 0.07568\n",
      "训练次数: 23 Epoch: 0011 log_lik= 0.6986805 train_kl= -0.00556 train_loss= 0.70424 train_acc= 0.29528 val_roc= 0.79754 val_ap= 0.79844 time= 0.07568\n",
      "训练次数: 23 Epoch: 0012 log_lik= 0.6834073 train_kl= -0.00669 train_loss= 0.69010 train_acc= 0.22578 val_roc= 0.79920 val_ap= 0.80269 time= 0.08280\n",
      "训练次数: 23 Epoch: 0013 log_lik= 0.6700952 train_kl= -0.00774 train_loss= 0.67784 train_acc= 0.21817 val_roc= 0.80533 val_ap= 0.80857 time= 0.07468\n",
      "训练次数: 23 Epoch: 0014 log_lik= 0.64873874 train_kl= -0.00872 train_loss= 0.65746 train_acc= 0.27291 val_roc= 0.81294 val_ap= 0.81639 time= 0.08066\n",
      "训练次数: 23 Epoch: 0015 log_lik= 0.63283294 train_kl= -0.00964 train_loss= 0.64247 train_acc= 0.31657 val_roc= 0.81664 val_ap= 0.81622 time= 0.07767\n",
      "训练次数: 23 Epoch: 0016 log_lik= 0.6182497 train_kl= -0.01051 train_loss= 0.62876 train_acc= 0.33702 val_roc= 0.81808 val_ap= 0.81883 time= 0.07967\n",
      "训练次数: 23 Epoch: 0017 log_lik= 0.6047057 train_kl= -0.01129 train_loss= 0.61600 train_acc= 0.35935 val_roc= 0.81620 val_ap= 0.81769 time= 0.07568\n",
      "训练次数: 23 Epoch: 0018 log_lik= 0.5988549 train_kl= -0.01199 train_loss= 0.61084 train_acc= 0.38453 val_roc= 0.81762 val_ap= 0.81749 time= 0.07468\n",
      "训练次数: 23 Epoch: 0019 log_lik= 0.59032726 train_kl= -0.01260 train_loss= 0.60293 train_acc= 0.42274 val_roc= 0.82089 val_ap= 0.81845 time= 0.07866\n",
      "训练次数: 23 Epoch: 0020 log_lik= 0.58458143 train_kl= -0.01312 train_loss= 0.59770 train_acc= 0.45270 val_roc= 0.82618 val_ap= 0.82595 time= 0.07468\n",
      "训练次数: 23 Epoch: 0021 log_lik= 0.57352614 train_kl= -0.01355 train_loss= 0.58708 train_acc= 0.47115 val_roc= 0.83591 val_ap= 0.83792 time= 0.07468\n",
      "训练次数: 23 Epoch: 0022 log_lik= 0.5620346 train_kl= -0.01393 train_loss= 0.57597 train_acc= 0.47703 val_roc= 0.85216 val_ap= 0.85191 time= 0.07767\n",
      "训练次数: 23 Epoch: 0023 log_lik= 0.54875445 train_kl= -0.01428 train_loss= 0.56303 train_acc= 0.47715 val_roc= 0.86575 val_ap= 0.86413 time= 0.07571\n",
      "训练次数: 23 Epoch: 0024 log_lik= 0.5387697 train_kl= -0.01458 train_loss= 0.55335 train_acc= 0.47815 val_roc= 0.87312 val_ap= 0.87268 time= 0.07568\n",
      "训练次数: 23 Epoch: 0025 log_lik= 0.5287058 train_kl= -0.01482 train_loss= 0.54352 train_acc= 0.48478 val_roc= 0.87736 val_ap= 0.87690 time= 0.07767\n",
      "训练次数: 23 Epoch: 0026 log_lik= 0.5236358 train_kl= -0.01501 train_loss= 0.53865 train_acc= 0.48846 val_roc= 0.87808 val_ap= 0.87562 time= 0.07568\n",
      "训练次数: 23 Epoch: 0027 log_lik= 0.5191684 train_kl= -0.01517 train_loss= 0.53434 train_acc= 0.49437 val_roc= 0.88022 val_ap= 0.87633 time= 0.07468\n",
      "训练次数: 23 Epoch: 0028 log_lik= 0.51729393 train_kl= -0.01530 train_loss= 0.53260 train_acc= 0.49210 val_roc= 0.88277 val_ap= 0.87896 time= 0.07568\n",
      "训练次数: 23 Epoch: 0029 log_lik= 0.5136833 train_kl= -0.01540 train_loss= 0.52909 train_acc= 0.49033 val_roc= 0.88475 val_ap= 0.88211 time= 0.07471\n",
      "训练次数: 23 Epoch: 0030 log_lik= 0.5099145 train_kl= -0.01549 train_loss= 0.52541 train_acc= 0.49199 val_roc= 0.88680 val_ap= 0.88462 time= 0.07819\n",
      "训练次数: 23 Epoch: 0031 log_lik= 0.5042536 train_kl= -0.01557 train_loss= 0.51982 train_acc= 0.49591 val_roc= 0.88918 val_ap= 0.88802 time= 0.07767\n",
      "训练次数: 23 Epoch: 0032 log_lik= 0.5002274 train_kl= -0.01561 train_loss= 0.51584 train_acc= 0.50071 val_roc= 0.89197 val_ap= 0.89181 time= 0.07667\n",
      "训练次数: 23 Epoch: 0033 log_lik= 0.49377665 train_kl= -0.01562 train_loss= 0.50939 train_acc= 0.50401 val_roc= 0.89377 val_ap= 0.89504 time= 0.07369\n",
      "训练次数: 23 Epoch: 0034 log_lik= 0.48958716 train_kl= -0.01561 train_loss= 0.50520 train_acc= 0.50354 val_roc= 0.89426 val_ap= 0.89762 time= 0.07468\n",
      "训练次数: 23 Epoch: 0035 log_lik= 0.4878167 train_kl= -0.01562 train_loss= 0.50343 train_acc= 0.50023 val_roc= 0.89368 val_ap= 0.89882 time= 0.07767\n",
      "训练次数: 23 Epoch: 0036 log_lik= 0.48494467 train_kl= -0.01563 train_loss= 0.50057 train_acc= 0.50162 val_roc= 0.89300 val_ap= 0.90002 time= 0.07568\n",
      "训练次数: 23 Epoch: 0037 log_lik= 0.4816385 train_kl= -0.01563 train_loss= 0.49726 train_acc= 0.50651 val_roc= 0.89277 val_ap= 0.90125 time= 0.08365\n",
      "训练次数: 23 Epoch: 0038 log_lik= 0.47851467 train_kl= -0.01560 train_loss= 0.49411 train_acc= 0.50971 val_roc= 0.89287 val_ap= 0.90221 time= 0.07667\n",
      "训练次数: 23 Epoch: 0039 log_lik= 0.47730616 train_kl= -0.01556 train_loss= 0.49286 train_acc= 0.50972 val_roc= 0.89278 val_ap= 0.90184 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 23 Epoch: 0040 log_lik= 0.47640768 train_kl= -0.01551 train_loss= 0.49191 train_acc= 0.50979 val_roc= 0.89357 val_ap= 0.90304 time= 0.08365\n",
      "训练次数: 23 Epoch: 0041 log_lik= 0.475521 train_kl= -0.01545 train_loss= 0.49097 train_acc= 0.50997 val_roc= 0.89433 val_ap= 0.90412 time= 0.07667\n",
      "训练次数: 23 Epoch: 0042 log_lik= 0.4728178 train_kl= -0.01538 train_loss= 0.48820 train_acc= 0.51216 val_roc= 0.89487 val_ap= 0.90438 time= 0.07568\n",
      "训练次数: 23 Epoch: 0043 log_lik= 0.4700841 train_kl= -0.01531 train_loss= 0.48539 train_acc= 0.51483 val_roc= 0.89576 val_ap= 0.90403 time= 0.07767\n",
      "训练次数: 23 Epoch: 0044 log_lik= 0.46897167 train_kl= -0.01523 train_loss= 0.48420 train_acc= 0.51490 val_roc= 0.89657 val_ap= 0.90407 time= 0.08364\n",
      "训练次数: 23 Epoch: 0045 log_lik= 0.46793205 train_kl= -0.01515 train_loss= 0.48308 train_acc= 0.51624 val_roc= 0.89780 val_ap= 0.90515 time= 0.07468\n",
      "训练次数: 23 Epoch: 0046 log_lik= 0.4665119 train_kl= -0.01505 train_loss= 0.48156 train_acc= 0.51722 val_roc= 0.89868 val_ap= 0.90502 time= 0.07468\n",
      "训练次数: 23 Epoch: 0047 log_lik= 0.46394044 train_kl= -0.01495 train_loss= 0.47889 train_acc= 0.51917 val_roc= 0.89938 val_ap= 0.90520 time= 0.07471\n",
      "训练次数: 23 Epoch: 0048 log_lik= 0.46243408 train_kl= -0.01486 train_loss= 0.47729 train_acc= 0.52087 val_roc= 0.90007 val_ap= 0.90547 time= 0.07568\n",
      "训练次数: 23 Epoch: 0049 log_lik= 0.46145964 train_kl= -0.01477 train_loss= 0.47623 train_acc= 0.52128 val_roc= 0.90103 val_ap= 0.90604 time= 0.07667\n",
      "训练次数: 23 Epoch: 0050 log_lik= 0.4603476 train_kl= -0.01468 train_loss= 0.47502 train_acc= 0.52140 val_roc= 0.90224 val_ap= 0.90733 time= 0.08364\n",
      "训练次数: 23 Epoch: 0051 log_lik= 0.45950356 train_kl= -0.01458 train_loss= 0.47408 train_acc= 0.52132 val_roc= 0.90282 val_ap= 0.90807 time= 0.07568\n",
      "训练次数: 23 Epoch: 0052 log_lik= 0.45779657 train_kl= -0.01447 train_loss= 0.47227 train_acc= 0.52327 val_roc= 0.90301 val_ap= 0.90822 time= 0.07568\n",
      "训练次数: 23 Epoch: 0053 log_lik= 0.45749494 train_kl= -0.01437 train_loss= 0.47186 train_acc= 0.52316 val_roc= 0.90366 val_ap= 0.90884 time= 0.07568\n",
      "训练次数: 23 Epoch: 0054 log_lik= 0.4564486 train_kl= -0.01427 train_loss= 0.47072 train_acc= 0.52367 val_roc= 0.90455 val_ap= 0.90985 time= 0.07767\n",
      "训练次数: 23 Epoch: 0055 log_lik= 0.4553264 train_kl= -0.01419 train_loss= 0.46951 train_acc= 0.52527 val_roc= 0.90564 val_ap= 0.91145 time= 0.07667\n",
      "训练次数: 23 Epoch: 0056 log_lik= 0.45475093 train_kl= -0.01409 train_loss= 0.46885 train_acc= 0.52519 val_roc= 0.90613 val_ap= 0.91215 time= 0.07468\n",
      "训练次数: 23 Epoch: 0057 log_lik= 0.45365533 train_kl= -0.01400 train_loss= 0.46765 train_acc= 0.52528 val_roc= 0.90611 val_ap= 0.91280 time= 0.08962\n",
      "训练次数: 23 Epoch: 0058 log_lik= 0.45250574 train_kl= -0.01389 train_loss= 0.46640 train_acc= 0.52559 val_roc= 0.90619 val_ap= 0.91294 time= 0.07866\n",
      "训练次数: 23 Epoch: 0059 log_lik= 0.45181045 train_kl= -0.01380 train_loss= 0.46561 train_acc= 0.52715 val_roc= 0.90692 val_ap= 0.91383 time= 0.07568\n",
      "训练次数: 23 Epoch: 0060 log_lik= 0.45113778 train_kl= -0.01370 train_loss= 0.46484 train_acc= 0.52787 val_roc= 0.90715 val_ap= 0.91422 time= 0.07568\n",
      "训练次数: 23 Epoch: 0061 log_lik= 0.45051938 train_kl= -0.01360 train_loss= 0.46411 train_acc= 0.52727 val_roc= 0.90740 val_ap= 0.91446 time= 0.07866\n",
      "训练次数: 23 Epoch: 0062 log_lik= 0.44968614 train_kl= -0.01349 train_loss= 0.46317 train_acc= 0.52712 val_roc= 0.90799 val_ap= 0.91469 time= 0.07682\n",
      "训练次数: 23 Epoch: 0063 log_lik= 0.44868645 train_kl= -0.01339 train_loss= 0.46207 train_acc= 0.52923 val_roc= 0.90808 val_ap= 0.91518 time= 0.07568\n",
      "训练次数: 23 Epoch: 0064 log_lik= 0.44889992 train_kl= -0.01329 train_loss= 0.46219 train_acc= 0.52953 val_roc= 0.90782 val_ap= 0.91501 time= 0.07468\n",
      "训练次数: 23 Epoch: 0065 log_lik= 0.44781807 train_kl= -0.01319 train_loss= 0.46101 train_acc= 0.52876 val_roc= 0.90834 val_ap= 0.91556 time= 0.07468\n",
      "训练次数: 23 Epoch: 0066 log_lik= 0.44699416 train_kl= -0.01308 train_loss= 0.46008 train_acc= 0.52975 val_roc= 0.90941 val_ap= 0.91642 time= 0.07767\n",
      "训练次数: 23 Epoch: 0067 log_lik= 0.44707653 train_kl= -0.01298 train_loss= 0.46006 train_acc= 0.52915 val_roc= 0.90958 val_ap= 0.91586 time= 0.07866\n",
      "训练次数: 23 Epoch: 0068 log_lik= 0.4463649 train_kl= -0.01288 train_loss= 0.45925 train_acc= 0.53091 val_roc= 0.90918 val_ap= 0.91548 time= 0.08364\n",
      "训练次数: 23 Epoch: 0069 log_lik= 0.44584587 train_kl= -0.01279 train_loss= 0.45864 train_acc= 0.53098 val_roc= 0.90850 val_ap= 0.91504 time= 0.09361\n",
      "训练次数: 23 Epoch: 0070 log_lik= 0.4457342 train_kl= -0.01270 train_loss= 0.45844 train_acc= 0.53033 val_roc= 0.90906 val_ap= 0.91548 time= 0.08265\n",
      "训练次数: 23 Epoch: 0071 log_lik= 0.44516182 train_kl= -0.01262 train_loss= 0.45778 train_acc= 0.53073 val_roc= 0.90953 val_ap= 0.91590 time= 0.08165\n",
      "训练次数: 23 Epoch: 0072 log_lik= 0.4446402 train_kl= -0.01253 train_loss= 0.45717 train_acc= 0.53069 val_roc= 0.90957 val_ap= 0.91572 time= 0.07519\n",
      "训练次数: 23 Epoch: 0073 log_lik= 0.44470853 train_kl= -0.01246 train_loss= 0.45717 train_acc= 0.53275 val_roc= 0.90902 val_ap= 0.91519 time= 0.07568\n",
      "训练次数: 23 Epoch: 0074 log_lik= 0.44426316 train_kl= -0.01240 train_loss= 0.45666 train_acc= 0.53267 val_roc= 0.90924 val_ap= 0.91568 time= 0.08265\n",
      "训练次数: 23 Epoch: 0075 log_lik= 0.44363165 train_kl= -0.01233 train_loss= 0.45596 train_acc= 0.53220 val_roc= 0.90997 val_ap= 0.91652 time= 0.08265\n",
      "训练次数: 23 Epoch: 0076 log_lik= 0.44354886 train_kl= -0.01226 train_loss= 0.45581 train_acc= 0.53072 val_roc= 0.90918 val_ap= 0.91565 time= 0.07468\n",
      "训练次数: 23 Epoch: 0077 log_lik= 0.4432272 train_kl= -0.01220 train_loss= 0.45542 train_acc= 0.53185 val_roc= 0.90892 val_ap= 0.91503 time= 0.07767\n",
      "训练次数: 23 Epoch: 0078 log_lik= 0.4428479 train_kl= -0.01215 train_loss= 0.45500 train_acc= 0.53303 val_roc= 0.90899 val_ap= 0.91499 time= 0.07667\n",
      "训练次数: 23 Epoch: 0079 log_lik= 0.44271803 train_kl= -0.01210 train_loss= 0.45482 train_acc= 0.53239 val_roc= 0.91012 val_ap= 0.91617 time= 0.07568\n",
      "训练次数: 23 Epoch: 0080 log_lik= 0.44187248 train_kl= -0.01206 train_loss= 0.45393 train_acc= 0.53294 val_roc= 0.91051 val_ap= 0.91678 time= 0.07520\n",
      "训练次数: 23 Epoch: 0081 log_lik= 0.4416234 train_kl= -0.01202 train_loss= 0.45364 train_acc= 0.53337 val_roc= 0.91002 val_ap= 0.91667 time= 0.07468\n",
      "训练次数: 23 Epoch: 0082 log_lik= 0.440846 train_kl= -0.01198 train_loss= 0.45282 train_acc= 0.53380 val_roc= 0.90948 val_ap= 0.91574 time= 0.07568\n",
      "训练次数: 23 Epoch: 0083 log_lik= 0.44108796 train_kl= -0.01194 train_loss= 0.45303 train_acc= 0.53414 val_roc= 0.91013 val_ap= 0.91609 time= 0.08663\n",
      "训练次数: 23 Epoch: 0084 log_lik= 0.44000277 train_kl= -0.01191 train_loss= 0.45191 train_acc= 0.53521 val_roc= 0.91099 val_ap= 0.91716 time= 0.07568\n",
      "训练次数: 23 Epoch: 0085 log_lik= 0.4401936 train_kl= -0.01188 train_loss= 0.45207 train_acc= 0.53463 val_roc= 0.91093 val_ap= 0.91714 time= 0.07468\n",
      "训练次数: 23 Epoch: 0086 log_lik= 0.4400518 train_kl= -0.01185 train_loss= 0.45190 train_acc= 0.53482 val_roc= 0.91019 val_ap= 0.91605 time= 0.07468\n",
      "训练次数: 23 Epoch: 0087 log_lik= 0.43968976 train_kl= -0.01182 train_loss= 0.45151 train_acc= 0.53580 val_roc= 0.90942 val_ap= 0.91522 time= 0.08165\n",
      "训练次数: 23 Epoch: 0088 log_lik= 0.4398652 train_kl= -0.01180 train_loss= 0.45167 train_acc= 0.53608 val_roc= 0.91035 val_ap= 0.91602 time= 0.07867\n",
      "训练次数: 23 Epoch: 0089 log_lik= 0.43888384 train_kl= -0.01178 train_loss= 0.45067 train_acc= 0.53718 val_roc= 0.91109 val_ap= 0.91706 time= 0.07667\n",
      "训练次数: 23 Epoch: 0090 log_lik= 0.4386213 train_kl= -0.01177 train_loss= 0.45039 train_acc= 0.53673 val_roc= 0.91015 val_ap= 0.91638 time= 0.07667\n",
      "训练次数: 23 Epoch: 0091 log_lik= 0.4384303 train_kl= -0.01176 train_loss= 0.45019 train_acc= 0.53688 val_roc= 0.90922 val_ap= 0.91532 time= 0.07468\n",
      "训练次数: 23 Epoch: 0092 log_lik= 0.43789038 train_kl= -0.01175 train_loss= 0.44964 train_acc= 0.53776 val_roc= 0.90963 val_ap= 0.91548 time= 0.08364\n",
      "训练次数: 23 Epoch: 0093 log_lik= 0.43751162 train_kl= -0.01174 train_loss= 0.44926 train_acc= 0.53801 val_roc= 0.91031 val_ap= 0.91568 time= 0.07369\n",
      "训练次数: 23 Epoch: 0094 log_lik= 0.43733093 train_kl= -0.01174 train_loss= 0.44907 train_acc= 0.53857 val_roc= 0.91083 val_ap= 0.91619 time= 0.07767\n",
      "训练次数: 23 Epoch: 0095 log_lik= 0.43721512 train_kl= -0.01174 train_loss= 0.44896 train_acc= 0.53979 val_roc= 0.91026 val_ap= 0.91635 time= 0.07468\n",
      "训练次数: 23 Epoch: 0096 log_lik= 0.43697324 train_kl= -0.01175 train_loss= 0.44872 train_acc= 0.53934 val_roc= 0.90964 val_ap= 0.91622 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 23 Epoch: 0097 log_lik= 0.43659875 train_kl= -0.01175 train_loss= 0.44835 train_acc= 0.53965 val_roc= 0.90992 val_ap= 0.91631 time= 0.07568\n",
      "训练次数: 23 Epoch: 0098 log_lik= 0.43635756 train_kl= -0.01174 train_loss= 0.44810 train_acc= 0.54024 val_roc= 0.91071 val_ap= 0.91634 time= 0.07668\n",
      "训练次数: 23 Epoch: 0099 log_lik= 0.43586174 train_kl= -0.01174 train_loss= 0.44761 train_acc= 0.53997 val_roc= 0.91061 val_ap= 0.91641 time= 0.07474\n",
      "训练次数: 23 Epoch: 0100 log_lik= 0.43590742 train_kl= -0.01176 train_loss= 0.44767 train_acc= 0.54021 val_roc= 0.91049 val_ap= 0.91665 time= 0.08464\n",
      "Optimization Finished!\n",
      "训练次数: 23 ROC score: 0.917340284954038\n",
      "训练次数: 23 AP score: 0.9220016667716238\n",
      "训练次数: 24 Epoch: 0001 log_lik= 1.6860615 train_kl= -0.00006 train_loss= 1.68613 train_acc= 0.49282 val_roc= 0.67270 val_ap= 0.71746 time= 1.61646\n",
      "训练次数: 24 Epoch: 0002 log_lik= 1.447366 train_kl= -0.00034 train_loss= 1.44771 train_acc= 0.45435 val_roc= 0.66909 val_ap= 0.71423 time= 0.08165\n",
      "训练次数: 24 Epoch: 0003 log_lik= 1.2804775 train_kl= -0.00068 train_loss= 1.28116 train_acc= 0.40046 val_roc= 0.67316 val_ap= 0.71826 time= 0.07568\n",
      "训练次数: 24 Epoch: 0004 log_lik= 1.1670687 train_kl= -0.00111 train_loss= 1.16818 train_acc= 0.34907 val_roc= 0.68320 val_ap= 0.72583 time= 0.07966\n",
      "训练次数: 24 Epoch: 0005 log_lik= 1.060136 train_kl= -0.00150 train_loss= 1.06164 train_acc= 0.32610 val_roc= 0.70400 val_ap= 0.74047 time= 0.08862\n",
      "训练次数: 24 Epoch: 0006 log_lik= 0.9395425 train_kl= -0.00190 train_loss= 0.94144 train_acc= 0.35950 val_roc= 0.74126 val_ap= 0.76688 time= 0.07568\n",
      "训练次数: 24 Epoch: 0007 log_lik= 0.8408449 train_kl= -0.00242 train_loss= 0.84326 train_acc= 0.37874 val_roc= 0.76729 val_ap= 0.78118 time= 0.07767\n",
      "训练次数: 24 Epoch: 0008 log_lik= 0.76862985 train_kl= -0.00310 train_loss= 0.77173 train_acc= 0.39862 val_roc= 0.75233 val_ap= 0.76139 time= 0.07369\n",
      "训练次数: 24 Epoch: 0009 log_lik= 0.7196542 train_kl= -0.00394 train_loss= 0.72360 train_acc= 0.38392 val_roc= 0.74408 val_ap= 0.75439 time= 0.07568\n",
      "训练次数: 24 Epoch: 0010 log_lik= 0.69534105 train_kl= -0.00489 train_loss= 0.70023 train_acc= 0.30142 val_roc= 0.79332 val_ap= 0.79893 time= 0.07468\n",
      "训练次数: 24 Epoch: 0011 log_lik= 0.68193597 train_kl= -0.00576 train_loss= 0.68770 train_acc= 0.25797 val_roc= 0.83556 val_ap= 0.84955 time= 0.07568\n",
      "训练次数: 24 Epoch: 0012 log_lik= 0.6681765 train_kl= -0.00655 train_loss= 0.67473 train_acc= 0.22876 val_roc= 0.84564 val_ap= 0.85383 time= 0.07468\n",
      "训练次数: 24 Epoch: 0013 log_lik= 0.6586868 train_kl= -0.00729 train_loss= 0.66598 train_acc= 0.22238 val_roc= 0.84671 val_ap= 0.85118 time= 0.07767\n",
      "训练次数: 24 Epoch: 0014 log_lik= 0.6415968 train_kl= -0.00800 train_loss= 0.64959 train_acc= 0.25634 val_roc= 0.84561 val_ap= 0.85249 time= 0.07468\n",
      "训练次数: 24 Epoch: 0015 log_lik= 0.6129634 train_kl= -0.00869 train_loss= 0.62165 train_acc= 0.32463 val_roc= 0.83984 val_ap= 0.84642 time= 0.07667\n",
      "训练次数: 24 Epoch: 0016 log_lik= 0.59344715 train_kl= -0.00935 train_loss= 0.60280 train_acc= 0.38005 val_roc= 0.83767 val_ap= 0.84552 time= 0.07667\n",
      "训练次数: 24 Epoch: 0017 log_lik= 0.5816284 train_kl= -0.00993 train_loss= 0.59156 train_acc= 0.40948 val_roc= 0.84120 val_ap= 0.85041 time= 0.07369\n",
      "训练次数: 24 Epoch: 0018 log_lik= 0.5699955 train_kl= -0.01038 train_loss= 0.58038 train_acc= 0.44397 val_roc= 0.84376 val_ap= 0.85401 time= 0.08066\n",
      "训练次数: 24 Epoch: 0019 log_lik= 0.5664936 train_kl= -0.01077 train_loss= 0.57726 train_acc= 0.45525 val_roc= 0.84710 val_ap= 0.85911 time= 0.07521\n",
      "训练次数: 24 Epoch: 0020 log_lik= 0.56295276 train_kl= -0.01113 train_loss= 0.57408 train_acc= 0.46643 val_roc= 0.85240 val_ap= 0.86458 time= 0.07568\n",
      "训练次数: 24 Epoch: 0021 log_lik= 0.5579708 train_kl= -0.01144 train_loss= 0.56941 train_acc= 0.47498 val_roc= 0.86276 val_ap= 0.87475 time= 0.07568\n",
      "训练次数: 24 Epoch: 0022 log_lik= 0.5506056 train_kl= -0.01166 train_loss= 0.56227 train_acc= 0.47836 val_roc= 0.87110 val_ap= 0.88309 time= 0.07619\n",
      "训练次数: 24 Epoch: 0023 log_lik= 0.54342675 train_kl= -0.01178 train_loss= 0.55521 train_acc= 0.47816 val_roc= 0.87672 val_ap= 0.88887 time= 0.07369\n",
      "训练次数: 24 Epoch: 0024 log_lik= 0.53499454 train_kl= -0.01182 train_loss= 0.54682 train_acc= 0.48258 val_roc= 0.88113 val_ap= 0.89464 time= 0.07468\n",
      "训练次数: 24 Epoch: 0025 log_lik= 0.52729106 train_kl= -0.01185 train_loss= 0.53914 train_acc= 0.48503 val_roc= 0.88524 val_ap= 0.89920 time= 0.07468\n",
      "训练次数: 24 Epoch: 0026 log_lik= 0.52063245 train_kl= -0.01188 train_loss= 0.53251 train_acc= 0.48762 val_roc= 0.88999 val_ap= 0.90284 time= 0.07869\n",
      "训练次数: 24 Epoch: 0027 log_lik= 0.5113603 train_kl= -0.01192 train_loss= 0.52328 train_acc= 0.49510 val_roc= 0.89478 val_ap= 0.90479 time= 0.08265\n",
      "训练次数: 24 Epoch: 0028 log_lik= 0.5038071 train_kl= -0.01193 train_loss= 0.51574 train_acc= 0.50369 val_roc= 0.89894 val_ap= 0.90613 time= 0.07668\n",
      "训练次数: 24 Epoch: 0029 log_lik= 0.50181544 train_kl= -0.01191 train_loss= 0.51372 train_acc= 0.50531 val_roc= 0.90267 val_ap= 0.90883 time= 0.08165\n",
      "训练次数: 24 Epoch: 0030 log_lik= 0.4988862 train_kl= -0.01186 train_loss= 0.51074 train_acc= 0.50507 val_roc= 0.90519 val_ap= 0.91101 time= 0.07468\n",
      "训练次数: 24 Epoch: 0031 log_lik= 0.49736235 train_kl= -0.01180 train_loss= 0.50916 train_acc= 0.50265 val_roc= 0.90723 val_ap= 0.91219 time= 0.08165\n",
      "训练次数: 24 Epoch: 0032 log_lik= 0.49768433 train_kl= -0.01174 train_loss= 0.50942 train_acc= 0.49620 val_roc= 0.90888 val_ap= 0.91229 time= 0.07680\n",
      "训练次数: 24 Epoch: 0033 log_lik= 0.4955716 train_kl= -0.01168 train_loss= 0.50725 train_acc= 0.49529 val_roc= 0.90982 val_ap= 0.91157 time= 0.07568\n",
      "训练次数: 24 Epoch: 0034 log_lik= 0.49284926 train_kl= -0.01162 train_loss= 0.50447 train_acc= 0.50061 val_roc= 0.91068 val_ap= 0.91163 time= 0.08265\n",
      "训练次数: 24 Epoch: 0035 log_lik= 0.48995993 train_kl= -0.01155 train_loss= 0.50151 train_acc= 0.50670 val_roc= 0.91078 val_ap= 0.91229 time= 0.07369\n",
      "训练次数: 24 Epoch: 0036 log_lik= 0.48622918 train_kl= -0.01148 train_loss= 0.49771 train_acc= 0.51195 val_roc= 0.91041 val_ap= 0.91357 time= 0.07468\n",
      "训练次数: 24 Epoch: 0037 log_lik= 0.4837357 train_kl= -0.01141 train_loss= 0.49515 train_acc= 0.51323 val_roc= 0.91010 val_ap= 0.91494 time= 0.07668\n",
      "训练次数: 24 Epoch: 0038 log_lik= 0.4832893 train_kl= -0.01136 train_loss= 0.49465 train_acc= 0.51324 val_roc= 0.90997 val_ap= 0.91500 time= 0.07568\n",
      "训练次数: 24 Epoch: 0039 log_lik= 0.48218995 train_kl= -0.01130 train_loss= 0.49349 train_acc= 0.51190 val_roc= 0.90996 val_ap= 0.91554 time= 0.07568\n",
      "训练次数: 24 Epoch: 0040 log_lik= 0.48187795 train_kl= -0.01122 train_loss= 0.49310 train_acc= 0.51054 val_roc= 0.90970 val_ap= 0.91576 time= 0.07568\n",
      "训练次数: 24 Epoch: 0041 log_lik= 0.47893143 train_kl= -0.01112 train_loss= 0.49005 train_acc= 0.51293 val_roc= 0.91005 val_ap= 0.91701 time= 0.07468\n",
      "训练次数: 24 Epoch: 0042 log_lik= 0.47584832 train_kl= -0.01101 train_loss= 0.48685 train_acc= 0.51545 val_roc= 0.90950 val_ap= 0.91763 time= 0.07369\n",
      "训练次数: 24 Epoch: 0043 log_lik= 0.4730628 train_kl= -0.01089 train_loss= 0.48396 train_acc= 0.51675 val_roc= 0.90847 val_ap= 0.91762 time= 0.07667\n",
      "训练次数: 24 Epoch: 0044 log_lik= 0.47190645 train_kl= -0.01079 train_loss= 0.48270 train_acc= 0.51660 val_roc= 0.90721 val_ap= 0.91662 time= 0.08464\n",
      "训练次数: 24 Epoch: 0045 log_lik= 0.47111723 train_kl= -0.01071 train_loss= 0.48182 train_acc= 0.51703 val_roc= 0.90604 val_ap= 0.91505 time= 0.07966\n",
      "训练次数: 24 Epoch: 0046 log_lik= 0.47079676 train_kl= -0.01064 train_loss= 0.48144 train_acc= 0.51671 val_roc= 0.90541 val_ap= 0.91393 time= 0.07468\n",
      "训练次数: 24 Epoch: 0047 log_lik= 0.46965802 train_kl= -0.01058 train_loss= 0.48024 train_acc= 0.51604 val_roc= 0.90545 val_ap= 0.91387 time= 0.07468\n",
      "训练次数: 24 Epoch: 0048 log_lik= 0.4688496 train_kl= -0.01053 train_loss= 0.47938 train_acc= 0.51547 val_roc= 0.90659 val_ap= 0.91566 time= 0.07369\n",
      "训练次数: 24 Epoch: 0049 log_lik= 0.4667521 train_kl= -0.01049 train_loss= 0.47724 train_acc= 0.51595 val_roc= 0.90739 val_ap= 0.91650 time= 0.07667\n",
      "训练次数: 24 Epoch: 0050 log_lik= 0.4659802 train_kl= -0.01045 train_loss= 0.47643 train_acc= 0.51622 val_roc= 0.90736 val_ap= 0.91621 time= 0.07468\n",
      "训练次数: 24 Epoch: 0051 log_lik= 0.46487352 train_kl= -0.01043 train_loss= 0.47531 train_acc= 0.51838 val_roc= 0.90729 val_ap= 0.91618 time= 0.07468\n",
      "训练次数: 24 Epoch: 0052 log_lik= 0.46275565 train_kl= -0.01043 train_loss= 0.47319 train_acc= 0.51883 val_roc= 0.90824 val_ap= 0.91736 time= 0.07369\n",
      "训练次数: 24 Epoch: 0053 log_lik= 0.46216497 train_kl= -0.01043 train_loss= 0.47260 train_acc= 0.52011 val_roc= 0.90847 val_ap= 0.91758 time= 0.07667\n",
      "训练次数: 24 Epoch: 0054 log_lik= 0.46194074 train_kl= -0.01044 train_loss= 0.47238 train_acc= 0.51857 val_roc= 0.90970 val_ap= 0.91841 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 24 Epoch: 0055 log_lik= 0.4607159 train_kl= -0.01045 train_loss= 0.47117 train_acc= 0.51919 val_roc= 0.91065 val_ap= 0.91888 time= 0.07966\n",
      "训练次数: 24 Epoch: 0056 log_lik= 0.45922324 train_kl= -0.01047 train_loss= 0.46969 train_acc= 0.51999 val_roc= 0.91171 val_ap= 0.91937 time= 0.07966\n",
      "训练次数: 24 Epoch: 0057 log_lik= 0.45844233 train_kl= -0.01049 train_loss= 0.46893 train_acc= 0.52153 val_roc= 0.91230 val_ap= 0.91948 time= 0.07966\n",
      "训练次数: 24 Epoch: 0058 log_lik= 0.45802572 train_kl= -0.01050 train_loss= 0.46853 train_acc= 0.52219 val_roc= 0.91276 val_ap= 0.91968 time= 0.07528\n",
      "训练次数: 24 Epoch: 0059 log_lik= 0.45713982 train_kl= -0.01051 train_loss= 0.46765 train_acc= 0.52219 val_roc= 0.91357 val_ap= 0.92079 time= 0.07667\n",
      "训练次数: 24 Epoch: 0060 log_lik= 0.45661938 train_kl= -0.01052 train_loss= 0.46714 train_acc= 0.52248 val_roc= 0.91420 val_ap= 0.92161 time= 0.07468\n",
      "训练次数: 24 Epoch: 0061 log_lik= 0.45538467 train_kl= -0.01054 train_loss= 0.46592 train_acc= 0.52302 val_roc= 0.91483 val_ap= 0.92201 time= 0.07667\n",
      "训练次数: 24 Epoch: 0062 log_lik= 0.45461628 train_kl= -0.01055 train_loss= 0.46517 train_acc= 0.52357 val_roc= 0.91571 val_ap= 0.92229 time= 0.07468\n",
      "训练次数: 24 Epoch: 0063 log_lik= 0.45415816 train_kl= -0.01057 train_loss= 0.46473 train_acc= 0.52534 val_roc= 0.91655 val_ap= 0.92297 time= 0.07468\n",
      "训练次数: 24 Epoch: 0064 log_lik= 0.4528587 train_kl= -0.01058 train_loss= 0.46344 train_acc= 0.52526 val_roc= 0.91681 val_ap= 0.92335 time= 0.07667\n",
      "训练次数: 24 Epoch: 0065 log_lik= 0.45198518 train_kl= -0.01059 train_loss= 0.46258 train_acc= 0.52477 val_roc= 0.91713 val_ap= 0.92402 time= 0.07468\n",
      "训练次数: 24 Epoch: 0066 log_lik= 0.45156848 train_kl= -0.01062 train_loss= 0.46219 train_acc= 0.52624 val_roc= 0.91800 val_ap= 0.92504 time= 0.07369\n",
      "训练次数: 24 Epoch: 0067 log_lik= 0.45129493 train_kl= -0.01065 train_loss= 0.46194 train_acc= 0.52729 val_roc= 0.91808 val_ap= 0.92550 time= 0.07767\n",
      "训练次数: 24 Epoch: 0068 log_lik= 0.45031834 train_kl= -0.01066 train_loss= 0.46098 train_acc= 0.52756 val_roc= 0.91775 val_ap= 0.92545 time= 0.07623\n",
      "训练次数: 24 Epoch: 0069 log_lik= 0.44904363 train_kl= -0.01069 train_loss= 0.45973 train_acc= 0.52841 val_roc= 0.91845 val_ap= 0.92568 time= 0.07568\n",
      "训练次数: 24 Epoch: 0070 log_lik= 0.449102 train_kl= -0.01072 train_loss= 0.45982 train_acc= 0.52839 val_roc= 0.91927 val_ap= 0.92602 time= 0.07966\n",
      "训练次数: 24 Epoch: 0071 log_lik= 0.44756028 train_kl= -0.01077 train_loss= 0.45833 train_acc= 0.52878 val_roc= 0.91915 val_ap= 0.92666 time= 0.07468\n",
      "训练次数: 24 Epoch: 0072 log_lik= 0.44667536 train_kl= -0.01080 train_loss= 0.45747 train_acc= 0.52950 val_roc= 0.91884 val_ap= 0.92726 time= 0.07668\n",
      "训练次数: 24 Epoch: 0073 log_lik= 0.44611126 train_kl= -0.01083 train_loss= 0.45694 train_acc= 0.53037 val_roc= 0.91981 val_ap= 0.92796 time= 0.07667\n",
      "训练次数: 24 Epoch: 0074 log_lik= 0.44562855 train_kl= -0.01088 train_loss= 0.45651 train_acc= 0.52950 val_roc= 0.92072 val_ap= 0.92816 time= 0.08962\n",
      "训练次数: 24 Epoch: 0075 log_lik= 0.44543362 train_kl= -0.01093 train_loss= 0.45637 train_acc= 0.53059 val_roc= 0.92050 val_ap= 0.92817 time= 0.07468\n",
      "训练次数: 24 Epoch: 0076 log_lik= 0.44457367 train_kl= -0.01096 train_loss= 0.45553 train_acc= 0.53124 val_roc= 0.92092 val_ap= 0.92846 time= 0.07767\n",
      "训练次数: 24 Epoch: 0077 log_lik= 0.44393122 train_kl= -0.01098 train_loss= 0.45491 train_acc= 0.53147 val_roc= 0.92187 val_ap= 0.92909 time= 0.07767\n",
      "训练次数: 24 Epoch: 0078 log_lik= 0.44346848 train_kl= -0.01102 train_loss= 0.45449 train_acc= 0.53268 val_roc= 0.92215 val_ap= 0.92936 time= 0.07667\n",
      "训练次数: 24 Epoch: 0079 log_lik= 0.44305772 train_kl= -0.01106 train_loss= 0.45412 train_acc= 0.53216 val_roc= 0.92087 val_ap= 0.92864 time= 0.07767\n",
      "训练次数: 24 Epoch: 0080 log_lik= 0.44244295 train_kl= -0.01109 train_loss= 0.45353 train_acc= 0.53136 val_roc= 0.92092 val_ap= 0.92855 time= 0.07667\n",
      "训练次数: 24 Epoch: 0081 log_lik= 0.44183552 train_kl= -0.01111 train_loss= 0.45294 train_acc= 0.53146 val_roc= 0.92210 val_ap= 0.92918 time= 0.07568\n",
      "训练次数: 24 Epoch: 0082 log_lik= 0.44098565 train_kl= -0.01112 train_loss= 0.45211 train_acc= 0.53280 val_roc= 0.92247 val_ap= 0.92814 time= 0.07767\n",
      "训练次数: 24 Epoch: 0083 log_lik= 0.44116923 train_kl= -0.01116 train_loss= 0.45233 train_acc= 0.53306 val_roc= 0.92148 val_ap= 0.92810 time= 0.07966\n",
      "训练次数: 24 Epoch: 0084 log_lik= 0.4402305 train_kl= -0.01119 train_loss= 0.45142 train_acc= 0.53393 val_roc= 0.92066 val_ap= 0.92816 time= 0.07667\n",
      "训练次数: 24 Epoch: 0085 log_lik= 0.43991667 train_kl= -0.01121 train_loss= 0.45113 train_acc= 0.53360 val_roc= 0.92118 val_ap= 0.92823 time= 0.07568\n",
      "训练次数: 24 Epoch: 0086 log_lik= 0.43936023 train_kl= -0.01122 train_loss= 0.45058 train_acc= 0.53409 val_roc= 0.92121 val_ap= 0.92778 time= 0.07568\n",
      "训练次数: 24 Epoch: 0087 log_lik= 0.4389428 train_kl= -0.01123 train_loss= 0.45017 train_acc= 0.53324 val_roc= 0.92017 val_ap= 0.92742 time= 0.07667\n",
      "训练次数: 24 Epoch: 0088 log_lik= 0.43876544 train_kl= -0.01124 train_loss= 0.45000 train_acc= 0.53426 val_roc= 0.91933 val_ap= 0.92690 time= 0.08265\n",
      "训练次数: 24 Epoch: 0089 log_lik= 0.4384388 train_kl= -0.01124 train_loss= 0.44968 train_acc= 0.53389 val_roc= 0.91983 val_ap= 0.92690 time= 0.07767\n",
      "训练次数: 24 Epoch: 0090 log_lik= 0.43705413 train_kl= -0.01126 train_loss= 0.44831 train_acc= 0.53438 val_roc= 0.92007 val_ap= 0.92663 time= 0.07568\n",
      "训练次数: 24 Epoch: 0091 log_lik= 0.43800202 train_kl= -0.01128 train_loss= 0.44929 train_acc= 0.53329 val_roc= 0.91956 val_ap= 0.92754 time= 0.07468\n",
      "训练次数: 24 Epoch: 0092 log_lik= 0.4367666 train_kl= -0.01130 train_loss= 0.44806 train_acc= 0.53415 val_roc= 0.91902 val_ap= 0.92732 time= 0.07468\n",
      "训练次数: 24 Epoch: 0093 log_lik= 0.43650997 train_kl= -0.01131 train_loss= 0.44782 train_acc= 0.53410 val_roc= 0.91952 val_ap= 0.92740 time= 0.07468\n",
      "训练次数: 24 Epoch: 0094 log_lik= 0.4360512 train_kl= -0.01134 train_loss= 0.44739 train_acc= 0.53454 val_roc= 0.91976 val_ap= 0.92659 time= 0.07767\n",
      "训练次数: 24 Epoch: 0095 log_lik= 0.435917 train_kl= -0.01135 train_loss= 0.44727 train_acc= 0.53500 val_roc= 0.91907 val_ap= 0.92665 time= 0.07568\n",
      "训练次数: 24 Epoch: 0096 log_lik= 0.4357372 train_kl= -0.01135 train_loss= 0.44709 train_acc= 0.53503 val_roc= 0.91795 val_ap= 0.92702 time= 0.07667\n",
      "训练次数: 24 Epoch: 0097 log_lik= 0.43545675 train_kl= -0.01137 train_loss= 0.44682 train_acc= 0.53490 val_roc= 0.91849 val_ap= 0.92771 time= 0.07767\n",
      "训练次数: 24 Epoch: 0098 log_lik= 0.43441668 train_kl= -0.01139 train_loss= 0.44581 train_acc= 0.53503 val_roc= 0.91962 val_ap= 0.92763 time= 0.07793\n",
      "训练次数: 24 Epoch: 0099 log_lik= 0.43423644 train_kl= -0.01140 train_loss= 0.44564 train_acc= 0.53535 val_roc= 0.91986 val_ap= 0.92765 time= 0.07468\n",
      "训练次数: 24 Epoch: 0100 log_lik= 0.4341021 train_kl= -0.01141 train_loss= 0.44551 train_acc= 0.53510 val_roc= 0.91947 val_ap= 0.92798 time= 0.07767\n",
      "Optimization Finished!\n",
      "训练次数: 24 ROC score: 0.9081658739274616\n",
      "训练次数: 24 AP score: 0.9190080086592298\n",
      "训练次数: 25 Epoch: 0001 log_lik= 1.7386633 train_kl= -0.00004 train_loss= 1.73871 train_acc= 0.49652 val_roc= 0.70657 val_ap= 0.73735 time= 1.72623\n",
      "训练次数: 25 Epoch: 0002 log_lik= 1.3604393 train_kl= -0.00034 train_loss= 1.36078 train_acc= 0.46081 val_roc= 0.70639 val_ap= 0.74017 time= 0.08265\n",
      "训练次数: 25 Epoch: 0003 log_lik= 1.1133213 train_kl= -0.00099 train_loss= 1.11431 train_acc= 0.40216 val_roc= 0.70845 val_ap= 0.74415 time= 0.07468\n",
      "训练次数: 25 Epoch: 0004 log_lik= 0.9617127 train_kl= -0.00189 train_loss= 0.96360 train_acc= 0.33831 val_roc= 0.71591 val_ap= 0.75319 time= 0.07867\n",
      "训练次数: 25 Epoch: 0005 log_lik= 0.8468583 train_kl= -0.00285 train_loss= 0.84971 train_acc= 0.29604 val_roc= 0.72937 val_ap= 0.76643 time= 0.08328\n",
      "训练次数: 25 Epoch: 0006 log_lik= 0.7703753 train_kl= -0.00387 train_loss= 0.77424 train_acc= 0.27521 val_roc= 0.75924 val_ap= 0.78882 time= 0.07767\n",
      "训练次数: 25 Epoch: 0007 log_lik= 0.721023 train_kl= -0.00494 train_loss= 0.72596 train_acc= 0.31452 val_roc= 0.78396 val_ap= 0.80462 time= 0.07767\n",
      "训练次数: 25 Epoch: 0008 log_lik= 0.71098447 train_kl= -0.00607 train_loss= 0.71705 train_acc= 0.31781 val_roc= 0.78025 val_ap= 0.80281 time= 0.07926\n",
      "训练次数: 25 Epoch: 0009 log_lik= 0.70117015 train_kl= -0.00723 train_loss= 0.70840 train_acc= 0.20557 val_roc= 0.78162 val_ap= 0.80518 time= 0.07468\n",
      "训练次数: 25 Epoch: 0010 log_lik= 0.7007726 train_kl= -0.00833 train_loss= 0.70911 train_acc= 0.11411 val_roc= 0.81143 val_ap= 0.83232 time= 0.07468\n",
      "训练次数: 25 Epoch: 0011 log_lik= 0.68532956 train_kl= -0.00925 train_loss= 0.69458 train_acc= 0.12105 val_roc= 0.83088 val_ap= 0.84626 time= 0.07521\n",
      "训练次数: 25 Epoch: 0012 log_lik= 0.6720969 train_kl= -0.01008 train_loss= 0.68218 train_acc= 0.18763 val_roc= 0.84159 val_ap= 0.84910 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 25 Epoch: 0013 log_lik= 0.65660226 train_kl= -0.01086 train_loss= 0.66746 train_acc= 0.24118 val_roc= 0.85946 val_ap= 0.86191 time= 0.07638\n",
      "训练次数: 25 Epoch: 0014 log_lik= 0.63272107 train_kl= -0.01159 train_loss= 0.64431 train_acc= 0.28236 val_roc= 0.86923 val_ap= 0.87200 time= 0.07519\n",
      "训练次数: 25 Epoch: 0015 log_lik= 0.6061691 train_kl= -0.01229 train_loss= 0.61846 train_acc= 0.28920 val_roc= 0.87121 val_ap= 0.87423 time= 0.07468\n",
      "训练次数: 25 Epoch: 0016 log_lik= 0.58948624 train_kl= -0.01293 train_loss= 0.60241 train_acc= 0.31577 val_roc= 0.86944 val_ap= 0.87182 time= 0.08365\n",
      "训练次数: 25 Epoch: 0017 log_lik= 0.5654996 train_kl= -0.01343 train_loss= 0.57893 train_acc= 0.39197 val_roc= 0.85984 val_ap= 0.86294 time= 0.07468\n",
      "训练次数: 25 Epoch: 0018 log_lik= 0.5523154 train_kl= -0.01386 train_loss= 0.56618 train_acc= 0.44933 val_roc= 0.85369 val_ap= 0.85746 time= 0.08066\n",
      "训练次数: 25 Epoch: 0019 log_lik= 0.5472977 train_kl= -0.01424 train_loss= 0.56154 train_acc= 0.47911 val_roc= 0.85943 val_ap= 0.86291 time= 0.07767\n",
      "训练次数: 25 Epoch: 0020 log_lik= 0.54704905 train_kl= -0.01457 train_loss= 0.56162 train_acc= 0.47747 val_roc= 0.86954 val_ap= 0.87479 time= 0.07568\n",
      "训练次数: 25 Epoch: 0021 log_lik= 0.5453594 train_kl= -0.01485 train_loss= 0.56021 train_acc= 0.47212 val_roc= 0.87400 val_ap= 0.88015 time= 0.07668\n",
      "训练次数: 25 Epoch: 0022 log_lik= 0.5419478 train_kl= -0.01506 train_loss= 0.55701 train_acc= 0.47788 val_roc= 0.87389 val_ap= 0.88067 time= 0.07468\n",
      "训练次数: 25 Epoch: 0023 log_lik= 0.53229225 train_kl= -0.01519 train_loss= 0.54748 train_acc= 0.49382 val_roc= 0.87393 val_ap= 0.88117 time= 0.07424\n",
      "训练次数: 25 Epoch: 0024 log_lik= 0.5216389 train_kl= -0.01525 train_loss= 0.53689 train_acc= 0.50802 val_roc= 0.87882 val_ap= 0.88652 time= 0.07369\n",
      "训练次数: 25 Epoch: 0025 log_lik= 0.5144502 train_kl= -0.01526 train_loss= 0.52971 train_acc= 0.50940 val_roc= 0.88454 val_ap= 0.89294 time= 0.07568\n",
      "训练次数: 25 Epoch: 0026 log_lik= 0.50908893 train_kl= -0.01525 train_loss= 0.52434 train_acc= 0.50477 val_roc= 0.88957 val_ap= 0.89784 time= 0.07468\n",
      "训练次数: 25 Epoch: 0027 log_lik= 0.50338244 train_kl= -0.01521 train_loss= 0.51859 train_acc= 0.50750 val_roc= 0.89005 val_ap= 0.89758 time= 0.07568\n",
      "训练次数: 25 Epoch: 0028 log_lik= 0.5006335 train_kl= -0.01515 train_loss= 0.51578 train_acc= 0.50889 val_roc= 0.88791 val_ap= 0.89518 time= 0.07966\n",
      "训练次数: 25 Epoch: 0029 log_lik= 0.49735457 train_kl= -0.01505 train_loss= 0.51241 train_acc= 0.51505 val_roc= 0.88820 val_ap= 0.89623 time= 0.07667\n",
      "训练次数: 25 Epoch: 0030 log_lik= 0.49361232 train_kl= -0.01494 train_loss= 0.50856 train_acc= 0.51977 val_roc= 0.88927 val_ap= 0.89814 time= 0.07468\n",
      "训练次数: 25 Epoch: 0031 log_lik= 0.49234277 train_kl= -0.01484 train_loss= 0.50718 train_acc= 0.51824 val_roc= 0.89109 val_ap= 0.89947 time= 0.08066\n",
      "训练次数: 25 Epoch: 0032 log_lik= 0.49093565 train_kl= -0.01474 train_loss= 0.50568 train_acc= 0.51643 val_roc= 0.89169 val_ap= 0.89936 time= 0.07767\n",
      "训练次数: 25 Epoch: 0033 log_lik= 0.4889131 train_kl= -0.01464 train_loss= 0.50356 train_acc= 0.51536 val_roc= 0.89164 val_ap= 0.89879 time= 0.07468\n",
      "训练次数: 25 Epoch: 0034 log_lik= 0.48741764 train_kl= -0.01453 train_loss= 0.50195 train_acc= 0.51663 val_roc= 0.89268 val_ap= 0.89995 time= 0.07468\n",
      "训练次数: 25 Epoch: 0035 log_lik= 0.4846913 train_kl= -0.01440 train_loss= 0.49909 train_acc= 0.51767 val_roc= 0.89501 val_ap= 0.90220 time= 0.08564\n",
      "训练次数: 25 Epoch: 0036 log_lik= 0.4808806 train_kl= -0.01427 train_loss= 0.49515 train_acc= 0.52162 val_roc= 0.89725 val_ap= 0.90433 time= 0.07468\n",
      "训练次数: 25 Epoch: 0037 log_lik= 0.4776827 train_kl= -0.01414 train_loss= 0.49182 train_acc= 0.52422 val_roc= 0.89932 val_ap= 0.90623 time= 0.07468\n",
      "训练次数: 25 Epoch: 0038 log_lik= 0.4758334 train_kl= -0.01402 train_loss= 0.48985 train_acc= 0.52290 val_roc= 0.90043 val_ap= 0.90755 time= 0.07369\n",
      "训练次数: 25 Epoch: 0039 log_lik= 0.47547722 train_kl= -0.01389 train_loss= 0.48937 train_acc= 0.52006 val_roc= 0.90140 val_ap= 0.90850 time= 0.07767\n",
      "训练次数: 25 Epoch: 0040 log_lik= 0.47341084 train_kl= -0.01376 train_loss= 0.48717 train_acc= 0.52128 val_roc= 0.90189 val_ap= 0.90868 time= 0.08030\n",
      "训练次数: 25 Epoch: 0041 log_lik= 0.47122142 train_kl= -0.01362 train_loss= 0.48484 train_acc= 0.52460 val_roc= 0.90341 val_ap= 0.90984 time= 0.07667\n",
      "训练次数: 25 Epoch: 0042 log_lik= 0.46879756 train_kl= -0.01350 train_loss= 0.48229 train_acc= 0.52700 val_roc= 0.90513 val_ap= 0.91155 time= 0.07767\n",
      "训练次数: 25 Epoch: 0043 log_lik= 0.46798843 train_kl= -0.01338 train_loss= 0.48136 train_acc= 0.52636 val_roc= 0.90611 val_ap= 0.91261 time= 0.07676\n",
      "训练次数: 25 Epoch: 0044 log_lik= 0.46723893 train_kl= -0.01326 train_loss= 0.48049 train_acc= 0.52363 val_roc= 0.90581 val_ap= 0.91209 time= 0.09063\n",
      "训练次数: 25 Epoch: 0045 log_lik= 0.4663185 train_kl= -0.01312 train_loss= 0.47944 train_acc= 0.52307 val_roc= 0.90581 val_ap= 0.91203 time= 0.07369\n",
      "训练次数: 25 Epoch: 0046 log_lik= 0.46460053 train_kl= -0.01298 train_loss= 0.47758 train_acc= 0.52522 val_roc= 0.90613 val_ap= 0.91222 time= 0.07667\n",
      "训练次数: 25 Epoch: 0047 log_lik= 0.462838 train_kl= -0.01284 train_loss= 0.47568 train_acc= 0.52565 val_roc= 0.90731 val_ap= 0.91382 time= 0.07468\n",
      "训练次数: 25 Epoch: 0048 log_lik= 0.46212927 train_kl= -0.01272 train_loss= 0.47485 train_acc= 0.52546 val_roc= 0.90801 val_ap= 0.91434 time= 0.07468\n",
      "训练次数: 25 Epoch: 0049 log_lik= 0.46074536 train_kl= -0.01261 train_loss= 0.47336 train_acc= 0.52461 val_roc= 0.90922 val_ap= 0.91578 time= 0.07667\n",
      "训练次数: 25 Epoch: 0050 log_lik= 0.45983246 train_kl= -0.01250 train_loss= 0.47234 train_acc= 0.52537 val_roc= 0.91029 val_ap= 0.91671 time= 0.07469\n",
      "训练次数: 25 Epoch: 0051 log_lik= 0.45874354 train_kl= -0.01240 train_loss= 0.47114 train_acc= 0.52673 val_roc= 0.91034 val_ap= 0.91678 time= 0.07369\n",
      "训练次数: 25 Epoch: 0052 log_lik= 0.45763588 train_kl= -0.01229 train_loss= 0.46992 train_acc= 0.52677 val_roc= 0.91031 val_ap= 0.91686 time= 0.07468\n",
      "训练次数: 25 Epoch: 0053 log_lik= 0.45687914 train_kl= -0.01219 train_loss= 0.46907 train_acc= 0.52530 val_roc= 0.91009 val_ap= 0.91710 time= 0.07568\n",
      "训练次数: 25 Epoch: 0054 log_lik= 0.4559964 train_kl= -0.01211 train_loss= 0.46810 train_acc= 0.52603 val_roc= 0.91106 val_ap= 0.91833 time= 0.07468\n",
      "训练次数: 25 Epoch: 0055 log_lik= 0.45534292 train_kl= -0.01204 train_loss= 0.46738 train_acc= 0.52616 val_roc= 0.91171 val_ap= 0.91927 time= 0.07767\n",
      "训练次数: 25 Epoch: 0056 log_lik= 0.454815 train_kl= -0.01198 train_loss= 0.46679 train_acc= 0.52697 val_roc= 0.91219 val_ap= 0.91988 time= 0.07568\n",
      "训练次数: 25 Epoch: 0057 log_lik= 0.45380664 train_kl= -0.01191 train_loss= 0.46572 train_acc= 0.52583 val_roc= 0.91177 val_ap= 0.91960 time= 0.08265\n",
      "训练次数: 25 Epoch: 0058 log_lik= 0.4540717 train_kl= -0.01185 train_loss= 0.46592 train_acc= 0.52682 val_roc= 0.91139 val_ap= 0.91945 time= 0.07670\n",
      "训练次数: 25 Epoch: 0059 log_lik= 0.45333332 train_kl= -0.01179 train_loss= 0.46512 train_acc= 0.52645 val_roc= 0.91210 val_ap= 0.92042 time= 0.07269\n",
      "训练次数: 25 Epoch: 0060 log_lik= 0.4526933 train_kl= -0.01173 train_loss= 0.46442 train_acc= 0.52767 val_roc= 0.91219 val_ap= 0.92079 time= 0.07468\n",
      "训练次数: 25 Epoch: 0061 log_lik= 0.45174518 train_kl= -0.01166 train_loss= 0.46341 train_acc= 0.52730 val_roc= 0.91193 val_ap= 0.92061 time= 0.07767\n",
      "训练次数: 25 Epoch: 0062 log_lik= 0.45181155 train_kl= -0.01160 train_loss= 0.46341 train_acc= 0.52709 val_roc= 0.91194 val_ap= 0.92054 time= 0.07568\n",
      "训练次数: 25 Epoch: 0063 log_lik= 0.4514337 train_kl= -0.01155 train_loss= 0.46298 train_acc= 0.52682 val_roc= 0.91193 val_ap= 0.92074 time= 0.07369\n",
      "训练次数: 25 Epoch: 0064 log_lik= 0.45060956 train_kl= -0.01150 train_loss= 0.46211 train_acc= 0.52804 val_roc= 0.91203 val_ap= 0.92126 time= 0.07568\n",
      "训练次数: 25 Epoch: 0065 log_lik= 0.45036775 train_kl= -0.01145 train_loss= 0.46182 train_acc= 0.52750 val_roc= 0.91146 val_ap= 0.92074 time= 0.07568\n",
      "训练次数: 25 Epoch: 0066 log_lik= 0.44952533 train_kl= -0.01140 train_loss= 0.46093 train_acc= 0.52791 val_roc= 0.91158 val_ap= 0.92051 time= 0.07468\n",
      "训练次数: 25 Epoch: 0067 log_lik= 0.4491622 train_kl= -0.01136 train_loss= 0.46052 train_acc= 0.52797 val_roc= 0.91169 val_ap= 0.92074 time= 0.07568\n",
      "训练次数: 25 Epoch: 0068 log_lik= 0.4481549 train_kl= -0.01132 train_loss= 0.45947 train_acc= 0.52808 val_roc= 0.91133 val_ap= 0.92053 time= 0.07468\n",
      "训练次数: 25 Epoch: 0069 log_lik= 0.44829696 train_kl= -0.01128 train_loss= 0.45957 train_acc= 0.52786 val_roc= 0.91122 val_ap= 0.92028 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 25 Epoch: 0070 log_lik= 0.4476963 train_kl= -0.01124 train_loss= 0.45894 train_acc= 0.52927 val_roc= 0.91138 val_ap= 0.92047 time= 0.07667\n",
      "训练次数: 25 Epoch: 0071 log_lik= 0.44686535 train_kl= -0.01122 train_loss= 0.45808 train_acc= 0.52860 val_roc= 0.91154 val_ap= 0.92073 time= 0.07568\n",
      "训练次数: 25 Epoch: 0072 log_lik= 0.44660872 train_kl= -0.01120 train_loss= 0.45781 train_acc= 0.52980 val_roc= 0.91073 val_ap= 0.91977 time= 0.07568\n",
      "训练次数: 25 Epoch: 0073 log_lik= 0.446541 train_kl= -0.01119 train_loss= 0.45773 train_acc= 0.52902 val_roc= 0.91039 val_ap= 0.91939 time= 0.07667\n",
      "训练次数: 25 Epoch: 0074 log_lik= 0.44559702 train_kl= -0.01118 train_loss= 0.45678 train_acc= 0.52943 val_roc= 0.91045 val_ap= 0.91968 time= 0.07667\n",
      "训练次数: 25 Epoch: 0075 log_lik= 0.4454174 train_kl= -0.01118 train_loss= 0.45659 train_acc= 0.52944 val_roc= 0.91097 val_ap= 0.92015 time= 0.07568\n",
      "训练次数: 25 Epoch: 0076 log_lik= 0.4449506 train_kl= -0.01118 train_loss= 0.45613 train_acc= 0.52974 val_roc= 0.91065 val_ap= 0.91994 time= 0.07369\n",
      "训练次数: 25 Epoch: 0077 log_lik= 0.44424802 train_kl= -0.01118 train_loss= 0.45543 train_acc= 0.53053 val_roc= 0.91013 val_ap= 0.91941 time= 0.07468\n",
      "训练次数: 25 Epoch: 0078 log_lik= 0.44446838 train_kl= -0.01118 train_loss= 0.45565 train_acc= 0.53024 val_roc= 0.90986 val_ap= 0.91927 time= 0.07468\n",
      "训练次数: 25 Epoch: 0079 log_lik= 0.44364497 train_kl= -0.01119 train_loss= 0.45483 train_acc= 0.53148 val_roc= 0.91012 val_ap= 0.91976 time= 0.07667\n",
      "训练次数: 25 Epoch: 0080 log_lik= 0.44338858 train_kl= -0.01120 train_loss= 0.45459 train_acc= 0.53083 val_roc= 0.91022 val_ap= 0.91983 time= 0.07468\n",
      "训练次数: 25 Epoch: 0081 log_lik= 0.44299495 train_kl= -0.01122 train_loss= 0.45422 train_acc= 0.53044 val_roc= 0.91018 val_ap= 0.91967 time= 0.07568\n",
      "训练次数: 25 Epoch: 0082 log_lik= 0.44191277 train_kl= -0.01124 train_loss= 0.45315 train_acc= 0.53174 val_roc= 0.90957 val_ap= 0.91906 time= 0.08066\n",
      "训练次数: 25 Epoch: 0083 log_lik= 0.44157797 train_kl= -0.01126 train_loss= 0.45283 train_acc= 0.53064 val_roc= 0.90983 val_ap= 0.91937 time= 0.08165\n",
      "训练次数: 25 Epoch: 0084 log_lik= 0.44152334 train_kl= -0.01127 train_loss= 0.45279 train_acc= 0.53098 val_roc= 0.90941 val_ap= 0.91918 time= 0.07568\n",
      "训练次数: 25 Epoch: 0085 log_lik= 0.4410062 train_kl= -0.01128 train_loss= 0.45229 train_acc= 0.53165 val_roc= 0.90886 val_ap= 0.91853 time= 0.07667\n",
      "训练次数: 25 Epoch: 0086 log_lik= 0.4407166 train_kl= -0.01129 train_loss= 0.45201 train_acc= 0.53031 val_roc= 0.90924 val_ap= 0.91877 time= 0.07867\n",
      "训练次数: 25 Epoch: 0087 log_lik= 0.44029772 train_kl= -0.01130 train_loss= 0.45160 train_acc= 0.53121 val_roc= 0.90958 val_ap= 0.91959 time= 0.07667\n",
      "训练次数: 25 Epoch: 0088 log_lik= 0.44013005 train_kl= -0.01132 train_loss= 0.45145 train_acc= 0.53214 val_roc= 0.90914 val_ap= 0.91946 time= 0.07966\n",
      "训练次数: 25 Epoch: 0089 log_lik= 0.43979093 train_kl= -0.01133 train_loss= 0.45112 train_acc= 0.53168 val_roc= 0.90815 val_ap= 0.91834 time= 0.07966\n",
      "训练次数: 25 Epoch: 0090 log_lik= 0.43918347 train_kl= -0.01135 train_loss= 0.45053 train_acc= 0.53200 val_roc= 0.90750 val_ap= 0.91795 time= 0.07468\n",
      "训练次数: 25 Epoch: 0091 log_lik= 0.43926892 train_kl= -0.01136 train_loss= 0.45063 train_acc= 0.53113 val_roc= 0.90808 val_ap= 0.91926 time= 0.07966\n",
      "训练次数: 25 Epoch: 0092 log_lik= 0.4387316 train_kl= -0.01138 train_loss= 0.45011 train_acc= 0.53262 val_roc= 0.90844 val_ap= 0.91946 time= 0.08265\n",
      "训练次数: 25 Epoch: 0093 log_lik= 0.43841484 train_kl= -0.01139 train_loss= 0.44981 train_acc= 0.53224 val_roc= 0.90822 val_ap= 0.91921 time= 0.07667\n",
      "训练次数: 25 Epoch: 0094 log_lik= 0.4381408 train_kl= -0.01141 train_loss= 0.44955 train_acc= 0.53216 val_roc= 0.90760 val_ap= 0.91835 time= 0.08913\n",
      "训练次数: 25 Epoch: 0095 log_lik= 0.43768555 train_kl= -0.01142 train_loss= 0.44911 train_acc= 0.53267 val_roc= 0.90807 val_ap= 0.91895 time= 0.07667\n",
      "训练次数: 25 Epoch: 0096 log_lik= 0.43704274 train_kl= -0.01144 train_loss= 0.44849 train_acc= 0.53279 val_roc= 0.90792 val_ap= 0.91912 time= 0.07867\n",
      "训练次数: 25 Epoch: 0097 log_lik= 0.43730304 train_kl= -0.01146 train_loss= 0.44876 train_acc= 0.53300 val_roc= 0.90689 val_ap= 0.91856 time= 0.07967\n",
      "训练次数: 25 Epoch: 0098 log_lik= 0.43685782 train_kl= -0.01147 train_loss= 0.44832 train_acc= 0.53218 val_roc= 0.90750 val_ap= 0.91900 time= 0.07867\n",
      "训练次数: 25 Epoch: 0099 log_lik= 0.43631133 train_kl= -0.01148 train_loss= 0.44779 train_acc= 0.53305 val_roc= 0.90752 val_ap= 0.91881 time= 0.07562\n",
      "训练次数: 25 Epoch: 0100 log_lik= 0.4358333 train_kl= -0.01151 train_loss= 0.44734 train_acc= 0.53289 val_roc= 0.90643 val_ap= 0.91773 time= 0.07468\n",
      "Optimization Finished!\n",
      "训练次数: 25 ROC score: 0.9215710278724945\n",
      "训练次数: 25 AP score: 0.9299146541315911\n",
      "训练次数: 26 Epoch: 0001 log_lik= 1.7974604 train_kl= -0.00005 train_loss= 1.79751 train_acc= 0.49640 val_roc= 0.68512 val_ap= 0.70362 time= 1.73866\n",
      "训练次数: 26 Epoch: 0002 log_lik= 1.5780108 train_kl= -0.00015 train_loss= 1.57816 train_acc= 0.48351 val_roc= 0.67929 val_ap= 0.70196 time= 0.08066\n",
      "训练次数: 26 Epoch: 0003 log_lik= 1.4516488 train_kl= -0.00036 train_loss= 1.45201 train_acc= 0.44646 val_roc= 0.68181 val_ap= 0.70646 time= 0.07667\n",
      "训练次数: 26 Epoch: 0004 log_lik= 1.3416823 train_kl= -0.00064 train_loss= 1.34232 train_acc= 0.40690 val_roc= 0.68740 val_ap= 0.71185 time= 0.07468\n",
      "训练次数: 26 Epoch: 0005 log_lik= 1.2358804 train_kl= -0.00091 train_loss= 1.23679 train_acc= 0.38877 val_roc= 0.69394 val_ap= 0.71841 time= 0.07668\n",
      "训练次数: 26 Epoch: 0006 log_lik= 1.0924932 train_kl= -0.00125 train_loss= 1.09375 train_acc= 0.37174 val_roc= 0.70179 val_ap= 0.72681 time= 0.07767\n",
      "训练次数: 26 Epoch: 0007 log_lik= 0.9796358 train_kl= -0.00168 train_loss= 0.98132 train_acc= 0.38320 val_roc= 0.71240 val_ap= 0.73724 time= 0.07667\n",
      "训练次数: 26 Epoch: 0008 log_lik= 0.890222 train_kl= -0.00226 train_loss= 0.89249 train_acc= 0.38421 val_roc= 0.72661 val_ap= 0.74996 time= 0.07568\n",
      "训练次数: 26 Epoch: 0009 log_lik= 0.794403 train_kl= -0.00301 train_loss= 0.79741 train_acc= 0.37763 val_roc= 0.74053 val_ap= 0.75935 time= 0.07468\n",
      "训练次数: 26 Epoch: 0010 log_lik= 0.7462812 train_kl= -0.00391 train_loss= 0.75019 train_acc= 0.35919 val_roc= 0.74794 val_ap= 0.75820 time= 0.07568\n",
      "训练次数: 26 Epoch: 0011 log_lik= 0.71094066 train_kl= -0.00491 train_loss= 0.71585 train_acc= 0.32788 val_roc= 0.75235 val_ap= 0.75785 time= 0.07468\n",
      "训练次数: 26 Epoch: 0012 log_lik= 0.69942415 train_kl= -0.00596 train_loss= 0.70538 train_acc= 0.25860 val_roc= 0.77066 val_ap= 0.77099 time= 0.08265\n",
      "训练次数: 26 Epoch: 0013 log_lik= 0.6909512 train_kl= -0.00695 train_loss= 0.69790 train_acc= 0.22601 val_roc= 0.79479 val_ap= 0.79227 time= 0.07568\n",
      "训练次数: 26 Epoch: 0014 log_lik= 0.6799258 train_kl= -0.00788 train_loss= 0.68781 train_acc= 0.22685 val_roc= 0.80951 val_ap= 0.80699 time= 0.08265\n",
      "训练次数: 26 Epoch: 0015 log_lik= 0.6720975 train_kl= -0.00876 train_loss= 0.68086 train_acc= 0.22124 val_roc= 0.81385 val_ap= 0.81189 time= 0.07767\n",
      "训练次数: 26 Epoch: 0016 log_lik= 0.65633273 train_kl= -0.00960 train_loss= 0.66593 train_acc= 0.22072 val_roc= 0.81360 val_ap= 0.80825 time= 0.07568\n",
      "训练次数: 26 Epoch: 0017 log_lik= 0.6396405 train_kl= -0.01038 train_loss= 0.65002 train_acc= 0.25215 val_roc= 0.81766 val_ap= 0.81427 time= 0.07568\n",
      "训练次数: 26 Epoch: 0018 log_lik= 0.616931 train_kl= -0.01106 train_loss= 0.62800 train_acc= 0.33145 val_roc= 0.82170 val_ap= 0.82031 time= 0.08663\n",
      "训练次数: 26 Epoch: 0019 log_lik= 0.5982254 train_kl= -0.01169 train_loss= 0.60992 train_acc= 0.40572 val_roc= 0.82137 val_ap= 0.81949 time= 0.08165\n",
      "训练次数: 26 Epoch: 0020 log_lik= 0.58202755 train_kl= -0.01228 train_loss= 0.59431 train_acc= 0.44890 val_roc= 0.82317 val_ap= 0.82186 time= 0.07568\n",
      "训练次数: 26 Epoch: 0021 log_lik= 0.57418835 train_kl= -0.01284 train_loss= 0.58702 train_acc= 0.45880 val_roc= 0.83010 val_ap= 0.82907 time= 0.07667\n",
      "训练次数: 26 Epoch: 0022 log_lik= 0.57080835 train_kl= -0.01329 train_loss= 0.58410 train_acc= 0.46459 val_roc= 0.83740 val_ap= 0.83673 time= 0.07767\n",
      "训练次数: 26 Epoch: 0023 log_lik= 0.56726396 train_kl= -0.01368 train_loss= 0.58094 train_acc= 0.47117 val_roc= 0.84441 val_ap= 0.84505 time= 0.07568\n",
      "训练次数: 26 Epoch: 0024 log_lik= 0.56163204 train_kl= -0.01399 train_loss= 0.57562 train_acc= 0.47565 val_roc= 0.85252 val_ap= 0.85409 time= 0.07468\n",
      "训练次数: 26 Epoch: 0025 log_lik= 0.5514694 train_kl= -0.01423 train_loss= 0.56570 train_acc= 0.48481 val_roc= 0.86082 val_ap= 0.86257 time= 0.07369\n",
      "训练次数: 26 Epoch: 0026 log_lik= 0.5403225 train_kl= -0.01441 train_loss= 0.55473 train_acc= 0.49007 val_roc= 0.87035 val_ap= 0.87148 time= 0.07767\n",
      "训练次数: 26 Epoch: 0027 log_lik= 0.52897274 train_kl= -0.01455 train_loss= 0.54353 train_acc= 0.49713 val_roc= 0.87720 val_ap= 0.87669 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 26 Epoch: 0028 log_lik= 0.52013445 train_kl= -0.01466 train_loss= 0.53480 train_acc= 0.50277 val_roc= 0.88252 val_ap= 0.88074 time= 0.07966\n",
      "训练次数: 26 Epoch: 0029 log_lik= 0.51377535 train_kl= -0.01474 train_loss= 0.52852 train_acc= 0.50560 val_roc= 0.88595 val_ap= 0.88390 time= 0.07468\n",
      "训练次数: 26 Epoch: 0030 log_lik= 0.512507 train_kl= -0.01480 train_loss= 0.52731 train_acc= 0.50456 val_roc= 0.88715 val_ap= 0.88506 time= 0.07468\n",
      "训练次数: 26 Epoch: 0031 log_lik= 0.51114047 train_kl= -0.01484 train_loss= 0.52598 train_acc= 0.50336 val_roc= 0.88652 val_ap= 0.88414 time= 0.07767\n",
      "训练次数: 26 Epoch: 0032 log_lik= 0.5092353 train_kl= -0.01484 train_loss= 0.52408 train_acc= 0.50677 val_roc= 0.88758 val_ap= 0.88522 time= 0.07568\n",
      "训练次数: 26 Epoch: 0033 log_lik= 0.50561184 train_kl= -0.01482 train_loss= 0.52044 train_acc= 0.51212 val_roc= 0.89031 val_ap= 0.88874 time= 0.07468\n",
      "训练次数: 26 Epoch: 0034 log_lik= 0.5010992 train_kl= -0.01479 train_loss= 0.51589 train_acc= 0.51731 val_roc= 0.89429 val_ap= 0.89372 time= 0.07767\n",
      "训练次数: 26 Epoch: 0035 log_lik= 0.4986595 train_kl= -0.01476 train_loss= 0.51342 train_acc= 0.51618 val_roc= 0.89848 val_ap= 0.89880 time= 0.07568\n",
      "训练次数: 26 Epoch: 0036 log_lik= 0.49642807 train_kl= -0.01472 train_loss= 0.51115 train_acc= 0.51302 val_roc= 0.90214 val_ap= 0.90322 time= 0.08066\n",
      "训练次数: 26 Epoch: 0037 log_lik= 0.4928428 train_kl= -0.01468 train_loss= 0.50753 train_acc= 0.51210 val_roc= 0.90627 val_ap= 0.90788 time= 0.07468\n",
      "训练次数: 26 Epoch: 0038 log_lik= 0.48795423 train_kl= -0.01463 train_loss= 0.50259 train_acc= 0.51447 val_roc= 0.91055 val_ap= 0.91245 time= 0.08165\n",
      "训练次数: 26 Epoch: 0039 log_lik= 0.4836079 train_kl= -0.01457 train_loss= 0.49818 train_acc= 0.51740 val_roc= 0.91330 val_ap= 0.91563 time= 0.07667\n",
      "训练次数: 26 Epoch: 0040 log_lik= 0.4801296 train_kl= -0.01449 train_loss= 0.49462 train_acc= 0.51960 val_roc= 0.91495 val_ap= 0.91696 time= 0.07568\n",
      "训练次数: 26 Epoch: 0041 log_lik= 0.47796595 train_kl= -0.01441 train_loss= 0.49238 train_acc= 0.51874 val_roc= 0.91645 val_ap= 0.91783 time= 0.07369\n",
      "训练次数: 26 Epoch: 0042 log_lik= 0.47706196 train_kl= -0.01433 train_loss= 0.49139 train_acc= 0.51749 val_roc= 0.91777 val_ap= 0.91872 time= 0.07468\n",
      "训练次数: 26 Epoch: 0043 log_lik= 0.4759199 train_kl= -0.01425 train_loss= 0.49017 train_acc= 0.51767 val_roc= 0.91863 val_ap= 0.91938 time= 0.07668\n",
      "训练次数: 26 Epoch: 0044 log_lik= 0.47390157 train_kl= -0.01416 train_loss= 0.48806 train_acc= 0.51985 val_roc= 0.91915 val_ap= 0.91954 time= 0.07667\n",
      "训练次数: 26 Epoch: 0045 log_lik= 0.47232 train_kl= -0.01407 train_loss= 0.48639 train_acc= 0.52176 val_roc= 0.92033 val_ap= 0.92077 time= 0.07468\n",
      "训练次数: 26 Epoch: 0046 log_lik= 0.47157258 train_kl= -0.01398 train_loss= 0.48555 train_acc= 0.52194 val_roc= 0.92141 val_ap= 0.92183 time= 0.07568\n",
      "训练次数: 26 Epoch: 0047 log_lik= 0.47075844 train_kl= -0.01388 train_loss= 0.48463 train_acc= 0.52205 val_roc= 0.92284 val_ap= 0.92319 time= 0.07568\n",
      "训练次数: 26 Epoch: 0048 log_lik= 0.4695734 train_kl= -0.01377 train_loss= 0.48334 train_acc= 0.52240 val_roc= 0.92390 val_ap= 0.92415 time= 0.08267\n",
      "训练次数: 26 Epoch: 0049 log_lik= 0.46816865 train_kl= -0.01364 train_loss= 0.48181 train_acc= 0.52392 val_roc= 0.92440 val_ap= 0.92484 time= 0.07568\n",
      "训练次数: 26 Epoch: 0050 log_lik= 0.46606064 train_kl= -0.01351 train_loss= 0.47957 train_acc= 0.52480 val_roc= 0.92510 val_ap= 0.92566 time= 0.07568\n",
      "训练次数: 26 Epoch: 0051 log_lik= 0.4649903 train_kl= -0.01338 train_loss= 0.47837 train_acc= 0.52564 val_roc= 0.92641 val_ap= 0.92661 time= 0.07468\n",
      "训练次数: 26 Epoch: 0052 log_lik= 0.46337384 train_kl= -0.01326 train_loss= 0.47663 train_acc= 0.52642 val_roc= 0.92757 val_ap= 0.92761 time= 0.07468\n",
      "训练次数: 26 Epoch: 0053 log_lik= 0.46268523 train_kl= -0.01314 train_loss= 0.47582 train_acc= 0.52639 val_roc= 0.92755 val_ap= 0.92775 time= 0.07568\n",
      "训练次数: 26 Epoch: 0054 log_lik= 0.46113762 train_kl= -0.01302 train_loss= 0.47416 train_acc= 0.52648 val_roc= 0.92700 val_ap= 0.92679 time= 0.07767\n",
      "训练次数: 26 Epoch: 0055 log_lik= 0.45968452 train_kl= -0.01291 train_loss= 0.47259 train_acc= 0.52611 val_roc= 0.92712 val_ap= 0.92615 time= 0.07966\n",
      "训练次数: 26 Epoch: 0056 log_lik= 0.45945084 train_kl= -0.01280 train_loss= 0.47225 train_acc= 0.52620 val_roc= 0.92712 val_ap= 0.92575 time= 0.07468\n",
      "训练次数: 26 Epoch: 0057 log_lik= 0.45952272 train_kl= -0.01270 train_loss= 0.47223 train_acc= 0.52715 val_roc= 0.92760 val_ap= 0.92623 time= 0.07966\n",
      "训练次数: 26 Epoch: 0058 log_lik= 0.4579909 train_kl= -0.01260 train_loss= 0.47059 train_acc= 0.52794 val_roc= 0.92690 val_ap= 0.92563 time= 0.07767\n",
      "训练次数: 26 Epoch: 0059 log_lik= 0.45681393 train_kl= -0.01250 train_loss= 0.46932 train_acc= 0.52680 val_roc= 0.92635 val_ap= 0.92497 time= 0.07568\n",
      "训练次数: 26 Epoch: 0060 log_lik= 0.45618814 train_kl= -0.01242 train_loss= 0.46861 train_acc= 0.52726 val_roc= 0.92630 val_ap= 0.92484 time= 0.08265\n",
      "训练次数: 26 Epoch: 0061 log_lik= 0.45529804 train_kl= -0.01234 train_loss= 0.46763 train_acc= 0.52777 val_roc= 0.92615 val_ap= 0.92450 time= 0.08565\n",
      "训练次数: 26 Epoch: 0062 log_lik= 0.4547772 train_kl= -0.01226 train_loss= 0.46704 train_acc= 0.52827 val_roc= 0.92500 val_ap= 0.92301 time= 0.07867\n",
      "训练次数: 26 Epoch: 0063 log_lik= 0.45416644 train_kl= -0.01218 train_loss= 0.46635 train_acc= 0.52817 val_roc= 0.92476 val_ap= 0.92302 time= 0.07468\n",
      "训练次数: 26 Epoch: 0064 log_lik= 0.45379546 train_kl= -0.01211 train_loss= 0.46591 train_acc= 0.52792 val_roc= 0.92562 val_ap= 0.92413 time= 0.07867\n",
      "训练次数: 26 Epoch: 0065 log_lik= 0.4537411 train_kl= -0.01204 train_loss= 0.46578 train_acc= 0.52772 val_roc= 0.92552 val_ap= 0.92414 time= 0.07767\n",
      "训练次数: 26 Epoch: 0066 log_lik= 0.4533121 train_kl= -0.01196 train_loss= 0.46527 train_acc= 0.52859 val_roc= 0.92488 val_ap= 0.92334 time= 0.07568\n",
      "训练次数: 26 Epoch: 0067 log_lik= 0.45220572 train_kl= -0.01189 train_loss= 0.46410 train_acc= 0.52872 val_roc= 0.92495 val_ap= 0.92345 time= 0.07468\n",
      "训练次数: 26 Epoch: 0068 log_lik= 0.45278352 train_kl= -0.01183 train_loss= 0.46461 train_acc= 0.52799 val_roc= 0.92573 val_ap= 0.92484 time= 0.07568\n",
      "训练次数: 26 Epoch: 0069 log_lik= 0.45159283 train_kl= -0.01176 train_loss= 0.46335 train_acc= 0.52841 val_roc= 0.92615 val_ap= 0.92564 time= 0.07568\n",
      "训练次数: 26 Epoch: 0070 log_lik= 0.45146674 train_kl= -0.01170 train_loss= 0.46317 train_acc= 0.52826 val_roc= 0.92596 val_ap= 0.92538 time= 0.08464\n",
      "训练次数: 26 Epoch: 0071 log_lik= 0.45070887 train_kl= -0.01164 train_loss= 0.46235 train_acc= 0.52881 val_roc= 0.92611 val_ap= 0.92534 time= 0.07369\n",
      "训练次数: 26 Epoch: 0072 log_lik= 0.45034984 train_kl= -0.01159 train_loss= 0.46194 train_acc= 0.52890 val_roc= 0.92618 val_ap= 0.92524 time= 0.07369\n",
      "训练次数: 26 Epoch: 0073 log_lik= 0.4501401 train_kl= -0.01154 train_loss= 0.46168 train_acc= 0.52825 val_roc= 0.92618 val_ap= 0.92554 time= 0.08165\n",
      "训练次数: 26 Epoch: 0074 log_lik= 0.44956958 train_kl= -0.01150 train_loss= 0.46106 train_acc= 0.52791 val_roc= 0.92661 val_ap= 0.92660 time= 0.08364\n",
      "训练次数: 26 Epoch: 0075 log_lik= 0.4498306 train_kl= -0.01145 train_loss= 0.46128 train_acc= 0.52784 val_roc= 0.92672 val_ap= 0.92711 time= 0.07468\n",
      "训练次数: 26 Epoch: 0076 log_lik= 0.44912055 train_kl= -0.01141 train_loss= 0.46053 train_acc= 0.52861 val_roc= 0.92664 val_ap= 0.92637 time= 0.07568\n",
      "训练次数: 26 Epoch: 0077 log_lik= 0.44860235 train_kl= -0.01138 train_loss= 0.45998 train_acc= 0.52751 val_roc= 0.92690 val_ap= 0.92624 time= 0.07368\n",
      "训练次数: 26 Epoch: 0078 log_lik= 0.44832745 train_kl= -0.01136 train_loss= 0.45968 train_acc= 0.52819 val_roc= 0.92770 val_ap= 0.92751 time= 0.07568\n",
      "训练次数: 26 Epoch: 0079 log_lik= 0.44821298 train_kl= -0.01134 train_loss= 0.45955 train_acc= 0.52868 val_roc= 0.92805 val_ap= 0.92815 time= 0.07570\n",
      "训练次数: 26 Epoch: 0080 log_lik= 0.44788617 train_kl= -0.01132 train_loss= 0.45920 train_acc= 0.52855 val_roc= 0.92761 val_ap= 0.92779 time= 0.07568\n",
      "训练次数: 26 Epoch: 0081 log_lik= 0.44727635 train_kl= -0.01131 train_loss= 0.45859 train_acc= 0.52866 val_roc= 0.92768 val_ap= 0.92801 time= 0.07568\n",
      "训练次数: 26 Epoch: 0082 log_lik= 0.44658947 train_kl= -0.01131 train_loss= 0.45790 train_acc= 0.52884 val_roc= 0.92845 val_ap= 0.92863 time= 0.07767\n",
      "训练次数: 26 Epoch: 0083 log_lik= 0.44659773 train_kl= -0.01131 train_loss= 0.45790 train_acc= 0.52897 val_roc= 0.92868 val_ap= 0.92892 time= 0.08165\n",
      "训练次数: 26 Epoch: 0084 log_lik= 0.44609195 train_kl= -0.01129 train_loss= 0.45739 train_acc= 0.52956 val_roc= 0.92832 val_ap= 0.92879 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 26 Epoch: 0085 log_lik= 0.4458646 train_kl= -0.01129 train_loss= 0.45716 train_acc= 0.52931 val_roc= 0.92780 val_ap= 0.92829 time= 0.07568\n",
      "训练次数: 26 Epoch: 0086 log_lik= 0.44510227 train_kl= -0.01130 train_loss= 0.45640 train_acc= 0.53036 val_roc= 0.92852 val_ap= 0.92873 time= 0.08265\n",
      "训练次数: 26 Epoch: 0087 log_lik= 0.44471318 train_kl= -0.01131 train_loss= 0.45603 train_acc= 0.53054 val_roc= 0.92835 val_ap= 0.92866 time= 0.07369\n",
      "训练次数: 26 Epoch: 0088 log_lik= 0.44443795 train_kl= -0.01132 train_loss= 0.45575 train_acc= 0.52982 val_roc= 0.92851 val_ap= 0.92894 time= 0.07468\n",
      "训练次数: 26 Epoch: 0089 log_lik= 0.44386968 train_kl= -0.01132 train_loss= 0.45519 train_acc= 0.53072 val_roc= 0.92913 val_ap= 0.93005 time= 0.07767\n",
      "训练次数: 26 Epoch: 0090 log_lik= 0.44407284 train_kl= -0.01133 train_loss= 0.45540 train_acc= 0.52988 val_roc= 0.92945 val_ap= 0.93054 time= 0.07468\n",
      "训练次数: 26 Epoch: 0091 log_lik= 0.4435841 train_kl= -0.01134 train_loss= 0.45493 train_acc= 0.52962 val_roc= 0.92914 val_ap= 0.93018 time= 0.08165\n",
      "训练次数: 26 Epoch: 0092 log_lik= 0.44321603 train_kl= -0.01136 train_loss= 0.45458 train_acc= 0.53048 val_roc= 0.92860 val_ap= 0.92929 time= 0.07369\n",
      "训练次数: 26 Epoch: 0093 log_lik= 0.442571 train_kl= -0.01137 train_loss= 0.45394 train_acc= 0.53072 val_roc= 0.92867 val_ap= 0.92979 time= 0.07568\n",
      "训练次数: 26 Epoch: 0094 log_lik= 0.44241977 train_kl= -0.01138 train_loss= 0.45380 train_acc= 0.53043 val_roc= 0.92877 val_ap= 0.93058 time= 0.07468\n",
      "训练次数: 26 Epoch: 0095 log_lik= 0.44173777 train_kl= -0.01139 train_loss= 0.45313 train_acc= 0.53158 val_roc= 0.92907 val_ap= 0.93109 time= 0.07668\n",
      "训练次数: 26 Epoch: 0096 log_lik= 0.4421864 train_kl= -0.01141 train_loss= 0.45359 train_acc= 0.53143 val_roc= 0.92844 val_ap= 0.92967 time= 0.08066\n",
      "训练次数: 26 Epoch: 0097 log_lik= 0.44131002 train_kl= -0.01143 train_loss= 0.45274 train_acc= 0.53178 val_roc= 0.92849 val_ap= 0.92964 time= 0.07667\n",
      "训练次数: 26 Epoch: 0098 log_lik= 0.4415049 train_kl= -0.01144 train_loss= 0.45295 train_acc= 0.53128 val_roc= 0.92871 val_ap= 0.93019 time= 0.07667\n",
      "训练次数: 26 Epoch: 0099 log_lik= 0.44054005 train_kl= -0.01146 train_loss= 0.45200 train_acc= 0.53239 val_roc= 0.92951 val_ap= 0.93175 time= 0.07369\n",
      "训练次数: 26 Epoch: 0100 log_lik= 0.44038403 train_kl= -0.01147 train_loss= 0.45186 train_acc= 0.53267 val_roc= 0.92997 val_ap= 0.93205 time= 0.07468\n",
      "Optimization Finished!\n",
      "训练次数: 26 ROC score: 0.9105422912263393\n",
      "训练次数: 26 AP score: 0.9176214334933721\n",
      "训练次数: 27 Epoch: 0001 log_lik= 1.8017823 train_kl= -0.00005 train_loss= 1.80183 train_acc= 0.49743 val_roc= 0.72235 val_ap= 0.74040 time= 1.74117\n",
      "训练次数: 27 Epoch: 0002 log_lik= 1.5283158 train_kl= -0.00018 train_loss= 1.52849 train_acc= 0.48210 val_roc= 0.71782 val_ap= 0.74158 time= 0.08862\n",
      "训练次数: 27 Epoch: 0003 log_lik= 1.4077412 train_kl= -0.00040 train_loss= 1.40814 train_acc= 0.47121 val_roc= 0.72366 val_ap= 0.74698 time= 0.07568\n",
      "训练次数: 27 Epoch: 0004 log_lik= 1.2726264 train_kl= -0.00073 train_loss= 1.27336 train_acc= 0.44647 val_roc= 0.72814 val_ap= 0.75182 time= 0.07568\n",
      "训练次数: 27 Epoch: 0005 log_lik= 1.1749374 train_kl= -0.00115 train_loss= 1.17609 train_acc= 0.42622 val_roc= 0.73333 val_ap= 0.75705 time= 0.08962\n",
      "训练次数: 27 Epoch: 0006 log_lik= 1.04223 train_kl= -0.00168 train_loss= 1.04391 train_acc= 0.41036 val_roc= 0.74623 val_ap= 0.76961 time= 0.07568\n",
      "训练次数: 27 Epoch: 0007 log_lik= 0.9206195 train_kl= -0.00231 train_loss= 0.92293 train_acc= 0.40618 val_roc= 0.78414 val_ap= 0.79778 time= 0.07468\n",
      "训练次数: 27 Epoch: 0008 log_lik= 0.81291395 train_kl= -0.00307 train_loss= 0.81598 train_acc= 0.42802 val_roc= 0.80484 val_ap= 0.80758 time= 0.07469\n",
      "训练次数: 27 Epoch: 0009 log_lik= 0.7601804 train_kl= -0.00398 train_loss= 0.76416 train_acc= 0.40793 val_roc= 0.77332 val_ap= 0.77801 time= 0.07568\n",
      "训练次数: 27 Epoch: 0010 log_lik= 0.7283036 train_kl= -0.00502 train_loss= 0.73332 train_acc= 0.27449 val_roc= 0.77773 val_ap= 0.78083 time= 0.07767\n",
      "训练次数: 27 Epoch: 0011 log_lik= 0.7157337 train_kl= -0.00602 train_loss= 0.72175 train_acc= 0.22024 val_roc= 0.81078 val_ap= 0.81193 time= 0.07568\n",
      "训练次数: 27 Epoch: 0012 log_lik= 0.6906848 train_kl= -0.00693 train_loss= 0.69762 train_acc= 0.24925 val_roc= 0.85363 val_ap= 0.84994 time= 0.07568\n",
      "训练次数: 27 Epoch: 0013 log_lik= 0.67825824 train_kl= -0.00779 train_loss= 0.68605 train_acc= 0.31265 val_roc= 0.86000 val_ap= 0.85282 time= 0.08165\n",
      "训练次数: 27 Epoch: 0014 log_lik= 0.6557682 train_kl= -0.00863 train_loss= 0.66440 train_acc= 0.33461 val_roc= 0.84539 val_ap= 0.84350 time= 0.07568\n",
      "训练次数: 27 Epoch: 0015 log_lik= 0.64074576 train_kl= -0.00943 train_loss= 0.65018 train_acc= 0.28011 val_roc= 0.84810 val_ap= 0.84657 time= 0.07568\n",
      "训练次数: 27 Epoch: 0016 log_lik= 0.6265944 train_kl= -0.01017 train_loss= 0.63677 train_acc= 0.27503 val_roc= 0.85856 val_ap= 0.85001 time= 0.07966\n",
      "训练次数: 27 Epoch: 0017 log_lik= 0.60636145 train_kl= -0.01082 train_loss= 0.61719 train_acc= 0.33894 val_roc= 0.86631 val_ap= 0.85612 time= 0.08265\n",
      "训练次数: 27 Epoch: 0018 log_lik= 0.5858753 train_kl= -0.01141 train_loss= 0.59729 train_acc= 0.40351 val_roc= 0.86503 val_ap= 0.85719 time= 0.07767\n",
      "训练次数: 27 Epoch: 0019 log_lik= 0.5740096 train_kl= -0.01196 train_loss= 0.58597 train_acc= 0.44340 val_roc= 0.86547 val_ap= 0.85853 time= 0.07468\n",
      "训练次数: 27 Epoch: 0020 log_lik= 0.56383264 train_kl= -0.01245 train_loss= 0.57628 train_acc= 0.46551 val_roc= 0.87123 val_ap= 0.86375 time= 0.07568\n",
      "训练次数: 27 Epoch: 0021 log_lik= 0.5541587 train_kl= -0.01288 train_loss= 0.56704 train_acc= 0.45779 val_roc= 0.87619 val_ap= 0.86694 time= 0.07568\n",
      "训练次数: 27 Epoch: 0022 log_lik= 0.5535706 train_kl= -0.01327 train_loss= 0.56684 train_acc= 0.45034 val_roc= 0.88125 val_ap= 0.87488 time= 0.08066\n",
      "训练次数: 27 Epoch: 0023 log_lik= 0.54670215 train_kl= -0.01359 train_loss= 0.56029 train_acc= 0.46496 val_roc= 0.88609 val_ap= 0.88389 time= 0.07524\n",
      "训练次数: 27 Epoch: 0024 log_lik= 0.5371898 train_kl= -0.01386 train_loss= 0.55105 train_acc= 0.48226 val_roc= 0.89362 val_ap= 0.89357 time= 0.07468\n",
      "训练次数: 27 Epoch: 0025 log_lik= 0.5244493 train_kl= -0.01408 train_loss= 0.53853 train_acc= 0.49194 val_roc= 0.90367 val_ap= 0.90475 time= 0.07867\n",
      "训练次数: 27 Epoch: 0026 log_lik= 0.5166882 train_kl= -0.01426 train_loss= 0.53094 train_acc= 0.48360 val_roc= 0.90951 val_ap= 0.91195 time= 0.07966\n",
      "训练次数: 27 Epoch: 0027 log_lik= 0.5131832 train_kl= -0.01439 train_loss= 0.52757 train_acc= 0.47785 val_roc= 0.91268 val_ap= 0.91642 time= 0.07568\n",
      "训练次数: 27 Epoch: 0028 log_lik= 0.5055443 train_kl= -0.01447 train_loss= 0.52001 train_acc= 0.48778 val_roc= 0.91282 val_ap= 0.91619 time= 0.07668\n",
      "训练次数: 27 Epoch: 0029 log_lik= 0.5013304 train_kl= -0.01452 train_loss= 0.51585 train_acc= 0.49990 val_roc= 0.91542 val_ap= 0.91792 time= 0.07667\n",
      "训练次数: 27 Epoch: 0030 log_lik= 0.49777582 train_kl= -0.01455 train_loss= 0.51232 train_acc= 0.50349 val_roc= 0.91888 val_ap= 0.92031 time= 0.07369\n",
      "训练次数: 27 Epoch: 0031 log_lik= 0.4963255 train_kl= -0.01457 train_loss= 0.51090 train_acc= 0.50042 val_roc= 0.92059 val_ap= 0.92032 time= 0.07867\n",
      "训练次数: 27 Epoch: 0032 log_lik= 0.4956811 train_kl= -0.01460 train_loss= 0.51028 train_acc= 0.49833 val_roc= 0.92102 val_ap= 0.92009 time= 0.08265\n",
      "训练次数: 27 Epoch: 0033 log_lik= 0.49056727 train_kl= -0.01463 train_loss= 0.50519 train_acc= 0.50821 val_roc= 0.92002 val_ap= 0.91846 time= 0.08165\n",
      "训练次数: 27 Epoch: 0034 log_lik= 0.48786595 train_kl= -0.01464 train_loss= 0.50251 train_acc= 0.51612 val_roc= 0.92112 val_ap= 0.91897 time= 0.07468\n",
      "训练次数: 27 Epoch: 0035 log_lik= 0.48399752 train_kl= -0.01465 train_loss= 0.49865 train_acc= 0.51965 val_roc= 0.92456 val_ap= 0.92196 time= 0.07468\n",
      "训练次数: 27 Epoch: 0036 log_lik= 0.4809607 train_kl= -0.01465 train_loss= 0.49561 train_acc= 0.51672 val_roc= 0.92582 val_ap= 0.92259 time= 0.07667\n",
      "训练次数: 27 Epoch: 0037 log_lik= 0.47845292 train_kl= -0.01465 train_loss= 0.49310 train_acc= 0.51699 val_roc= 0.92510 val_ap= 0.92190 time= 0.07867\n",
      "训练次数: 27 Epoch: 0038 log_lik= 0.4748288 train_kl= -0.01464 train_loss= 0.48947 train_acc= 0.52029 val_roc= 0.92465 val_ap= 0.92138 time= 0.07568\n",
      "训练次数: 27 Epoch: 0039 log_lik= 0.47172067 train_kl= -0.01461 train_loss= 0.48633 train_acc= 0.52352 val_roc= 0.92540 val_ap= 0.92292 time= 0.07470\n",
      "训练次数: 27 Epoch: 0040 log_lik= 0.46936524 train_kl= -0.01456 train_loss= 0.48393 train_acc= 0.52513 val_roc= 0.92656 val_ap= 0.92399 time= 0.07468\n",
      "训练次数: 27 Epoch: 0041 log_lik= 0.46767595 train_kl= -0.01451 train_loss= 0.48219 train_acc= 0.52527 val_roc= 0.92689 val_ap= 0.92429 time= 0.07667\n",
      "训练次数: 27 Epoch: 0042 log_lik= 0.46670005 train_kl= -0.01447 train_loss= 0.48117 train_acc= 0.52434 val_roc= 0.92618 val_ap= 0.92409 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 27 Epoch: 0043 log_lik= 0.46488702 train_kl= -0.01443 train_loss= 0.47932 train_acc= 0.52532 val_roc= 0.92524 val_ap= 0.92360 time= 0.08066\n",
      "训练次数: 27 Epoch: 0044 log_lik= 0.46346557 train_kl= -0.01438 train_loss= 0.47785 train_acc= 0.52894 val_roc= 0.92479 val_ap= 0.92358 time= 0.07867\n",
      "训练次数: 27 Epoch: 0045 log_lik= 0.46250874 train_kl= -0.01432 train_loss= 0.47683 train_acc= 0.52766 val_roc= 0.92567 val_ap= 0.92475 time= 0.07468\n",
      "训练次数: 27 Epoch: 0046 log_lik= 0.46213558 train_kl= -0.01424 train_loss= 0.47637 train_acc= 0.52660 val_roc= 0.92689 val_ap= 0.92634 time= 0.07674\n",
      "训练次数: 27 Epoch: 0047 log_lik= 0.4617702 train_kl= -0.01415 train_loss= 0.47592 train_acc= 0.52478 val_roc= 0.92755 val_ap= 0.92704 time= 0.07561\n",
      "训练次数: 27 Epoch: 0048 log_lik= 0.46046945 train_kl= -0.01405 train_loss= 0.47452 train_acc= 0.52594 val_roc= 0.92722 val_ap= 0.92730 time= 0.07468\n",
      "训练次数: 27 Epoch: 0049 log_lik= 0.45843998 train_kl= -0.01395 train_loss= 0.47239 train_acc= 0.52682 val_roc= 0.92755 val_ap= 0.92801 time= 0.07767\n",
      "训练次数: 27 Epoch: 0050 log_lik= 0.458459 train_kl= -0.01384 train_loss= 0.47230 train_acc= 0.52603 val_roc= 0.92819 val_ap= 0.92905 time= 0.07667\n",
      "训练次数: 27 Epoch: 0051 log_lik= 0.4577356 train_kl= -0.01372 train_loss= 0.47146 train_acc= 0.52593 val_roc= 0.92982 val_ap= 0.93133 time= 0.07427\n",
      "训练次数: 27 Epoch: 0052 log_lik= 0.45607328 train_kl= -0.01362 train_loss= 0.46969 train_acc= 0.52708 val_roc= 0.93069 val_ap= 0.93279 time= 0.07568\n",
      "训练次数: 27 Epoch: 0053 log_lik= 0.4557274 train_kl= -0.01352 train_loss= 0.46925 train_acc= 0.52700 val_roc= 0.93100 val_ap= 0.93359 time= 0.07568\n",
      "训练次数: 27 Epoch: 0054 log_lik= 0.4547175 train_kl= -0.01342 train_loss= 0.46814 train_acc= 0.52595 val_roc= 0.93104 val_ap= 0.93423 time= 0.07369\n",
      "训练次数: 27 Epoch: 0055 log_lik= 0.45371938 train_kl= -0.01331 train_loss= 0.46703 train_acc= 0.52563 val_roc= 0.93167 val_ap= 0.93512 time= 0.07767\n",
      "训练次数: 27 Epoch: 0056 log_lik= 0.4536111 train_kl= -0.01320 train_loss= 0.46681 train_acc= 0.52666 val_roc= 0.93228 val_ap= 0.93569 time= 0.07468\n",
      "训练次数: 27 Epoch: 0057 log_lik= 0.4528889 train_kl= -0.01309 train_loss= 0.46598 train_acc= 0.52592 val_roc= 0.93298 val_ap= 0.93674 time= 0.08165\n",
      "训练次数: 27 Epoch: 0058 log_lik= 0.4528416 train_kl= -0.01299 train_loss= 0.46583 train_acc= 0.52714 val_roc= 0.93363 val_ap= 0.93786 time= 0.07767\n",
      "训练次数: 27 Epoch: 0059 log_lik= 0.45154467 train_kl= -0.01289 train_loss= 0.46444 train_acc= 0.52763 val_roc= 0.93351 val_ap= 0.93818 time= 0.07468\n",
      "训练次数: 27 Epoch: 0060 log_lik= 0.4523119 train_kl= -0.01280 train_loss= 0.46511 train_acc= 0.52634 val_roc= 0.93360 val_ap= 0.93849 time= 0.07369\n",
      "训练次数: 27 Epoch: 0061 log_lik= 0.45138413 train_kl= -0.01272 train_loss= 0.46410 train_acc= 0.52782 val_roc= 0.93415 val_ap= 0.93886 time= 0.07767\n",
      "训练次数: 27 Epoch: 0062 log_lik= 0.45021847 train_kl= -0.01264 train_loss= 0.46285 train_acc= 0.52878 val_roc= 0.93487 val_ap= 0.93951 time= 0.07667\n",
      "训练次数: 27 Epoch: 0063 log_lik= 0.45082942 train_kl= -0.01256 train_loss= 0.46339 train_acc= 0.52923 val_roc= 0.93523 val_ap= 0.94025 time= 0.07568\n",
      "训练次数: 27 Epoch: 0064 log_lik= 0.44997525 train_kl= -0.01249 train_loss= 0.46246 train_acc= 0.52880 val_roc= 0.93497 val_ap= 0.94023 time= 0.08265\n",
      "训练次数: 27 Epoch: 0065 log_lik= 0.4495377 train_kl= -0.01242 train_loss= 0.46196 train_acc= 0.52996 val_roc= 0.93465 val_ap= 0.93984 time= 0.07468\n",
      "训练次数: 27 Epoch: 0066 log_lik= 0.44892433 train_kl= -0.01235 train_loss= 0.46127 train_acc= 0.52991 val_roc= 0.93503 val_ap= 0.94002 time= 0.07767\n",
      "训练次数: 27 Epoch: 0067 log_lik= 0.4489727 train_kl= -0.01227 train_loss= 0.46125 train_acc= 0.52896 val_roc= 0.93516 val_ap= 0.94002 time= 0.07469\n",
      "训练次数: 27 Epoch: 0068 log_lik= 0.44877797 train_kl= -0.01221 train_loss= 0.46099 train_acc= 0.52900 val_roc= 0.93494 val_ap= 0.93993 time= 0.07966\n",
      "训练次数: 27 Epoch: 0069 log_lik= 0.44823417 train_kl= -0.01216 train_loss= 0.46039 train_acc= 0.53123 val_roc= 0.93490 val_ap= 0.94014 time= 0.07468\n",
      "训练次数: 27 Epoch: 0070 log_lik= 0.447684 train_kl= -0.01210 train_loss= 0.45979 train_acc= 0.53048 val_roc= 0.93459 val_ap= 0.94008 time= 0.07966\n",
      "训练次数: 27 Epoch: 0071 log_lik= 0.44729888 train_kl= -0.01205 train_loss= 0.45934 train_acc= 0.53097 val_roc= 0.93416 val_ap= 0.93978 time= 0.08267\n",
      "训练次数: 27 Epoch: 0072 log_lik= 0.4472444 train_kl= -0.01199 train_loss= 0.45924 train_acc= 0.53132 val_roc= 0.93390 val_ap= 0.93945 time= 0.07468\n",
      "训练次数: 27 Epoch: 0073 log_lik= 0.44718853 train_kl= -0.01195 train_loss= 0.45914 train_acc= 0.53113 val_roc= 0.93420 val_ap= 0.93943 time= 0.07767\n",
      "训练次数: 27 Epoch: 0074 log_lik= 0.44671193 train_kl= -0.01191 train_loss= 0.45863 train_acc= 0.53136 val_roc= 0.93387 val_ap= 0.93930 time= 0.07369\n",
      "训练次数: 27 Epoch: 0075 log_lik= 0.44647905 train_kl= -0.01187 train_loss= 0.45835 train_acc= 0.53037 val_roc= 0.93342 val_ap= 0.93906 time= 0.07868\n",
      "训练次数: 27 Epoch: 0076 log_lik= 0.44676554 train_kl= -0.01182 train_loss= 0.45859 train_acc= 0.53071 val_roc= 0.93319 val_ap= 0.93927 time= 0.07767\n",
      "训练次数: 27 Epoch: 0077 log_lik= 0.44595492 train_kl= -0.01179 train_loss= 0.45775 train_acc= 0.53017 val_roc= 0.93298 val_ap= 0.93901 time= 0.07568\n",
      "训练次数: 27 Epoch: 0078 log_lik= 0.44589397 train_kl= -0.01177 train_loss= 0.45766 train_acc= 0.53129 val_roc= 0.93341 val_ap= 0.93934 time= 0.07568\n",
      "训练次数: 27 Epoch: 0079 log_lik= 0.4452839 train_kl= -0.01174 train_loss= 0.45702 train_acc= 0.53093 val_roc= 0.93311 val_ap= 0.93949 time= 0.07667\n",
      "训练次数: 27 Epoch: 0080 log_lik= 0.44492757 train_kl= -0.01171 train_loss= 0.45664 train_acc= 0.53031 val_roc= 0.93277 val_ap= 0.93937 time= 0.07468\n",
      "训练次数: 27 Epoch: 0081 log_lik= 0.4451468 train_kl= -0.01170 train_loss= 0.45684 train_acc= 0.53020 val_roc= 0.93225 val_ap= 0.93882 time= 0.08165\n",
      "训练次数: 27 Epoch: 0082 log_lik= 0.444963 train_kl= -0.01168 train_loss= 0.45665 train_acc= 0.53083 val_roc= 0.93233 val_ap= 0.93911 time= 0.07468\n",
      "训练次数: 27 Epoch: 0083 log_lik= 0.4446284 train_kl= -0.01166 train_loss= 0.45629 train_acc= 0.53217 val_roc= 0.93280 val_ap= 0.94010 time= 0.07767\n",
      "训练次数: 27 Epoch: 0084 log_lik= 0.44479546 train_kl= -0.01164 train_loss= 0.45643 train_acc= 0.53086 val_roc= 0.93230 val_ap= 0.93967 time= 0.07568\n",
      "训练次数: 27 Epoch: 0085 log_lik= 0.44471043 train_kl= -0.01164 train_loss= 0.45635 train_acc= 0.53034 val_roc= 0.93175 val_ap= 0.93896 time= 0.07589\n",
      "训练次数: 27 Epoch: 0086 log_lik= 0.4439071 train_kl= -0.01164 train_loss= 0.45555 train_acc= 0.53161 val_roc= 0.93195 val_ap= 0.93935 time= 0.07468\n",
      "训练次数: 27 Epoch: 0087 log_lik= 0.44360283 train_kl= -0.01163 train_loss= 0.45523 train_acc= 0.53176 val_roc= 0.93211 val_ap= 0.93997 time= 0.07568\n",
      "训练次数: 27 Epoch: 0088 log_lik= 0.44326162 train_kl= -0.01162 train_loss= 0.45488 train_acc= 0.53164 val_roc= 0.93185 val_ap= 0.93991 time= 0.08265\n",
      "训练次数: 27 Epoch: 0089 log_lik= 0.44307956 train_kl= -0.01162 train_loss= 0.45470 train_acc= 0.53164 val_roc= 0.93120 val_ap= 0.93914 time= 0.08265\n",
      "训练次数: 27 Epoch: 0090 log_lik= 0.442863 train_kl= -0.01163 train_loss= 0.45449 train_acc= 0.53096 val_roc= 0.93076 val_ap= 0.93882 time= 0.07966\n",
      "训练次数: 27 Epoch: 0091 log_lik= 0.44235438 train_kl= -0.01163 train_loss= 0.45399 train_acc= 0.53276 val_roc= 0.93108 val_ap= 0.93931 time= 0.07867\n",
      "训练次数: 27 Epoch: 0092 log_lik= 0.44230607 train_kl= -0.01162 train_loss= 0.45393 train_acc= 0.53355 val_roc= 0.93165 val_ap= 0.94017 time= 0.07668\n",
      "训练次数: 27 Epoch: 0093 log_lik= 0.4428241 train_kl= -0.01162 train_loss= 0.45444 train_acc= 0.53149 val_roc= 0.93108 val_ap= 0.93975 time= 0.07568\n",
      "训练次数: 27 Epoch: 0094 log_lik= 0.4418483 train_kl= -0.01163 train_loss= 0.45348 train_acc= 0.53202 val_roc= 0.92997 val_ap= 0.93857 time= 0.07476\n",
      "训练次数: 27 Epoch: 0095 log_lik= 0.44170153 train_kl= -0.01164 train_loss= 0.45334 train_acc= 0.53339 val_roc= 0.93056 val_ap= 0.93927 time= 0.07568\n",
      "训练次数: 27 Epoch: 0096 log_lik= 0.44190568 train_kl= -0.01164 train_loss= 0.45355 train_acc= 0.53370 val_roc= 0.93126 val_ap= 0.93996 time= 0.07468\n",
      "训练次数: 27 Epoch: 0097 log_lik= 0.44128245 train_kl= -0.01164 train_loss= 0.45292 train_acc= 0.53289 val_roc= 0.93076 val_ap= 0.93981 time= 0.07867\n",
      "训练次数: 27 Epoch: 0098 log_lik= 0.44094458 train_kl= -0.01164 train_loss= 0.45259 train_acc= 0.53270 val_roc= 0.92995 val_ap= 0.93929 time= 0.07667\n",
      "训练次数: 27 Epoch: 0099 log_lik= 0.44083175 train_kl= -0.01165 train_loss= 0.45249 train_acc= 0.53305 val_roc= 0.92967 val_ap= 0.93898 time= 0.07468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 27 Epoch: 0100 log_lik= 0.44099212 train_kl= -0.01166 train_loss= 0.45265 train_acc= 0.53313 val_roc= 0.93049 val_ap= 0.93959 time= 0.07570\n",
      "Optimization Finished!\n",
      "训练次数: 27 ROC score: 0.9214270025816533\n",
      "训练次数: 27 AP score: 0.9313626539558025\n",
      "训练次数: 28 Epoch: 0001 log_lik= 1.7779363 train_kl= -0.00003 train_loss= 1.77797 train_acc= 0.49819 val_roc= 0.67853 val_ap= 0.68766 time= 1.74516\n",
      "训练次数: 28 Epoch: 0002 log_lik= 1.4973966 train_kl= -0.00026 train_loss= 1.49765 train_acc= 0.47873 val_roc= 0.68414 val_ap= 0.69609 time= 0.09062\n",
      "训练次数: 28 Epoch: 0003 log_lik= 1.3809887 train_kl= -0.00067 train_loss= 1.38166 train_acc= 0.44045 val_roc= 0.68816 val_ap= 0.70224 time= 0.07667\n",
      "训练次数: 28 Epoch: 0004 log_lik= 1.2255329 train_kl= -0.00094 train_loss= 1.22647 train_acc= 0.42779 val_roc= 0.68904 val_ap= 0.70601 time= 0.07867\n",
      "训练次数: 28 Epoch: 0005 log_lik= 1.1019332 train_kl= -0.00131 train_loss= 1.10325 train_acc= 0.39663 val_roc= 0.69524 val_ap= 0.71183 time= 0.07568\n",
      "训练次数: 28 Epoch: 0006 log_lik= 0.9744688 train_kl= -0.00182 train_loss= 0.97629 train_acc= 0.38275 val_roc= 0.71642 val_ap= 0.72913 time= 0.07668\n",
      "训练次数: 28 Epoch: 0007 log_lik= 0.8708458 train_kl= -0.00247 train_loss= 0.87331 train_acc= 0.38848 val_roc= 0.74695 val_ap= 0.75332 time= 0.07767\n",
      "训练次数: 28 Epoch: 0008 log_lik= 0.7908285 train_kl= -0.00328 train_loss= 0.79411 train_acc= 0.39579 val_roc= 0.75767 val_ap= 0.76177 time= 0.07568\n",
      "训练次数: 28 Epoch: 0009 log_lik= 0.74358606 train_kl= -0.00425 train_loss= 0.74784 train_acc= 0.37487 val_roc= 0.74254 val_ap= 0.74954 time= 0.07767\n",
      "训练次数: 28 Epoch: 0010 log_lik= 0.7234318 train_kl= -0.00532 train_loss= 0.72875 train_acc= 0.28441 val_roc= 0.74208 val_ap= 0.74954 time= 0.07966\n",
      "训练次数: 28 Epoch: 0011 log_lik= 0.7096003 train_kl= -0.00638 train_loss= 0.71598 train_acc= 0.22046 val_roc= 0.75360 val_ap= 0.76130 time= 0.07686\n",
      "训练次数: 28 Epoch: 0012 log_lik= 0.69776666 train_kl= -0.00738 train_loss= 0.70515 train_acc= 0.20615 val_roc= 0.76867 val_ap= 0.77666 time= 0.07568\n",
      "训练次数: 28 Epoch: 0013 log_lik= 0.6842303 train_kl= -0.00831 train_loss= 0.69254 train_acc= 0.22406 val_roc= 0.77619 val_ap= 0.78242 time= 0.08265\n",
      "训练次数: 28 Epoch: 0014 log_lik= 0.67267615 train_kl= -0.00920 train_loss= 0.68188 train_acc= 0.21746 val_roc= 0.78266 val_ap= 0.78804 time= 0.07767\n",
      "训练次数: 28 Epoch: 0015 log_lik= 0.66379344 train_kl= -0.01003 train_loss= 0.67382 train_acc= 0.17733 val_roc= 0.79682 val_ap= 0.79890 time= 0.07468\n",
      "训练次数: 28 Epoch: 0016 log_lik= 0.64648163 train_kl= -0.01075 train_loss= 0.65723 train_acc= 0.20904 val_roc= 0.81044 val_ap= 0.80614 time= 0.07468\n",
      "训练次数: 28 Epoch: 0017 log_lik= 0.6286793 train_kl= -0.01138 train_loss= 0.64006 train_acc= 0.26789 val_roc= 0.81281 val_ap= 0.80480 time= 0.07667\n",
      "训练次数: 28 Epoch: 0018 log_lik= 0.60722077 train_kl= -0.01195 train_loss= 0.61917 train_acc= 0.35662 val_roc= 0.81469 val_ap= 0.80589 time= 0.07568\n",
      "训练次数: 28 Epoch: 0019 log_lik= 0.5882764 train_kl= -0.01249 train_loss= 0.60077 train_acc= 0.40401 val_roc= 0.81787 val_ap= 0.81198 time= 0.07867\n",
      "训练次数: 28 Epoch: 0020 log_lik= 0.5769865 train_kl= -0.01299 train_loss= 0.58997 train_acc= 0.40891 val_roc= 0.82881 val_ap= 0.82095 time= 0.07767\n",
      "训练次数: 28 Epoch: 0021 log_lik= 0.56884885 train_kl= -0.01340 train_loss= 0.58225 train_acc= 0.42279 val_roc= 0.84159 val_ap= 0.82910 time= 0.07468\n",
      "训练次数: 28 Epoch: 0022 log_lik= 0.55650824 train_kl= -0.01372 train_loss= 0.57023 train_acc= 0.45506 val_roc= 0.84970 val_ap= 0.83528 time= 0.07468\n",
      "训练次数: 28 Epoch: 0023 log_lik= 0.54907334 train_kl= -0.01400 train_loss= 0.56308 train_acc= 0.46872 val_roc= 0.85634 val_ap= 0.84309 time= 0.07668\n",
      "训练次数: 28 Epoch: 0024 log_lik= 0.5466817 train_kl= -0.01427 train_loss= 0.56095 train_acc= 0.47483 val_roc= 0.86164 val_ap= 0.84914 time= 0.07489\n",
      "训练次数: 28 Epoch: 0025 log_lik= 0.5448248 train_kl= -0.01451 train_loss= 0.55934 train_acc= 0.48602 val_roc= 0.86774 val_ap= 0.85538 time= 0.07767\n",
      "训练次数: 28 Epoch: 0026 log_lik= 0.538517 train_kl= -0.01471 train_loss= 0.55322 train_acc= 0.49533 val_roc= 0.87221 val_ap= 0.86215 time= 0.07867\n",
      "训练次数: 28 Epoch: 0027 log_lik= 0.5312535 train_kl= -0.01483 train_loss= 0.54608 train_acc= 0.49828 val_roc= 0.87649 val_ap= 0.86838 time= 0.07667\n",
      "训练次数: 28 Epoch: 0028 log_lik= 0.52243215 train_kl= -0.01484 train_loss= 0.53727 train_acc= 0.50081 val_roc= 0.88177 val_ap= 0.87581 time= 0.07468\n",
      "训练次数: 28 Epoch: 0029 log_lik= 0.51250404 train_kl= -0.01476 train_loss= 0.52726 train_acc= 0.50461 val_roc= 0.88493 val_ap= 0.88084 time= 0.07369\n",
      "训练次数: 28 Epoch: 0030 log_lik= 0.50520355 train_kl= -0.01464 train_loss= 0.51984 train_acc= 0.50565 val_roc= 0.88600 val_ap= 0.88483 time= 0.07767\n",
      "训练次数: 28 Epoch: 0031 log_lik= 0.49902275 train_kl= -0.01453 train_loss= 0.51355 train_acc= 0.50359 val_roc= 0.88788 val_ap= 0.88892 time= 0.07568\n",
      "训练次数: 28 Epoch: 0032 log_lik= 0.49510375 train_kl= -0.01444 train_loss= 0.50954 train_acc= 0.50219 val_roc= 0.88755 val_ap= 0.89077 time= 0.07568\n",
      "训练次数: 28 Epoch: 0033 log_lik= 0.49258158 train_kl= -0.01434 train_loss= 0.50693 train_acc= 0.50342 val_roc= 0.88865 val_ap= 0.89356 time= 0.07568\n",
      "训练次数: 28 Epoch: 0034 log_lik= 0.48942822 train_kl= -0.01424 train_loss= 0.50367 train_acc= 0.50500 val_roc= 0.88965 val_ap= 0.89558 time= 0.07568\n",
      "训练次数: 28 Epoch: 0035 log_lik= 0.48733896 train_kl= -0.01412 train_loss= 0.50146 train_acc= 0.50444 val_roc= 0.89140 val_ap= 0.89820 time= 0.08066\n",
      "训练次数: 28 Epoch: 0036 log_lik= 0.48518485 train_kl= -0.01402 train_loss= 0.49921 train_acc= 0.50083 val_roc= 0.89317 val_ap= 0.90045 time= 0.07568\n",
      "训练次数: 28 Epoch: 0037 log_lik= 0.4837525 train_kl= -0.01393 train_loss= 0.49768 train_acc= 0.50189 val_roc= 0.89346 val_ap= 0.90130 time= 0.07568\n",
      "训练次数: 28 Epoch: 0038 log_lik= 0.48191053 train_kl= -0.01384 train_loss= 0.49575 train_acc= 0.50509 val_roc= 0.89459 val_ap= 0.90303 time= 0.07966\n",
      "训练次数: 28 Epoch: 0039 log_lik= 0.48001277 train_kl= -0.01374 train_loss= 0.49375 train_acc= 0.50865 val_roc= 0.89699 val_ap= 0.90580 time= 0.07767\n",
      "训练次数: 28 Epoch: 0040 log_lik= 0.47655353 train_kl= -0.01363 train_loss= 0.49018 train_acc= 0.50853 val_roc= 0.89865 val_ap= 0.90750 time= 0.07468\n",
      "训练次数: 28 Epoch: 0041 log_lik= 0.4743553 train_kl= -0.01351 train_loss= 0.48787 train_acc= 0.50649 val_roc= 0.89922 val_ap= 0.90757 time= 0.07468\n",
      "训练次数: 28 Epoch: 0042 log_lik= 0.47173604 train_kl= -0.01341 train_loss= 0.48515 train_acc= 0.50858 val_roc= 0.89939 val_ap= 0.90713 time= 0.07468\n",
      "训练次数: 28 Epoch: 0043 log_lik= 0.46962634 train_kl= -0.01332 train_loss= 0.48294 train_acc= 0.51297 val_roc= 0.90055 val_ap= 0.90807 time= 0.07667\n",
      "训练次数: 28 Epoch: 0044 log_lik= 0.46698314 train_kl= -0.01323 train_loss= 0.48022 train_acc= 0.51506 val_roc= 0.90183 val_ap= 0.90902 time= 0.08464\n",
      "训练次数: 28 Epoch: 0045 log_lik= 0.46567416 train_kl= -0.01315 train_loss= 0.47882 train_acc= 0.51721 val_roc= 0.90191 val_ap= 0.90892 time= 0.07468\n",
      "训练次数: 28 Epoch: 0046 log_lik= 0.46408808 train_kl= -0.01305 train_loss= 0.47714 train_acc= 0.51783 val_roc= 0.90056 val_ap= 0.90660 time= 0.07568\n",
      "训练次数: 28 Epoch: 0047 log_lik= 0.46306077 train_kl= -0.01296 train_loss= 0.47602 train_acc= 0.51989 val_roc= 0.90157 val_ap= 0.90613 time= 0.07468\n",
      "训练次数: 28 Epoch: 0048 log_lik= 0.46215162 train_kl= -0.01287 train_loss= 0.47502 train_acc= 0.52012 val_roc= 0.90247 val_ap= 0.90587 time= 0.07469\n",
      "训练次数: 28 Epoch: 0049 log_lik= 0.4609614 train_kl= -0.01278 train_loss= 0.47374 train_acc= 0.52156 val_roc= 0.90198 val_ap= 0.90516 time= 0.07767\n",
      "训练次数: 28 Epoch: 0050 log_lik= 0.45956293 train_kl= -0.01267 train_loss= 0.47224 train_acc= 0.52151 val_roc= 0.90092 val_ap= 0.90465 time= 0.07867\n",
      "训练次数: 28 Epoch: 0051 log_lik= 0.45891732 train_kl= -0.01255 train_loss= 0.47147 train_acc= 0.52142 val_roc= 0.90087 val_ap= 0.90450 time= 0.08365\n",
      "训练次数: 28 Epoch: 0052 log_lik= 0.45739383 train_kl= -0.01244 train_loss= 0.46984 train_acc= 0.52125 val_roc= 0.90110 val_ap= 0.90357 time= 0.08066\n",
      "训练次数: 28 Epoch: 0053 log_lik= 0.4559483 train_kl= -0.01234 train_loss= 0.46829 train_acc= 0.52159 val_roc= 0.89998 val_ap= 0.90205 time= 0.07468\n",
      "训练次数: 28 Epoch: 0054 log_lik= 0.45555097 train_kl= -0.01224 train_loss= 0.46779 train_acc= 0.52251 val_roc= 0.89760 val_ap= 0.90032 time= 0.07468\n",
      "训练次数: 28 Epoch: 0055 log_lik= 0.45476398 train_kl= -0.01215 train_loss= 0.46692 train_acc= 0.52219 val_roc= 0.89750 val_ap= 0.90045 time= 0.07667\n",
      "训练次数: 28 Epoch: 0056 log_lik= 0.4538959 train_kl= -0.01208 train_loss= 0.46598 train_acc= 0.52200 val_roc= 0.89841 val_ap= 0.90090 time= 0.07667\n",
      "训练次数: 28 Epoch: 0057 log_lik= 0.45313665 train_kl= -0.01202 train_loss= 0.46515 train_acc= 0.52233 val_roc= 0.89753 val_ap= 0.89930 time= 0.08119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 28 Epoch: 0058 log_lik= 0.45229828 train_kl= -0.01196 train_loss= 0.46426 train_acc= 0.52209 val_roc= 0.89633 val_ap= 0.89813 time= 0.07568\n",
      "训练次数: 28 Epoch: 0059 log_lik= 0.45125923 train_kl= -0.01191 train_loss= 0.46316 train_acc= 0.52374 val_roc= 0.89536 val_ap= 0.89641 time= 0.07667\n",
      "训练次数: 28 Epoch: 0060 log_lik= 0.45088926 train_kl= -0.01186 train_loss= 0.46275 train_acc= 0.52369 val_roc= 0.89640 val_ap= 0.89734 time= 0.07568\n",
      "训练次数: 28 Epoch: 0061 log_lik= 0.45056564 train_kl= -0.01182 train_loss= 0.46239 train_acc= 0.52272 val_roc= 0.89787 val_ap= 0.89874 time= 0.07767\n",
      "训练次数: 28 Epoch: 0062 log_lik= 0.44980943 train_kl= -0.01178 train_loss= 0.46159 train_acc= 0.52372 val_roc= 0.89851 val_ap= 0.89970 time= 0.07767\n",
      "训练次数: 28 Epoch: 0063 log_lik= 0.44922638 train_kl= -0.01175 train_loss= 0.46098 train_acc= 0.52404 val_roc= 0.89829 val_ap= 0.89962 time= 0.07568\n",
      "训练次数: 28 Epoch: 0064 log_lik= 0.44835708 train_kl= -0.01174 train_loss= 0.46010 train_acc= 0.52530 val_roc= 0.89848 val_ap= 0.89933 time= 0.07568\n",
      "训练次数: 28 Epoch: 0065 log_lik= 0.4481796 train_kl= -0.01174 train_loss= 0.45992 train_acc= 0.52492 val_roc= 0.90050 val_ap= 0.90010 time= 0.07867\n",
      "训练次数: 28 Epoch: 0066 log_lik= 0.44759244 train_kl= -0.01176 train_loss= 0.45935 train_acc= 0.52527 val_roc= 0.90160 val_ap= 0.90129 time= 0.07468\n",
      "训练次数: 28 Epoch: 0067 log_lik= 0.44672543 train_kl= -0.01177 train_loss= 0.45849 train_acc= 0.52575 val_roc= 0.90124 val_ap= 0.90179 time= 0.07867\n",
      "训练次数: 28 Epoch: 0068 log_lik= 0.4463761 train_kl= -0.01176 train_loss= 0.45813 train_acc= 0.52641 val_roc= 0.90149 val_ap= 0.90217 time= 0.08165\n",
      "训练次数: 28 Epoch: 0069 log_lik= 0.4458072 train_kl= -0.01175 train_loss= 0.45756 train_acc= 0.52634 val_roc= 0.90312 val_ap= 0.90263 time= 0.07469\n",
      "训练次数: 28 Epoch: 0070 log_lik= 0.44512096 train_kl= -0.01177 train_loss= 0.45689 train_acc= 0.52721 val_roc= 0.90387 val_ap= 0.90231 time= 0.07966\n",
      "训练次数: 28 Epoch: 0071 log_lik= 0.44519514 train_kl= -0.01178 train_loss= 0.45698 train_acc= 0.52696 val_roc= 0.90386 val_ap= 0.90282 time= 0.07568\n",
      "训练次数: 28 Epoch: 0072 log_lik= 0.44423422 train_kl= -0.01178 train_loss= 0.45602 train_acc= 0.52793 val_roc= 0.90464 val_ap= 0.90462 time= 0.07568\n",
      "训练次数: 28 Epoch: 0073 log_lik= 0.44406402 train_kl= -0.01179 train_loss= 0.45585 train_acc= 0.52796 val_roc= 0.90609 val_ap= 0.90603 time= 0.07867\n",
      "训练次数: 28 Epoch: 0074 log_lik= 0.4438333 train_kl= -0.01180 train_loss= 0.45563 train_acc= 0.52719 val_roc= 0.90604 val_ap= 0.90566 time= 0.07667\n",
      "训练次数: 28 Epoch: 0075 log_lik= 0.4428633 train_kl= -0.01181 train_loss= 0.45467 train_acc= 0.52878 val_roc= 0.90538 val_ap= 0.90466 time= 0.07568\n",
      "训练次数: 28 Epoch: 0076 log_lik= 0.44282705 train_kl= -0.01181 train_loss= 0.45464 train_acc= 0.52828 val_roc= 0.90590 val_ap= 0.90520 time= 0.07568\n",
      "训练次数: 28 Epoch: 0077 log_lik= 0.4422654 train_kl= -0.01183 train_loss= 0.45409 train_acc= 0.52838 val_roc= 0.90752 val_ap= 0.90726 time= 0.07667\n",
      "训练次数: 28 Epoch: 0078 log_lik= 0.44185886 train_kl= -0.01183 train_loss= 0.45369 train_acc= 0.52912 val_roc= 0.90742 val_ap= 0.90752 time= 0.07767\n",
      "训练次数: 28 Epoch: 0079 log_lik= 0.44130117 train_kl= -0.01183 train_loss= 0.45313 train_acc= 0.52804 val_roc= 0.90746 val_ap= 0.90783 time= 0.08265\n",
      "训练次数: 28 Epoch: 0080 log_lik= 0.44094136 train_kl= -0.01184 train_loss= 0.45278 train_acc= 0.52912 val_roc= 0.90812 val_ap= 0.90806 time= 0.07568\n",
      "训练次数: 28 Epoch: 0081 log_lik= 0.44034317 train_kl= -0.01186 train_loss= 0.45220 train_acc= 0.52964 val_roc= 0.90984 val_ap= 0.90948 time= 0.07468\n",
      "训练次数: 28 Epoch: 0082 log_lik= 0.44003874 train_kl= -0.01188 train_loss= 0.45192 train_acc= 0.53124 val_roc= 0.91002 val_ap= 0.91017 time= 0.07767\n",
      "训练次数: 28 Epoch: 0083 log_lik= 0.4392213 train_kl= -0.01189 train_loss= 0.45111 train_acc= 0.53050 val_roc= 0.91000 val_ap= 0.91150 time= 0.07468\n",
      "训练次数: 28 Epoch: 0084 log_lik= 0.43935418 train_kl= -0.01190 train_loss= 0.45125 train_acc= 0.53124 val_roc= 0.91156 val_ap= 0.91233 time= 0.08265\n",
      "训练次数: 28 Epoch: 0085 log_lik= 0.43852383 train_kl= -0.01192 train_loss= 0.45044 train_acc= 0.53194 val_roc= 0.91220 val_ap= 0.91227 time= 0.07867\n",
      "训练次数: 28 Epoch: 0086 log_lik= 0.43809408 train_kl= -0.01195 train_loss= 0.45004 train_acc= 0.53294 val_roc= 0.91216 val_ap= 0.91259 time= 0.07468\n",
      "训练次数: 28 Epoch: 0087 log_lik= 0.43772542 train_kl= -0.01196 train_loss= 0.44969 train_acc= 0.53308 val_roc= 0.91272 val_ap= 0.91396 time= 0.07468\n",
      "训练次数: 28 Epoch: 0088 log_lik= 0.43719992 train_kl= -0.01197 train_loss= 0.44917 train_acc= 0.53379 val_roc= 0.91470 val_ap= 0.91526 time= 0.07668\n",
      "训练次数: 28 Epoch: 0089 log_lik= 0.43706945 train_kl= -0.01199 train_loss= 0.44906 train_acc= 0.53426 val_roc= 0.91495 val_ap= 0.91594 time= 0.07468\n",
      "训练次数: 28 Epoch: 0090 log_lik= 0.43627092 train_kl= -0.01201 train_loss= 0.44828 train_acc= 0.53405 val_roc= 0.91375 val_ap= 0.91552 time= 0.08265\n",
      "训练次数: 28 Epoch: 0091 log_lik= 0.43594712 train_kl= -0.01201 train_loss= 0.44796 train_acc= 0.53336 val_roc= 0.91477 val_ap= 0.91689 time= 0.07468\n",
      "训练次数: 28 Epoch: 0092 log_lik= 0.4354581 train_kl= -0.01201 train_loss= 0.44746 train_acc= 0.53410 val_roc= 0.91609 val_ap= 0.91784 time= 0.07667\n",
      "训练次数: 28 Epoch: 0093 log_lik= 0.4353006 train_kl= -0.01203 train_loss= 0.44733 train_acc= 0.53325 val_roc= 0.91528 val_ap= 0.91720 time= 0.07568\n",
      "训练次数: 28 Epoch: 0094 log_lik= 0.4342972 train_kl= -0.01204 train_loss= 0.44634 train_acc= 0.53381 val_roc= 0.91461 val_ap= 0.91660 time= 0.07668\n",
      "训练次数: 28 Epoch: 0095 log_lik= 0.43434206 train_kl= -0.01204 train_loss= 0.44638 train_acc= 0.53413 val_roc= 0.91521 val_ap= 0.91731 time= 0.08564\n",
      "训练次数: 28 Epoch: 0096 log_lik= 0.43403393 train_kl= -0.01206 train_loss= 0.44609 train_acc= 0.53450 val_roc= 0.91576 val_ap= 0.91782 time= 0.07568\n",
      "训练次数: 28 Epoch: 0097 log_lik= 0.4337766 train_kl= -0.01205 train_loss= 0.44582 train_acc= 0.53356 val_roc= 0.91489 val_ap= 0.91756 time= 0.07468\n",
      "训练次数: 28 Epoch: 0098 log_lik= 0.43361554 train_kl= -0.01205 train_loss= 0.44566 train_acc= 0.53294 val_roc= 0.91425 val_ap= 0.91667 time= 0.07966\n",
      "训练次数: 28 Epoch: 0099 log_lik= 0.43292844 train_kl= -0.01207 train_loss= 0.44500 train_acc= 0.53401 val_roc= 0.91483 val_ap= 0.91641 time= 0.07568\n",
      "训练次数: 28 Epoch: 0100 log_lik= 0.43292335 train_kl= -0.01208 train_loss= 0.44501 train_acc= 0.53431 val_roc= 0.91457 val_ap= 0.91618 time= 0.07468\n",
      "Optimization Finished!\n",
      "训练次数: 28 ROC score: 0.924332712824372\n",
      "训练次数: 28 AP score: 0.9323660162355252\n",
      "训练次数: 29 Epoch: 0001 log_lik= 1.7941504 train_kl= -0.00005 train_loss= 1.79420 train_acc= 0.49490 val_roc= 0.70011 val_ap= 0.71780 time= 1.76707\n",
      "训练次数: 29 Epoch: 0002 log_lik= 1.4177098 train_kl= -0.00025 train_loss= 1.41796 train_acc= 0.46881 val_roc= 0.68025 val_ap= 0.70329 time= 0.08663\n",
      "训练次数: 29 Epoch: 0003 log_lik= 1.2522328 train_kl= -0.00083 train_loss= 1.25306 train_acc= 0.36633 val_roc= 0.68013 val_ap= 0.70384 time= 0.07767\n",
      "训练次数: 29 Epoch: 0004 log_lik= 1.1464338 train_kl= -0.00133 train_loss= 1.14776 train_acc= 0.30011 val_roc= 0.68354 val_ap= 0.70723 time= 0.07568\n",
      "训练次数: 29 Epoch: 0005 log_lik= 1.0813313 train_kl= -0.00168 train_loss= 1.08301 train_acc= 0.26341 val_roc= 0.68927 val_ap= 0.71174 time= 0.07468\n",
      "训练次数: 29 Epoch: 0006 log_lik= 0.9887483 train_kl= -0.00195 train_loss= 0.99069 train_acc= 0.26481 val_roc= 0.70182 val_ap= 0.72111 time= 0.07468\n",
      "训练次数: 29 Epoch: 0007 log_lik= 0.901013 train_kl= -0.00219 train_loss= 0.90320 train_acc= 0.29217 val_roc= 0.72028 val_ap= 0.73640 time= 0.07468\n",
      "训练次数: 29 Epoch: 0008 log_lik= 0.8300404 train_kl= -0.00248 train_loss= 0.83253 train_acc= 0.31132 val_roc= 0.73825 val_ap= 0.75235 time= 0.07468\n",
      "训练次数: 29 Epoch: 0009 log_lik= 0.78386897 train_kl= -0.00286 train_loss= 0.78673 train_acc= 0.33130 val_roc= 0.75227 val_ap= 0.76419 time= 0.07667\n",
      "训练次数: 29 Epoch: 0010 log_lik= 0.75196517 train_kl= -0.00330 train_loss= 0.75527 train_acc= 0.32543 val_roc= 0.76160 val_ap= 0.77483 time= 0.09161\n",
      "训练次数: 29 Epoch: 0011 log_lik= 0.7277581 train_kl= -0.00381 train_loss= 0.73157 train_acc= 0.30526 val_roc= 0.76900 val_ap= 0.78282 time= 0.07818\n",
      "训练次数: 29 Epoch: 0012 log_lik= 0.7122872 train_kl= -0.00437 train_loss= 0.71666 train_acc= 0.27698 val_roc= 0.78025 val_ap= 0.79319 time= 0.07519\n",
      "训练次数: 29 Epoch: 0013 log_lik= 0.6990913 train_kl= -0.00494 train_loss= 0.70403 train_acc= 0.25513 val_roc= 0.79625 val_ap= 0.80519 time= 0.07667\n",
      "训练次数: 29 Epoch: 0014 log_lik= 0.6856203 train_kl= -0.00549 train_loss= 0.69111 train_acc= 0.24067 val_roc= 0.81859 val_ap= 0.82195 time= 0.07568\n",
      "训练次数: 29 Epoch: 0015 log_lik= 0.67107105 train_kl= -0.00599 train_loss= 0.67706 train_acc= 0.25167 val_roc= 0.83827 val_ap= 0.83540 time= 0.07668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 29 Epoch: 0016 log_lik= 0.65588725 train_kl= -0.00646 train_loss= 0.66235 train_acc= 0.27652 val_roc= 0.84752 val_ap= 0.84268 time= 0.07667\n",
      "训练次数: 29 Epoch: 0017 log_lik= 0.64033586 train_kl= -0.00690 train_loss= 0.64724 train_acc= 0.30473 val_roc= 0.85051 val_ap= 0.84502 time= 0.07867\n",
      "训练次数: 29 Epoch: 0018 log_lik= 0.62502646 train_kl= -0.00730 train_loss= 0.63233 train_acc= 0.34315 val_roc= 0.85175 val_ap= 0.84668 time= 0.07468\n",
      "训练次数: 29 Epoch: 0019 log_lik= 0.61158854 train_kl= -0.00769 train_loss= 0.61928 train_acc= 0.36401 val_roc= 0.85368 val_ap= 0.84854 time= 0.07468\n",
      "训练次数: 29 Epoch: 0020 log_lik= 0.5964217 train_kl= -0.00805 train_loss= 0.60447 train_acc= 0.38909 val_roc= 0.85680 val_ap= 0.85109 time= 0.07667\n",
      "训练次数: 29 Epoch: 0021 log_lik= 0.58532447 train_kl= -0.00837 train_loss= 0.59369 train_acc= 0.41287 val_roc= 0.85969 val_ap= 0.85237 time= 0.07468\n",
      "训练次数: 29 Epoch: 0022 log_lik= 0.5742976 train_kl= -0.00863 train_loss= 0.58293 train_acc= 0.44001 val_roc= 0.86354 val_ap= 0.85726 time= 0.07867\n",
      "训练次数: 29 Epoch: 0023 log_lik= 0.56461614 train_kl= -0.00885 train_loss= 0.57347 train_acc= 0.46590 val_roc= 0.86663 val_ap= 0.86239 time= 0.07867\n",
      "训练次数: 29 Epoch: 0024 log_lik= 0.56079537 train_kl= -0.00903 train_loss= 0.56982 train_acc= 0.47429 val_roc= 0.87077 val_ap= 0.86709 time= 0.07468\n",
      "训练次数: 29 Epoch: 0025 log_lik= 0.5561703 train_kl= -0.00919 train_loss= 0.56536 train_acc= 0.47996 val_roc= 0.87468 val_ap= 0.87014 time= 0.07668\n",
      "训练次数: 29 Epoch: 0026 log_lik= 0.5502637 train_kl= -0.00933 train_loss= 0.55959 train_acc= 0.48460 val_roc= 0.87784 val_ap= 0.87234 time= 0.07568\n",
      "训练次数: 29 Epoch: 0027 log_lik= 0.54264015 train_kl= -0.00944 train_loss= 0.55208 train_acc= 0.48951 val_roc= 0.88122 val_ap= 0.87583 time= 0.07468\n",
      "训练次数: 29 Epoch: 0028 log_lik= 0.53724533 train_kl= -0.00950 train_loss= 0.54674 train_acc= 0.48989 val_roc= 0.88450 val_ap= 0.87911 time= 0.07468\n",
      "训练次数: 29 Epoch: 0029 log_lik= 0.5308773 train_kl= -0.00952 train_loss= 0.54040 train_acc= 0.49235 val_roc= 0.88678 val_ap= 0.88085 time= 0.07767\n",
      "训练次数: 29 Epoch: 0030 log_lik= 0.5266944 train_kl= -0.00953 train_loss= 0.53623 train_acc= 0.49104 val_roc= 0.88956 val_ap= 0.88320 time= 0.07917\n",
      "训练次数: 29 Epoch: 0031 log_lik= 0.5222914 train_kl= -0.00954 train_loss= 0.53183 train_acc= 0.48994 val_roc= 0.89210 val_ap= 0.88543 time= 0.07667\n",
      "训练次数: 29 Epoch: 0032 log_lik= 0.51807106 train_kl= -0.00955 train_loss= 0.52762 train_acc= 0.48944 val_roc= 0.89401 val_ap= 0.88651 time= 0.08663\n",
      "训练次数: 29 Epoch: 0033 log_lik= 0.51634854 train_kl= -0.00955 train_loss= 0.52590 train_acc= 0.48790 val_roc= 0.89646 val_ap= 0.88861 time= 0.07468\n",
      "训练次数: 29 Epoch: 0034 log_lik= 0.51303554 train_kl= -0.00956 train_loss= 0.52259 train_acc= 0.49011 val_roc= 0.89745 val_ap= 0.89008 time= 0.07468\n",
      "训练次数: 29 Epoch: 0035 log_lik= 0.51153505 train_kl= -0.00957 train_loss= 0.52111 train_acc= 0.49326 val_roc= 0.89838 val_ap= 0.89021 time= 0.07369\n",
      "训练次数: 29 Epoch: 0036 log_lik= 0.5075107 train_kl= -0.00960 train_loss= 0.51711 train_acc= 0.49816 val_roc= 0.89886 val_ap= 0.89090 time= 0.07568\n",
      "训练次数: 29 Epoch: 0037 log_lik= 0.5028955 train_kl= -0.00965 train_loss= 0.51254 train_acc= 0.50370 val_roc= 0.90016 val_ap= 0.89137 time= 0.07369\n",
      "训练次数: 29 Epoch: 0038 log_lik= 0.49931797 train_kl= -0.00970 train_loss= 0.50902 train_acc= 0.50845 val_roc= 0.90124 val_ap= 0.89130 time= 0.07468\n",
      "训练次数: 29 Epoch: 0039 log_lik= 0.4958521 train_kl= -0.00976 train_loss= 0.50561 train_acc= 0.51136 val_roc= 0.90286 val_ap= 0.89213 time= 0.07468\n",
      "训练次数: 29 Epoch: 0040 log_lik= 0.49358904 train_kl= -0.00981 train_loss= 0.50340 train_acc= 0.51266 val_roc= 0.90513 val_ap= 0.89540 time= 0.07767\n",
      "训练次数: 29 Epoch: 0041 log_lik= 0.49029034 train_kl= -0.00984 train_loss= 0.50013 train_acc= 0.51276 val_roc= 0.90671 val_ap= 0.89717 time= 0.07568\n",
      "训练次数: 29 Epoch: 0042 log_lik= 0.4878321 train_kl= -0.00986 train_loss= 0.49769 train_acc= 0.51502 val_roc= 0.90753 val_ap= 0.89762 time= 0.07866\n",
      "训练次数: 29 Epoch: 0043 log_lik= 0.48708412 train_kl= -0.00988 train_loss= 0.49697 train_acc= 0.51504 val_roc= 0.90817 val_ap= 0.89787 time= 0.07767\n",
      "训练次数: 29 Epoch: 0044 log_lik= 0.48618987 train_kl= -0.00991 train_loss= 0.49610 train_acc= 0.51539 val_roc= 0.90811 val_ap= 0.89693 time= 0.08165\n",
      "训练次数: 29 Epoch: 0045 log_lik= 0.48428434 train_kl= -0.00995 train_loss= 0.49424 train_acc= 0.51709 val_roc= 0.90830 val_ap= 0.89748 time= 0.07667\n",
      "训练次数: 29 Epoch: 0046 log_lik= 0.48307914 train_kl= -0.00999 train_loss= 0.49307 train_acc= 0.51918 val_roc= 0.90914 val_ap= 0.89956 time= 0.08265\n",
      "训练次数: 29 Epoch: 0047 log_lik= 0.47931156 train_kl= -0.01002 train_loss= 0.48933 train_acc= 0.52104 val_roc= 0.91022 val_ap= 0.90224 time= 0.07867\n",
      "训练次数: 29 Epoch: 0048 log_lik= 0.4771118 train_kl= -0.01005 train_loss= 0.48716 train_acc= 0.52264 val_roc= 0.91081 val_ap= 0.90439 time= 0.07668\n",
      "训练次数: 29 Epoch: 0049 log_lik= 0.475175 train_kl= -0.01008 train_loss= 0.48525 train_acc= 0.52288 val_roc= 0.91075 val_ap= 0.90455 time= 0.07767\n",
      "训练次数: 29 Epoch: 0050 log_lik= 0.47237638 train_kl= -0.01012 train_loss= 0.48250 train_acc= 0.52368 val_roc= 0.91019 val_ap= 0.90321 time= 0.07468\n",
      "训练次数: 29 Epoch: 0051 log_lik= 0.47171694 train_kl= -0.01017 train_loss= 0.48189 train_acc= 0.52562 val_roc= 0.91078 val_ap= 0.90543 time= 0.07470\n",
      "训练次数: 29 Epoch: 0052 log_lik= 0.47050947 train_kl= -0.01020 train_loss= 0.48071 train_acc= 0.52548 val_roc= 0.91099 val_ap= 0.90752 time= 0.08364\n",
      "训练次数: 29 Epoch: 0053 log_lik= 0.4690957 train_kl= -0.01021 train_loss= 0.47931 train_acc= 0.52625 val_roc= 0.91067 val_ap= 0.90863 time= 0.07577\n",
      "训练次数: 29 Epoch: 0054 log_lik= 0.46813965 train_kl= -0.01024 train_loss= 0.47838 train_acc= 0.52687 val_roc= 0.90968 val_ap= 0.90815 time= 0.07767\n",
      "训练次数: 29 Epoch: 0055 log_lik= 0.46650818 train_kl= -0.01029 train_loss= 0.47680 train_acc= 0.52723 val_roc= 0.90967 val_ap= 0.90866 time= 0.07767\n",
      "训练次数: 29 Epoch: 0056 log_lik= 0.46565878 train_kl= -0.01035 train_loss= 0.47601 train_acc= 0.52847 val_roc= 0.91088 val_ap= 0.91025 time= 0.08165\n",
      "训练次数: 29 Epoch: 0057 log_lik= 0.46505585 train_kl= -0.01040 train_loss= 0.47546 train_acc= 0.52836 val_roc= 0.91276 val_ap= 0.91229 time= 0.07668\n",
      "训练次数: 29 Epoch: 0058 log_lik= 0.464337 train_kl= -0.01045 train_loss= 0.47479 train_acc= 0.52768 val_roc= 0.91352 val_ap= 0.91303 time= 0.08265\n",
      "训练次数: 29 Epoch: 0059 log_lik= 0.46339348 train_kl= -0.01050 train_loss= 0.47390 train_acc= 0.52886 val_roc= 0.91304 val_ap= 0.91264 time= 0.07468\n",
      "训练次数: 29 Epoch: 0060 log_lik= 0.4618535 train_kl= -0.01056 train_loss= 0.47241 train_acc= 0.52993 val_roc= 0.91297 val_ap= 0.91330 time= 0.07468\n",
      "训练次数: 29 Epoch: 0061 log_lik= 0.46097147 train_kl= -0.01061 train_loss= 0.47158 train_acc= 0.53030 val_roc= 0.91412 val_ap= 0.91459 time= 0.07468\n",
      "训练次数: 29 Epoch: 0062 log_lik= 0.4607819 train_kl= -0.01066 train_loss= 0.47144 train_acc= 0.53049 val_roc= 0.91644 val_ap= 0.91710 time= 0.07668\n",
      "训练次数: 29 Epoch: 0063 log_lik= 0.45961508 train_kl= -0.01069 train_loss= 0.47031 train_acc= 0.53007 val_roc= 0.91804 val_ap= 0.91795 time= 0.07667\n",
      "训练次数: 29 Epoch: 0064 log_lik= 0.4589196 train_kl= -0.01073 train_loss= 0.46965 train_acc= 0.53127 val_roc= 0.91849 val_ap= 0.91794 time= 0.07568\n",
      "训练次数: 29 Epoch: 0065 log_lik= 0.4579095 train_kl= -0.01078 train_loss= 0.46869 train_acc= 0.53156 val_roc= 0.91907 val_ap= 0.91886 time= 0.07468\n",
      "训练次数: 29 Epoch: 0066 log_lik= 0.45693392 train_kl= -0.01081 train_loss= 0.46775 train_acc= 0.53317 val_roc= 0.91949 val_ap= 0.91980 time= 0.07369\n",
      "训练次数: 29 Epoch: 0067 log_lik= 0.4555439 train_kl= -0.01085 train_loss= 0.46639 train_acc= 0.53328 val_roc= 0.92020 val_ap= 0.92033 time= 0.07667\n",
      "训练次数: 29 Epoch: 0068 log_lik= 0.45508176 train_kl= -0.01089 train_loss= 0.46597 train_acc= 0.53307 val_roc= 0.92121 val_ap= 0.92128 time= 0.07667\n",
      "训练次数: 29 Epoch: 0069 log_lik= 0.45417675 train_kl= -0.01094 train_loss= 0.46511 train_acc= 0.53433 val_roc= 0.92207 val_ap= 0.92183 time= 0.07667\n",
      "训练次数: 29 Epoch: 0070 log_lik= 0.45394498 train_kl= -0.01099 train_loss= 0.46493 train_acc= 0.53200 val_roc= 0.92316 val_ap= 0.92291 time= 0.07668\n",
      "训练次数: 29 Epoch: 0071 log_lik= 0.45235127 train_kl= -0.01102 train_loss= 0.46337 train_acc= 0.53434 val_roc= 0.92355 val_ap= 0.92381 time= 0.07468\n",
      "训练次数: 29 Epoch: 0072 log_lik= 0.4517191 train_kl= -0.01107 train_loss= 0.46279 train_acc= 0.53381 val_roc= 0.92281 val_ap= 0.92381 time= 0.07568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 29 Epoch: 0073 log_lik= 0.45056883 train_kl= -0.01114 train_loss= 0.46171 train_acc= 0.53486 val_roc= 0.92297 val_ap= 0.92360 time= 0.07867\n",
      "训练次数: 29 Epoch: 0074 log_lik= 0.44932643 train_kl= -0.01120 train_loss= 0.46053 train_acc= 0.53698 val_roc= 0.92351 val_ap= 0.92399 time= 0.07568\n",
      "训练次数: 29 Epoch: 0075 log_lik= 0.4491924 train_kl= -0.01126 train_loss= 0.46045 train_acc= 0.53620 val_roc= 0.92387 val_ap= 0.92393 time= 0.08165\n",
      "训练次数: 29 Epoch: 0076 log_lik= 0.44829202 train_kl= -0.01131 train_loss= 0.45960 train_acc= 0.53686 val_roc= 0.92407 val_ap= 0.92471 time= 0.09062\n",
      "训练次数: 29 Epoch: 0077 log_lik= 0.44769445 train_kl= -0.01136 train_loss= 0.45906 train_acc= 0.53777 val_roc= 0.92382 val_ap= 0.92527 time= 0.07568\n",
      "训练次数: 29 Epoch: 0078 log_lik= 0.44664606 train_kl= -0.01140 train_loss= 0.45804 train_acc= 0.53765 val_roc= 0.92309 val_ap= 0.92572 time= 0.07568\n",
      "训练次数: 29 Epoch: 0079 log_lik= 0.4463422 train_kl= -0.01143 train_loss= 0.45777 train_acc= 0.53934 val_roc= 0.92231 val_ap= 0.92543 time= 0.07568\n",
      "训练次数: 29 Epoch: 0080 log_lik= 0.4456347 train_kl= -0.01147 train_loss= 0.45711 train_acc= 0.53811 val_roc= 0.92314 val_ap= 0.92594 time= 0.07468\n",
      "训练次数: 29 Epoch: 0081 log_lik= 0.4449794 train_kl= -0.01149 train_loss= 0.45647 train_acc= 0.53939 val_roc= 0.92368 val_ap= 0.92676 time= 0.07667\n",
      "训练次数: 29 Epoch: 0082 log_lik= 0.44467914 train_kl= -0.01150 train_loss= 0.45618 train_acc= 0.53856 val_roc= 0.92323 val_ap= 0.92727 time= 0.07667\n",
      "训练次数: 29 Epoch: 0083 log_lik= 0.44381484 train_kl= -0.01154 train_loss= 0.45536 train_acc= 0.54043 val_roc= 0.92342 val_ap= 0.92769 time= 0.07568\n",
      "训练次数: 29 Epoch: 0084 log_lik= 0.44405335 train_kl= -0.01156 train_loss= 0.45561 train_acc= 0.53971 val_roc= 0.92369 val_ap= 0.92811 time= 0.08066\n",
      "训练次数: 29 Epoch: 0085 log_lik= 0.44319096 train_kl= -0.01155 train_loss= 0.45474 train_acc= 0.53954 val_roc= 0.92420 val_ap= 0.92850 time= 0.07966\n",
      "训练次数: 29 Epoch: 0086 log_lik= 0.44275248 train_kl= -0.01156 train_loss= 0.45431 train_acc= 0.53925 val_roc= 0.92333 val_ap= 0.92812 time= 0.07468\n",
      "训练次数: 29 Epoch: 0087 log_lik= 0.44252804 train_kl= -0.01159 train_loss= 0.45411 train_acc= 0.54091 val_roc= 0.92449 val_ap= 0.92953 time= 0.07468\n",
      "训练次数: 29 Epoch: 0088 log_lik= 0.44224912 train_kl= -0.01158 train_loss= 0.45383 train_acc= 0.54028 val_roc= 0.92543 val_ap= 0.93035 time= 0.08165\n",
      "训练次数: 29 Epoch: 0089 log_lik= 0.44190642 train_kl= -0.01158 train_loss= 0.45349 train_acc= 0.54131 val_roc= 0.92452 val_ap= 0.92953 time= 0.07568\n",
      "训练次数: 29 Epoch: 0090 log_lik= 0.4415825 train_kl= -0.01159 train_loss= 0.45317 train_acc= 0.54125 val_roc= 0.92501 val_ap= 0.93014 time= 0.07667\n",
      "训练次数: 29 Epoch: 0091 log_lik= 0.441178 train_kl= -0.01159 train_loss= 0.45277 train_acc= 0.54106 val_roc= 0.92566 val_ap= 0.93079 time= 0.07667\n",
      "训练次数: 29 Epoch: 0092 log_lik= 0.44060662 train_kl= -0.01159 train_loss= 0.45219 train_acc= 0.54257 val_roc= 0.92620 val_ap= 0.93123 time= 0.07369\n",
      "训练次数: 29 Epoch: 0093 log_lik= 0.44034564 train_kl= -0.01158 train_loss= 0.45193 train_acc= 0.54170 val_roc= 0.92565 val_ap= 0.93069 time= 0.07468\n",
      "训练次数: 29 Epoch: 0094 log_lik= 0.4400138 train_kl= -0.01158 train_loss= 0.45160 train_acc= 0.54100 val_roc= 0.92588 val_ap= 0.93094 time= 0.07670\n",
      "训练次数: 29 Epoch: 0095 log_lik= 0.43971556 train_kl= -0.01157 train_loss= 0.45129 train_acc= 0.54128 val_roc= 0.92621 val_ap= 0.93151 time= 0.07468\n",
      "训练次数: 29 Epoch: 0096 log_lik= 0.43948552 train_kl= -0.01157 train_loss= 0.45106 train_acc= 0.54198 val_roc= 0.92622 val_ap= 0.93158 time= 0.07767\n",
      "训练次数: 29 Epoch: 0097 log_lik= 0.43942347 train_kl= -0.01157 train_loss= 0.45099 train_acc= 0.54200 val_roc= 0.92630 val_ap= 0.93136 time= 0.07767\n",
      "训练次数: 29 Epoch: 0098 log_lik= 0.43892276 train_kl= -0.01156 train_loss= 0.45048 train_acc= 0.54314 val_roc= 0.92638 val_ap= 0.93126 time= 0.07667\n",
      "训练次数: 29 Epoch: 0099 log_lik= 0.4387557 train_kl= -0.01155 train_loss= 0.45031 train_acc= 0.54183 val_roc= 0.92567 val_ap= 0.93073 time= 0.07369\n",
      "训练次数: 29 Epoch: 0100 log_lik= 0.4385337 train_kl= -0.01155 train_loss= 0.45008 train_acc= 0.54230 val_roc= 0.92557 val_ap= 0.93115 time= 0.07667\n",
      "Optimization Finished!\n",
      "训练次数: 29 ROC score: 0.9143337570077307\n",
      "训练次数: 29 AP score: 0.9204708674848792\n",
      "训练次数: 30 Epoch: 0001 log_lik= 1.7956702 train_kl= -0.00004 train_loss= 1.79571 train_acc= 0.49624 val_roc= 0.67837 val_ap= 0.69749 time= 1.80737\n",
      "训练次数: 30 Epoch: 0002 log_lik= 1.5600381 train_kl= -0.00014 train_loss= 1.56018 train_acc= 0.48578 val_roc= 0.67723 val_ap= 0.69647 time= 0.08066\n",
      "训练次数: 30 Epoch: 0003 log_lik= 1.4311792 train_kl= -0.00044 train_loss= 1.43162 train_acc= 0.44827 val_roc= 0.67863 val_ap= 0.69979 time= 0.07668\n",
      "训练次数: 30 Epoch: 0004 log_lik= 1.3524324 train_kl= -0.00080 train_loss= 1.35323 train_acc= 0.39537 val_roc= 0.68137 val_ap= 0.70265 time= 0.07767\n",
      "训练次数: 30 Epoch: 0005 log_lik= 1.2506512 train_kl= -0.00111 train_loss= 1.25176 train_acc= 0.36081 val_roc= 0.68604 val_ap= 0.70937 time= 0.07568\n",
      "训练次数: 30 Epoch: 0006 log_lik= 1.132291 train_kl= -0.00135 train_loss= 1.13364 train_acc= 0.36057 val_roc= 0.68746 val_ap= 0.71384 time= 0.07568\n",
      "训练次数: 30 Epoch: 0007 log_lik= 1.0284048 train_kl= -0.00164 train_loss= 1.03004 train_acc= 0.35206 val_roc= 0.68149 val_ap= 0.71119 time= 0.07966\n",
      "训练次数: 30 Epoch: 0008 log_lik= 0.9308262 train_kl= -0.00200 train_loss= 0.93282 train_acc= 0.32564 val_roc= 0.68526 val_ap= 0.71221 time= 0.07568\n",
      "训练次数: 30 Epoch: 0009 log_lik= 0.8642917 train_kl= -0.00241 train_loss= 0.86670 train_acc= 0.33531 val_roc= 0.71179 val_ap= 0.72890 time= 0.07767\n",
      "训练次数: 30 Epoch: 0010 log_lik= 0.7883898 train_kl= -0.00289 train_loss= 0.79128 train_acc= 0.36951 val_roc= 0.75715 val_ap= 0.76061 time= 0.07568\n",
      "训练次数: 30 Epoch: 0011 log_lik= 0.74304724 train_kl= -0.00349 train_loss= 0.74654 train_acc= 0.39932 val_roc= 0.77723 val_ap= 0.77864 time= 0.09263\n",
      "训练次数: 30 Epoch: 0012 log_lik= 0.7206259 train_kl= -0.00419 train_loss= 0.72481 train_acc= 0.39768 val_roc= 0.76656 val_ap= 0.77563 time= 0.07568\n",
      "训练次数: 30 Epoch: 0013 log_lik= 0.69984436 train_kl= -0.00496 train_loss= 0.70481 train_acc= 0.33679 val_roc= 0.76059 val_ap= 0.77268 time= 0.07668\n",
      "训练次数: 30 Epoch: 0014 log_lik= 0.6884873 train_kl= -0.00577 train_loss= 0.69425 train_acc= 0.24091 val_roc= 0.77237 val_ap= 0.78132 time= 0.08664\n",
      "训练次数: 30 Epoch: 0015 log_lik= 0.68054557 train_kl= -0.00649 train_loss= 0.68703 train_acc= 0.20629 val_roc= 0.79361 val_ap= 0.79434 time= 0.07568\n",
      "训练次数: 30 Epoch: 0016 log_lik= 0.6601379 train_kl= -0.00711 train_loss= 0.66725 train_acc= 0.23225 val_roc= 0.80433 val_ap= 0.79640 time= 0.07667\n",
      "训练次数: 30 Epoch: 0017 log_lik= 0.64207697 train_kl= -0.00766 train_loss= 0.64974 train_acc= 0.28984 val_roc= 0.80623 val_ap= 0.79636 time= 0.07867\n",
      "训练次数: 30 Epoch: 0018 log_lik= 0.6328759 train_kl= -0.00816 train_loss= 0.64103 train_acc= 0.32333 val_roc= 0.80805 val_ap= 0.79996 time= 0.07767\n",
      "训练次数: 30 Epoch: 0019 log_lik= 0.6217063 train_kl= -0.00861 train_loss= 0.63031 train_acc= 0.35459 val_roc= 0.80965 val_ap= 0.80412 time= 0.07767\n",
      "训练次数: 30 Epoch: 0020 log_lik= 0.6181654 train_kl= -0.00900 train_loss= 0.62717 train_acc= 0.36752 val_roc= 0.81148 val_ap= 0.80757 time= 0.07767\n",
      "训练次数: 30 Epoch: 0021 log_lik= 0.61031795 train_kl= -0.00936 train_loss= 0.61968 train_acc= 0.39920 val_roc= 0.81129 val_ap= 0.80606 time= 0.07767\n",
      "训练次数: 30 Epoch: 0022 log_lik= 0.6015092 train_kl= -0.00968 train_loss= 0.61119 train_acc= 0.42353 val_roc= 0.81031 val_ap= 0.80395 time= 0.07767\n",
      "训练次数: 30 Epoch: 0023 log_lik= 0.5950434 train_kl= -0.00997 train_loss= 0.60501 train_acc= 0.44398 val_roc= 0.81156 val_ap= 0.80516 time= 0.07468\n",
      "训练次数: 30 Epoch: 0024 log_lik= 0.58976 train_kl= -0.01022 train_loss= 0.59998 train_acc= 0.45686 val_roc= 0.81593 val_ap= 0.80993 time= 0.07767\n",
      "训练次数: 30 Epoch: 0025 log_lik= 0.5813365 train_kl= -0.01043 train_loss= 0.59177 train_acc= 0.46423 val_roc= 0.82245 val_ap= 0.81668 time= 0.07568\n",
      "训练次数: 30 Epoch: 0026 log_lik= 0.5688842 train_kl= -0.01059 train_loss= 0.57947 train_acc= 0.47520 val_roc= 0.82989 val_ap= 0.82424 time= 0.07667\n",
      "训练次数: 30 Epoch: 0027 log_lik= 0.55566996 train_kl= -0.01069 train_loss= 0.56636 train_acc= 0.48308 val_roc= 0.83513 val_ap= 0.83095 time= 0.07867\n",
      "训练次数: 30 Epoch: 0028 log_lik= 0.5450082 train_kl= -0.01076 train_loss= 0.55577 train_acc= 0.48710 val_roc= 0.83728 val_ap= 0.83354 time= 0.07767\n",
      "训练次数: 30 Epoch: 0029 log_lik= 0.5412211 train_kl= -0.01081 train_loss= 0.55203 train_acc= 0.48610 val_roc= 0.83978 val_ap= 0.83543 time= 0.07468\n",
      "训练次数: 30 Epoch: 0030 log_lik= 0.5396488 train_kl= -0.01083 train_loss= 0.55048 train_acc= 0.47853 val_roc= 0.84159 val_ap= 0.83474 time= 0.08165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 30 Epoch: 0031 log_lik= 0.5371282 train_kl= -0.01084 train_loss= 0.54797 train_acc= 0.47827 val_roc= 0.84388 val_ap= 0.83343 time= 0.07966\n",
      "训练次数: 30 Epoch: 0032 log_lik= 0.5333793 train_kl= -0.01083 train_loss= 0.54421 train_acc= 0.47935 val_roc= 0.84457 val_ap= 0.83092 time= 0.08464\n",
      "训练次数: 30 Epoch: 0033 log_lik= 0.52976894 train_kl= -0.01082 train_loss= 0.54059 train_acc= 0.48417 val_roc= 0.84454 val_ap= 0.82914 time= 0.07468\n",
      "训练次数: 30 Epoch: 0034 log_lik= 0.5266355 train_kl= -0.01079 train_loss= 0.53743 train_acc= 0.48685 val_roc= 0.84463 val_ap= 0.82903 time= 0.07667\n",
      "训练次数: 30 Epoch: 0035 log_lik= 0.52335745 train_kl= -0.01074 train_loss= 0.53410 train_acc= 0.49330 val_roc= 0.84622 val_ap= 0.83233 time= 0.07468\n",
      "训练次数: 30 Epoch: 0036 log_lik= 0.5196588 train_kl= -0.01067 train_loss= 0.53033 train_acc= 0.50145 val_roc= 0.84687 val_ap= 0.83285 time= 0.07568\n",
      "训练次数: 30 Epoch: 0037 log_lik= 0.51665056 train_kl= -0.01059 train_loss= 0.52724 train_acc= 0.50253 val_roc= 0.84769 val_ap= 0.83495 time= 0.07767\n",
      "训练次数: 30 Epoch: 0038 log_lik= 0.5157269 train_kl= -0.01052 train_loss= 0.52624 train_acc= 0.50039 val_roc= 0.84831 val_ap= 0.83575 time= 0.07568\n",
      "训练次数: 30 Epoch: 0039 log_lik= 0.51329494 train_kl= -0.01045 train_loss= 0.52374 train_acc= 0.49873 val_roc= 0.85038 val_ap= 0.83680 time= 0.07468\n",
      "训练次数: 30 Epoch: 0040 log_lik= 0.5095023 train_kl= -0.01039 train_loss= 0.51989 train_acc= 0.50119 val_roc= 0.85336 val_ap= 0.83956 time= 0.07767\n",
      "训练次数: 30 Epoch: 0041 log_lik= 0.5059636 train_kl= -0.01033 train_loss= 0.51630 train_acc= 0.50232 val_roc= 0.85661 val_ap= 0.84298 time= 0.07468\n",
      "训练次数: 30 Epoch: 0042 log_lik= 0.50264394 train_kl= -0.01028 train_loss= 0.51292 train_acc= 0.50526 val_roc= 0.85903 val_ap= 0.84594 time= 0.07468\n",
      "训练次数: 30 Epoch: 0043 log_lik= 0.49910736 train_kl= -0.01020 train_loss= 0.50931 train_acc= 0.50758 val_roc= 0.86001 val_ap= 0.84762 time= 0.07867\n",
      "训练次数: 30 Epoch: 0044 log_lik= 0.49672863 train_kl= -0.01012 train_loss= 0.50685 train_acc= 0.51060 val_roc= 0.85986 val_ap= 0.84807 time= 0.07568\n",
      "训练次数: 30 Epoch: 0045 log_lik= 0.49420005 train_kl= -0.01003 train_loss= 0.50424 train_acc= 0.51224 val_roc= 0.85976 val_ap= 0.84790 time= 0.07468\n",
      "训练次数: 30 Epoch: 0046 log_lik= 0.49228138 train_kl= -0.00996 train_loss= 0.50224 train_acc= 0.51176 val_roc= 0.86125 val_ap= 0.84943 time= 0.08365\n",
      "训练次数: 30 Epoch: 0047 log_lik= 0.48954615 train_kl= -0.00988 train_loss= 0.49943 train_acc= 0.51229 val_roc= 0.86393 val_ap= 0.85210 time= 0.07667\n",
      "训练次数: 30 Epoch: 0048 log_lik= 0.4877819 train_kl= -0.00982 train_loss= 0.49760 train_acc= 0.51328 val_roc= 0.86618 val_ap= 0.85382 time= 0.07568\n",
      "训练次数: 30 Epoch: 0049 log_lik= 0.48716173 train_kl= -0.00975 train_loss= 0.49691 train_acc= 0.51258 val_roc= 0.86730 val_ap= 0.85495 time= 0.07966\n",
      "训练次数: 30 Epoch: 0050 log_lik= 0.48573595 train_kl= -0.00968 train_loss= 0.49542 train_acc= 0.51426 val_roc= 0.86757 val_ap= 0.85538 time= 0.07767\n",
      "训练次数: 30 Epoch: 0051 log_lik= 0.48353073 train_kl= -0.00962 train_loss= 0.49315 train_acc= 0.51542 val_roc= 0.86800 val_ap= 0.85586 time= 0.07468\n",
      "训练次数: 30 Epoch: 0052 log_lik= 0.48184168 train_kl= -0.00956 train_loss= 0.49140 train_acc= 0.51462 val_roc= 0.86952 val_ap= 0.85833 time= 0.07767\n",
      "训练次数: 30 Epoch: 0053 log_lik= 0.48106003 train_kl= -0.00951 train_loss= 0.49057 train_acc= 0.51647 val_roc= 0.87126 val_ap= 0.86074 time= 0.07568\n",
      "训练次数: 30 Epoch: 0054 log_lik= 0.47945 train_kl= -0.00949 train_loss= 0.48894 train_acc= 0.51742 val_roc= 0.87367 val_ap= 0.86438 time= 0.07468\n",
      "训练次数: 30 Epoch: 0055 log_lik= 0.47832128 train_kl= -0.00949 train_loss= 0.48781 train_acc= 0.51878 val_roc= 0.87533 val_ap= 0.86562 time= 0.07767\n",
      "训练次数: 30 Epoch: 0056 log_lik= 0.47690076 train_kl= -0.00950 train_loss= 0.48640 train_acc= 0.51910 val_roc= 0.87542 val_ap= 0.86412 time= 0.08066\n",
      "训练次数: 30 Epoch: 0057 log_lik= 0.4758438 train_kl= -0.00951 train_loss= 0.48535 train_acc= 0.52027 val_roc= 0.87581 val_ap= 0.86382 time= 0.07767\n",
      "训练次数: 30 Epoch: 0058 log_lik= 0.4744877 train_kl= -0.00952 train_loss= 0.48401 train_acc= 0.51931 val_roc= 0.87685 val_ap= 0.86506 time= 0.07966\n",
      "训练次数: 30 Epoch: 0059 log_lik= 0.4739029 train_kl= -0.00955 train_loss= 0.48345 train_acc= 0.51820 val_roc= 0.87941 val_ap= 0.86804 time= 0.07468\n",
      "训练次数: 30 Epoch: 0060 log_lik= 0.47259888 train_kl= -0.00958 train_loss= 0.48218 train_acc= 0.52004 val_roc= 0.88165 val_ap= 0.87133 time= 0.08265\n",
      "训练次数: 30 Epoch: 0061 log_lik= 0.4713032 train_kl= -0.00963 train_loss= 0.48093 train_acc= 0.52273 val_roc= 0.88330 val_ap= 0.87337 time= 0.07568\n",
      "训练次数: 30 Epoch: 0062 log_lik= 0.47054175 train_kl= -0.00967 train_loss= 0.48021 train_acc= 0.52248 val_roc= 0.88417 val_ap= 0.87316 time= 0.08364\n",
      "训练次数: 30 Epoch: 0063 log_lik= 0.46926722 train_kl= -0.00971 train_loss= 0.47897 train_acc= 0.52330 val_roc= 0.88473 val_ap= 0.87318 time= 0.07668\n",
      "训练次数: 30 Epoch: 0064 log_lik= 0.46845895 train_kl= -0.00974 train_loss= 0.47820 train_acc= 0.52219 val_roc= 0.88477 val_ap= 0.87337 time= 0.07724\n",
      "训练次数: 30 Epoch: 0065 log_lik= 0.4677138 train_kl= -0.00978 train_loss= 0.47750 train_acc= 0.52325 val_roc= 0.88629 val_ap= 0.87430 time= 0.07468\n",
      "训练次数: 30 Epoch: 0066 log_lik= 0.46693543 train_kl= -0.00984 train_loss= 0.47677 train_acc= 0.52259 val_roc= 0.88748 val_ap= 0.87492 time= 0.07568\n",
      "训练次数: 30 Epoch: 0067 log_lik= 0.46581224 train_kl= -0.00990 train_loss= 0.47571 train_acc= 0.52472 val_roc= 0.88901 val_ap= 0.87724 time= 0.07867\n",
      "训练次数: 30 Epoch: 0068 log_lik= 0.4651469 train_kl= -0.00995 train_loss= 0.47509 train_acc= 0.52375 val_roc= 0.89030 val_ap= 0.87846 time= 0.07568\n",
      "训练次数: 30 Epoch: 0069 log_lik= 0.4644923 train_kl= -0.00998 train_loss= 0.47448 train_acc= 0.52484 val_roc= 0.89090 val_ap= 0.87838 time= 0.07667\n",
      "训练次数: 30 Epoch: 0070 log_lik= 0.46316552 train_kl= -0.01002 train_loss= 0.47318 train_acc= 0.52501 val_roc= 0.89135 val_ap= 0.87824 time= 0.08464\n",
      "训练次数: 30 Epoch: 0071 log_lik= 0.4632328 train_kl= -0.01006 train_loss= 0.47329 train_acc= 0.52349 val_roc= 0.89151 val_ap= 0.87781 time= 0.07767\n",
      "训练次数: 30 Epoch: 0072 log_lik= 0.46200284 train_kl= -0.01010 train_loss= 0.47210 train_acc= 0.52560 val_roc= 0.89239 val_ap= 0.87876 time= 0.07369\n",
      "训练次数: 30 Epoch: 0073 log_lik= 0.46112645 train_kl= -0.01015 train_loss= 0.47128 train_acc= 0.52458 val_roc= 0.89326 val_ap= 0.88032 time= 0.07667\n",
      "训练次数: 30 Epoch: 0074 log_lik= 0.4609267 train_kl= -0.01019 train_loss= 0.47112 train_acc= 0.52474 val_roc= 0.89437 val_ap= 0.88173 time= 0.07468\n",
      "训练次数: 30 Epoch: 0075 log_lik= 0.45964217 train_kl= -0.01023 train_loss= 0.46987 train_acc= 0.52365 val_roc= 0.89504 val_ap= 0.88245 time= 0.07468\n",
      "训练次数: 30 Epoch: 0076 log_lik= 0.4594673 train_kl= -0.01026 train_loss= 0.46972 train_acc= 0.52468 val_roc= 0.89529 val_ap= 0.88264 time= 0.07667\n",
      "训练次数: 30 Epoch: 0077 log_lik= 0.45873606 train_kl= -0.01029 train_loss= 0.46902 train_acc= 0.52690 val_roc= 0.89546 val_ap= 0.88251 time= 0.07668\n",
      "训练次数: 30 Epoch: 0078 log_lik= 0.45731872 train_kl= -0.01033 train_loss= 0.46765 train_acc= 0.52495 val_roc= 0.89657 val_ap= 0.88408 time= 0.07468\n",
      "训练次数: 30 Epoch: 0079 log_lik= 0.45728278 train_kl= -0.01038 train_loss= 0.46766 train_acc= 0.52622 val_roc= 0.89748 val_ap= 0.88515 time= 0.07468\n",
      "训练次数: 30 Epoch: 0080 log_lik= 0.45646036 train_kl= -0.01042 train_loss= 0.46688 train_acc= 0.52617 val_roc= 0.89842 val_ap= 0.88618 time= 0.07468\n",
      "训练次数: 30 Epoch: 0081 log_lik= 0.45580876 train_kl= -0.01046 train_loss= 0.46627 train_acc= 0.52731 val_roc= 0.89873 val_ap= 0.88657 time= 0.07767\n",
      "训练次数: 30 Epoch: 0082 log_lik= 0.45489082 train_kl= -0.01049 train_loss= 0.46539 train_acc= 0.52765 val_roc= 0.89933 val_ap= 0.88709 time= 0.07828\n",
      "训练次数: 30 Epoch: 0083 log_lik= 0.45457724 train_kl= -0.01053 train_loss= 0.46511 train_acc= 0.52780 val_roc= 0.89967 val_ap= 0.88729 time= 0.07568\n",
      "训练次数: 30 Epoch: 0084 log_lik= 0.45318097 train_kl= -0.01057 train_loss= 0.46375 train_acc= 0.52918 val_roc= 0.90068 val_ap= 0.88895 time= 0.07767\n",
      "训练次数: 30 Epoch: 0085 log_lik= 0.45312262 train_kl= -0.01061 train_loss= 0.46373 train_acc= 0.52810 val_roc= 0.90218 val_ap= 0.89079 time= 0.07568\n",
      "训练次数: 30 Epoch: 0086 log_lik= 0.4524566 train_kl= -0.01064 train_loss= 0.46310 train_acc= 0.52925 val_roc= 0.90269 val_ap= 0.89138 time= 0.07575\n",
      "训练次数: 30 Epoch: 0087 log_lik= 0.45272344 train_kl= -0.01067 train_loss= 0.46340 train_acc= 0.52864 val_roc= 0.90273 val_ap= 0.89113 time= 0.08364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 30 Epoch: 0088 log_lik= 0.4515523 train_kl= -0.01070 train_loss= 0.46226 train_acc= 0.52968 val_roc= 0.90301 val_ap= 0.89115 time= 0.08165\n",
      "训练次数: 30 Epoch: 0089 log_lik= 0.45102078 train_kl= -0.01073 train_loss= 0.46176 train_acc= 0.52994 val_roc= 0.90403 val_ap= 0.89254 time= 0.07568\n",
      "训练次数: 30 Epoch: 0090 log_lik= 0.45120478 train_kl= -0.01076 train_loss= 0.46197 train_acc= 0.53043 val_roc= 0.90530 val_ap= 0.89405 time= 0.07867\n",
      "训练次数: 30 Epoch: 0091 log_lik= 0.45029378 train_kl= -0.01079 train_loss= 0.46108 train_acc= 0.53111 val_roc= 0.90597 val_ap= 0.89433 time= 0.07767\n",
      "训练次数: 30 Epoch: 0092 log_lik= 0.45019487 train_kl= -0.01081 train_loss= 0.46101 train_acc= 0.53201 val_roc= 0.90567 val_ap= 0.89338 time= 0.07667\n",
      "训练次数: 30 Epoch: 0093 log_lik= 0.44991362 train_kl= -0.01084 train_loss= 0.46075 train_acc= 0.53168 val_roc= 0.90526 val_ap= 0.89244 time= 0.07568\n",
      "训练次数: 30 Epoch: 0094 log_lik= 0.4496726 train_kl= -0.01086 train_loss= 0.46053 train_acc= 0.53107 val_roc= 0.90604 val_ap= 0.89370 time= 0.07966\n",
      "训练次数: 30 Epoch: 0095 log_lik= 0.4489582 train_kl= -0.01088 train_loss= 0.45984 train_acc= 0.53090 val_roc= 0.90704 val_ap= 0.89448 time= 0.07767\n",
      "训练次数: 30 Epoch: 0096 log_lik= 0.44893265 train_kl= -0.01090 train_loss= 0.45983 train_acc= 0.53242 val_roc= 0.90772 val_ap= 0.89531 time= 0.07767\n",
      "训练次数: 30 Epoch: 0097 log_lik= 0.4483427 train_kl= -0.01091 train_loss= 0.45926 train_acc= 0.53212 val_roc= 0.90726 val_ap= 0.89463 time= 0.07867\n",
      "训练次数: 30 Epoch: 0098 log_lik= 0.4482508 train_kl= -0.01092 train_loss= 0.45917 train_acc= 0.53206 val_roc= 0.90698 val_ap= 0.89338 time= 0.07568\n",
      "训练次数: 30 Epoch: 0099 log_lik= 0.44804895 train_kl= -0.01093 train_loss= 0.45898 train_acc= 0.53143 val_roc= 0.90695 val_ap= 0.89352 time= 0.08189\n",
      "训练次数: 30 Epoch: 0100 log_lik= 0.44776386 train_kl= -0.01094 train_loss= 0.45870 train_acc= 0.53160 val_roc= 0.90714 val_ap= 0.89392 time= 0.07568\n",
      "Optimization Finished!\n",
      "训练次数: 30 ROC score: 0.9212865779230833\n",
      "训练次数: 30 AP score: 0.9242826695736127\n",
      "Average Test ROC score: 0.918578122318279\n",
      "Average Test AP score: 0.9242749948978172\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始VAE结果\n",
    "# 实验30次结果\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1eee390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "训练次数: 1 Epoch: 0001 log_lik= 0.7820681 train_kl= 0.00805 train_loss= 0.79012 train_acc= 0.07911 val_roc= 0.71340 val_ap= 0.73726 time= 8.05920\n",
      "训练次数: 1 Epoch: 0002 log_lik= 0.8028706 train_kl= 0.00831 train_loss= 0.81118 train_acc= 0.00160 val_roc= 0.71445 val_ap= 0.74419 time= 0.12447\n",
      "训练次数: 1 Epoch: 0003 log_lik= 0.72450846 train_kl= 0.00816 train_loss= 0.73267 train_acc= 0.00873 val_roc= 0.70120 val_ap= 0.73817 time= 0.10854\n",
      "训练次数: 1 Epoch: 0004 log_lik= 0.74238646 train_kl= 0.00825 train_loss= 0.75063 train_acc= 0.00769 val_roc= 0.75817 val_ap= 0.77846 time= 0.12049\n",
      "训练次数: 1 Epoch: 0005 log_lik= 0.7042623 train_kl= 0.00818 train_loss= 0.71244 train_acc= 0.05555 val_roc= 0.81719 val_ap= 0.82621 time= 0.11053\n",
      "训练次数: 1 Epoch: 0006 log_lik= 0.68816113 train_kl= 0.00816 train_loss= 0.69632 train_acc= 0.16630 val_roc= 0.82935 val_ap= 0.82796 time= 0.11750\n",
      "训练次数: 1 Epoch: 0007 log_lik= 0.66772884 train_kl= 0.00818 train_loss= 0.67591 train_acc= 0.22629 val_roc= 0.82530 val_ap= 0.82054 time= 0.11651\n",
      "训练次数: 1 Epoch: 0008 log_lik= 0.6471079 train_kl= 0.00824 train_loss= 0.65535 train_acc= 0.25764 val_roc= 0.83428 val_ap= 0.83086 time= 0.10954\n",
      "训练次数: 1 Epoch: 0009 log_lik= 0.6200315 train_kl= 0.00831 train_loss= 0.62834 train_acc= 0.32488 val_roc= 0.83938 val_ap= 0.83719 time= 0.10953\n",
      "训练次数: 1 Epoch: 0010 log_lik= 0.60067874 train_kl= 0.00837 train_loss= 0.60905 train_acc= 0.37419 val_roc= 0.84497 val_ap= 0.84308 time= 0.10655\n",
      "训练次数: 1 Epoch: 0011 log_lik= 0.59114194 train_kl= 0.00842 train_loss= 0.59957 train_acc= 0.40197 val_roc= 0.85397 val_ap= 0.85333 time= 0.10953\n",
      "训练次数: 1 Epoch: 0012 log_lik= 0.5776571 train_kl= 0.00845 train_loss= 0.58611 train_acc= 0.42615 val_roc= 0.86069 val_ap= 0.86141 time= 0.12550\n",
      "训练次数: 1 Epoch: 0013 log_lik= 0.56187856 train_kl= 0.00845 train_loss= 0.57033 train_acc= 0.45318 val_roc= 0.86550 val_ap= 0.86678 time= 0.10854\n",
      "训练次数: 1 Epoch: 0014 log_lik= 0.54916024 train_kl= 0.00844 train_loss= 0.55760 train_acc= 0.47687 val_roc= 0.86884 val_ap= 0.87097 time= 0.11053\n",
      "训练次数: 1 Epoch: 0015 log_lik= 0.53987163 train_kl= 0.00844 train_loss= 0.54831 train_acc= 0.49560 val_roc= 0.87155 val_ap= 0.87379 time= 0.10953\n",
      "训练次数: 1 Epoch: 0016 log_lik= 0.53348726 train_kl= 0.00844 train_loss= 0.54193 train_acc= 0.50572 val_roc= 0.87448 val_ap= 0.87670 time= 0.10954\n",
      "训练次数: 1 Epoch: 0017 log_lik= 0.5287644 train_kl= 0.00845 train_loss= 0.53722 train_acc= 0.51015 val_roc= 0.87691 val_ap= 0.87873 time= 0.10754\n",
      "训练次数: 1 Epoch: 0018 log_lik= 0.52536553 train_kl= 0.00847 train_loss= 0.53384 train_acc= 0.51096 val_roc= 0.87788 val_ap= 0.87804 time= 0.12148\n",
      "训练次数: 1 Epoch: 0019 log_lik= 0.52391744 train_kl= 0.00850 train_loss= 0.53242 train_acc= 0.51142 val_roc= 0.87804 val_ap= 0.87835 time= 0.10655\n",
      "训练次数: 1 Epoch: 0020 log_lik= 0.52334654 train_kl= 0.00852 train_loss= 0.53187 train_acc= 0.51259 val_roc= 0.87808 val_ap= 0.87907 time= 0.11203\n",
      "训练次数: 1 Epoch: 0021 log_lik= 0.52092415 train_kl= 0.00854 train_loss= 0.52946 train_acc= 0.51739 val_roc= 0.87841 val_ap= 0.88218 time= 0.10754\n",
      "训练次数: 1 Epoch: 0022 log_lik= 0.51725733 train_kl= 0.00854 train_loss= 0.52580 train_acc= 0.52215 val_roc= 0.87917 val_ap= 0.88455 time= 0.12049\n",
      "训练次数: 1 Epoch: 0023 log_lik= 0.51375955 train_kl= 0.00854 train_loss= 0.52230 train_acc= 0.52517 val_roc= 0.88190 val_ap= 0.88841 time= 0.11949\n",
      "训练次数: 1 Epoch: 0024 log_lik= 0.5107371 train_kl= 0.00853 train_loss= 0.51927 train_acc= 0.52728 val_roc= 0.88449 val_ap= 0.89199 time= 0.10655\n",
      "训练次数: 1 Epoch: 0025 log_lik= 0.5080346 train_kl= 0.00852 train_loss= 0.51656 train_acc= 0.52857 val_roc= 0.88686 val_ap= 0.89443 time= 0.10854\n",
      "训练次数: 1 Epoch: 0026 log_lik= 0.5046668 train_kl= 0.00852 train_loss= 0.51318 train_acc= 0.53112 val_roc= 0.88882 val_ap= 0.89611 time= 0.10854\n",
      "训练次数: 1 Epoch: 0027 log_lik= 0.5014153 train_kl= 0.00851 train_loss= 0.50993 train_acc= 0.53466 val_roc= 0.89082 val_ap= 0.89720 time= 0.10954\n",
      "训练次数: 1 Epoch: 0028 log_lik= 0.49864155 train_kl= 0.00851 train_loss= 0.50715 train_acc= 0.53613 val_roc= 0.89278 val_ap= 0.89881 time= 0.12547\n",
      "训练次数: 1 Epoch: 0029 log_lik= 0.49604252 train_kl= 0.00852 train_loss= 0.50456 train_acc= 0.53817 val_roc= 0.89410 val_ap= 0.89972 time= 0.11849\n",
      "训练次数: 1 Epoch: 0030 log_lik= 0.493426 train_kl= 0.00853 train_loss= 0.50195 train_acc= 0.54023 val_roc= 0.89565 val_ap= 0.90131 time= 0.11153\n",
      "训练次数: 1 Epoch: 0031 log_lik= 0.49058926 train_kl= 0.00854 train_loss= 0.49913 train_acc= 0.54224 val_roc= 0.89774 val_ap= 0.90295 time= 0.10953\n",
      "训练次数: 1 Epoch: 0032 log_lik= 0.4871916 train_kl= 0.00855 train_loss= 0.49574 train_acc= 0.54451 val_roc= 0.89909 val_ap= 0.90380 time= 0.10953\n",
      "训练次数: 1 Epoch: 0033 log_lik= 0.4844665 train_kl= 0.00856 train_loss= 0.49302 train_acc= 0.54754 val_roc= 0.90009 val_ap= 0.90401 time= 0.10953\n",
      "训练次数: 1 Epoch: 0034 log_lik= 0.4819346 train_kl= 0.00857 train_loss= 0.49050 train_acc= 0.55002 val_roc= 0.90104 val_ap= 0.90421 time= 0.12845\n",
      "训练次数: 1 Epoch: 0035 log_lik= 0.47939694 train_kl= 0.00857 train_loss= 0.48797 train_acc= 0.55339 val_roc= 0.90130 val_ap= 0.90429 time= 0.12049\n",
      "训练次数: 1 Epoch: 0036 log_lik= 0.47734696 train_kl= 0.00858 train_loss= 0.48593 train_acc= 0.55626 val_roc= 0.90238 val_ap= 0.90494 time= 0.12081\n",
      "训练次数: 1 Epoch: 0037 log_lik= 0.47586104 train_kl= 0.00859 train_loss= 0.48445 train_acc= 0.55848 val_roc= 0.90329 val_ap= 0.90455 time= 0.12447\n",
      "训练次数: 1 Epoch: 0038 log_lik= 0.47446862 train_kl= 0.00859 train_loss= 0.48306 train_acc= 0.55940 val_roc= 0.90418 val_ap= 0.90381 time= 0.12148\n",
      "训练次数: 1 Epoch: 0039 log_lik= 0.47312644 train_kl= 0.00859 train_loss= 0.48172 train_acc= 0.55962 val_roc= 0.90457 val_ap= 0.90277 time= 0.10754\n",
      "训练次数: 1 Epoch: 0040 log_lik= 0.47198865 train_kl= 0.00859 train_loss= 0.48058 train_acc= 0.56057 val_roc= 0.90502 val_ap= 0.90250 time= 0.10953\n",
      "训练次数: 1 Epoch: 0041 log_lik= 0.47094125 train_kl= 0.00859 train_loss= 0.47953 train_acc= 0.56146 val_roc= 0.90593 val_ap= 0.90274 time= 0.12148\n",
      "训练次数: 1 Epoch: 0042 log_lik= 0.46965086 train_kl= 0.00858 train_loss= 0.47823 train_acc= 0.56187 val_roc= 0.90633 val_ap= 0.90282 time= 0.12547\n",
      "训练次数: 1 Epoch: 0043 log_lik= 0.46837813 train_kl= 0.00858 train_loss= 0.47695 train_acc= 0.56338 val_roc= 0.90707 val_ap= 0.90374 time= 0.11650\n",
      "训练次数: 1 Epoch: 0044 log_lik= 0.46696967 train_kl= 0.00858 train_loss= 0.47555 train_acc= 0.56426 val_roc= 0.90782 val_ap= 0.90596 time= 0.11053\n",
      "训练次数: 1 Epoch: 0045 log_lik= 0.46556908 train_kl= 0.00858 train_loss= 0.47415 train_acc= 0.56557 val_roc= 0.90922 val_ap= 0.90876 time= 0.10754\n",
      "训练次数: 1 Epoch: 0046 log_lik= 0.4642541 train_kl= 0.00858 train_loss= 0.47284 train_acc= 0.56661 val_roc= 0.91002 val_ap= 0.91066 time= 0.11750\n",
      "训练次数: 1 Epoch: 0047 log_lik= 0.4632509 train_kl= 0.00859 train_loss= 0.47184 train_acc= 0.56760 val_roc= 0.91073 val_ap= 0.91232 time= 0.12144\n",
      "训练次数: 1 Epoch: 0048 log_lik= 0.4624426 train_kl= 0.00860 train_loss= 0.47104 train_acc= 0.56877 val_roc= 0.91139 val_ap= 0.91316 time= 0.11451\n",
      "训练次数: 1 Epoch: 0049 log_lik= 0.4616704 train_kl= 0.00861 train_loss= 0.47028 train_acc= 0.56943 val_roc= 0.91139 val_ap= 0.91367 time= 0.11602\n",
      "训练次数: 1 Epoch: 0050 log_lik= 0.46090943 train_kl= 0.00861 train_loss= 0.46952 train_acc= 0.57034 val_roc= 0.91126 val_ap= 0.91375 time= 0.10854\n",
      "训练次数: 1 Epoch: 0051 log_lik= 0.46022913 train_kl= 0.00861 train_loss= 0.46884 train_acc= 0.57070 val_roc= 0.91094 val_ap= 0.91388 time= 0.11153\n",
      "训练次数: 1 Epoch: 0052 log_lik= 0.45953748 train_kl= 0.00861 train_loss= 0.46815 train_acc= 0.57011 val_roc= 0.91125 val_ap= 0.91459 time= 0.11153\n",
      "训练次数: 1 Epoch: 0053 log_lik= 0.45876974 train_kl= 0.00861 train_loss= 0.46738 train_acc= 0.57020 val_roc= 0.91185 val_ap= 0.91597 time= 0.11153\n",
      "训练次数: 1 Epoch: 0054 log_lik= 0.45790797 train_kl= 0.00861 train_loss= 0.46651 train_acc= 0.57114 val_roc= 0.91308 val_ap= 0.91769 time= 0.11053\n",
      "训练次数: 1 Epoch: 0055 log_lik= 0.4570145 train_kl= 0.00860 train_loss= 0.46562 train_acc= 0.57193 val_roc= 0.91385 val_ap= 0.91881 time= 0.11152\n",
      "训练次数: 1 Epoch: 0056 log_lik= 0.45592397 train_kl= 0.00860 train_loss= 0.46453 train_acc= 0.57318 val_roc= 0.91456 val_ap= 0.91985 time= 0.12846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 1 Epoch: 0057 log_lik= 0.4552712 train_kl= 0.00861 train_loss= 0.46388 train_acc= 0.57331 val_roc= 0.91529 val_ap= 0.92070 time= 0.10854\n",
      "训练次数: 1 Epoch: 0058 log_lik= 0.45451254 train_kl= 0.00861 train_loss= 0.46312 train_acc= 0.57374 val_roc= 0.91535 val_ap= 0.92058 time= 0.10854\n",
      "训练次数: 1 Epoch: 0059 log_lik= 0.45395756 train_kl= 0.00861 train_loss= 0.46257 train_acc= 0.57343 val_roc= 0.91554 val_ap= 0.92071 time= 0.11252\n",
      "训练次数: 1 Epoch: 0060 log_lik= 0.45319128 train_kl= 0.00861 train_loss= 0.46180 train_acc= 0.57292 val_roc= 0.91521 val_ap= 0.92002 time= 0.11352\n",
      "训练次数: 1 Epoch: 0061 log_lik= 0.4525591 train_kl= 0.00862 train_loss= 0.46118 train_acc= 0.57329 val_roc= 0.91542 val_ap= 0.92025 time= 0.12547\n",
      "训练次数: 1 Epoch: 0062 log_lik= 0.45190468 train_kl= 0.00862 train_loss= 0.46053 train_acc= 0.57294 val_roc= 0.91573 val_ap= 0.92066 time= 0.11352\n",
      "训练次数: 1 Epoch: 0063 log_lik= 0.45124996 train_kl= 0.00862 train_loss= 0.45987 train_acc= 0.57326 val_roc= 0.91605 val_ap= 0.92154 time= 0.11503\n",
      "训练次数: 1 Epoch: 0064 log_lik= 0.45057416 train_kl= 0.00863 train_loss= 0.45920 train_acc= 0.57361 val_roc= 0.91664 val_ap= 0.92269 time= 0.12542\n",
      "训练次数: 1 Epoch: 0065 log_lik= 0.44972247 train_kl= 0.00863 train_loss= 0.45835 train_acc= 0.57443 val_roc= 0.91729 val_ap= 0.92361 time= 0.10854\n",
      "训练次数: 1 Epoch: 0066 log_lik= 0.44903097 train_kl= 0.00863 train_loss= 0.45766 train_acc= 0.57542 val_roc= 0.91787 val_ap= 0.92472 time= 0.11053\n",
      "训练次数: 1 Epoch: 0067 log_lik= 0.44824836 train_kl= 0.00863 train_loss= 0.45688 train_acc= 0.57630 val_roc= 0.91806 val_ap= 0.92534 time= 0.12447\n",
      "训练次数: 1 Epoch: 0068 log_lik= 0.44745082 train_kl= 0.00863 train_loss= 0.45608 train_acc= 0.57732 val_roc= 0.91765 val_ap= 0.92494 time= 0.11053\n",
      "训练次数: 1 Epoch: 0069 log_lik= 0.44683555 train_kl= 0.00863 train_loss= 0.45547 train_acc= 0.57807 val_roc= 0.91761 val_ap= 0.92501 time= 0.11352\n",
      "训练次数: 1 Epoch: 0070 log_lik= 0.44608414 train_kl= 0.00863 train_loss= 0.45472 train_acc= 0.57899 val_roc= 0.91767 val_ap= 0.92493 time= 0.11153\n",
      "训练次数: 1 Epoch: 0071 log_lik= 0.4454399 train_kl= 0.00864 train_loss= 0.45408 train_acc= 0.57934 val_roc= 0.91774 val_ap= 0.92517 time= 0.12049\n",
      "训练次数: 1 Epoch: 0072 log_lik= 0.4448774 train_kl= 0.00864 train_loss= 0.45352 train_acc= 0.57987 val_roc= 0.91761 val_ap= 0.92503 time= 0.11053\n",
      "训练次数: 1 Epoch: 0073 log_lik= 0.4442536 train_kl= 0.00864 train_loss= 0.45290 train_acc= 0.58125 val_roc= 0.91756 val_ap= 0.92507 time= 0.11252\n",
      "训练次数: 1 Epoch: 0074 log_lik= 0.4434471 train_kl= 0.00865 train_loss= 0.45210 train_acc= 0.58131 val_roc= 0.91769 val_ap= 0.92517 time= 0.12746\n",
      "训练次数: 1 Epoch: 0075 log_lik= 0.44276792 train_kl= 0.00865 train_loss= 0.45142 train_acc= 0.58328 val_roc= 0.91774 val_ap= 0.92525 time= 0.12248\n",
      "训练次数: 1 Epoch: 0076 log_lik= 0.44212064 train_kl= 0.00866 train_loss= 0.45078 train_acc= 0.58295 val_roc= 0.91806 val_ap= 0.92549 time= 0.10754\n",
      "训练次数: 1 Epoch: 0077 log_lik= 0.44156 train_kl= 0.00866 train_loss= 0.45022 train_acc= 0.58339 val_roc= 0.91823 val_ap= 0.92531 time= 0.11152\n",
      "训练次数: 1 Epoch: 0078 log_lik= 0.44088185 train_kl= 0.00866 train_loss= 0.44954 train_acc= 0.58412 val_roc= 0.91839 val_ap= 0.92532 time= 0.11551\n",
      "训练次数: 1 Epoch: 0079 log_lik= 0.4402478 train_kl= 0.00866 train_loss= 0.44891 train_acc= 0.58442 val_roc= 0.91869 val_ap= 0.92546 time= 0.11850\n",
      "训练次数: 1 Epoch: 0080 log_lik= 0.43961477 train_kl= 0.00867 train_loss= 0.44828 train_acc= 0.58535 val_roc= 0.91939 val_ap= 0.92622 time= 0.10954\n",
      "训练次数: 1 Epoch: 0081 log_lik= 0.43907434 train_kl= 0.00867 train_loss= 0.44774 train_acc= 0.58556 val_roc= 0.91978 val_ap= 0.92671 time= 0.11153\n",
      "训练次数: 1 Epoch: 0082 log_lik= 0.43845826 train_kl= 0.00867 train_loss= 0.44713 train_acc= 0.58601 val_roc= 0.92053 val_ap= 0.92756 time= 0.11053\n",
      "训练次数: 1 Epoch: 0083 log_lik= 0.4378697 train_kl= 0.00867 train_loss= 0.44654 train_acc= 0.58632 val_roc= 0.92098 val_ap= 0.92798 time= 0.12447\n",
      "训练次数: 1 Epoch: 0084 log_lik= 0.43730447 train_kl= 0.00867 train_loss= 0.44598 train_acc= 0.58689 val_roc= 0.92166 val_ap= 0.92872 time= 0.10655\n",
      "训练次数: 1 Epoch: 0085 log_lik= 0.43676093 train_kl= 0.00867 train_loss= 0.44543 train_acc= 0.58784 val_roc= 0.92181 val_ap= 0.92898 time= 0.11252\n",
      "训练次数: 1 Epoch: 0086 log_lik= 0.4362775 train_kl= 0.00868 train_loss= 0.44495 train_acc= 0.58834 val_roc= 0.92199 val_ap= 0.92909 time= 0.10953\n",
      "训练次数: 1 Epoch: 0087 log_lik= 0.43575507 train_kl= 0.00868 train_loss= 0.44443 train_acc= 0.58883 val_roc= 0.92222 val_ap= 0.92928 time= 0.12149\n",
      "训练次数: 1 Epoch: 0088 log_lik= 0.43530217 train_kl= 0.00868 train_loss= 0.44398 train_acc= 0.58954 val_roc= 0.92251 val_ap= 0.92947 time= 0.12248\n",
      "训练次数: 1 Epoch: 0089 log_lik= 0.4348011 train_kl= 0.00868 train_loss= 0.44349 train_acc= 0.59017 val_roc= 0.92291 val_ap= 0.92996 time= 0.10754\n",
      "训练次数: 1 Epoch: 0090 log_lik= 0.43430135 train_kl= 0.00869 train_loss= 0.44299 train_acc= 0.59072 val_roc= 0.92323 val_ap= 0.93029 time= 0.10754\n",
      "训练次数: 1 Epoch: 0091 log_lik= 0.4339021 train_kl= 0.00869 train_loss= 0.44259 train_acc= 0.59090 val_roc= 0.92320 val_ap= 0.93030 time= 0.10754\n",
      "训练次数: 1 Epoch: 0092 log_lik= 0.43340775 train_kl= 0.00869 train_loss= 0.44210 train_acc= 0.59155 val_roc= 0.92339 val_ap= 0.93075 time= 0.10953\n",
      "训练次数: 1 Epoch: 0093 log_lik= 0.43298334 train_kl= 0.00869 train_loss= 0.44167 train_acc= 0.59169 val_roc= 0.92332 val_ap= 0.93064 time= 0.10854\n",
      "训练次数: 1 Epoch: 0094 log_lik= 0.432537 train_kl= 0.00869 train_loss= 0.44123 train_acc= 0.59291 val_roc= 0.92367 val_ap= 0.93108 time= 0.11650\n",
      "训练次数: 1 Epoch: 0095 log_lik= 0.43210766 train_kl= 0.00869 train_loss= 0.44080 train_acc= 0.59299 val_roc= 0.92394 val_ap= 0.93145 time= 0.11153\n",
      "训练次数: 1 Epoch: 0096 log_lik= 0.43176898 train_kl= 0.00869 train_loss= 0.44046 train_acc= 0.59327 val_roc= 0.92411 val_ap= 0.93166 time= 0.11252\n",
      "训练次数: 1 Epoch: 0097 log_lik= 0.43137294 train_kl= 0.00869 train_loss= 0.44006 train_acc= 0.59367 val_roc= 0.92426 val_ap= 0.93191 time= 0.12049\n",
      "训练次数: 1 Epoch: 0098 log_lik= 0.43097457 train_kl= 0.00869 train_loss= 0.43967 train_acc= 0.59422 val_roc= 0.92440 val_ap= 0.93211 time= 0.10854\n",
      "训练次数: 1 Epoch: 0099 log_lik= 0.43056014 train_kl= 0.00869 train_loss= 0.43925 train_acc= 0.59465 val_roc= 0.92482 val_ap= 0.93262 time= 0.11153\n",
      "训练次数: 1 Epoch: 0100 log_lik= 0.43030602 train_kl= 0.00869 train_loss= 0.43900 train_acc= 0.59450 val_roc= 0.92485 val_ap= 0.93277 time= 0.10854\n",
      "Optimization Finished!\n",
      "训练次数: 1 ROC score: 0.9108627474984606\n",
      "训练次数: 1 AP score: 0.9123503736079794\n",
      "训练次数: 2 Epoch: 0001 log_lik= 0.77892864 train_kl= 0.00806 train_loss= 0.78699 train_acc= 0.02730 val_roc= 0.65712 val_ap= 0.68579 time= 7.63669\n",
      "训练次数: 2 Epoch: 0002 log_lik= 0.9666541 train_kl= 0.00845 train_loss= 0.97511 train_acc= 0.00159 val_roc= 0.69452 val_ap= 0.71664 time= 0.12746\n",
      "训练次数: 2 Epoch: 0003 log_lik= 0.73300725 train_kl= 0.00814 train_loss= 0.74115 train_acc= 0.00425 val_roc= 0.63648 val_ap= 0.66843 time= 0.12447\n",
      "训练次数: 2 Epoch: 0004 log_lik= 0.74626493 train_kl= 0.00821 train_loss= 0.75448 train_acc= 0.00433 val_roc= 0.65665 val_ap= 0.68836 time= 0.10953\n",
      "训练次数: 2 Epoch: 0005 log_lik= 0.7357521 train_kl= 0.00820 train_loss= 0.74395 train_acc= 0.00437 val_roc= 0.69340 val_ap= 0.71961 time= 0.11153\n",
      "训练次数: 2 Epoch: 0006 log_lik= 0.72077626 train_kl= 0.00817 train_loss= 0.72894 train_acc= 0.01083 val_roc= 0.72905 val_ap= 0.73593 time= 0.11650\n",
      "训练次数: 2 Epoch: 0007 log_lik= 0.7086562 train_kl= 0.00816 train_loss= 0.71682 train_acc= 0.05087 val_roc= 0.75957 val_ap= 0.75256 time= 0.11252\n",
      "训练次数: 2 Epoch: 0008 log_lik= 0.6881594 train_kl= 0.00818 train_loss= 0.69634 train_acc= 0.15416 val_roc= 0.78164 val_ap= 0.77426 time= 0.11551\n",
      "训练次数: 2 Epoch: 0009 log_lik= 0.6600423 train_kl= 0.00821 train_loss= 0.66826 train_acc= 0.25087 val_roc= 0.78996 val_ap= 0.78237 time= 0.10555\n",
      "训练次数: 2 Epoch: 0010 log_lik= 0.6336248 train_kl= 0.00826 train_loss= 0.64189 train_acc= 0.31427 val_roc= 0.79426 val_ap= 0.78612 time= 0.11551\n",
      "训练次数: 2 Epoch: 0011 log_lik= 0.6159407 train_kl= 0.00832 train_loss= 0.62426 train_acc= 0.36533 val_roc= 0.80130 val_ap= 0.78792 time= 0.11056\n",
      "训练次数: 2 Epoch: 0012 log_lik= 0.6037081 train_kl= 0.00838 train_loss= 0.61208 train_acc= 0.41401 val_roc= 0.80498 val_ap= 0.78718 time= 0.12148\n",
      "训练次数: 2 Epoch: 0013 log_lik= 0.5955239 train_kl= 0.00842 train_loss= 0.60394 train_acc= 0.45387 val_roc= 0.80822 val_ap= 0.78988 time= 0.11153\n",
      "训练次数: 2 Epoch: 0014 log_lik= 0.5854877 train_kl= 0.00844 train_loss= 0.59392 train_acc= 0.48114 val_roc= 0.81304 val_ap= 0.79582 time= 0.10655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0015 log_lik= 0.5723237 train_kl= 0.00843 train_loss= 0.58076 train_acc= 0.49574 val_roc= 0.81662 val_ap= 0.80116 time= 0.11252\n",
      "训练次数: 2 Epoch: 0016 log_lik= 0.5621043 train_kl= 0.00842 train_loss= 0.57053 train_acc= 0.50228 val_roc= 0.81856 val_ap= 0.80116 time= 0.10655\n",
      "训练次数: 2 Epoch: 0017 log_lik= 0.558001 train_kl= 0.00841 train_loss= 0.56641 train_acc= 0.50491 val_roc= 0.82158 val_ap= 0.80193 time= 0.10754\n",
      "训练次数: 2 Epoch: 0018 log_lik= 0.5551754 train_kl= 0.00841 train_loss= 0.56358 train_acc= 0.50992 val_roc= 0.82489 val_ap= 0.80466 time= 0.11949\n",
      "训练次数: 2 Epoch: 0019 log_lik= 0.5517498 train_kl= 0.00841 train_loss= 0.56016 train_acc= 0.51583 val_roc= 0.82776 val_ap= 0.80912 time= 0.11153\n",
      "训练次数: 2 Epoch: 0020 log_lik= 0.54832006 train_kl= 0.00841 train_loss= 0.55673 train_acc= 0.52139 val_roc= 0.82965 val_ap= 0.81243 time= 0.11156\n",
      "训练次数: 2 Epoch: 0021 log_lik= 0.54519886 train_kl= 0.00843 train_loss= 0.55363 train_acc= 0.52515 val_roc= 0.83130 val_ap= 0.81518 time= 0.11153\n",
      "训练次数: 2 Epoch: 0022 log_lik= 0.54275256 train_kl= 0.00845 train_loss= 0.55120 train_acc= 0.52728 val_roc= 0.83241 val_ap= 0.81659 time= 0.11252\n",
      "训练次数: 2 Epoch: 0023 log_lik= 0.5397491 train_kl= 0.00846 train_loss= 0.54821 train_acc= 0.52960 val_roc= 0.83416 val_ap= 0.81731 time= 0.10954\n",
      "训练次数: 2 Epoch: 0024 log_lik= 0.5362963 train_kl= 0.00847 train_loss= 0.54476 train_acc= 0.52978 val_roc= 0.83579 val_ap= 0.81943 time= 0.10854\n",
      "训练次数: 2 Epoch: 0025 log_lik= 0.53328377 train_kl= 0.00847 train_loss= 0.54175 train_acc= 0.52939 val_roc= 0.83802 val_ap= 0.82173 time= 0.10607\n",
      "训练次数: 2 Epoch: 0026 log_lik= 0.53103447 train_kl= 0.00847 train_loss= 0.53950 train_acc= 0.52830 val_roc= 0.84043 val_ap= 0.82453 time= 0.11352\n",
      "训练次数: 2 Epoch: 0027 log_lik= 0.5287457 train_kl= 0.00846 train_loss= 0.53721 train_acc= 0.52722 val_roc= 0.84421 val_ap= 0.82905 time= 0.10655\n",
      "训练次数: 2 Epoch: 0028 log_lik= 0.5256716 train_kl= 0.00846 train_loss= 0.53413 train_acc= 0.52823 val_roc= 0.84743 val_ap= 0.83384 time= 0.10754\n",
      "训练次数: 2 Epoch: 0029 log_lik= 0.522488 train_kl= 0.00845 train_loss= 0.53094 train_acc= 0.53003 val_roc= 0.85006 val_ap= 0.83764 time= 0.10854\n",
      "训练次数: 2 Epoch: 0030 log_lik= 0.5196738 train_kl= 0.00845 train_loss= 0.52812 train_acc= 0.53073 val_roc= 0.85232 val_ap= 0.84000 time= 0.12248\n",
      "训练次数: 2 Epoch: 0031 log_lik= 0.5172558 train_kl= 0.00846 train_loss= 0.52571 train_acc= 0.53096 val_roc= 0.85540 val_ap= 0.84383 time= 0.11352\n",
      "训练次数: 2 Epoch: 0032 log_lik= 0.51436424 train_kl= 0.00847 train_loss= 0.52283 train_acc= 0.53045 val_roc= 0.85801 val_ap= 0.84740 time= 0.11650\n",
      "训练次数: 2 Epoch: 0033 log_lik= 0.51158917 train_kl= 0.00848 train_loss= 0.52007 train_acc= 0.53122 val_roc= 0.85989 val_ap= 0.84952 time= 0.11949\n",
      "训练次数: 2 Epoch: 0034 log_lik= 0.5090291 train_kl= 0.00849 train_loss= 0.51752 train_acc= 0.53135 val_roc= 0.86187 val_ap= 0.85204 time= 0.11153\n",
      "训练次数: 2 Epoch: 0035 log_lik= 0.5067584 train_kl= 0.00850 train_loss= 0.51526 train_acc= 0.53216 val_roc= 0.86422 val_ap= 0.85505 time= 0.11053\n",
      "训练次数: 2 Epoch: 0036 log_lik= 0.50408256 train_kl= 0.00851 train_loss= 0.51259 train_acc= 0.53453 val_roc= 0.86611 val_ap= 0.85670 time= 0.11053\n",
      "训练次数: 2 Epoch: 0037 log_lik= 0.50152683 train_kl= 0.00851 train_loss= 0.51003 train_acc= 0.53780 val_roc= 0.86660 val_ap= 0.85754 time= 0.10964\n",
      "训练次数: 2 Epoch: 0038 log_lik= 0.49911356 train_kl= 0.00851 train_loss= 0.50762 train_acc= 0.53989 val_roc= 0.86719 val_ap= 0.85745 time= 0.10754\n",
      "训练次数: 2 Epoch: 0039 log_lik= 0.4973124 train_kl= 0.00851 train_loss= 0.50582 train_acc= 0.54176 val_roc= 0.86847 val_ap= 0.85853 time= 0.10655\n",
      "训练次数: 2 Epoch: 0040 log_lik= 0.49519938 train_kl= 0.00852 train_loss= 0.50372 train_acc= 0.54314 val_roc= 0.87016 val_ap= 0.86087 time= 0.11053\n",
      "训练次数: 2 Epoch: 0041 log_lik= 0.49332654 train_kl= 0.00852 train_loss= 0.50185 train_acc= 0.54291 val_roc= 0.87243 val_ap= 0.86433 time= 0.10455\n",
      "训练次数: 2 Epoch: 0042 log_lik= 0.49131343 train_kl= 0.00853 train_loss= 0.49984 train_acc= 0.54283 val_roc= 0.87425 val_ap= 0.86799 time= 0.10854\n",
      "训练次数: 2 Epoch: 0043 log_lik= 0.48912966 train_kl= 0.00854 train_loss= 0.49766 train_acc= 0.54304 val_roc= 0.87604 val_ap= 0.87090 time= 0.10854\n",
      "训练次数: 2 Epoch: 0044 log_lik= 0.48672915 train_kl= 0.00854 train_loss= 0.49527 train_acc= 0.54347 val_roc= 0.87807 val_ap= 0.87362 time= 0.11451\n",
      "训练次数: 2 Epoch: 0045 log_lik= 0.4847044 train_kl= 0.00855 train_loss= 0.49325 train_acc= 0.54420 val_roc= 0.88016 val_ap= 0.87600 time= 0.10655\n",
      "训练次数: 2 Epoch: 0046 log_lik= 0.4826104 train_kl= 0.00855 train_loss= 0.49116 train_acc= 0.54437 val_roc= 0.88138 val_ap= 0.87795 time= 0.10555\n",
      "训练次数: 2 Epoch: 0047 log_lik= 0.48038274 train_kl= 0.00855 train_loss= 0.48893 train_acc= 0.54486 val_roc= 0.88279 val_ap= 0.88115 time= 0.11053\n",
      "训练次数: 2 Epoch: 0048 log_lik= 0.4779777 train_kl= 0.00855 train_loss= 0.48653 train_acc= 0.54575 val_roc= 0.88431 val_ap= 0.88504 time= 0.11750\n",
      "训练次数: 2 Epoch: 0049 log_lik= 0.47590148 train_kl= 0.00856 train_loss= 0.48446 train_acc= 0.54640 val_roc= 0.88548 val_ap= 0.88849 time= 0.11451\n",
      "训练次数: 2 Epoch: 0050 log_lik= 0.47383028 train_kl= 0.00856 train_loss= 0.48239 train_acc= 0.54648 val_roc= 0.88648 val_ap= 0.89134 time= 0.12846\n",
      "训练次数: 2 Epoch: 0051 log_lik= 0.4723617 train_kl= 0.00857 train_loss= 0.48093 train_acc= 0.54646 val_roc= 0.88749 val_ap= 0.89415 time= 0.11352\n",
      "训练次数: 2 Epoch: 0052 log_lik= 0.4704669 train_kl= 0.00857 train_loss= 0.47904 train_acc= 0.54654 val_roc= 0.88862 val_ap= 0.89727 time= 0.11252\n",
      "训练次数: 2 Epoch: 0053 log_lik= 0.4689091 train_kl= 0.00858 train_loss= 0.47749 train_acc= 0.54697 val_roc= 0.88965 val_ap= 0.89982 time= 0.11949\n",
      "训练次数: 2 Epoch: 0054 log_lik= 0.46765822 train_kl= 0.00859 train_loss= 0.47625 train_acc= 0.54761 val_roc= 0.89067 val_ap= 0.90193 time= 0.11152\n",
      "训练次数: 2 Epoch: 0055 log_lik= 0.4664966 train_kl= 0.00860 train_loss= 0.47509 train_acc= 0.54728 val_roc= 0.89119 val_ap= 0.90328 time= 0.12348\n",
      "训练次数: 2 Epoch: 0056 log_lik= 0.4654252 train_kl= 0.00860 train_loss= 0.47403 train_acc= 0.54814 val_roc= 0.89156 val_ap= 0.90432 time= 0.10754\n",
      "训练次数: 2 Epoch: 0057 log_lik= 0.46437973 train_kl= 0.00861 train_loss= 0.47299 train_acc= 0.54837 val_roc= 0.89182 val_ap= 0.90508 time= 0.11165\n",
      "训练次数: 2 Epoch: 0058 log_lik= 0.463212 train_kl= 0.00861 train_loss= 0.47182 train_acc= 0.54920 val_roc= 0.89222 val_ap= 0.90607 time= 0.11252\n",
      "训练次数: 2 Epoch: 0059 log_lik= 0.46207982 train_kl= 0.00862 train_loss= 0.47070 train_acc= 0.54988 val_roc= 0.89270 val_ap= 0.90705 time= 0.11054\n",
      "训练次数: 2 Epoch: 0060 log_lik= 0.46085146 train_kl= 0.00862 train_loss= 0.46947 train_acc= 0.55018 val_roc= 0.89297 val_ap= 0.90800 time= 0.10854\n",
      "训练次数: 2 Epoch: 0061 log_lik= 0.45978823 train_kl= 0.00862 train_loss= 0.46841 train_acc= 0.55151 val_roc= 0.89261 val_ap= 0.90809 time= 0.11551\n",
      "训练次数: 2 Epoch: 0062 log_lik= 0.45859838 train_kl= 0.00862 train_loss= 0.46722 train_acc= 0.55252 val_roc= 0.89278 val_ap= 0.90864 time= 0.11451\n",
      "训练次数: 2 Epoch: 0063 log_lik= 0.45757148 train_kl= 0.00862 train_loss= 0.46619 train_acc= 0.55387 val_roc= 0.89273 val_ap= 0.90866 time= 0.11053\n",
      "训练次数: 2 Epoch: 0064 log_lik= 0.4566472 train_kl= 0.00862 train_loss= 0.46526 train_acc= 0.55518 val_roc= 0.89244 val_ap= 0.90884 time= 0.10754\n",
      "训练次数: 2 Epoch: 0065 log_lik= 0.45558563 train_kl= 0.00862 train_loss= 0.46420 train_acc= 0.55620 val_roc= 0.89280 val_ap= 0.90928 time= 0.11152\n",
      "训练次数: 2 Epoch: 0066 log_lik= 0.4546767 train_kl= 0.00862 train_loss= 0.46330 train_acc= 0.55750 val_roc= 0.89338 val_ap= 0.90973 time= 0.10854\n",
      "训练次数: 2 Epoch: 0067 log_lik= 0.45387357 train_kl= 0.00862 train_loss= 0.46250 train_acc= 0.55838 val_roc= 0.89435 val_ap= 0.91048 time= 0.11053\n",
      "训练次数: 2 Epoch: 0068 log_lik= 0.4531491 train_kl= 0.00863 train_loss= 0.46177 train_acc= 0.55904 val_roc= 0.89527 val_ap= 0.91126 time= 0.11252\n",
      "训练次数: 2 Epoch: 0069 log_lik= 0.452372 train_kl= 0.00863 train_loss= 0.46100 train_acc= 0.56025 val_roc= 0.89566 val_ap= 0.91143 time= 0.11055\n",
      "训练次数: 2 Epoch: 0070 log_lik= 0.45173922 train_kl= 0.00863 train_loss= 0.46037 train_acc= 0.56130 val_roc= 0.89612 val_ap= 0.91172 time= 0.11850\n",
      "训练次数: 2 Epoch: 0071 log_lik= 0.45109594 train_kl= 0.00863 train_loss= 0.45973 train_acc= 0.56202 val_roc= 0.89679 val_ap= 0.91246 time= 0.11053\n",
      "训练次数: 2 Epoch: 0072 log_lik= 0.45045194 train_kl= 0.00864 train_loss= 0.45909 train_acc= 0.56257 val_roc= 0.89748 val_ap= 0.91294 time= 0.11252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0073 log_lik= 0.44999683 train_kl= 0.00864 train_loss= 0.45863 train_acc= 0.56310 val_roc= 0.89821 val_ap= 0.91353 time= 0.11053\n",
      "训练次数: 2 Epoch: 0074 log_lik= 0.44939098 train_kl= 0.00864 train_loss= 0.45803 train_acc= 0.56301 val_roc= 0.89810 val_ap= 0.91326 time= 0.10754\n",
      "训练次数: 2 Epoch: 0075 log_lik= 0.4488671 train_kl= 0.00864 train_loss= 0.45750 train_acc= 0.56318 val_roc= 0.89857 val_ap= 0.91376 time= 0.11053\n",
      "训练次数: 2 Epoch: 0076 log_lik= 0.44834903 train_kl= 0.00864 train_loss= 0.45699 train_acc= 0.56346 val_roc= 0.89900 val_ap= 0.91417 time= 0.11252\n",
      "训练次数: 2 Epoch: 0077 log_lik= 0.447736 train_kl= 0.00864 train_loss= 0.45637 train_acc= 0.56397 val_roc= 0.89893 val_ap= 0.91404 time= 0.11451\n",
      "训练次数: 2 Epoch: 0078 log_lik= 0.44741344 train_kl= 0.00864 train_loss= 0.45605 train_acc= 0.56389 val_roc= 0.89896 val_ap= 0.91390 time= 0.12398\n",
      "训练次数: 2 Epoch: 0079 log_lik= 0.446941 train_kl= 0.00864 train_loss= 0.45558 train_acc= 0.56363 val_roc= 0.89910 val_ap= 0.91404 time= 0.10456\n",
      "训练次数: 2 Epoch: 0080 log_lik= 0.44655845 train_kl= 0.00864 train_loss= 0.45520 train_acc= 0.56415 val_roc= 0.89933 val_ap= 0.91416 time= 0.11750\n",
      "训练次数: 2 Epoch: 0081 log_lik= 0.44606563 train_kl= 0.00864 train_loss= 0.45471 train_acc= 0.56410 val_roc= 0.89930 val_ap= 0.91401 time= 0.10819\n",
      "训练次数: 2 Epoch: 0082 log_lik= 0.4457186 train_kl= 0.00864 train_loss= 0.45436 train_acc= 0.56410 val_roc= 0.89925 val_ap= 0.91388 time= 0.12100\n",
      "训练次数: 2 Epoch: 0083 log_lik= 0.44533226 train_kl= 0.00864 train_loss= 0.45398 train_acc= 0.56453 val_roc= 0.89943 val_ap= 0.91387 time= 0.10854\n",
      "训练次数: 2 Epoch: 0084 log_lik= 0.44493034 train_kl= 0.00865 train_loss= 0.45358 train_acc= 0.56471 val_roc= 0.89910 val_ap= 0.91321 time= 0.11949\n",
      "训练次数: 2 Epoch: 0085 log_lik= 0.44459438 train_kl= 0.00865 train_loss= 0.45324 train_acc= 0.56469 val_roc= 0.89860 val_ap= 0.91256 time= 0.10854\n",
      "训练次数: 2 Epoch: 0086 log_lik= 0.4441981 train_kl= 0.00865 train_loss= 0.45285 train_acc= 0.56498 val_roc= 0.89857 val_ap= 0.91236 time= 0.11053\n",
      "训练次数: 2 Epoch: 0087 log_lik= 0.44385433 train_kl= 0.00865 train_loss= 0.45251 train_acc= 0.56512 val_roc= 0.89825 val_ap= 0.91182 time= 0.10655\n",
      "训练次数: 2 Epoch: 0088 log_lik= 0.44346344 train_kl= 0.00865 train_loss= 0.45212 train_acc= 0.56610 val_roc= 0.89808 val_ap= 0.91141 time= 0.11053\n",
      "训练次数: 2 Epoch: 0089 log_lik= 0.44315505 train_kl= 0.00865 train_loss= 0.45181 train_acc= 0.56652 val_roc= 0.89829 val_ap= 0.91137 time= 0.10953\n",
      "训练次数: 2 Epoch: 0090 log_lik= 0.44280297 train_kl= 0.00865 train_loss= 0.45146 train_acc= 0.56664 val_roc= 0.89838 val_ap= 0.91127 time= 0.10755\n",
      "训练次数: 2 Epoch: 0091 log_lik= 0.44250342 train_kl= 0.00865 train_loss= 0.45116 train_acc= 0.56736 val_roc= 0.89849 val_ap= 0.91119 time= 0.12447\n",
      "训练次数: 2 Epoch: 0092 log_lik= 0.44222444 train_kl= 0.00866 train_loss= 0.45088 train_acc= 0.56793 val_roc= 0.89841 val_ap= 0.91108 time= 0.10655\n",
      "训练次数: 2 Epoch: 0093 log_lik= 0.44195062 train_kl= 0.00866 train_loss= 0.45061 train_acc= 0.56836 val_roc= 0.89810 val_ap= 0.91081 time= 0.11053\n",
      "训练次数: 2 Epoch: 0094 log_lik= 0.4416563 train_kl= 0.00866 train_loss= 0.45031 train_acc= 0.56885 val_roc= 0.89803 val_ap= 0.91075 time= 0.11452\n",
      "训练次数: 2 Epoch: 0095 log_lik= 0.4413586 train_kl= 0.00866 train_loss= 0.45002 train_acc= 0.56910 val_roc= 0.89831 val_ap= 0.91095 time= 0.10854\n",
      "训练次数: 2 Epoch: 0096 log_lik= 0.44105873 train_kl= 0.00866 train_loss= 0.44972 train_acc= 0.56975 val_roc= 0.89844 val_ap= 0.91122 time= 0.10953\n",
      "训练次数: 2 Epoch: 0097 log_lik= 0.44079152 train_kl= 0.00866 train_loss= 0.44945 train_acc= 0.57014 val_roc= 0.89858 val_ap= 0.91135 time= 0.10356\n",
      "训练次数: 2 Epoch: 0098 log_lik= 0.44055364 train_kl= 0.00866 train_loss= 0.44922 train_acc= 0.57031 val_roc= 0.89870 val_ap= 0.91143 time= 0.11850\n",
      "训练次数: 2 Epoch: 0099 log_lik= 0.44024453 train_kl= 0.00867 train_loss= 0.44891 train_acc= 0.57100 val_roc= 0.89890 val_ap= 0.91178 time= 0.11551\n",
      "训练次数: 2 Epoch: 0100 log_lik= 0.44000885 train_kl= 0.00867 train_loss= 0.44868 train_acc= 0.57131 val_roc= 0.89919 val_ap= 0.91213 time= 0.11252\n",
      "Optimization Finished!\n",
      "训练次数: 2 ROC score: 0.9196698940333923\n",
      "训练次数: 2 AP score: 0.9284163718765874\n",
      "训练次数: 3 Epoch: 0001 log_lik= 0.7814334 train_kl= 0.00806 train_loss= 0.78949 train_acc= 0.03359 val_roc= 0.71745 val_ap= 0.74263 time= 7.63298\n",
      "训练次数: 3 Epoch: 0002 log_lik= 0.87021714 train_kl= 0.00837 train_loss= 0.87858 train_acc= 0.00159 val_roc= 0.73850 val_ap= 0.76885 time= 0.12299\n",
      "训练次数: 3 Epoch: 0003 log_lik= 0.7416042 train_kl= 0.00812 train_loss= 0.74972 train_acc= 0.00323 val_roc= 0.67856 val_ap= 0.71194 time= 0.12746\n",
      "训练次数: 3 Epoch: 0004 log_lik= 0.73712456 train_kl= 0.00815 train_loss= 0.74527 train_acc= 0.00488 val_roc= 0.68797 val_ap= 0.71993 time= 0.12646\n",
      "训练次数: 3 Epoch: 0005 log_lik= 0.7440778 train_kl= 0.00821 train_loss= 0.75229 train_acc= 0.00267 val_roc= 0.71656 val_ap= 0.74895 time= 0.11551\n",
      "训练次数: 3 Epoch: 0006 log_lik= 0.72476244 train_kl= 0.00818 train_loss= 0.73295 train_acc= 0.00454 val_roc= 0.75798 val_ap= 0.78550 time= 0.10805\n",
      "训练次数: 3 Epoch: 0007 log_lik= 0.7096271 train_kl= 0.00816 train_loss= 0.71779 train_acc= 0.01944 val_roc= 0.79210 val_ap= 0.80820 time= 0.10655\n",
      "训练次数: 3 Epoch: 0008 log_lik= 0.6934117 train_kl= 0.00816 train_loss= 0.70158 train_acc= 0.07883 val_roc= 0.80942 val_ap= 0.81935 time= 0.11077\n",
      "训练次数: 3 Epoch: 0009 log_lik= 0.6724764 train_kl= 0.00819 train_loss= 0.68066 train_acc= 0.15813 val_roc= 0.81823 val_ap= 0.82470 time= 0.15338\n",
      "训练次数: 3 Epoch: 0010 log_lik= 0.6486406 train_kl= 0.00823 train_loss= 0.65687 train_acc= 0.22114 val_roc= 0.82697 val_ap= 0.83001 time= 0.11352\n",
      "训练次数: 3 Epoch: 0011 log_lik= 0.6266223 train_kl= 0.00828 train_loss= 0.63490 train_acc= 0.27703 val_roc= 0.83523 val_ap= 0.83737 time= 0.11360\n",
      "训练次数: 3 Epoch: 0012 log_lik= 0.60598415 train_kl= 0.00832 train_loss= 0.61430 train_acc= 0.33220 val_roc= 0.83928 val_ap= 0.84201 time= 0.14689\n",
      "训练次数: 3 Epoch: 0013 log_lik= 0.5871819 train_kl= 0.00836 train_loss= 0.59554 train_acc= 0.38402 val_roc= 0.83996 val_ap= 0.84326 time= 0.10854\n",
      "训练次数: 3 Epoch: 0014 log_lik= 0.5747029 train_kl= 0.00840 train_loss= 0.58310 train_acc= 0.42409 val_roc= 0.84104 val_ap= 0.84598 time= 0.11053\n",
      "训练次数: 3 Epoch: 0015 log_lik= 0.5666662 train_kl= 0.00844 train_loss= 0.57511 train_acc= 0.45074 val_roc= 0.84548 val_ap= 0.85242 time= 0.11053\n",
      "训练次数: 3 Epoch: 0016 log_lik= 0.55901384 train_kl= 0.00848 train_loss= 0.56749 train_acc= 0.46853 val_roc= 0.85080 val_ap= 0.86014 time= 0.11053\n",
      "训练次数: 3 Epoch: 0017 log_lik= 0.55203843 train_kl= 0.00851 train_loss= 0.56055 train_acc= 0.48020 val_roc= 0.85541 val_ap= 0.86625 time= 0.10655\n",
      "训练次数: 3 Epoch: 0018 log_lik= 0.5474844 train_kl= 0.00852 train_loss= 0.55601 train_acc= 0.48733 val_roc= 0.85752 val_ap= 0.86908 time= 0.11403\n",
      "训练次数: 3 Epoch: 0019 log_lik= 0.54276097 train_kl= 0.00852 train_loss= 0.55129 train_acc= 0.49224 val_roc= 0.86047 val_ap= 0.87327 time= 0.11352\n",
      "训练次数: 3 Epoch: 0020 log_lik= 0.53706837 train_kl= 0.00851 train_loss= 0.54558 train_acc= 0.49738 val_roc= 0.86302 val_ap= 0.87706 time= 0.10755\n",
      "训练次数: 3 Epoch: 0021 log_lik= 0.53171897 train_kl= 0.00849 train_loss= 0.54021 train_acc= 0.50163 val_roc= 0.86511 val_ap= 0.87971 time= 0.10953\n",
      "训练次数: 3 Epoch: 0022 log_lik= 0.5280009 train_kl= 0.00847 train_loss= 0.53647 train_acc= 0.50551 val_roc= 0.86773 val_ap= 0.88233 time= 0.11105\n",
      "训练次数: 3 Epoch: 0023 log_lik= 0.5252475 train_kl= 0.00846 train_loss= 0.53371 train_acc= 0.50879 val_roc= 0.87039 val_ap= 0.88439 time= 0.10954\n",
      "训练次数: 3 Epoch: 0024 log_lik= 0.5216616 train_kl= 0.00845 train_loss= 0.53012 train_acc= 0.51129 val_roc= 0.87231 val_ap= 0.88491 time= 0.11153\n",
      "训练次数: 3 Epoch: 0025 log_lik= 0.5184431 train_kl= 0.00846 train_loss= 0.52690 train_acc= 0.51337 val_roc= 0.87366 val_ap= 0.88473 time= 0.11153\n",
      "训练次数: 3 Epoch: 0026 log_lik= 0.51541626 train_kl= 0.00847 train_loss= 0.52389 train_acc= 0.51487 val_roc= 0.87463 val_ap= 0.88426 time= 0.12499\n",
      "训练次数: 3 Epoch: 0027 log_lik= 0.5121424 train_kl= 0.00849 train_loss= 0.52063 train_acc= 0.51637 val_roc= 0.87564 val_ap= 0.88534 time= 0.11452\n",
      "训练次数: 3 Epoch: 0028 log_lik= 0.5079364 train_kl= 0.00851 train_loss= 0.51644 train_acc= 0.51925 val_roc= 0.87743 val_ap= 0.88772 time= 0.10854\n",
      "训练次数: 3 Epoch: 0029 log_lik= 0.5035519 train_kl= 0.00852 train_loss= 0.51207 train_acc= 0.52242 val_roc= 0.87857 val_ap= 0.88925 time= 0.11551\n",
      "训练次数: 3 Epoch: 0030 log_lik= 0.4997011 train_kl= 0.00854 train_loss= 0.50824 train_acc= 0.52625 val_roc= 0.87966 val_ap= 0.88993 time= 0.10905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0031 log_lik= 0.49624053 train_kl= 0.00855 train_loss= 0.50479 train_acc= 0.52939 val_roc= 0.88028 val_ap= 0.88996 time= 0.10954\n",
      "训练次数: 3 Epoch: 0032 log_lik= 0.492931 train_kl= 0.00857 train_loss= 0.50150 train_acc= 0.53379 val_roc= 0.88110 val_ap= 0.89001 time= 0.11004\n",
      "训练次数: 3 Epoch: 0033 log_lik= 0.48968902 train_kl= 0.00858 train_loss= 0.49827 train_acc= 0.53815 val_roc= 0.88161 val_ap= 0.88959 time= 0.11104\n",
      "训练次数: 3 Epoch: 0034 log_lik= 0.48636788 train_kl= 0.00859 train_loss= 0.49496 train_acc= 0.54363 val_roc= 0.88197 val_ap= 0.88971 time= 0.14439\n",
      "训练次数: 3 Epoch: 0035 log_lik= 0.48361027 train_kl= 0.00860 train_loss= 0.49221 train_acc= 0.54866 val_roc= 0.88238 val_ap= 0.88963 time= 0.10953\n",
      "训练次数: 3 Epoch: 0036 log_lik= 0.48129183 train_kl= 0.00860 train_loss= 0.48990 train_acc= 0.55481 val_roc= 0.88347 val_ap= 0.89028 time= 0.11551\n",
      "训练次数: 3 Epoch: 0037 log_lik= 0.4790577 train_kl= 0.00860 train_loss= 0.48766 train_acc= 0.55968 val_roc= 0.88433 val_ap= 0.89084 time= 0.11553\n",
      "训练次数: 3 Epoch: 0038 log_lik= 0.47721973 train_kl= 0.00860 train_loss= 0.48582 train_acc= 0.56381 val_roc= 0.88547 val_ap= 0.89062 time= 0.11053\n",
      "训练次数: 3 Epoch: 0039 log_lik= 0.47569966 train_kl= 0.00860 train_loss= 0.48430 train_acc= 0.56741 val_roc= 0.88687 val_ap= 0.89081 time= 0.10256\n",
      "训练次数: 3 Epoch: 0040 log_lik= 0.47446552 train_kl= 0.00860 train_loss= 0.48307 train_acc= 0.56974 val_roc= 0.88845 val_ap= 0.89153 time= 0.10256\n",
      "训练次数: 3 Epoch: 0041 log_lik= 0.47310516 train_kl= 0.00860 train_loss= 0.48171 train_acc= 0.57097 val_roc= 0.88986 val_ap= 0.89216 time= 0.09858\n",
      "训练次数: 3 Epoch: 0042 log_lik= 0.47190192 train_kl= 0.00861 train_loss= 0.48051 train_acc= 0.57173 val_roc= 0.89062 val_ap= 0.89213 time= 0.11244\n",
      "训练次数: 3 Epoch: 0043 log_lik= 0.47097054 train_kl= 0.00861 train_loss= 0.47958 train_acc= 0.57259 val_roc= 0.89104 val_ap= 0.89202 time= 0.11650\n",
      "训练次数: 3 Epoch: 0044 log_lik= 0.47008768 train_kl= 0.00861 train_loss= 0.47870 train_acc= 0.57340 val_roc= 0.89179 val_ap= 0.89265 time= 0.10655\n",
      "训练次数: 3 Epoch: 0045 log_lik= 0.46881488 train_kl= 0.00862 train_loss= 0.47743 train_acc= 0.57436 val_roc= 0.89265 val_ap= 0.89365 time= 0.10555\n",
      "训练次数: 3 Epoch: 0046 log_lik= 0.46757504 train_kl= 0.00862 train_loss= 0.47619 train_acc= 0.57533 val_roc= 0.89383 val_ap= 0.89544 time= 0.10555\n",
      "训练次数: 3 Epoch: 0047 log_lik= 0.4662708 train_kl= 0.00862 train_loss= 0.47489 train_acc= 0.57550 val_roc= 0.89546 val_ap= 0.89747 time= 0.12447\n",
      "训练次数: 3 Epoch: 0048 log_lik= 0.4649806 train_kl= 0.00862 train_loss= 0.47360 train_acc= 0.57615 val_roc= 0.89656 val_ap= 0.89918 time= 0.10157\n",
      "训练次数: 3 Epoch: 0049 log_lik= 0.46366918 train_kl= 0.00862 train_loss= 0.47228 train_acc= 0.57663 val_roc= 0.89716 val_ap= 0.89991 time= 0.11053\n",
      "训练次数: 3 Epoch: 0050 log_lik= 0.46255222 train_kl= 0.00861 train_loss= 0.47116 train_acc= 0.57638 val_roc= 0.89777 val_ap= 0.90050 time= 0.11053\n",
      "训练次数: 3 Epoch: 0051 log_lik= 0.46130744 train_kl= 0.00861 train_loss= 0.46992 train_acc= 0.57667 val_roc= 0.89884 val_ap= 0.90173 time= 0.10655\n",
      "训练次数: 3 Epoch: 0052 log_lik= 0.46025074 train_kl= 0.00861 train_loss= 0.46886 train_acc= 0.57639 val_roc= 0.89935 val_ap= 0.90279 time= 0.10655\n",
      "训练次数: 3 Epoch: 0053 log_lik= 0.45918334 train_kl= 0.00861 train_loss= 0.46779 train_acc= 0.57648 val_roc= 0.89993 val_ap= 0.90325 time= 0.16032\n",
      "训练次数: 3 Epoch: 0054 log_lik= 0.45820424 train_kl= 0.00861 train_loss= 0.46681 train_acc= 0.57652 val_roc= 0.90042 val_ap= 0.90396 time= 0.11004\n",
      "训练次数: 3 Epoch: 0055 log_lik= 0.45739463 train_kl= 0.00861 train_loss= 0.46600 train_acc= 0.57702 val_roc= 0.90111 val_ap= 0.90472 time= 0.10157\n",
      "训练次数: 3 Epoch: 0056 log_lik= 0.45652837 train_kl= 0.00861 train_loss= 0.46514 train_acc= 0.57657 val_roc= 0.90168 val_ap= 0.90559 time= 0.10655\n",
      "训练次数: 3 Epoch: 0057 log_lik= 0.45586026 train_kl= 0.00861 train_loss= 0.46447 train_acc= 0.57700 val_roc= 0.90183 val_ap= 0.90613 time= 0.11152\n",
      "训练次数: 3 Epoch: 0058 log_lik= 0.45509374 train_kl= 0.00862 train_loss= 0.46371 train_acc= 0.57692 val_roc= 0.90240 val_ap= 0.90673 time= 0.10257\n",
      "训练次数: 3 Epoch: 0059 log_lik= 0.454297 train_kl= 0.00862 train_loss= 0.46292 train_acc= 0.57703 val_roc= 0.90256 val_ap= 0.90696 time= 0.10157\n",
      "训练次数: 3 Epoch: 0060 log_lik= 0.45366648 train_kl= 0.00862 train_loss= 0.46229 train_acc= 0.57751 val_roc= 0.90285 val_ap= 0.90734 time= 0.10964\n",
      "训练次数: 3 Epoch: 0061 log_lik= 0.45302114 train_kl= 0.00863 train_loss= 0.46165 train_acc= 0.57764 val_roc= 0.90335 val_ap= 0.90773 time= 0.12547\n",
      "训练次数: 3 Epoch: 0062 log_lik= 0.452354 train_kl= 0.00863 train_loss= 0.46098 train_acc= 0.57874 val_roc= 0.90399 val_ap= 0.90838 time= 0.11252\n",
      "训练次数: 3 Epoch: 0063 log_lik= 0.45169482 train_kl= 0.00863 train_loss= 0.46032 train_acc= 0.57906 val_roc= 0.90506 val_ap= 0.90949 time= 0.11451\n",
      "训练次数: 3 Epoch: 0064 log_lik= 0.4510252 train_kl= 0.00863 train_loss= 0.45966 train_acc= 0.57950 val_roc= 0.90523 val_ap= 0.90973 time= 0.10456\n",
      "训练次数: 3 Epoch: 0065 log_lik= 0.45035866 train_kl= 0.00863 train_loss= 0.45899 train_acc= 0.57928 val_roc= 0.90569 val_ap= 0.91030 time= 0.11949\n",
      "训练次数: 3 Epoch: 0066 log_lik= 0.4497395 train_kl= 0.00863 train_loss= 0.45837 train_acc= 0.57926 val_roc= 0.90639 val_ap= 0.91115 time= 0.12327\n",
      "训练次数: 3 Epoch: 0067 log_lik= 0.4491988 train_kl= 0.00864 train_loss= 0.45783 train_acc= 0.57886 val_roc= 0.90705 val_ap= 0.91175 time= 0.12148\n",
      "训练次数: 3 Epoch: 0068 log_lik= 0.44859684 train_kl= 0.00864 train_loss= 0.45723 train_acc= 0.57941 val_roc= 0.90734 val_ap= 0.91168 time= 0.12348\n",
      "训练次数: 3 Epoch: 0069 log_lik= 0.4480171 train_kl= 0.00864 train_loss= 0.45665 train_acc= 0.57907 val_roc= 0.90804 val_ap= 0.91232 time= 0.14059\n",
      "训练次数: 3 Epoch: 0070 log_lik= 0.44751853 train_kl= 0.00864 train_loss= 0.45616 train_acc= 0.57904 val_roc= 0.90837 val_ap= 0.91247 time= 0.11451\n",
      "训练次数: 3 Epoch: 0071 log_lik= 0.4470455 train_kl= 0.00864 train_loss= 0.45569 train_acc= 0.57906 val_roc= 0.90834 val_ap= 0.91208 time= 0.11451\n",
      "训练次数: 3 Epoch: 0072 log_lik= 0.44658372 train_kl= 0.00864 train_loss= 0.45522 train_acc= 0.57973 val_roc= 0.90849 val_ap= 0.91231 time= 0.10854\n",
      "训练次数: 3 Epoch: 0073 log_lik= 0.44603947 train_kl= 0.00864 train_loss= 0.45468 train_acc= 0.57952 val_roc= 0.90901 val_ap= 0.91267 time= 0.12248\n",
      "训练次数: 3 Epoch: 0074 log_lik= 0.44561565 train_kl= 0.00864 train_loss= 0.45426 train_acc= 0.57923 val_roc= 0.90974 val_ap= 0.91333 time= 0.12248\n",
      "训练次数: 3 Epoch: 0075 log_lik= 0.44514483 train_kl= 0.00864 train_loss= 0.45379 train_acc= 0.57919 val_roc= 0.90974 val_ap= 0.91289 time= 0.11352\n",
      "训练次数: 3 Epoch: 0076 log_lik= 0.44456097 train_kl= 0.00864 train_loss= 0.45320 train_acc= 0.57886 val_roc= 0.91008 val_ap= 0.91298 time= 0.12498\n",
      "训练次数: 3 Epoch: 0077 log_lik= 0.44398305 train_kl= 0.00864 train_loss= 0.45263 train_acc= 0.57983 val_roc= 0.91051 val_ap= 0.91341 time= 0.11153\n",
      "训练次数: 3 Epoch: 0078 log_lik= 0.443426 train_kl= 0.00864 train_loss= 0.45207 train_acc= 0.57970 val_roc= 0.91112 val_ap= 0.91409 time= 0.13045\n",
      "训练次数: 3 Epoch: 0079 log_lik= 0.44305354 train_kl= 0.00865 train_loss= 0.45170 train_acc= 0.57975 val_roc= 0.91164 val_ap= 0.91449 time= 0.11451\n",
      "训练次数: 3 Epoch: 0080 log_lik= 0.44256508 train_kl= 0.00865 train_loss= 0.45121 train_acc= 0.58013 val_roc= 0.91223 val_ap= 0.91493 time= 0.11045\n",
      "训练次数: 3 Epoch: 0081 log_lik= 0.44205245 train_kl= 0.00865 train_loss= 0.45070 train_acc= 0.58025 val_roc= 0.91281 val_ap= 0.91535 time= 0.12049\n",
      "训练次数: 3 Epoch: 0082 log_lik= 0.4415928 train_kl= 0.00865 train_loss= 0.45024 train_acc= 0.58024 val_roc= 0.91328 val_ap= 0.91566 time= 0.10555\n",
      "训练次数: 3 Epoch: 0083 log_lik= 0.4411526 train_kl= 0.00865 train_loss= 0.44981 train_acc= 0.58014 val_roc= 0.91363 val_ap= 0.91600 time= 0.11153\n",
      "训练次数: 3 Epoch: 0084 log_lik= 0.44067532 train_kl= 0.00866 train_loss= 0.44933 train_acc= 0.58035 val_roc= 0.91402 val_ap= 0.91637 time= 0.10655\n",
      "训练次数: 3 Epoch: 0085 log_lik= 0.4402362 train_kl= 0.00866 train_loss= 0.44889 train_acc= 0.58045 val_roc= 0.91424 val_ap= 0.91657 time= 0.11153\n",
      "训练次数: 3 Epoch: 0086 log_lik= 0.43981755 train_kl= 0.00866 train_loss= 0.44848 train_acc= 0.58073 val_roc= 0.91451 val_ap= 0.91670 time= 0.10555\n",
      "训练次数: 3 Epoch: 0087 log_lik= 0.43938905 train_kl= 0.00866 train_loss= 0.44805 train_acc= 0.58054 val_roc= 0.91469 val_ap= 0.91709 time= 0.12248\n",
      "训练次数: 3 Epoch: 0088 log_lik= 0.4390192 train_kl= 0.00866 train_loss= 0.44768 train_acc= 0.58054 val_roc= 0.91489 val_ap= 0.91722 time= 0.11551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0089 log_lik= 0.43859273 train_kl= 0.00866 train_loss= 0.44726 train_acc= 0.58051 val_roc= 0.91498 val_ap= 0.91750 time= 0.10356\n",
      "训练次数: 3 Epoch: 0090 log_lik= 0.43817198 train_kl= 0.00867 train_loss= 0.44684 train_acc= 0.58056 val_roc= 0.91524 val_ap= 0.91776 time= 0.12148\n",
      "训练次数: 3 Epoch: 0091 log_lik= 0.43792635 train_kl= 0.00867 train_loss= 0.44659 train_acc= 0.58025 val_roc= 0.91537 val_ap= 0.91795 time= 0.11551\n",
      "训练次数: 3 Epoch: 0092 log_lik= 0.43744335 train_kl= 0.00867 train_loss= 0.44611 train_acc= 0.58069 val_roc= 0.91554 val_ap= 0.91830 time= 0.10057\n",
      "训练次数: 3 Epoch: 0093 log_lik= 0.43706724 train_kl= 0.00867 train_loss= 0.44574 train_acc= 0.58090 val_roc= 0.91577 val_ap= 0.91855 time= 0.10057\n",
      "训练次数: 3 Epoch: 0094 log_lik= 0.4367086 train_kl= 0.00867 train_loss= 0.44538 train_acc= 0.58050 val_roc= 0.91602 val_ap= 0.91887 time= 0.10356\n",
      "训练次数: 3 Epoch: 0095 log_lik= 0.43635884 train_kl= 0.00867 train_loss= 0.44503 train_acc= 0.58057 val_roc= 0.91613 val_ap= 0.91919 time= 0.11055\n",
      "训练次数: 3 Epoch: 0096 log_lik= 0.43594489 train_kl= 0.00867 train_loss= 0.44462 train_acc= 0.58061 val_roc= 0.91649 val_ap= 0.91981 time= 0.10555\n",
      "训练次数: 3 Epoch: 0097 log_lik= 0.435597 train_kl= 0.00867 train_loss= 0.44427 train_acc= 0.58036 val_roc= 0.91684 val_ap= 0.92027 time= 0.14638\n",
      "训练次数: 3 Epoch: 0098 log_lik= 0.43520087 train_kl= 0.00867 train_loss= 0.44387 train_acc= 0.58068 val_roc= 0.91742 val_ap= 0.92088 time= 0.10256\n",
      "训练次数: 3 Epoch: 0099 log_lik= 0.4347973 train_kl= 0.00867 train_loss= 0.44347 train_acc= 0.58072 val_roc= 0.91788 val_ap= 0.92140 time= 0.10854\n",
      "训练次数: 3 Epoch: 0100 log_lik= 0.43432072 train_kl= 0.00868 train_loss= 0.44300 train_acc= 0.58088 val_roc= 0.91816 val_ap= 0.92185 time= 0.10441\n",
      "Optimization Finished!\n",
      "训练次数: 3 ROC score: 0.9155147643926274\n",
      "训练次数: 3 AP score: 0.9165712427770246\n",
      "训练次数: 4 Epoch: 0001 log_lik= 0.7797111 train_kl= 0.00806 train_loss= 0.78777 train_acc= 0.05078 val_roc= 0.72398 val_ap= 0.73484 time= 7.52841\n",
      "训练次数: 4 Epoch: 0002 log_lik= 0.7991685 train_kl= 0.00830 train_loss= 0.80747 train_acc= 0.00160 val_roc= 0.76388 val_ap= 0.77061 time= 0.12646\n",
      "训练次数: 4 Epoch: 0003 log_lik= 0.7260032 train_kl= 0.00814 train_loss= 0.73414 train_acc= 0.01926 val_roc= 0.65518 val_ap= 0.68329 time= 0.10953\n",
      "训练次数: 4 Epoch: 0004 log_lik= 0.73634577 train_kl= 0.00820 train_loss= 0.74455 train_acc= 0.05859 val_roc= 0.72187 val_ap= 0.72502 time= 0.10954\n",
      "训练次数: 4 Epoch: 0005 log_lik= 0.7117658 train_kl= 0.00815 train_loss= 0.71992 train_acc= 0.26104 val_roc= 0.80042 val_ap= 0.79899 time= 0.11252\n",
      "训练次数: 4 Epoch: 0006 log_lik= 0.6889303 train_kl= 0.00815 train_loss= 0.69708 train_acc= 0.36168 val_roc= 0.82760 val_ap= 0.83810 time= 0.11153\n",
      "训练次数: 4 Epoch: 0007 log_lik= 0.6543962 train_kl= 0.00818 train_loss= 0.66258 train_acc= 0.34481 val_roc= 0.83287 val_ap= 0.84207 time= 0.11352\n",
      "训练次数: 4 Epoch: 0008 log_lik= 0.6168794 train_kl= 0.00827 train_loss= 0.62515 train_acc= 0.33042 val_roc= 0.83733 val_ap= 0.84270 time= 0.11053\n",
      "训练次数: 4 Epoch: 0009 log_lik= 0.59622 train_kl= 0.00838 train_loss= 0.60460 train_acc= 0.35309 val_roc= 0.84677 val_ap= 0.84914 time= 0.14339\n",
      "训练次数: 4 Epoch: 0010 log_lik= 0.57628596 train_kl= 0.00847 train_loss= 0.58476 train_acc= 0.40762 val_roc= 0.85081 val_ap= 0.84942 time= 0.12348\n",
      "训练次数: 4 Epoch: 0011 log_lik= 0.56734127 train_kl= 0.00855 train_loss= 0.57589 train_acc= 0.45009 val_roc= 0.85976 val_ap= 0.86184 time= 0.12148\n",
      "训练次数: 4 Epoch: 0012 log_lik= 0.55196524 train_kl= 0.00859 train_loss= 0.56055 train_acc= 0.47253 val_roc= 0.87145 val_ap= 0.87764 time= 0.12148\n",
      "训练次数: 4 Epoch: 0013 log_lik= 0.5398165 train_kl= 0.00860 train_loss= 0.54842 train_acc= 0.47820 val_roc= 0.88100 val_ap= 0.88863 time= 0.10458\n",
      "训练次数: 4 Epoch: 0014 log_lik= 0.5292149 train_kl= 0.00861 train_loss= 0.53782 train_acc= 0.48399 val_roc= 0.88984 val_ap= 0.89725 time= 0.10456\n",
      "训练次数: 4 Epoch: 0015 log_lik= 0.51789457 train_kl= 0.00860 train_loss= 0.52649 train_acc= 0.49348 val_roc= 0.89590 val_ap= 0.90213 time= 0.10309\n",
      "训练次数: 4 Epoch: 0016 log_lik= 0.5111375 train_kl= 0.00858 train_loss= 0.51972 train_acc= 0.49887 val_roc= 0.90072 val_ap= 0.90562 time= 0.10456\n",
      "训练次数: 4 Epoch: 0017 log_lik= 0.50602233 train_kl= 0.00857 train_loss= 0.51459 train_acc= 0.50062 val_roc= 0.90496 val_ap= 0.90897 time= 0.12447\n",
      "训练次数: 4 Epoch: 0018 log_lik= 0.5018704 train_kl= 0.00855 train_loss= 0.51042 train_acc= 0.50035 val_roc= 0.90782 val_ap= 0.90977 time= 0.11652\n",
      "训练次数: 4 Epoch: 0019 log_lik= 0.49967393 train_kl= 0.00854 train_loss= 0.50821 train_acc= 0.49910 val_roc= 0.90873 val_ap= 0.90807 time= 0.10555\n",
      "训练次数: 4 Epoch: 0020 log_lik= 0.4972798 train_kl= 0.00853 train_loss= 0.50581 train_acc= 0.50066 val_roc= 0.90987 val_ap= 0.90818 time= 0.10256\n",
      "训练次数: 4 Epoch: 0021 log_lik= 0.49455038 train_kl= 0.00853 train_loss= 0.50308 train_acc= 0.50449 val_roc= 0.91019 val_ap= 0.90848 time= 0.10256\n",
      "训练次数: 4 Epoch: 0022 log_lik= 0.49163213 train_kl= 0.00854 train_loss= 0.50017 train_acc= 0.50942 val_roc= 0.91019 val_ap= 0.90908 time= 0.10456\n",
      "训练次数: 4 Epoch: 0023 log_lik= 0.48824292 train_kl= 0.00855 train_loss= 0.49679 train_acc= 0.51456 val_roc= 0.91010 val_ap= 0.91076 time= 0.10278\n",
      "训练次数: 4 Epoch: 0024 log_lik= 0.48439375 train_kl= 0.00856 train_loss= 0.49296 train_acc= 0.51874 val_roc= 0.91000 val_ap= 0.91179 time= 0.10455\n",
      "训练次数: 4 Epoch: 0025 log_lik= 0.48128 train_kl= 0.00857 train_loss= 0.48985 train_acc= 0.52174 val_roc= 0.90906 val_ap= 0.91130 time= 0.10555\n",
      "训练次数: 4 Epoch: 0026 log_lik= 0.47841236 train_kl= 0.00859 train_loss= 0.48700 train_acc= 0.52429 val_roc= 0.90882 val_ap= 0.91085 time= 0.11551\n",
      "训练次数: 4 Epoch: 0027 log_lik= 0.47534603 train_kl= 0.00860 train_loss= 0.48394 train_acc= 0.52652 val_roc= 0.90912 val_ap= 0.91190 time= 0.11651\n",
      "训练次数: 4 Epoch: 0028 log_lik= 0.47190922 train_kl= 0.00861 train_loss= 0.48051 train_acc= 0.52816 val_roc= 0.90967 val_ap= 0.91277 time= 0.11451\n",
      "训练次数: 4 Epoch: 0029 log_lik= 0.4690804 train_kl= 0.00861 train_loss= 0.47769 train_acc= 0.52943 val_roc= 0.91045 val_ap= 0.91363 time= 0.11005\n",
      "训练次数: 4 Epoch: 0030 log_lik= 0.46670884 train_kl= 0.00862 train_loss= 0.47533 train_acc= 0.53089 val_roc= 0.91155 val_ap= 0.91532 time= 0.10655\n",
      "训练次数: 4 Epoch: 0031 log_lik= 0.46473953 train_kl= 0.00863 train_loss= 0.47337 train_acc= 0.53224 val_roc= 0.91246 val_ap= 0.91731 time= 0.10754\n",
      "训练次数: 4 Epoch: 0032 log_lik= 0.46295163 train_kl= 0.00863 train_loss= 0.47158 train_acc= 0.53334 val_roc= 0.91343 val_ap= 0.91872 time= 0.12199\n",
      "训练次数: 4 Epoch: 0033 log_lik= 0.46157387 train_kl= 0.00863 train_loss= 0.47020 train_acc= 0.53504 val_roc= 0.91457 val_ap= 0.92038 time= 0.11352\n",
      "训练次数: 4 Epoch: 0034 log_lik= 0.46033823 train_kl= 0.00863 train_loss= 0.46897 train_acc= 0.53582 val_roc= 0.91522 val_ap= 0.92097 time= 0.12348\n",
      "训练次数: 4 Epoch: 0035 log_lik= 0.45884877 train_kl= 0.00863 train_loss= 0.46747 train_acc= 0.53687 val_roc= 0.91581 val_ap= 0.92111 time= 0.10754\n",
      "训练次数: 4 Epoch: 0036 log_lik= 0.45731562 train_kl= 0.00862 train_loss= 0.46594 train_acc= 0.53837 val_roc= 0.91636 val_ap= 0.92141 time= 0.10555\n",
      "训练次数: 4 Epoch: 0037 log_lik= 0.4560321 train_kl= 0.00862 train_loss= 0.46466 train_acc= 0.53956 val_roc= 0.91739 val_ap= 0.92232 time= 0.12447\n",
      "训练次数: 4 Epoch: 0038 log_lik= 0.45511776 train_kl= 0.00863 train_loss= 0.46375 train_acc= 0.54071 val_roc= 0.91887 val_ap= 0.92373 time= 0.12646\n",
      "训练次数: 4 Epoch: 0039 log_lik= 0.45423877 train_kl= 0.00863 train_loss= 0.46287 train_acc= 0.54262 val_roc= 0.91960 val_ap= 0.92450 time= 0.12248\n",
      "训练次数: 4 Epoch: 0040 log_lik= 0.4531663 train_kl= 0.00864 train_loss= 0.46181 train_acc= 0.54466 val_roc= 0.92021 val_ap= 0.92519 time= 0.10655\n",
      "训练次数: 4 Epoch: 0041 log_lik= 0.45195574 train_kl= 0.00865 train_loss= 0.46060 train_acc= 0.54650 val_roc= 0.92054 val_ap= 0.92542 time= 0.12455\n",
      "训练次数: 4 Epoch: 0042 log_lik= 0.45086253 train_kl= 0.00865 train_loss= 0.45951 train_acc= 0.54813 val_roc= 0.92157 val_ap= 0.92627 time= 0.10953\n",
      "训练次数: 4 Epoch: 0043 log_lik= 0.44965455 train_kl= 0.00866 train_loss= 0.45831 train_acc= 0.54947 val_roc= 0.92192 val_ap= 0.92640 time= 0.11053\n",
      "训练次数: 4 Epoch: 0044 log_lik= 0.44855526 train_kl= 0.00866 train_loss= 0.45721 train_acc= 0.55047 val_roc= 0.92207 val_ap= 0.92624 time= 0.11252\n",
      "训练次数: 4 Epoch: 0045 log_lik= 0.44752637 train_kl= 0.00866 train_loss= 0.45619 train_acc= 0.55154 val_roc= 0.92241 val_ap= 0.92607 time= 0.12148\n",
      "训练次数: 4 Epoch: 0046 log_lik= 0.4463898 train_kl= 0.00866 train_loss= 0.45505 train_acc= 0.55189 val_roc= 0.92258 val_ap= 0.92583 time= 0.11252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 Epoch: 0047 log_lik= 0.44575486 train_kl= 0.00866 train_loss= 0.45441 train_acc= 0.55322 val_roc= 0.92281 val_ap= 0.92543 time= 0.12547\n",
      "训练次数: 4 Epoch: 0048 log_lik= 0.44496086 train_kl= 0.00866 train_loss= 0.45362 train_acc= 0.55424 val_roc= 0.92278 val_ap= 0.92529 time= 0.12148\n",
      "训练次数: 4 Epoch: 0049 log_lik= 0.44412297 train_kl= 0.00865 train_loss= 0.45278 train_acc= 0.55475 val_roc= 0.92323 val_ap= 0.92596 time= 0.11451\n",
      "训练次数: 4 Epoch: 0050 log_lik= 0.44349867 train_kl= 0.00865 train_loss= 0.45215 train_acc= 0.55545 val_roc= 0.92372 val_ap= 0.92615 time= 0.11551\n",
      "训练次数: 4 Epoch: 0051 log_lik= 0.4429462 train_kl= 0.00866 train_loss= 0.45160 train_acc= 0.55593 val_roc= 0.92393 val_ap= 0.92624 time= 0.11451\n",
      "训练次数: 4 Epoch: 0052 log_lik= 0.4423348 train_kl= 0.00866 train_loss= 0.45099 train_acc= 0.55653 val_roc= 0.92434 val_ap= 0.92658 time= 0.11949\n",
      "训练次数: 4 Epoch: 0053 log_lik= 0.44161636 train_kl= 0.00866 train_loss= 0.45028 train_acc= 0.55718 val_roc= 0.92475 val_ap= 0.92699 time= 0.10953\n",
      "训练次数: 4 Epoch: 0054 log_lik= 0.44097453 train_kl= 0.00867 train_loss= 0.44964 train_acc= 0.55758 val_roc= 0.92504 val_ap= 0.92732 time= 0.11053\n",
      "训练次数: 4 Epoch: 0055 log_lik= 0.440313 train_kl= 0.00867 train_loss= 0.44898 train_acc= 0.55783 val_roc= 0.92530 val_ap= 0.92748 time= 0.11252\n",
      "训练次数: 4 Epoch: 0056 log_lik= 0.43970647 train_kl= 0.00867 train_loss= 0.44838 train_acc= 0.55889 val_roc= 0.92580 val_ap= 0.92822 time= 0.11053\n",
      "训练次数: 4 Epoch: 0057 log_lik= 0.43916 train_kl= 0.00867 train_loss= 0.44783 train_acc= 0.56028 val_roc= 0.92667 val_ap= 0.92910 time= 0.12049\n",
      "训练次数: 4 Epoch: 0058 log_lik= 0.43839923 train_kl= 0.00867 train_loss= 0.44707 train_acc= 0.56117 val_roc= 0.92750 val_ap= 0.93001 time= 0.11949\n",
      "训练次数: 4 Epoch: 0059 log_lik= 0.43783674 train_kl= 0.00868 train_loss= 0.44651 train_acc= 0.56220 val_roc= 0.92828 val_ap= 0.93092 time= 0.10754\n",
      "训练次数: 4 Epoch: 0060 log_lik= 0.4374185 train_kl= 0.00868 train_loss= 0.44609 train_acc= 0.56217 val_roc= 0.92855 val_ap= 0.93123 time= 0.12447\n",
      "训练次数: 4 Epoch: 0061 log_lik= 0.43690532 train_kl= 0.00868 train_loss= 0.44558 train_acc= 0.56275 val_roc= 0.92909 val_ap= 0.93201 time= 0.11053\n",
      "训练次数: 4 Epoch: 0062 log_lik= 0.43643424 train_kl= 0.00868 train_loss= 0.44511 train_acc= 0.56370 val_roc= 0.92916 val_ap= 0.93212 time= 0.11750\n",
      "训练次数: 4 Epoch: 0063 log_lik= 0.43596333 train_kl= 0.00868 train_loss= 0.44464 train_acc= 0.56412 val_roc= 0.92912 val_ap= 0.93234 time= 0.11055\n",
      "训练次数: 4 Epoch: 0064 log_lik= 0.4355248 train_kl= 0.00868 train_loss= 0.44420 train_acc= 0.56506 val_roc= 0.92952 val_ap= 0.93276 time= 0.11053\n",
      "训练次数: 4 Epoch: 0065 log_lik= 0.4350756 train_kl= 0.00868 train_loss= 0.44375 train_acc= 0.56564 val_roc= 0.93037 val_ap= 0.93350 time= 0.10953\n",
      "训练次数: 4 Epoch: 0066 log_lik= 0.4346741 train_kl= 0.00868 train_loss= 0.44335 train_acc= 0.56644 val_roc= 0.93043 val_ap= 0.93383 time= 0.12250\n",
      "训练次数: 4 Epoch: 0067 log_lik= 0.43431708 train_kl= 0.00868 train_loss= 0.44300 train_acc= 0.56742 val_roc= 0.93071 val_ap= 0.93415 time= 0.11451\n",
      "训练次数: 4 Epoch: 0068 log_lik= 0.43393984 train_kl= 0.00868 train_loss= 0.44262 train_acc= 0.56783 val_roc= 0.93033 val_ap= 0.93389 time= 0.11949\n",
      "训练次数: 4 Epoch: 0069 log_lik= 0.43350706 train_kl= 0.00869 train_loss= 0.44219 train_acc= 0.56826 val_roc= 0.92988 val_ap= 0.93357 time= 0.11750\n",
      "训练次数: 4 Epoch: 0070 log_lik= 0.4330618 train_kl= 0.00869 train_loss= 0.44175 train_acc= 0.56834 val_roc= 0.92964 val_ap= 0.93342 time= 0.11352\n",
      "训练次数: 4 Epoch: 0071 log_lik= 0.4327484 train_kl= 0.00869 train_loss= 0.44144 train_acc= 0.56852 val_roc= 0.92952 val_ap= 0.93333 time= 0.11153\n",
      "训练次数: 4 Epoch: 0072 log_lik= 0.4323804 train_kl= 0.00869 train_loss= 0.44107 train_acc= 0.56914 val_roc= 0.92959 val_ap= 0.93331 time= 0.11053\n",
      "训练次数: 4 Epoch: 0073 log_lik= 0.43205512 train_kl= 0.00869 train_loss= 0.44075 train_acc= 0.56929 val_roc= 0.92953 val_ap= 0.93330 time= 0.11850\n",
      "训练次数: 4 Epoch: 0074 log_lik= 0.43165255 train_kl= 0.00869 train_loss= 0.44034 train_acc= 0.56949 val_roc= 0.92942 val_ap= 0.93322 time= 0.12348\n",
      "训练次数: 4 Epoch: 0075 log_lik= 0.43132892 train_kl= 0.00869 train_loss= 0.44002 train_acc= 0.56976 val_roc= 0.92894 val_ap= 0.93291 time= 0.10854\n",
      "训练次数: 4 Epoch: 0076 log_lik= 0.4309952 train_kl= 0.00869 train_loss= 0.43968 train_acc= 0.57009 val_roc= 0.92881 val_ap= 0.93297 time= 0.10854\n",
      "训练次数: 4 Epoch: 0077 log_lik= 0.43062878 train_kl= 0.00869 train_loss= 0.43932 train_acc= 0.56994 val_roc= 0.92864 val_ap= 0.93280 time= 0.12447\n",
      "训练次数: 4 Epoch: 0078 log_lik= 0.43027925 train_kl= 0.00869 train_loss= 0.43897 train_acc= 0.56985 val_roc= 0.92828 val_ap= 0.93249 time= 0.12049\n",
      "训练次数: 4 Epoch: 0079 log_lik= 0.4299892 train_kl= 0.00869 train_loss= 0.43868 train_acc= 0.57019 val_roc= 0.92806 val_ap= 0.93235 time= 0.11053\n",
      "训练次数: 4 Epoch: 0080 log_lik= 0.42963883 train_kl= 0.00869 train_loss= 0.43833 train_acc= 0.57050 val_roc= 0.92789 val_ap= 0.93223 time= 0.10953\n",
      "训练次数: 4 Epoch: 0081 log_lik= 0.42928034 train_kl= 0.00869 train_loss= 0.43797 train_acc= 0.57084 val_roc= 0.92780 val_ap= 0.93215 time= 0.11152\n",
      "训练次数: 4 Epoch: 0082 log_lik= 0.42889836 train_kl= 0.00870 train_loss= 0.43760 train_acc= 0.57118 val_roc= 0.92792 val_ap= 0.93230 time= 0.10754\n",
      "训练次数: 4 Epoch: 0083 log_lik= 0.42855445 train_kl= 0.00870 train_loss= 0.43725 train_acc= 0.57173 val_roc= 0.92768 val_ap= 0.93224 time= 0.10854\n",
      "训练次数: 4 Epoch: 0084 log_lik= 0.4282959 train_kl= 0.00870 train_loss= 0.43700 train_acc= 0.57137 val_roc= 0.92761 val_ap= 0.93221 time= 0.12503\n",
      "训练次数: 4 Epoch: 0085 log_lik= 0.42800575 train_kl= 0.00870 train_loss= 0.43671 train_acc= 0.57154 val_roc= 0.92774 val_ap= 0.93243 time= 0.11252\n",
      "训练次数: 4 Epoch: 0086 log_lik= 0.42767778 train_kl= 0.00870 train_loss= 0.43638 train_acc= 0.57167 val_roc= 0.92768 val_ap= 0.93221 time= 0.11053\n",
      "训练次数: 4 Epoch: 0087 log_lik= 0.42739385 train_kl= 0.00870 train_loss= 0.43610 train_acc= 0.57214 val_roc= 0.92745 val_ap= 0.93190 time= 0.10953\n",
      "训练次数: 4 Epoch: 0088 log_lik= 0.42711416 train_kl= 0.00871 train_loss= 0.43582 train_acc= 0.57261 val_roc= 0.92742 val_ap= 0.93183 time= 0.12148\n",
      "训练次数: 4 Epoch: 0089 log_lik= 0.4267324 train_kl= 0.00871 train_loss= 0.43544 train_acc= 0.57273 val_roc= 0.92764 val_ap= 0.93197 time= 0.10754\n",
      "训练次数: 4 Epoch: 0090 log_lik= 0.42648476 train_kl= 0.00871 train_loss= 0.43519 train_acc= 0.57277 val_roc= 0.92763 val_ap= 0.93193 time= 0.10854\n",
      "训练次数: 4 Epoch: 0091 log_lik= 0.42619264 train_kl= 0.00871 train_loss= 0.43490 train_acc= 0.57318 val_roc= 0.92744 val_ap= 0.93185 time= 0.10854\n",
      "训练次数: 4 Epoch: 0092 log_lik= 0.42590916 train_kl= 0.00871 train_loss= 0.43462 train_acc= 0.57349 val_roc= 0.92745 val_ap= 0.93184 time= 0.11153\n",
      "训练次数: 4 Epoch: 0093 log_lik= 0.42563757 train_kl= 0.00871 train_loss= 0.43435 train_acc= 0.57336 val_roc= 0.92735 val_ap= 0.93168 time= 0.11451\n",
      "训练次数: 4 Epoch: 0094 log_lik= 0.42534578 train_kl= 0.00871 train_loss= 0.43406 train_acc= 0.57356 val_roc= 0.92711 val_ap= 0.93131 time= 0.10953\n",
      "训练次数: 4 Epoch: 0095 log_lik= 0.42502263 train_kl= 0.00871 train_loss= 0.43373 train_acc= 0.57357 val_roc= 0.92728 val_ap= 0.93140 time= 0.11252\n",
      "训练次数: 4 Epoch: 0096 log_lik= 0.42466557 train_kl= 0.00871 train_loss= 0.43338 train_acc= 0.57349 val_roc= 0.92731 val_ap= 0.93157 time= 0.11551\n",
      "训练次数: 4 Epoch: 0097 log_lik= 0.4243785 train_kl= 0.00871 train_loss= 0.43309 train_acc= 0.57370 val_roc= 0.92732 val_ap= 0.93157 time= 0.11452\n",
      "训练次数: 4 Epoch: 0098 log_lik= 0.42411575 train_kl= 0.00871 train_loss= 0.43283 train_acc= 0.57362 val_roc= 0.92751 val_ap= 0.93176 time= 0.11651\n",
      "训练次数: 4 Epoch: 0099 log_lik= 0.42373726 train_kl= 0.00872 train_loss= 0.43245 train_acc= 0.57413 val_roc= 0.92771 val_ap= 0.93193 time= 0.11252\n",
      "训练次数: 4 Epoch: 0100 log_lik= 0.4234503 train_kl= 0.00872 train_loss= 0.43217 train_acc= 0.57386 val_roc= 0.92794 val_ap= 0.93210 time= 0.11152\n",
      "Optimization Finished!\n",
      "训练次数: 4 ROC score: 0.9171674546050287\n",
      "训练次数: 4 AP score: 0.922206450362789\n",
      "训练次数: 5 Epoch: 0001 log_lik= 0.78334713 train_kl= 0.00805 train_loss= 0.79140 train_acc= 0.08098 val_roc= 0.72742 val_ap= 0.76261 time= 8.40614\n",
      "训练次数: 5 Epoch: 0002 log_lik= 0.8377858 train_kl= 0.00834 train_loss= 0.84612 train_acc= 0.00160 val_roc= 0.79799 val_ap= 0.81698 time= 0.12771\n",
      "训练次数: 5 Epoch: 0003 log_lik= 0.74800265 train_kl= 0.00810 train_loss= 0.75610 train_acc= 0.00948 val_roc= 0.73205 val_ap= 0.75309 time= 0.11451\n",
      "训练次数: 5 Epoch: 0004 log_lik= 0.7420881 train_kl= 0.00812 train_loss= 0.75021 train_acc= 0.00404 val_roc= 0.70961 val_ap= 0.73605 time= 0.13016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0005 log_lik= 0.7601814 train_kl= 0.00823 train_loss= 0.76841 train_acc= 0.00182 val_roc= 0.72746 val_ap= 0.75092 time= 0.11252\n",
      "训练次数: 5 Epoch: 0006 log_lik= 0.7405884 train_kl= 0.00819 train_loss= 0.74878 train_acc= 0.00222 val_roc= 0.75475 val_ap= 0.77285 time= 0.12549\n",
      "训练次数: 5 Epoch: 0007 log_lik= 0.7287261 train_kl= 0.00815 train_loss= 0.73688 train_acc= 0.00666 val_roc= 0.78346 val_ap= 0.79164 time= 0.11352\n",
      "训练次数: 5 Epoch: 0008 log_lik= 0.7235205 train_kl= 0.00814 train_loss= 0.73166 train_acc= 0.02139 val_roc= 0.79806 val_ap= 0.79803 time= 0.13245\n",
      "训练次数: 5 Epoch: 0009 log_lik= 0.7161634 train_kl= 0.00814 train_loss= 0.72430 train_acc= 0.04558 val_roc= 0.80270 val_ap= 0.79850 time= 0.11153\n",
      "训练次数: 5 Epoch: 0010 log_lik= 0.70409095 train_kl= 0.00816 train_loss= 0.71225 train_acc= 0.06870 val_roc= 0.80770 val_ap= 0.80116 time= 0.11212\n",
      "训练次数: 5 Epoch: 0011 log_lik= 0.6892366 train_kl= 0.00819 train_loss= 0.69742 train_acc= 0.10006 val_roc= 0.81314 val_ap= 0.80705 time= 0.11451\n",
      "训练次数: 5 Epoch: 0012 log_lik= 0.67210317 train_kl= 0.00822 train_loss= 0.68032 train_acc= 0.13552 val_roc= 0.81870 val_ap= 0.81328 time= 0.12348\n",
      "训练次数: 5 Epoch: 0013 log_lik= 0.6546827 train_kl= 0.00826 train_loss= 0.66294 train_acc= 0.17895 val_roc= 0.82095 val_ap= 0.81583 time= 0.11053\n",
      "训练次数: 5 Epoch: 0014 log_lik= 0.6391548 train_kl= 0.00830 train_loss= 0.64745 train_acc= 0.22951 val_roc= 0.81901 val_ap= 0.81531 time= 0.11451\n",
      "训练次数: 5 Epoch: 0015 log_lik= 0.62582815 train_kl= 0.00834 train_loss= 0.63416 train_acc= 0.28076 val_roc= 0.81703 val_ap= 0.81391 time= 0.12955\n",
      "训练次数: 5 Epoch: 0016 log_lik= 0.6120792 train_kl= 0.00837 train_loss= 0.62045 train_acc= 0.33550 val_roc= 0.81648 val_ap= 0.81247 time= 0.10953\n",
      "训练次数: 5 Epoch: 0017 log_lik= 0.5972839 train_kl= 0.00840 train_loss= 0.60569 train_acc= 0.38789 val_roc= 0.81956 val_ap= 0.81401 time= 0.11053\n",
      "训练次数: 5 Epoch: 0018 log_lik= 0.5850488 train_kl= 0.00843 train_loss= 0.59348 train_acc= 0.42939 val_roc= 0.82395 val_ap= 0.81772 time= 0.11254\n",
      "训练次数: 5 Epoch: 0019 log_lik= 0.57487285 train_kl= 0.00845 train_loss= 0.58332 train_acc= 0.45704 val_roc= 0.82725 val_ap= 0.82215 time= 0.11451\n",
      "训练次数: 5 Epoch: 0020 log_lik= 0.564214 train_kl= 0.00845 train_loss= 0.57267 train_acc= 0.47704 val_roc= 0.83147 val_ap= 0.82759 time= 0.11153\n",
      "训练次数: 5 Epoch: 0021 log_lik= 0.5545921 train_kl= 0.00845 train_loss= 0.56304 train_acc= 0.48936 val_roc= 0.83520 val_ap= 0.83257 time= 0.12349\n",
      "训练次数: 5 Epoch: 0022 log_lik= 0.54604816 train_kl= 0.00845 train_loss= 0.55450 train_acc= 0.49720 val_roc= 0.83786 val_ap= 0.83554 time= 0.12447\n",
      "训练次数: 5 Epoch: 0023 log_lik= 0.53995067 train_kl= 0.00845 train_loss= 0.54840 train_acc= 0.50033 val_roc= 0.83975 val_ap= 0.83888 time= 0.12547\n",
      "训练次数: 5 Epoch: 0024 log_lik= 0.5357049 train_kl= 0.00846 train_loss= 0.54416 train_acc= 0.50267 val_roc= 0.84230 val_ap= 0.84376 time= 0.12049\n",
      "训练次数: 5 Epoch: 0025 log_lik= 0.5311005 train_kl= 0.00847 train_loss= 0.53957 train_acc= 0.50648 val_roc= 0.84610 val_ap= 0.84933 time= 0.10754\n",
      "训练次数: 5 Epoch: 0026 log_lik= 0.5263637 train_kl= 0.00848 train_loss= 0.53485 train_acc= 0.51092 val_roc= 0.84969 val_ap= 0.85500 time= 0.12447\n",
      "训练次数: 5 Epoch: 0027 log_lik= 0.52229893 train_kl= 0.00850 train_loss= 0.53080 train_acc= 0.51263 val_roc= 0.85437 val_ap= 0.86195 time= 0.12547\n",
      "训练次数: 5 Epoch: 0028 log_lik= 0.51883155 train_kl= 0.00852 train_loss= 0.52735 train_acc= 0.51270 val_roc= 0.85936 val_ap= 0.86889 time= 0.12049\n",
      "训练次数: 5 Epoch: 0029 log_lik= 0.51388365 train_kl= 0.00853 train_loss= 0.52241 train_acc= 0.51213 val_roc= 0.86386 val_ap= 0.87432 time= 0.12051\n",
      "训练次数: 5 Epoch: 0030 log_lik= 0.50845593 train_kl= 0.00854 train_loss= 0.51700 train_acc= 0.51335 val_roc= 0.86897 val_ap= 0.88011 time= 0.11252\n",
      "训练次数: 5 Epoch: 0031 log_lik= 0.5038439 train_kl= 0.00855 train_loss= 0.51239 train_acc= 0.51377 val_roc= 0.87335 val_ap= 0.88488 time= 0.11158\n",
      "训练次数: 5 Epoch: 0032 log_lik= 0.49940103 train_kl= 0.00855 train_loss= 0.50796 train_acc= 0.51628 val_roc= 0.87564 val_ap= 0.88732 time= 0.12646\n",
      "训练次数: 5 Epoch: 0033 log_lik= 0.4950395 train_kl= 0.00856 train_loss= 0.50360 train_acc= 0.52134 val_roc= 0.87802 val_ap= 0.89009 time= 0.12447\n",
      "训练次数: 5 Epoch: 0034 log_lik= 0.49094975 train_kl= 0.00857 train_loss= 0.49952 train_acc= 0.52686 val_roc= 0.87932 val_ap= 0.89117 time= 0.12447\n",
      "训练次数: 5 Epoch: 0035 log_lik= 0.48757514 train_kl= 0.00857 train_loss= 0.49615 train_acc= 0.53196 val_roc= 0.88096 val_ap= 0.89243 time= 0.11252\n",
      "训练次数: 5 Epoch: 0036 log_lik= 0.48523942 train_kl= 0.00858 train_loss= 0.49382 train_acc= 0.53562 val_roc= 0.88213 val_ap= 0.89324 time= 0.11352\n",
      "训练次数: 5 Epoch: 0037 log_lik= 0.48320666 train_kl= 0.00859 train_loss= 0.49179 train_acc= 0.53710 val_roc= 0.88414 val_ap= 0.89432 time= 0.11352\n",
      "训练次数: 5 Epoch: 0038 log_lik= 0.48153657 train_kl= 0.00859 train_loss= 0.49012 train_acc= 0.53775 val_roc= 0.88566 val_ap= 0.89534 time= 0.11750\n",
      "训练次数: 5 Epoch: 0039 log_lik= 0.4803007 train_kl= 0.00859 train_loss= 0.48889 train_acc= 0.53683 val_roc= 0.88611 val_ap= 0.89515 time= 0.12148\n",
      "训练次数: 5 Epoch: 0040 log_lik= 0.47940245 train_kl= 0.00859 train_loss= 0.48799 train_acc= 0.53656 val_roc= 0.88590 val_ap= 0.89474 time= 0.11053\n",
      "训练次数: 5 Epoch: 0041 log_lik= 0.4784105 train_kl= 0.00858 train_loss= 0.48699 train_acc= 0.53720 val_roc= 0.88530 val_ap= 0.89420 time= 0.11303\n",
      "训练次数: 5 Epoch: 0042 log_lik= 0.4774056 train_kl= 0.00858 train_loss= 0.48598 train_acc= 0.53829 val_roc= 0.88496 val_ap= 0.89395 time= 0.12348\n",
      "训练次数: 5 Epoch: 0043 log_lik= 0.47583354 train_kl= 0.00857 train_loss= 0.48441 train_acc= 0.53945 val_roc= 0.88518 val_ap= 0.89369 time= 0.11054\n",
      "训练次数: 5 Epoch: 0044 log_lik= 0.47454336 train_kl= 0.00857 train_loss= 0.48312 train_acc= 0.54104 val_roc= 0.88534 val_ap= 0.89366 time= 0.10953\n",
      "训练次数: 5 Epoch: 0045 log_lik= 0.47317648 train_kl= 0.00857 train_loss= 0.48175 train_acc= 0.54256 val_roc= 0.88655 val_ap= 0.89449 time= 0.11252\n",
      "训练次数: 5 Epoch: 0046 log_lik= 0.47149092 train_kl= 0.00857 train_loss= 0.48007 train_acc= 0.54428 val_roc= 0.88746 val_ap= 0.89528 time= 0.11252\n",
      "训练次数: 5 Epoch: 0047 log_lik= 0.4697355 train_kl= 0.00858 train_loss= 0.47831 train_acc= 0.54570 val_roc= 0.88905 val_ap= 0.89661 time= 0.11153\n",
      "训练次数: 5 Epoch: 0048 log_lik= 0.4683328 train_kl= 0.00858 train_loss= 0.47691 train_acc= 0.54758 val_roc= 0.89028 val_ap= 0.89759 time= 0.10854\n",
      "训练次数: 5 Epoch: 0049 log_lik= 0.46711338 train_kl= 0.00858 train_loss= 0.47569 train_acc= 0.54821 val_roc= 0.89105 val_ap= 0.89798 time= 0.12248\n",
      "训练次数: 5 Epoch: 0050 log_lik= 0.4659797 train_kl= 0.00859 train_loss= 0.47457 train_acc= 0.54918 val_roc= 0.89184 val_ap= 0.89822 time= 0.11352\n",
      "训练次数: 5 Epoch: 0051 log_lik= 0.46479657 train_kl= 0.00859 train_loss= 0.47339 train_acc= 0.55001 val_roc= 0.89239 val_ap= 0.89849 time= 0.10854\n",
      "训练次数: 5 Epoch: 0052 log_lik= 0.46368074 train_kl= 0.00860 train_loss= 0.47228 train_acc= 0.55098 val_roc= 0.89247 val_ap= 0.89853 time= 0.12270\n",
      "训练次数: 5 Epoch: 0053 log_lik= 0.46280706 train_kl= 0.00860 train_loss= 0.47141 train_acc= 0.55160 val_roc= 0.89273 val_ap= 0.89880 time= 0.11949\n",
      "训练次数: 5 Epoch: 0054 log_lik= 0.4619015 train_kl= 0.00860 train_loss= 0.47050 train_acc= 0.55196 val_roc= 0.89261 val_ap= 0.89903 time= 0.10953\n",
      "训练次数: 5 Epoch: 0055 log_lik= 0.46103668 train_kl= 0.00860 train_loss= 0.46964 train_acc= 0.55251 val_roc= 0.89319 val_ap= 0.90004 time= 0.11352\n",
      "训练次数: 5 Epoch: 0056 log_lik= 0.46018323 train_kl= 0.00860 train_loss= 0.46878 train_acc= 0.55352 val_roc= 0.89348 val_ap= 0.90044 time= 0.11849\n",
      "训练次数: 5 Epoch: 0057 log_lik= 0.4593373 train_kl= 0.00860 train_loss= 0.46794 train_acc= 0.55390 val_roc= 0.89396 val_ap= 0.90115 time= 0.11451\n",
      "训练次数: 5 Epoch: 0058 log_lik= 0.4584408 train_kl= 0.00860 train_loss= 0.46704 train_acc= 0.55519 val_roc= 0.89406 val_ap= 0.90169 time= 0.11053\n",
      "训练次数: 5 Epoch: 0059 log_lik= 0.45769328 train_kl= 0.00860 train_loss= 0.46629 train_acc= 0.55632 val_roc= 0.89488 val_ap= 0.90298 time= 0.11053\n",
      "训练次数: 5 Epoch: 0060 log_lik= 0.4568456 train_kl= 0.00860 train_loss= 0.46545 train_acc= 0.55746 val_roc= 0.89591 val_ap= 0.90408 time= 0.11153\n",
      "训练次数: 5 Epoch: 0061 log_lik= 0.45595878 train_kl= 0.00860 train_loss= 0.46456 train_acc= 0.55850 val_roc= 0.89675 val_ap= 0.90492 time= 0.11252\n",
      "训练次数: 5 Epoch: 0062 log_lik= 0.4551721 train_kl= 0.00861 train_loss= 0.46378 train_acc= 0.55981 val_roc= 0.89732 val_ap= 0.90543 time= 0.12945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0063 log_lik= 0.4542884 train_kl= 0.00861 train_loss= 0.46290 train_acc= 0.56123 val_roc= 0.89896 val_ap= 0.90669 time= 0.11551\n",
      "训练次数: 5 Epoch: 0064 log_lik= 0.45335534 train_kl= 0.00861 train_loss= 0.46197 train_acc= 0.56293 val_roc= 0.90009 val_ap= 0.90744 time= 0.11456\n",
      "训练次数: 5 Epoch: 0065 log_lik= 0.45253986 train_kl= 0.00862 train_loss= 0.46116 train_acc= 0.56437 val_roc= 0.90100 val_ap= 0.90809 time= 0.12447\n",
      "训练次数: 5 Epoch: 0066 log_lik= 0.45164055 train_kl= 0.00862 train_loss= 0.46026 train_acc= 0.56501 val_roc= 0.90233 val_ap= 0.90938 time= 0.10854\n",
      "训练次数: 5 Epoch: 0067 log_lik= 0.45083562 train_kl= 0.00862 train_loss= 0.45946 train_acc= 0.56562 val_roc= 0.90292 val_ap= 0.91011 time= 0.11850\n",
      "训练次数: 5 Epoch: 0068 log_lik= 0.44991907 train_kl= 0.00863 train_loss= 0.45855 train_acc= 0.56685 val_roc= 0.90374 val_ap= 0.91081 time= 0.12447\n",
      "训练次数: 5 Epoch: 0069 log_lik= 0.44910908 train_kl= 0.00863 train_loss= 0.45774 train_acc= 0.56803 val_roc= 0.90484 val_ap= 0.91192 time= 0.12148\n",
      "训练次数: 5 Epoch: 0070 log_lik= 0.44808793 train_kl= 0.00863 train_loss= 0.45672 train_acc= 0.56909 val_roc= 0.90555 val_ap= 0.91269 time= 0.10854\n",
      "训练次数: 5 Epoch: 0071 log_lik= 0.4471019 train_kl= 0.00863 train_loss= 0.45574 train_acc= 0.57111 val_roc= 0.90642 val_ap= 0.91375 time= 0.11153\n",
      "训练次数: 5 Epoch: 0072 log_lik= 0.44620556 train_kl= 0.00864 train_loss= 0.45484 train_acc= 0.57222 val_roc= 0.90711 val_ap= 0.91466 time= 0.11451\n",
      "训练次数: 5 Epoch: 0073 log_lik= 0.44514507 train_kl= 0.00864 train_loss= 0.45378 train_acc= 0.57405 val_roc= 0.90772 val_ap= 0.91545 time= 0.11153\n",
      "训练次数: 5 Epoch: 0074 log_lik= 0.4441979 train_kl= 0.00864 train_loss= 0.45284 train_acc= 0.57574 val_roc= 0.90827 val_ap= 0.91621 time= 0.11156\n",
      "训练次数: 5 Epoch: 0075 log_lik= 0.44320184 train_kl= 0.00865 train_loss= 0.45185 train_acc= 0.57771 val_roc= 0.90869 val_ap= 0.91674 time= 0.12447\n",
      "训练次数: 5 Epoch: 0076 log_lik= 0.4422738 train_kl= 0.00865 train_loss= 0.45092 train_acc= 0.57893 val_roc= 0.90944 val_ap= 0.91753 time= 0.11650\n",
      "训练次数: 5 Epoch: 0077 log_lik= 0.4415038 train_kl= 0.00865 train_loss= 0.45016 train_acc= 0.58023 val_roc= 0.90944 val_ap= 0.91771 time= 0.11153\n",
      "训练次数: 5 Epoch: 0078 log_lik= 0.44057178 train_kl= 0.00866 train_loss= 0.44923 train_acc= 0.58171 val_roc= 0.90986 val_ap= 0.91841 time= 0.11153\n",
      "训练次数: 5 Epoch: 0079 log_lik= 0.43982202 train_kl= 0.00867 train_loss= 0.44849 train_acc= 0.58299 val_roc= 0.91015 val_ap= 0.91886 time= 0.11153\n",
      "训练次数: 5 Epoch: 0080 log_lik= 0.43908212 train_kl= 0.00867 train_loss= 0.44775 train_acc= 0.58444 val_roc= 0.91015 val_ap= 0.91899 time= 0.12049\n",
      "训练次数: 5 Epoch: 0081 log_lik= 0.4383568 train_kl= 0.00868 train_loss= 0.44703 train_acc= 0.58548 val_roc= 0.91005 val_ap= 0.91908 time= 0.10954\n",
      "训练次数: 5 Epoch: 0082 log_lik= 0.4377116 train_kl= 0.00868 train_loss= 0.44639 train_acc= 0.58679 val_roc= 0.90961 val_ap= 0.91857 time= 0.10655\n",
      "训练次数: 5 Epoch: 0083 log_lik= 0.43704572 train_kl= 0.00868 train_loss= 0.44573 train_acc= 0.58811 val_roc= 0.90927 val_ap= 0.91844 time= 0.12845\n",
      "训练次数: 5 Epoch: 0084 log_lik= 0.4364925 train_kl= 0.00868 train_loss= 0.44518 train_acc= 0.58812 val_roc= 0.90912 val_ap= 0.91856 time= 0.12646\n",
      "训练次数: 5 Epoch: 0085 log_lik= 0.43584767 train_kl= 0.00869 train_loss= 0.44453 train_acc= 0.58941 val_roc= 0.90906 val_ap= 0.91873 time= 0.12447\n",
      "训练次数: 5 Epoch: 0086 log_lik= 0.4352798 train_kl= 0.00869 train_loss= 0.44397 train_acc= 0.59009 val_roc= 0.90924 val_ap= 0.91899 time= 0.12348\n",
      "训练次数: 5 Epoch: 0087 log_lik= 0.434713 train_kl= 0.00869 train_loss= 0.44340 train_acc= 0.59045 val_roc= 0.90929 val_ap= 0.91913 time= 0.11153\n",
      "训练次数: 5 Epoch: 0088 log_lik= 0.4340838 train_kl= 0.00869 train_loss= 0.44277 train_acc= 0.59139 val_roc= 0.90945 val_ap= 0.91923 time= 0.11551\n",
      "训练次数: 5 Epoch: 0089 log_lik= 0.43361735 train_kl= 0.00869 train_loss= 0.44231 train_acc= 0.59234 val_roc= 0.90955 val_ap= 0.91966 time= 0.11752\n",
      "训练次数: 5 Epoch: 0090 log_lik= 0.4330583 train_kl= 0.00869 train_loss= 0.44175 train_acc= 0.59230 val_roc= 0.90971 val_ap= 0.92002 time= 0.11153\n",
      "训练次数: 5 Epoch: 0091 log_lik= 0.43248126 train_kl= 0.00869 train_loss= 0.44117 train_acc= 0.59270 val_roc= 0.90973 val_ap= 0.92030 time= 0.11452\n",
      "训练次数: 5 Epoch: 0092 log_lik= 0.432108 train_kl= 0.00869 train_loss= 0.44080 train_acc= 0.59336 val_roc= 0.90999 val_ap= 0.92093 time= 0.12148\n",
      "训练次数: 5 Epoch: 0093 log_lik= 0.43161422 train_kl= 0.00869 train_loss= 0.44031 train_acc= 0.59318 val_roc= 0.91012 val_ap= 0.92137 time= 0.10953\n",
      "训练次数: 5 Epoch: 0094 log_lik= 0.43118215 train_kl= 0.00870 train_loss= 0.43988 train_acc= 0.59370 val_roc= 0.91028 val_ap= 0.92169 time= 0.11352\n",
      "训练次数: 5 Epoch: 0095 log_lik= 0.43068492 train_kl= 0.00870 train_loss= 0.43938 train_acc= 0.59397 val_roc= 0.91055 val_ap= 0.92204 time= 0.10854\n",
      "训练次数: 5 Epoch: 0096 log_lik= 0.4302905 train_kl= 0.00870 train_loss= 0.43899 train_acc= 0.59397 val_roc= 0.91096 val_ap= 0.92260 time= 0.11053\n",
      "训练次数: 5 Epoch: 0097 log_lik= 0.42993054 train_kl= 0.00870 train_loss= 0.43863 train_acc= 0.59371 val_roc= 0.91035 val_ap= 0.92223 time= 0.11651\n",
      "训练次数: 5 Epoch: 0098 log_lik= 0.42952672 train_kl= 0.00870 train_loss= 0.43823 train_acc= 0.59387 val_roc= 0.91022 val_ap= 0.92242 time= 0.11252\n",
      "训练次数: 5 Epoch: 0099 log_lik= 0.42920563 train_kl= 0.00870 train_loss= 0.43791 train_acc= 0.59382 val_roc= 0.91003 val_ap= 0.92265 time= 0.11252\n",
      "训练次数: 5 Epoch: 0100 log_lik= 0.42883188 train_kl= 0.00870 train_loss= 0.43753 train_acc= 0.59399 val_roc= 0.90992 val_ap= 0.92297 time= 0.11850\n",
      "Optimization Finished!\n",
      "训练次数: 5 ROC score: 0.9039279297444632\n",
      "训练次数: 5 AP score: 0.9148463767755352\n",
      "训练次数: 6 Epoch: 0001 log_lik= 0.7833473 train_kl= 0.00805 train_loss= 0.79140 train_acc= 0.07933 val_roc= 0.67793 val_ap= 0.69964 time= 8.30470\n",
      "训练次数: 6 Epoch: 0002 log_lik= 0.8506009 train_kl= 0.00834 train_loss= 0.85895 train_acc= 0.00160 val_roc= 0.76368 val_ap= 0.77641 time= 0.12646\n",
      "训练次数: 6 Epoch: 0003 log_lik= 0.7466291 train_kl= 0.00810 train_loss= 0.75473 train_acc= 0.01012 val_roc= 0.71507 val_ap= 0.72696 time= 0.12248\n",
      "训练次数: 6 Epoch: 0004 log_lik= 0.7391659 train_kl= 0.00814 train_loss= 0.74731 train_acc= 0.00454 val_roc= 0.70681 val_ap= 0.72205 time= 0.12447\n",
      "训练次数: 6 Epoch: 0005 log_lik= 0.76840967 train_kl= 0.00825 train_loss= 0.77666 train_acc= 0.00176 val_roc= 0.72801 val_ap= 0.74812 time= 0.11352\n",
      "训练次数: 6 Epoch: 0006 log_lik= 0.733338 train_kl= 0.00819 train_loss= 0.74153 train_acc= 0.00463 val_roc= 0.75115 val_ap= 0.77489 time= 0.12547\n",
      "训练次数: 6 Epoch: 0007 log_lik= 0.7240932 train_kl= 0.00814 train_loss= 0.73224 train_acc= 0.02453 val_roc= 0.76187 val_ap= 0.77023 time= 0.12149\n",
      "训练次数: 6 Epoch: 0008 log_lik= 0.7200866 train_kl= 0.00813 train_loss= 0.72822 train_acc= 0.08121 val_roc= 0.76598 val_ap= 0.75757 time= 0.11153\n",
      "训练次数: 6 Epoch: 0009 log_lik= 0.7098183 train_kl= 0.00814 train_loss= 0.71796 train_acc= 0.15783 val_roc= 0.76848 val_ap= 0.75822 time= 0.11252\n",
      "训练次数: 6 Epoch: 0010 log_lik= 0.6923588 train_kl= 0.00816 train_loss= 0.70052 train_acc= 0.22525 val_roc= 0.77027 val_ap= 0.76364 time= 0.11252\n",
      "训练次数: 6 Epoch: 0011 log_lik= 0.6716039 train_kl= 0.00820 train_loss= 0.67980 train_acc= 0.27447 val_roc= 0.77277 val_ap= 0.77232 time= 0.13045\n",
      "训练次数: 6 Epoch: 0012 log_lik= 0.6519317 train_kl= 0.00825 train_loss= 0.66018 train_acc= 0.30478 val_roc= 0.77744 val_ap= 0.78015 time= 0.11650\n",
      "训练次数: 6 Epoch: 0013 log_lik= 0.64056045 train_kl= 0.00831 train_loss= 0.64887 train_acc= 0.32117 val_roc= 0.78756 val_ap= 0.78887 time= 0.11451\n",
      "训练次数: 6 Epoch: 0014 log_lik= 0.63226306 train_kl= 0.00837 train_loss= 0.64063 train_acc= 0.33695 val_roc= 0.79670 val_ap= 0.79319 time= 0.11252\n",
      "训练次数: 6 Epoch: 0015 log_lik= 0.6227222 train_kl= 0.00842 train_loss= 0.63114 train_acc= 0.35372 val_roc= 0.80289 val_ap= 0.79428 time= 0.12447\n",
      "训练次数: 6 Epoch: 0016 log_lik= 0.6121054 train_kl= 0.00844 train_loss= 0.62055 train_acc= 0.37598 val_roc= 0.81124 val_ap= 0.80155 time= 0.11650\n",
      "训练次数: 6 Epoch: 0017 log_lik= 0.5986047 train_kl= 0.00845 train_loss= 0.60705 train_acc= 0.39957 val_roc= 0.82122 val_ap= 0.81245 time= 0.12547\n",
      "训练次数: 6 Epoch: 0018 log_lik= 0.5844189 train_kl= 0.00844 train_loss= 0.59286 train_acc= 0.41716 val_roc= 0.83095 val_ap= 0.82503 time= 0.11252\n",
      "训练次数: 6 Epoch: 0019 log_lik= 0.5714399 train_kl= 0.00843 train_loss= 0.57987 train_acc= 0.42916 val_roc= 0.84036 val_ap= 0.83727 time= 0.11153\n",
      "训练次数: 6 Epoch: 0020 log_lik= 0.56060886 train_kl= 0.00842 train_loss= 0.56903 train_acc= 0.44187 val_roc= 0.84831 val_ap= 0.84658 time= 0.11153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0021 log_lik= 0.55216455 train_kl= 0.00843 train_loss= 0.56059 train_acc= 0.45293 val_roc= 0.85376 val_ap= 0.85321 time= 0.11551\n",
      "训练次数: 6 Epoch: 0022 log_lik= 0.54545814 train_kl= 0.00844 train_loss= 0.55389 train_acc= 0.46259 val_roc= 0.85845 val_ap= 0.85920 time= 0.11028\n",
      "训练次数: 6 Epoch: 0023 log_lik= 0.54032767 train_kl= 0.00845 train_loss= 0.54878 train_acc= 0.47102 val_roc= 0.86125 val_ap= 0.86222 time= 0.12348\n",
      "训练次数: 6 Epoch: 0024 log_lik= 0.5351583 train_kl= 0.00847 train_loss= 0.54363 train_acc= 0.47798 val_roc= 0.86284 val_ap= 0.86470 time= 0.11451\n",
      "训练次数: 6 Epoch: 0025 log_lik= 0.5307448 train_kl= 0.00849 train_loss= 0.53923 train_acc= 0.48326 val_roc= 0.86323 val_ap= 0.86477 time= 0.11053\n",
      "训练次数: 6 Epoch: 0026 log_lik= 0.52651966 train_kl= 0.00850 train_loss= 0.53502 train_acc= 0.49010 val_roc= 0.86232 val_ap= 0.86413 time= 0.12148\n",
      "训练次数: 6 Epoch: 0027 log_lik= 0.52321625 train_kl= 0.00852 train_loss= 0.53173 train_acc= 0.49825 val_roc= 0.86150 val_ap= 0.86345 time= 0.11252\n",
      "训练次数: 6 Epoch: 0028 log_lik= 0.51967406 train_kl= 0.00852 train_loss= 0.52820 train_acc= 0.50501 val_roc= 0.86213 val_ap= 0.86418 time= 0.12049\n",
      "训练次数: 6 Epoch: 0029 log_lik= 0.5158182 train_kl= 0.00852 train_loss= 0.52434 train_acc= 0.51136 val_roc= 0.86400 val_ap= 0.86635 time= 0.10854\n",
      "训练次数: 6 Epoch: 0030 log_lik= 0.51170033 train_kl= 0.00852 train_loss= 0.52022 train_acc= 0.51693 val_roc= 0.86754 val_ap= 0.87100 time= 0.11352\n",
      "训练次数: 6 Epoch: 0031 log_lik= 0.5077496 train_kl= 0.00851 train_loss= 0.51626 train_acc= 0.51999 val_roc= 0.86984 val_ap= 0.87428 time= 0.11850\n",
      "训练次数: 6 Epoch: 0032 log_lik= 0.50514907 train_kl= 0.00851 train_loss= 0.51366 train_acc= 0.52134 val_roc= 0.87171 val_ap= 0.87760 time= 0.11849\n",
      "训练次数: 6 Epoch: 0033 log_lik= 0.50326025 train_kl= 0.00851 train_loss= 0.51177 train_acc= 0.52195 val_roc= 0.87179 val_ap= 0.87823 time= 0.14837\n",
      "训练次数: 6 Epoch: 0034 log_lik= 0.50153023 train_kl= 0.00851 train_loss= 0.51004 train_acc= 0.52397 val_roc= 0.87097 val_ap= 0.87716 time= 0.11352\n",
      "训练次数: 6 Epoch: 0035 log_lik= 0.49957687 train_kl= 0.00852 train_loss= 0.50810 train_acc= 0.52667 val_roc= 0.87022 val_ap= 0.87621 time= 0.11252\n",
      "训练次数: 6 Epoch: 0036 log_lik= 0.4975891 train_kl= 0.00853 train_loss= 0.50612 train_acc= 0.52897 val_roc= 0.86906 val_ap= 0.87479 time= 0.12945\n",
      "训练次数: 6 Epoch: 0037 log_lik= 0.49581397 train_kl= 0.00853 train_loss= 0.50435 train_acc= 0.53046 val_roc= 0.86890 val_ap= 0.87470 time= 0.11451\n",
      "训练次数: 6 Epoch: 0038 log_lik= 0.49377462 train_kl= 0.00854 train_loss= 0.50231 train_acc= 0.53120 val_roc= 0.87009 val_ap= 0.87658 time= 0.11053\n",
      "训练次数: 6 Epoch: 0039 log_lik= 0.49115956 train_kl= 0.00854 train_loss= 0.49970 train_acc= 0.53125 val_roc= 0.87119 val_ap= 0.87884 time= 0.11153\n",
      "训练次数: 6 Epoch: 0040 log_lik= 0.48897228 train_kl= 0.00854 train_loss= 0.49751 train_acc= 0.53008 val_roc= 0.87136 val_ap= 0.88050 time= 0.11153\n",
      "训练次数: 6 Epoch: 0041 log_lik= 0.48722777 train_kl= 0.00854 train_loss= 0.49576 train_acc= 0.52964 val_roc= 0.87146 val_ap= 0.88164 time= 0.12148\n",
      "训练次数: 6 Epoch: 0042 log_lik= 0.4854972 train_kl= 0.00854 train_loss= 0.49403 train_acc= 0.52998 val_roc= 0.87201 val_ap= 0.88250 time= 0.11451\n",
      "训练次数: 6 Epoch: 0043 log_lik= 0.483592 train_kl= 0.00854 train_loss= 0.49213 train_acc= 0.53118 val_roc= 0.87163 val_ap= 0.88216 time= 0.12347\n",
      "训练次数: 6 Epoch: 0044 log_lik= 0.48168197 train_kl= 0.00854 train_loss= 0.49022 train_acc= 0.53331 val_roc= 0.87108 val_ap= 0.88206 time= 0.11053\n",
      "训练次数: 6 Epoch: 0045 log_lik= 0.4803081 train_kl= 0.00855 train_loss= 0.48886 train_acc= 0.53530 val_roc= 0.87040 val_ap= 0.88171 time= 0.11451\n",
      "训练次数: 6 Epoch: 0046 log_lik= 0.4791537 train_kl= 0.00855 train_loss= 0.48771 train_acc= 0.53595 val_roc= 0.87038 val_ap= 0.88197 time= 0.10853\n",
      "训练次数: 6 Epoch: 0047 log_lik= 0.47780296 train_kl= 0.00856 train_loss= 0.48636 train_acc= 0.53712 val_roc= 0.87052 val_ap= 0.88276 time= 0.11153\n",
      "训练次数: 6 Epoch: 0048 log_lik= 0.47621575 train_kl= 0.00856 train_loss= 0.48478 train_acc= 0.53915 val_roc= 0.87095 val_ap= 0.88401 time= 0.11850\n",
      "训练次数: 6 Epoch: 0049 log_lik= 0.4747582 train_kl= 0.00856 train_loss= 0.48332 train_acc= 0.54065 val_roc= 0.87137 val_ap= 0.88494 time= 0.12148\n",
      "训练次数: 6 Epoch: 0050 log_lik= 0.47341302 train_kl= 0.00856 train_loss= 0.48198 train_acc= 0.54352 val_roc= 0.87149 val_ap= 0.88545 time= 0.11352\n",
      "训练次数: 6 Epoch: 0051 log_lik= 0.4719993 train_kl= 0.00857 train_loss= 0.48056 train_acc= 0.54530 val_roc= 0.87149 val_ap= 0.88572 time= 0.12049\n",
      "训练次数: 6 Epoch: 0052 log_lik= 0.47052202 train_kl= 0.00857 train_loss= 0.47909 train_acc= 0.54740 val_roc= 0.87165 val_ap= 0.88585 time= 0.11053\n",
      "训练次数: 6 Epoch: 0053 log_lik= 0.46910244 train_kl= 0.00857 train_loss= 0.47767 train_acc= 0.54924 val_roc= 0.87171 val_ap= 0.88610 time= 0.11750\n",
      "训练次数: 6 Epoch: 0054 log_lik= 0.46784934 train_kl= 0.00858 train_loss= 0.47642 train_acc= 0.55110 val_roc= 0.87208 val_ap= 0.88648 time= 0.11352\n",
      "训练次数: 6 Epoch: 0055 log_lik= 0.46645874 train_kl= 0.00858 train_loss= 0.47504 train_acc= 0.55348 val_roc= 0.87301 val_ap= 0.88718 time= 0.11650\n",
      "训练次数: 6 Epoch: 0056 log_lik= 0.4652482 train_kl= 0.00859 train_loss= 0.47384 train_acc= 0.55527 val_roc= 0.87382 val_ap= 0.88792 time= 0.11053\n",
      "训练次数: 6 Epoch: 0057 log_lik= 0.46389234 train_kl= 0.00859 train_loss= 0.47249 train_acc= 0.55728 val_roc= 0.87441 val_ap= 0.88839 time= 0.12547\n",
      "训练次数: 6 Epoch: 0058 log_lik= 0.4626366 train_kl= 0.00860 train_loss= 0.47124 train_acc= 0.55843 val_roc= 0.87542 val_ap= 0.88949 time= 0.11352\n",
      "训练次数: 6 Epoch: 0059 log_lik= 0.4614627 train_kl= 0.00861 train_loss= 0.47007 train_acc= 0.55965 val_roc= 0.87546 val_ap= 0.88944 time= 0.11252\n",
      "训练次数: 6 Epoch: 0060 log_lik= 0.46005917 train_kl= 0.00861 train_loss= 0.46867 train_acc= 0.56030 val_roc= 0.87529 val_ap= 0.88915 time= 0.12447\n",
      "训练次数: 6 Epoch: 0061 log_lik= 0.45891103 train_kl= 0.00862 train_loss= 0.46753 train_acc= 0.56162 val_roc= 0.87481 val_ap= 0.88850 time= 0.12547\n",
      "训练次数: 6 Epoch: 0062 log_lik= 0.45781907 train_kl= 0.00862 train_loss= 0.46644 train_acc= 0.56233 val_roc= 0.87516 val_ap= 0.88885 time= 0.10953\n",
      "训练次数: 6 Epoch: 0063 log_lik= 0.45655516 train_kl= 0.00862 train_loss= 0.46517 train_acc= 0.56320 val_roc= 0.87613 val_ap= 0.88993 time= 0.11510\n",
      "训练次数: 6 Epoch: 0064 log_lik= 0.45522085 train_kl= 0.00862 train_loss= 0.46384 train_acc= 0.56415 val_roc= 0.87629 val_ap= 0.89003 time= 0.12447\n",
      "训练次数: 6 Epoch: 0065 log_lik= 0.45415103 train_kl= 0.00862 train_loss= 0.46277 train_acc= 0.56598 val_roc= 0.87717 val_ap= 0.89106 time= 0.12050\n",
      "训练次数: 6 Epoch: 0066 log_lik= 0.4531781 train_kl= 0.00862 train_loss= 0.46180 train_acc= 0.56622 val_roc= 0.87773 val_ap= 0.89196 time= 0.12649\n",
      "训练次数: 6 Epoch: 0067 log_lik= 0.4520015 train_kl= 0.00862 train_loss= 0.46063 train_acc= 0.56721 val_roc= 0.87756 val_ap= 0.89186 time= 0.11352\n",
      "训练次数: 6 Epoch: 0068 log_lik= 0.45093492 train_kl= 0.00863 train_loss= 0.45956 train_acc= 0.56896 val_roc= 0.87840 val_ap= 0.89257 time= 0.12845\n",
      "训练次数: 6 Epoch: 0069 log_lik= 0.4499245 train_kl= 0.00863 train_loss= 0.45856 train_acc= 0.56945 val_roc= 0.87856 val_ap= 0.89291 time= 0.12646\n",
      "训练次数: 6 Epoch: 0070 log_lik= 0.44901785 train_kl= 0.00864 train_loss= 0.45766 train_acc= 0.57076 val_roc= 0.87918 val_ap= 0.89370 time= 0.11053\n",
      "训练次数: 6 Epoch: 0071 log_lik= 0.44812024 train_kl= 0.00864 train_loss= 0.45676 train_acc= 0.57121 val_roc= 0.88016 val_ap= 0.89484 time= 0.11551\n",
      "训练次数: 6 Epoch: 0072 log_lik= 0.4471948 train_kl= 0.00864 train_loss= 0.45584 train_acc= 0.57110 val_roc= 0.88037 val_ap= 0.89501 time= 0.13144\n",
      "训练次数: 6 Epoch: 0073 log_lik= 0.44651484 train_kl= 0.00865 train_loss= 0.45516 train_acc= 0.57236 val_roc= 0.87990 val_ap= 0.89436 time= 0.11451\n",
      "训练次数: 6 Epoch: 0074 log_lik= 0.44560745 train_kl= 0.00865 train_loss= 0.45426 train_acc= 0.57274 val_roc= 0.87966 val_ap= 0.89395 time= 0.11053\n",
      "训练次数: 6 Epoch: 0075 log_lik= 0.44494075 train_kl= 0.00865 train_loss= 0.45359 train_acc= 0.57303 val_roc= 0.87938 val_ap= 0.89377 time= 0.13244\n",
      "训练次数: 6 Epoch: 0076 log_lik= 0.44418934 train_kl= 0.00865 train_loss= 0.45284 train_acc= 0.57378 val_roc= 0.87914 val_ap= 0.89348 time= 0.10854\n",
      "训练次数: 6 Epoch: 0077 log_lik= 0.44352403 train_kl= 0.00865 train_loss= 0.45218 train_acc= 0.57470 val_roc= 0.87925 val_ap= 0.89349 time= 0.11152\n",
      "训练次数: 6 Epoch: 0078 log_lik= 0.44296446 train_kl= 0.00865 train_loss= 0.45162 train_acc= 0.57509 val_roc= 0.87899 val_ap= 0.89306 time= 0.12547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0079 log_lik= 0.44233474 train_kl= 0.00866 train_loss= 0.45099 train_acc= 0.57519 val_roc= 0.87880 val_ap= 0.89273 time= 0.11252\n",
      "训练次数: 6 Epoch: 0080 log_lik= 0.44170067 train_kl= 0.00866 train_loss= 0.45036 train_acc= 0.57596 val_roc= 0.87846 val_ap= 0.89229 time= 0.11352\n",
      "训练次数: 6 Epoch: 0081 log_lik= 0.44109866 train_kl= 0.00866 train_loss= 0.44976 train_acc= 0.57638 val_roc= 0.87841 val_ap= 0.89225 time= 0.10754\n",
      "训练次数: 6 Epoch: 0082 log_lik= 0.44054675 train_kl= 0.00866 train_loss= 0.44921 train_acc= 0.57669 val_roc= 0.87844 val_ap= 0.89210 time= 0.11252\n",
      "训练次数: 6 Epoch: 0083 log_lik= 0.439969 train_kl= 0.00867 train_loss= 0.44864 train_acc= 0.57709 val_roc= 0.87853 val_ap= 0.89216 time= 0.12248\n",
      "训练次数: 6 Epoch: 0084 log_lik= 0.43941766 train_kl= 0.00867 train_loss= 0.44809 train_acc= 0.57796 val_roc= 0.87801 val_ap= 0.89137 time= 0.10953\n",
      "训练次数: 6 Epoch: 0085 log_lik= 0.43894878 train_kl= 0.00867 train_loss= 0.44762 train_acc= 0.57765 val_roc= 0.87749 val_ap= 0.89082 time= 0.11053\n",
      "训练次数: 6 Epoch: 0086 log_lik= 0.4384898 train_kl= 0.00867 train_loss= 0.44716 train_acc= 0.57819 val_roc= 0.87756 val_ap= 0.89090 time= 0.11252\n",
      "训练次数: 6 Epoch: 0087 log_lik= 0.43801403 train_kl= 0.00867 train_loss= 0.44668 train_acc= 0.57870 val_roc= 0.87779 val_ap= 0.89132 time= 0.12248\n",
      "训练次数: 6 Epoch: 0088 log_lik= 0.4375435 train_kl= 0.00867 train_loss= 0.44621 train_acc= 0.57894 val_roc= 0.87792 val_ap= 0.89181 time= 0.11053\n",
      "训练次数: 6 Epoch: 0089 log_lik= 0.4370788 train_kl= 0.00867 train_loss= 0.44575 train_acc= 0.57937 val_roc= 0.87805 val_ap= 0.89220 time= 0.11053\n",
      "训练次数: 6 Epoch: 0090 log_lik= 0.43655324 train_kl= 0.00867 train_loss= 0.44523 train_acc= 0.58006 val_roc= 0.87788 val_ap= 0.89227 time= 0.11451\n",
      "训练次数: 6 Epoch: 0091 log_lik= 0.43619424 train_kl= 0.00867 train_loss= 0.44487 train_acc= 0.58029 val_roc= 0.87804 val_ap= 0.89256 time= 0.11153\n",
      "训练次数: 6 Epoch: 0092 log_lik= 0.43578482 train_kl= 0.00868 train_loss= 0.44446 train_acc= 0.58097 val_roc= 0.87843 val_ap= 0.89319 time= 0.11053\n",
      "训练次数: 6 Epoch: 0093 log_lik= 0.4353302 train_kl= 0.00868 train_loss= 0.44401 train_acc= 0.58108 val_roc= 0.87873 val_ap= 0.89373 time= 0.12347\n",
      "训练次数: 6 Epoch: 0094 log_lik= 0.43491292 train_kl= 0.00868 train_loss= 0.44359 train_acc= 0.58149 val_roc= 0.87895 val_ap= 0.89419 time= 0.11551\n",
      "训练次数: 6 Epoch: 0095 log_lik= 0.43446144 train_kl= 0.00868 train_loss= 0.44314 train_acc= 0.58191 val_roc= 0.87901 val_ap= 0.89433 time= 0.11403\n",
      "训练次数: 6 Epoch: 0096 log_lik= 0.43411702 train_kl= 0.00868 train_loss= 0.44280 train_acc= 0.58235 val_roc= 0.87895 val_ap= 0.89448 time= 0.12248\n",
      "训练次数: 6 Epoch: 0097 log_lik= 0.4337583 train_kl= 0.00868 train_loss= 0.44244 train_acc= 0.58296 val_roc= 0.87928 val_ap= 0.89532 time= 0.11253\n",
      "训练次数: 6 Epoch: 0098 log_lik= 0.43338808 train_kl= 0.00868 train_loss= 0.44207 train_acc= 0.58263 val_roc= 0.87963 val_ap= 0.89583 time= 0.11495\n",
      "训练次数: 6 Epoch: 0099 log_lik= 0.43310213 train_kl= 0.00868 train_loss= 0.44179 train_acc= 0.58307 val_roc= 0.87996 val_ap= 0.89639 time= 0.12347\n",
      "训练次数: 6 Epoch: 0100 log_lik= 0.4327443 train_kl= 0.00868 train_loss= 0.44143 train_acc= 0.58302 val_roc= 0.88025 val_ap= 0.89675 time= 0.11153\n",
      "Optimization Finished!\n",
      "训练次数: 6 ROC score: 0.901648729516903\n",
      "训练次数: 6 AP score: 0.9114662564523994\n",
      "训练次数: 7 Epoch: 0001 log_lik= 0.7829496 train_kl= 0.00805 train_loss= 0.79100 train_acc= 0.16188 val_roc= 0.67950 val_ap= 0.70227 time= 8.89157\n",
      "训练次数: 7 Epoch: 0002 log_lik= 0.7894905 train_kl= 0.00829 train_loss= 0.79778 train_acc= 0.00160 val_roc= 0.74198 val_ap= 0.73424 time= 0.13443\n",
      "训练次数: 7 Epoch: 0003 log_lik= 0.73472416 train_kl= 0.00811 train_loss= 0.74283 train_acc= 0.04944 val_roc= 0.69499 val_ap= 0.68574 time= 0.12148\n",
      "训练次数: 7 Epoch: 0004 log_lik= 0.72069806 train_kl= 0.00813 train_loss= 0.72883 train_acc= 0.09002 val_roc= 0.69265 val_ap= 0.69287 time= 0.12049\n",
      "训练次数: 7 Epoch: 0005 log_lik= 0.7080637 train_kl= 0.00822 train_loss= 0.71628 train_acc= 0.05244 val_roc= 0.74941 val_ap= 0.74261 time= 0.11153\n",
      "训练次数: 7 Epoch: 0006 log_lik= 0.6801284 train_kl= 0.00823 train_loss= 0.68836 train_acc= 0.09454 val_roc= 0.77429 val_ap= 0.75839 time= 0.12447\n",
      "训练次数: 7 Epoch: 0007 log_lik= 0.654701 train_kl= 0.00825 train_loss= 0.66295 train_acc= 0.16555 val_roc= 0.78136 val_ap= 0.76312 time= 0.12348\n",
      "训练次数: 7 Epoch: 0008 log_lik= 0.62778544 train_kl= 0.00827 train_loss= 0.63606 train_acc= 0.26402 val_roc= 0.77833 val_ap= 0.75755 time= 0.11053\n",
      "训练次数: 7 Epoch: 0009 log_lik= 0.60406905 train_kl= 0.00831 train_loss= 0.61238 train_acc= 0.35041 val_roc= 0.77837 val_ap= 0.75554 time= 0.12646\n",
      "训练次数: 7 Epoch: 0010 log_lik= 0.5903986 train_kl= 0.00836 train_loss= 0.59876 train_acc= 0.40013 val_roc= 0.78717 val_ap= 0.76749 time= 0.11551\n",
      "训练次数: 7 Epoch: 0011 log_lik= 0.5712863 train_kl= 0.00840 train_loss= 0.57969 train_acc= 0.43596 val_roc= 0.80056 val_ap= 0.78291 time= 0.11044\n",
      "训练次数: 7 Epoch: 0012 log_lik= 0.55689645 train_kl= 0.00844 train_loss= 0.56533 train_acc= 0.45756 val_roc= 0.81360 val_ap= 0.79931 time= 0.12347\n",
      "训练次数: 7 Epoch: 0013 log_lik= 0.5459204 train_kl= 0.00848 train_loss= 0.55440 train_acc= 0.46272 val_roc= 0.82764 val_ap= 0.81926 time= 0.11949\n",
      "训练次数: 7 Epoch: 0014 log_lik= 0.5321483 train_kl= 0.00851 train_loss= 0.54066 train_acc= 0.46421 val_roc= 0.83954 val_ap= 0.83709 time= 0.11949\n",
      "训练次数: 7 Epoch: 0015 log_lik= 0.520165 train_kl= 0.00855 train_loss= 0.52871 train_acc= 0.46897 val_roc= 0.84862 val_ap= 0.85086 time= 0.12546\n",
      "训练次数: 7 Epoch: 0016 log_lik= 0.5113377 train_kl= 0.00857 train_loss= 0.51991 train_acc= 0.47920 val_roc= 0.85421 val_ap= 0.85827 time= 0.11651\n",
      "训练次数: 7 Epoch: 0017 log_lik= 0.5046048 train_kl= 0.00859 train_loss= 0.51320 train_acc= 0.49165 val_roc= 0.85882 val_ap= 0.86147 time= 0.11650\n",
      "训练次数: 7 Epoch: 0018 log_lik= 0.4982312 train_kl= 0.00860 train_loss= 0.50683 train_acc= 0.50639 val_roc= 0.86089 val_ap= 0.86053 time= 0.12945\n",
      "训练次数: 7 Epoch: 0019 log_lik= 0.49442336 train_kl= 0.00861 train_loss= 0.50303 train_acc= 0.51800 val_roc= 0.86144 val_ap= 0.85931 time= 0.11153\n",
      "训练次数: 7 Epoch: 0020 log_lik= 0.49163917 train_kl= 0.00861 train_loss= 0.50025 train_acc= 0.52380 val_roc= 0.86410 val_ap= 0.86245 time= 0.12348\n",
      "训练次数: 7 Epoch: 0021 log_lik= 0.48854527 train_kl= 0.00861 train_loss= 0.49715 train_acc= 0.52534 val_roc= 0.86847 val_ap= 0.86847 time= 0.11849\n",
      "训练次数: 7 Epoch: 0022 log_lik= 0.4864296 train_kl= 0.00861 train_loss= 0.49504 train_acc= 0.52389 val_roc= 0.87253 val_ap= 0.87448 time= 0.12903\n",
      "训练次数: 7 Epoch: 0023 log_lik= 0.48451814 train_kl= 0.00861 train_loss= 0.49313 train_acc= 0.52267 val_roc= 0.87591 val_ap= 0.87935 time= 0.13044\n",
      "训练次数: 7 Epoch: 0024 log_lik= 0.48196894 train_kl= 0.00861 train_loss= 0.49058 train_acc= 0.52530 val_roc= 0.87851 val_ap= 0.88270 time= 0.11053\n",
      "训练次数: 7 Epoch: 0025 log_lik= 0.47805914 train_kl= 0.00861 train_loss= 0.48667 train_acc= 0.53029 val_roc= 0.88032 val_ap= 0.88407 time= 0.11153\n",
      "训练次数: 7 Epoch: 0026 log_lik= 0.47462654 train_kl= 0.00861 train_loss= 0.48323 train_acc= 0.53555 val_roc= 0.88201 val_ap= 0.88653 time= 0.11404\n",
      "训练次数: 7 Epoch: 0027 log_lik= 0.47115162 train_kl= 0.00861 train_loss= 0.47976 train_acc= 0.54104 val_roc= 0.88415 val_ap= 0.88969 time= 0.11451\n",
      "训练次数: 7 Epoch: 0028 log_lik= 0.46741673 train_kl= 0.00861 train_loss= 0.47603 train_acc= 0.54527 val_roc= 0.88689 val_ap= 0.89322 time= 0.11651\n",
      "训练次数: 7 Epoch: 0029 log_lik= 0.46432832 train_kl= 0.00861 train_loss= 0.47294 train_acc= 0.54840 val_roc= 0.88913 val_ap= 0.89565 time= 0.11252\n",
      "训练次数: 7 Epoch: 0030 log_lik= 0.4618658 train_kl= 0.00862 train_loss= 0.47048 train_acc= 0.55037 val_roc= 0.89079 val_ap= 0.89692 time= 0.11252\n",
      "训练次数: 7 Epoch: 0031 log_lik= 0.46024415 train_kl= 0.00863 train_loss= 0.46887 train_acc= 0.55218 val_roc= 0.89252 val_ap= 0.89838 time= 0.11850\n",
      "训练次数: 7 Epoch: 0032 log_lik= 0.4589061 train_kl= 0.00864 train_loss= 0.46754 train_acc= 0.55365 val_roc= 0.89407 val_ap= 0.90017 time= 0.11651\n",
      "训练次数: 7 Epoch: 0033 log_lik= 0.45749462 train_kl= 0.00864 train_loss= 0.46614 train_acc= 0.55518 val_roc= 0.89569 val_ap= 0.90129 time= 0.12261\n",
      "训练次数: 7 Epoch: 0034 log_lik= 0.45641822 train_kl= 0.00865 train_loss= 0.46507 train_acc= 0.55612 val_roc= 0.89699 val_ap= 0.90174 time= 0.11053\n",
      "训练次数: 7 Epoch: 0035 log_lik= 0.4554544 train_kl= 0.00865 train_loss= 0.46411 train_acc= 0.55657 val_roc= 0.89822 val_ap= 0.90300 time= 0.12355\n",
      "训练次数: 7 Epoch: 0036 log_lik= 0.4545255 train_kl= 0.00865 train_loss= 0.46318 train_acc= 0.55701 val_roc= 0.89954 val_ap= 0.90392 time= 0.12049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0037 log_lik= 0.45325842 train_kl= 0.00865 train_loss= 0.46191 train_acc= 0.55740 val_roc= 0.90127 val_ap= 0.90505 time= 0.11850\n",
      "训练次数: 7 Epoch: 0038 log_lik= 0.45186862 train_kl= 0.00865 train_loss= 0.46051 train_acc= 0.55846 val_roc= 0.90270 val_ap= 0.90629 time= 0.11454\n",
      "训练次数: 7 Epoch: 0039 log_lik= 0.45077312 train_kl= 0.00864 train_loss= 0.45941 train_acc= 0.55905 val_roc= 0.90429 val_ap= 0.90803 time= 0.11352\n",
      "训练次数: 7 Epoch: 0040 log_lik= 0.44982037 train_kl= 0.00864 train_loss= 0.45846 train_acc= 0.56028 val_roc= 0.90549 val_ap= 0.90874 time= 0.11153\n",
      "训练次数: 7 Epoch: 0041 log_lik= 0.44875225 train_kl= 0.00863 train_loss= 0.45739 train_acc= 0.56067 val_roc= 0.90658 val_ap= 0.90980 time= 0.12148\n",
      "训练次数: 7 Epoch: 0042 log_lik= 0.4474745 train_kl= 0.00864 train_loss= 0.45611 train_acc= 0.56091 val_roc= 0.90762 val_ap= 0.91110 time= 0.10754\n",
      "训练次数: 7 Epoch: 0043 log_lik= 0.44632313 train_kl= 0.00864 train_loss= 0.45496 train_acc= 0.56187 val_roc= 0.90815 val_ap= 0.91200 time= 0.12049\n",
      "训练次数: 7 Epoch: 0044 log_lik= 0.445465 train_kl= 0.00865 train_loss= 0.45411 train_acc= 0.56182 val_roc= 0.90873 val_ap= 0.91294 time= 0.10706\n",
      "训练次数: 7 Epoch: 0045 log_lik= 0.44484058 train_kl= 0.00865 train_loss= 0.45349 train_acc= 0.56111 val_roc= 0.90895 val_ap= 0.91380 time= 0.12646\n",
      "训练次数: 7 Epoch: 0046 log_lik= 0.444002 train_kl= 0.00866 train_loss= 0.45266 train_acc= 0.56157 val_roc= 0.90914 val_ap= 0.91427 time= 0.12447\n",
      "训练次数: 7 Epoch: 0047 log_lik= 0.44324014 train_kl= 0.00867 train_loss= 0.45191 train_acc= 0.56167 val_roc= 0.90908 val_ap= 0.91443 time= 0.10648\n",
      "训练次数: 7 Epoch: 0048 log_lik= 0.44256547 train_kl= 0.00867 train_loss= 0.45124 train_acc= 0.56234 val_roc= 0.90911 val_ap= 0.91418 time= 0.12048\n",
      "训练次数: 7 Epoch: 0049 log_lik= 0.4418718 train_kl= 0.00867 train_loss= 0.45055 train_acc= 0.56281 val_roc= 0.90886 val_ap= 0.91405 time= 0.11651\n",
      "训练次数: 7 Epoch: 0050 log_lik= 0.4410398 train_kl= 0.00867 train_loss= 0.44971 train_acc= 0.56369 val_roc= 0.90908 val_ap= 0.91457 time= 0.11475\n",
      "训练次数: 7 Epoch: 0051 log_lik= 0.4401983 train_kl= 0.00867 train_loss= 0.44887 train_acc= 0.56434 val_roc= 0.90903 val_ap= 0.91488 time= 0.12547\n",
      "训练次数: 7 Epoch: 0052 log_lik= 0.43972468 train_kl= 0.00867 train_loss= 0.44840 train_acc= 0.56514 val_roc= 0.90888 val_ap= 0.91448 time= 0.12746\n",
      "训练次数: 7 Epoch: 0053 log_lik= 0.43916205 train_kl= 0.00867 train_loss= 0.44783 train_acc= 0.56486 val_roc= 0.90893 val_ap= 0.91445 time= 0.11053\n",
      "训练次数: 7 Epoch: 0054 log_lik= 0.43857685 train_kl= 0.00867 train_loss= 0.44724 train_acc= 0.56552 val_roc= 0.90932 val_ap= 0.91425 time= 0.12845\n",
      "训练次数: 7 Epoch: 0055 log_lik= 0.4380395 train_kl= 0.00867 train_loss= 0.44671 train_acc= 0.56571 val_roc= 0.90934 val_ap= 0.91383 time= 0.12845\n",
      "训练次数: 7 Epoch: 0056 log_lik= 0.43752044 train_kl= 0.00867 train_loss= 0.44619 train_acc= 0.56624 val_roc= 0.90928 val_ap= 0.91326 time= 0.11807\n",
      "训练次数: 7 Epoch: 0057 log_lik= 0.43684933 train_kl= 0.00867 train_loss= 0.44552 train_acc= 0.56696 val_roc= 0.90916 val_ap= 0.91285 time= 0.11153\n",
      "训练次数: 7 Epoch: 0058 log_lik= 0.43639645 train_kl= 0.00867 train_loss= 0.44507 train_acc= 0.56710 val_roc= 0.90927 val_ap= 0.91273 time= 0.12352\n",
      "训练次数: 7 Epoch: 0059 log_lik= 0.43595302 train_kl= 0.00867 train_loss= 0.44463 train_acc= 0.56746 val_roc= 0.90908 val_ap= 0.91259 time= 0.12547\n",
      "训练次数: 7 Epoch: 0060 log_lik= 0.43550003 train_kl= 0.00868 train_loss= 0.44418 train_acc= 0.56725 val_roc= 0.90890 val_ap= 0.91283 time= 0.11352\n",
      "训练次数: 7 Epoch: 0061 log_lik= 0.4349191 train_kl= 0.00868 train_loss= 0.44360 train_acc= 0.56739 val_roc= 0.90909 val_ap= 0.91299 time= 0.12547\n",
      "训练次数: 7 Epoch: 0062 log_lik= 0.43465465 train_kl= 0.00868 train_loss= 0.44334 train_acc= 0.56727 val_roc= 0.90921 val_ap= 0.91256 time= 0.11551\n",
      "训练次数: 7 Epoch: 0063 log_lik= 0.43421867 train_kl= 0.00868 train_loss= 0.44290 train_acc= 0.56754 val_roc= 0.90935 val_ap= 0.91270 time= 0.11451\n",
      "训练次数: 7 Epoch: 0064 log_lik= 0.43378058 train_kl= 0.00869 train_loss= 0.44247 train_acc= 0.56737 val_roc= 0.90931 val_ap= 0.91297 time= 0.11153\n",
      "训练次数: 7 Epoch: 0065 log_lik= 0.43335274 train_kl= 0.00869 train_loss= 0.44204 train_acc= 0.56774 val_roc= 0.90932 val_ap= 0.91309 time= 0.11303\n",
      "训练次数: 7 Epoch: 0066 log_lik= 0.43288064 train_kl= 0.00869 train_loss= 0.44157 train_acc= 0.56777 val_roc= 0.90937 val_ap= 0.91326 time= 0.12646\n",
      "训练次数: 7 Epoch: 0067 log_lik= 0.43248653 train_kl= 0.00869 train_loss= 0.44117 train_acc= 0.56744 val_roc= 0.90993 val_ap= 0.91394 time= 0.12348\n",
      "训练次数: 7 Epoch: 0068 log_lik= 0.4321152 train_kl= 0.00868 train_loss= 0.44080 train_acc= 0.56762 val_roc= 0.91045 val_ap= 0.91446 time= 0.11352\n",
      "训练次数: 7 Epoch: 0069 log_lik= 0.431701 train_kl= 0.00868 train_loss= 0.44039 train_acc= 0.56807 val_roc= 0.91073 val_ap= 0.91467 time= 0.12447\n",
      "训练次数: 7 Epoch: 0070 log_lik= 0.4313597 train_kl= 0.00868 train_loss= 0.44004 train_acc= 0.56822 val_roc= 0.91091 val_ap= 0.91514 time= 0.11152\n",
      "训练次数: 7 Epoch: 0071 log_lik= 0.4308965 train_kl= 0.00869 train_loss= 0.43958 train_acc= 0.56863 val_roc= 0.91116 val_ap= 0.91552 time= 0.12348\n",
      "训练次数: 7 Epoch: 0072 log_lik= 0.4305578 train_kl= 0.00869 train_loss= 0.43924 train_acc= 0.56868 val_roc= 0.91100 val_ap= 0.91574 time= 0.11451\n",
      "训练次数: 7 Epoch: 0073 log_lik= 0.4301713 train_kl= 0.00869 train_loss= 0.43886 train_acc= 0.56918 val_roc= 0.91139 val_ap= 0.91633 time= 0.13244\n",
      "训练次数: 7 Epoch: 0074 log_lik= 0.4298589 train_kl= 0.00869 train_loss= 0.43855 train_acc= 0.56930 val_roc= 0.91148 val_ap= 0.91653 time= 0.12347\n",
      "训练次数: 7 Epoch: 0075 log_lik= 0.42941436 train_kl= 0.00869 train_loss= 0.43811 train_acc= 0.56933 val_roc= 0.91178 val_ap= 0.91681 time= 0.12049\n",
      "训练次数: 7 Epoch: 0076 log_lik= 0.4290242 train_kl= 0.00870 train_loss= 0.43772 train_acc= 0.56984 val_roc= 0.91185 val_ap= 0.91696 time= 0.12447\n",
      "训练次数: 7 Epoch: 0077 log_lik= 0.42865813 train_kl= 0.00870 train_loss= 0.43736 train_acc= 0.57020 val_roc= 0.91191 val_ap= 0.91737 time= 0.11850\n",
      "训练次数: 7 Epoch: 0078 log_lik= 0.42827293 train_kl= 0.00870 train_loss= 0.43697 train_acc= 0.57076 val_roc= 0.91171 val_ap= 0.91720 time= 0.12447\n",
      "训练次数: 7 Epoch: 0079 log_lik= 0.42784807 train_kl= 0.00870 train_loss= 0.43655 train_acc= 0.57104 val_roc= 0.91128 val_ap= 0.91690 time= 0.12369\n",
      "训练次数: 7 Epoch: 0080 log_lik= 0.42755142 train_kl= 0.00870 train_loss= 0.43625 train_acc= 0.57179 val_roc= 0.91129 val_ap= 0.91689 time= 0.11252\n",
      "训练次数: 7 Epoch: 0081 log_lik= 0.4271047 train_kl= 0.00870 train_loss= 0.43580 train_acc= 0.57205 val_roc= 0.91086 val_ap= 0.91647 time= 0.11949\n",
      "训练次数: 7 Epoch: 0082 log_lik= 0.42667857 train_kl= 0.00870 train_loss= 0.43538 train_acc= 0.57251 val_roc= 0.91062 val_ap= 0.91621 time= 0.11252\n",
      "训练次数: 7 Epoch: 0083 log_lik= 0.42636293 train_kl= 0.00870 train_loss= 0.43506 train_acc= 0.57263 val_roc= 0.91031 val_ap= 0.91572 time= 0.11651\n",
      "训练次数: 7 Epoch: 0084 log_lik= 0.4259713 train_kl= 0.00870 train_loss= 0.43467 train_acc= 0.57327 val_roc= 0.91002 val_ap= 0.91517 time= 0.12746\n",
      "训练次数: 7 Epoch: 0085 log_lik= 0.4255864 train_kl= 0.00870 train_loss= 0.43429 train_acc= 0.57292 val_roc= 0.90948 val_ap= 0.91424 time= 0.12348\n",
      "训练次数: 7 Epoch: 0086 log_lik= 0.4252058 train_kl= 0.00871 train_loss= 0.43391 train_acc= 0.57278 val_roc= 0.90922 val_ap= 0.91334 time= 0.11451\n",
      "训练次数: 7 Epoch: 0087 log_lik= 0.42487484 train_kl= 0.00871 train_loss= 0.43358 train_acc= 0.57370 val_roc= 0.90896 val_ap= 0.91281 time= 0.11651\n",
      "训练次数: 7 Epoch: 0088 log_lik= 0.42451358 train_kl= 0.00871 train_loss= 0.43323 train_acc= 0.57395 val_roc= 0.90919 val_ap= 0.91298 time= 0.11451\n",
      "训练次数: 7 Epoch: 0089 log_lik= 0.42418677 train_kl= 0.00872 train_loss= 0.43290 train_acc= 0.57413 val_roc= 0.90886 val_ap= 0.91260 time= 0.11801\n",
      "训练次数: 7 Epoch: 0090 log_lik= 0.4238335 train_kl= 0.00872 train_loss= 0.43255 train_acc= 0.57421 val_roc= 0.90854 val_ap= 0.91214 time= 0.12447\n",
      "训练次数: 7 Epoch: 0091 log_lik= 0.42357877 train_kl= 0.00872 train_loss= 0.43230 train_acc= 0.57421 val_roc= 0.90838 val_ap= 0.91213 time= 0.11153\n",
      "训练次数: 7 Epoch: 0092 log_lik= 0.42326736 train_kl= 0.00872 train_loss= 0.43199 train_acc= 0.57479 val_roc= 0.90815 val_ap= 0.91179 time= 0.12447\n",
      "训练次数: 7 Epoch: 0093 log_lik= 0.42296302 train_kl= 0.00872 train_loss= 0.43169 train_acc= 0.57481 val_roc= 0.90795 val_ap= 0.91157 time= 0.11451\n",
      "训练次数: 7 Epoch: 0094 log_lik= 0.42265975 train_kl= 0.00872 train_loss= 0.43138 train_acc= 0.57479 val_roc= 0.90792 val_ap= 0.91165 time= 0.11152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0095 log_lik= 0.42240936 train_kl= 0.00873 train_loss= 0.43113 train_acc= 0.57524 val_roc= 0.90779 val_ap= 0.91159 time= 0.11153\n",
      "训练次数: 7 Epoch: 0096 log_lik= 0.4221282 train_kl= 0.00873 train_loss= 0.43085 train_acc= 0.57558 val_roc= 0.90746 val_ap= 0.91163 time= 0.11352\n",
      "训练次数: 7 Epoch: 0097 log_lik= 0.42184508 train_kl= 0.00873 train_loss= 0.43057 train_acc= 0.57584 val_roc= 0.90737 val_ap= 0.91199 time= 0.11451\n",
      "训练次数: 7 Epoch: 0098 log_lik= 0.4216003 train_kl= 0.00873 train_loss= 0.43033 train_acc= 0.57529 val_roc= 0.90729 val_ap= 0.91220 time= 0.12348\n",
      "训练次数: 7 Epoch: 0099 log_lik= 0.42139584 train_kl= 0.00873 train_loss= 0.43012 train_acc= 0.57570 val_roc= 0.90675 val_ap= 0.91183 time= 0.10854\n",
      "训练次数: 7 Epoch: 0100 log_lik= 0.42125347 train_kl= 0.00873 train_loss= 0.42998 train_acc= 0.57556 val_roc= 0.90694 val_ap= 0.91222 time= 0.11900\n",
      "Optimization Finished!\n",
      "训练次数: 7 ROC score: 0.9027433217272954\n",
      "训练次数: 7 AP score: 0.8991131583659846\n",
      "训练次数: 8 Epoch: 0001 log_lik= 0.7841961 train_kl= 0.00805 train_loss= 0.79225 train_acc= 0.10038 val_roc= 0.66780 val_ap= 0.69376 time= 9.04866\n",
      "训练次数: 8 Epoch: 0002 log_lik= 0.7920997 train_kl= 0.00829 train_loss= 0.80039 train_acc= 0.00160 val_roc= 0.75863 val_ap= 0.77164 time= 0.13243\n",
      "训练次数: 8 Epoch: 0003 log_lik= 0.7433555 train_kl= 0.00810 train_loss= 0.75146 train_acc= 0.02896 val_roc= 0.71107 val_ap= 0.73396 time= 0.12646\n",
      "训练次数: 8 Epoch: 0004 log_lik= 0.73113394 train_kl= 0.00817 train_loss= 0.73931 train_acc= 0.01023 val_roc= 0.73167 val_ap= 0.75487 time= 0.12646\n",
      "训练次数: 8 Epoch: 0005 log_lik= 0.7252482 train_kl= 0.00821 train_loss= 0.73346 train_acc= 0.00962 val_roc= 0.78852 val_ap= 0.80251 time= 0.11352\n",
      "训练次数: 8 Epoch: 0006 log_lik= 0.7030566 train_kl= 0.00819 train_loss= 0.71124 train_acc= 0.01989 val_roc= 0.82397 val_ap= 0.82392 time= 0.11651\n",
      "训练次数: 8 Epoch: 0007 log_lik= 0.68101203 train_kl= 0.00819 train_loss= 0.68920 train_acc= 0.06923 val_roc= 0.83585 val_ap= 0.82973 time= 0.11750\n",
      "训练次数: 8 Epoch: 0008 log_lik= 0.6519947 train_kl= 0.00822 train_loss= 0.66022 train_acc= 0.16930 val_roc= 0.84703 val_ap= 0.84195 time= 0.11651\n",
      "训练次数: 8 Epoch: 0009 log_lik= 0.6186497 train_kl= 0.00827 train_loss= 0.62692 train_acc= 0.26241 val_roc= 0.86130 val_ap= 0.85587 time= 0.13144\n",
      "训练次数: 8 Epoch: 0010 log_lik= 0.587174 train_kl= 0.00834 train_loss= 0.59551 train_acc= 0.33750 val_roc= 0.87098 val_ap= 0.86583 time= 0.13144\n",
      "训练次数: 8 Epoch: 0011 log_lik= 0.56079984 train_kl= 0.00841 train_loss= 0.56921 train_acc= 0.40846 val_roc= 0.87311 val_ap= 0.86709 time= 0.10854\n",
      "训练次数: 8 Epoch: 0012 log_lik= 0.54363894 train_kl= 0.00849 train_loss= 0.55212 train_acc= 0.46395 val_roc= 0.88005 val_ap= 0.87472 time= 0.11949\n",
      "训练次数: 8 Epoch: 0013 log_lik= 0.5280539 train_kl= 0.00856 train_loss= 0.53661 train_acc= 0.50072 val_roc= 0.88595 val_ap= 0.88115 time= 0.11551\n",
      "训练次数: 8 Epoch: 0014 log_lik= 0.5171615 train_kl= 0.00861 train_loss= 0.52578 train_acc= 0.52044 val_roc= 0.88936 val_ap= 0.88392 time= 0.11651\n",
      "训练次数: 8 Epoch: 0015 log_lik= 0.5075709 train_kl= 0.00865 train_loss= 0.51622 train_acc= 0.53376 val_roc= 0.89151 val_ap= 0.88319 time= 0.12845\n",
      "训练次数: 8 Epoch: 0016 log_lik= 0.49969754 train_kl= 0.00866 train_loss= 0.50836 train_acc= 0.54216 val_roc= 0.89398 val_ap= 0.88361 time= 0.11752\n",
      "训练次数: 8 Epoch: 0017 log_lik= 0.49517208 train_kl= 0.00867 train_loss= 0.50384 train_acc= 0.54444 val_roc= 0.89637 val_ap= 0.88420 time= 0.12646\n",
      "训练次数: 8 Epoch: 0018 log_lik= 0.49106953 train_kl= 0.00867 train_loss= 0.49974 train_acc= 0.54303 val_roc= 0.89835 val_ap= 0.88289 time= 0.12448\n",
      "训练次数: 8 Epoch: 0019 log_lik= 0.48868477 train_kl= 0.00866 train_loss= 0.49735 train_acc= 0.54053 val_roc= 0.89974 val_ap= 0.88261 time= 0.12051\n",
      "训练次数: 8 Epoch: 0020 log_lik= 0.4858374 train_kl= 0.00865 train_loss= 0.49449 train_acc= 0.54081 val_roc= 0.89894 val_ap= 0.88099 time= 0.11352\n",
      "训练次数: 8 Epoch: 0021 log_lik= 0.48230997 train_kl= 0.00863 train_loss= 0.49094 train_acc= 0.54486 val_roc= 0.89935 val_ap= 0.88163 time= 0.11153\n",
      "训练次数: 8 Epoch: 0022 log_lik= 0.47892264 train_kl= 0.00861 train_loss= 0.48753 train_acc= 0.54977 val_roc= 0.90084 val_ap= 0.88499 time= 0.11352\n",
      "训练次数: 8 Epoch: 0023 log_lik= 0.47573483 train_kl= 0.00859 train_loss= 0.48433 train_acc= 0.55322 val_roc= 0.90302 val_ap= 0.88829 time= 0.12049\n",
      "训练次数: 8 Epoch: 0024 log_lik= 0.4722392 train_kl= 0.00859 train_loss= 0.48083 train_acc= 0.55518 val_roc= 0.90604 val_ap= 0.89394 time= 0.11551\n",
      "训练次数: 8 Epoch: 0025 log_lik= 0.46880126 train_kl= 0.00859 train_loss= 0.47739 train_acc= 0.55618 val_roc= 0.90922 val_ap= 0.89845 time= 0.12546\n",
      "训练次数: 8 Epoch: 0026 log_lik= 0.4653183 train_kl= 0.00860 train_loss= 0.47392 train_acc= 0.55754 val_roc= 0.91226 val_ap= 0.90341 time= 0.13244\n",
      "训练次数: 8 Epoch: 0027 log_lik= 0.46212313 train_kl= 0.00862 train_loss= 0.47074 train_acc= 0.55787 val_roc= 0.91541 val_ap= 0.90858 time= 0.11352\n",
      "训练次数: 8 Epoch: 0028 log_lik= 0.45966643 train_kl= 0.00863 train_loss= 0.46830 train_acc= 0.55918 val_roc= 0.91885 val_ap= 0.91382 time= 0.12348\n",
      "训练次数: 8 Epoch: 0029 log_lik= 0.4580285 train_kl= 0.00865 train_loss= 0.46668 train_acc= 0.55958 val_roc= 0.92142 val_ap= 0.91638 time= 0.11952\n",
      "训练次数: 8 Epoch: 0030 log_lik= 0.45675543 train_kl= 0.00867 train_loss= 0.46542 train_acc= 0.56036 val_roc= 0.92322 val_ap= 0.91867 time= 0.11252\n",
      "训练次数: 8 Epoch: 0031 log_lik= 0.45585775 train_kl= 0.00868 train_loss= 0.46454 train_acc= 0.56129 val_roc= 0.92505 val_ap= 0.92073 time= 0.11551\n",
      "训练次数: 8 Epoch: 0032 log_lik= 0.45459992 train_kl= 0.00869 train_loss= 0.46329 train_acc= 0.56292 val_roc= 0.92602 val_ap= 0.92223 time= 0.11850\n",
      "训练次数: 8 Epoch: 0033 log_lik= 0.45309955 train_kl= 0.00869 train_loss= 0.46179 train_acc= 0.56440 val_roc= 0.92615 val_ap= 0.92255 time= 0.13237\n",
      "训练次数: 8 Epoch: 0034 log_lik= 0.45152947 train_kl= 0.00869 train_loss= 0.46022 train_acc= 0.56676 val_roc= 0.92614 val_ap= 0.92318 time= 0.12746\n",
      "训练次数: 8 Epoch: 0035 log_lik= 0.44982666 train_kl= 0.00868 train_loss= 0.45850 train_acc= 0.56866 val_roc= 0.92579 val_ap= 0.92302 time= 0.11452\n",
      "训练次数: 8 Epoch: 0036 log_lik= 0.44826138 train_kl= 0.00867 train_loss= 0.45693 train_acc= 0.57042 val_roc= 0.92546 val_ap= 0.92282 time= 0.12797\n",
      "训练次数: 8 Epoch: 0037 log_lik= 0.44697475 train_kl= 0.00866 train_loss= 0.45563 train_acc= 0.57135 val_roc= 0.92458 val_ap= 0.92162 time= 0.11551\n",
      "训练次数: 8 Epoch: 0038 log_lik= 0.4459809 train_kl= 0.00865 train_loss= 0.45463 train_acc= 0.57175 val_roc= 0.92449 val_ap= 0.92118 time= 0.12248\n",
      "训练次数: 8 Epoch: 0039 log_lik= 0.44486758 train_kl= 0.00865 train_loss= 0.45352 train_acc= 0.57239 val_roc= 0.92482 val_ap= 0.92153 time= 0.12658\n",
      "训练次数: 8 Epoch: 0040 log_lik= 0.44386098 train_kl= 0.00866 train_loss= 0.45252 train_acc= 0.57280 val_roc= 0.92559 val_ap= 0.92296 time= 0.11252\n",
      "训练次数: 8 Epoch: 0041 log_lik= 0.4429605 train_kl= 0.00867 train_loss= 0.45163 train_acc= 0.57346 val_roc= 0.92638 val_ap= 0.92440 time= 0.11750\n",
      "训练次数: 8 Epoch: 0042 log_lik= 0.44209167 train_kl= 0.00868 train_loss= 0.45077 train_acc= 0.57400 val_roc= 0.92676 val_ap= 0.92485 time= 0.11451\n",
      "训练次数: 8 Epoch: 0043 log_lik= 0.4413737 train_kl= 0.00868 train_loss= 0.45006 train_acc= 0.57371 val_roc= 0.92741 val_ap= 0.92571 time= 0.11352\n",
      "训练次数: 8 Epoch: 0044 log_lik= 0.4405776 train_kl= 0.00869 train_loss= 0.44927 train_acc= 0.57470 val_roc= 0.92807 val_ap= 0.92619 time= 0.11352\n",
      "训练次数: 8 Epoch: 0045 log_lik= 0.43988597 train_kl= 0.00869 train_loss= 0.44858 train_acc= 0.57438 val_roc= 0.92877 val_ap= 0.92693 time= 0.11453\n",
      "训练次数: 8 Epoch: 0046 log_lik= 0.4391641 train_kl= 0.00869 train_loss= 0.44786 train_acc= 0.57419 val_roc= 0.93010 val_ap= 0.92879 time= 0.12646\n",
      "训练次数: 8 Epoch: 0047 log_lik= 0.43847987 train_kl= 0.00869 train_loss= 0.44717 train_acc= 0.57480 val_roc= 0.93104 val_ap= 0.93039 time= 0.11451\n",
      "训练次数: 8 Epoch: 0048 log_lik= 0.43778 train_kl= 0.00869 train_loss= 0.44647 train_acc= 0.57527 val_roc= 0.93189 val_ap= 0.93146 time= 0.11949\n",
      "训练次数: 8 Epoch: 0049 log_lik= 0.43720105 train_kl= 0.00868 train_loss= 0.44588 train_acc= 0.57511 val_roc= 0.93260 val_ap= 0.93248 time= 0.12347\n",
      "训练次数: 8 Epoch: 0050 log_lik= 0.4364188 train_kl= 0.00868 train_loss= 0.44510 train_acc= 0.57572 val_roc= 0.93322 val_ap= 0.93293 time= 0.11949\n",
      "训练次数: 8 Epoch: 0051 log_lik= 0.43583366 train_kl= 0.00867 train_loss= 0.44451 train_acc= 0.57575 val_roc= 0.93406 val_ap= 0.93337 time= 0.12547\n",
      "训练次数: 8 Epoch: 0052 log_lik= 0.4353324 train_kl= 0.00867 train_loss= 0.44401 train_acc= 0.57548 val_roc= 0.93522 val_ap= 0.93428 time= 0.12547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 8 Epoch: 0053 log_lik= 0.4347118 train_kl= 0.00867 train_loss= 0.44339 train_acc= 0.57550 val_roc= 0.93632 val_ap= 0.93550 time= 0.11949\n",
      "训练次数: 8 Epoch: 0054 log_lik= 0.43410593 train_kl= 0.00868 train_loss= 0.44279 train_acc= 0.57549 val_roc= 0.93711 val_ap= 0.93625 time= 0.11651\n",
      "训练次数: 8 Epoch: 0055 log_lik= 0.4335739 train_kl= 0.00869 train_loss= 0.44226 train_acc= 0.57575 val_roc= 0.93750 val_ap= 0.93673 time= 0.11153\n",
      "训练次数: 8 Epoch: 0056 log_lik= 0.43302375 train_kl= 0.00869 train_loss= 0.44172 train_acc= 0.57638 val_roc= 0.93780 val_ap= 0.93730 time= 0.12547\n",
      "训练次数: 8 Epoch: 0057 log_lik= 0.4324425 train_kl= 0.00870 train_loss= 0.44114 train_acc= 0.57591 val_roc= 0.93796 val_ap= 0.93742 time= 0.11551\n",
      "训练次数: 8 Epoch: 0058 log_lik= 0.43200198 train_kl= 0.00870 train_loss= 0.44071 train_acc= 0.57620 val_roc= 0.93796 val_ap= 0.93762 time= 0.11750\n",
      "训练次数: 8 Epoch: 0059 log_lik= 0.43144378 train_kl= 0.00871 train_loss= 0.44015 train_acc= 0.57731 val_roc= 0.93827 val_ap= 0.93813 time= 0.12598\n",
      "训练次数: 8 Epoch: 0060 log_lik= 0.4309642 train_kl= 0.00871 train_loss= 0.43967 train_acc= 0.57755 val_roc= 0.93899 val_ap= 0.93873 time= 0.11551\n",
      "训练次数: 8 Epoch: 0061 log_lik= 0.4305102 train_kl= 0.00871 train_loss= 0.43922 train_acc= 0.57835 val_roc= 0.93945 val_ap= 0.93919 time= 0.11651\n",
      "训练次数: 8 Epoch: 0062 log_lik= 0.43002415 train_kl= 0.00871 train_loss= 0.43873 train_acc= 0.57799 val_roc= 0.93996 val_ap= 0.93988 time= 0.11352\n",
      "训练次数: 8 Epoch: 0063 log_lik= 0.42959845 train_kl= 0.00871 train_loss= 0.43831 train_acc= 0.57839 val_roc= 0.94005 val_ap= 0.93995 time= 0.11750\n",
      "训练次数: 8 Epoch: 0064 log_lik= 0.42915413 train_kl= 0.00870 train_loss= 0.43786 train_acc= 0.57850 val_roc= 0.94009 val_ap= 0.94009 time= 0.11451\n",
      "训练次数: 8 Epoch: 0065 log_lik= 0.42869052 train_kl= 0.00870 train_loss= 0.43739 train_acc= 0.57961 val_roc= 0.94049 val_ap= 0.94050 time= 0.11849\n",
      "训练次数: 8 Epoch: 0066 log_lik= 0.42838237 train_kl= 0.00870 train_loss= 0.43708 train_acc= 0.57904 val_roc= 0.94093 val_ap= 0.94097 time= 0.11949\n",
      "训练次数: 8 Epoch: 0067 log_lik= 0.4280534 train_kl= 0.00870 train_loss= 0.43676 train_acc= 0.57975 val_roc= 0.94126 val_ap= 0.94135 time= 0.11650\n",
      "训练次数: 8 Epoch: 0068 log_lik= 0.4276828 train_kl= 0.00870 train_loss= 0.43639 train_acc= 0.57943 val_roc= 0.94179 val_ap= 0.94195 time= 0.12945\n",
      "训练次数: 8 Epoch: 0069 log_lik= 0.4273156 train_kl= 0.00871 train_loss= 0.43602 train_acc= 0.57924 val_roc= 0.94229 val_ap= 0.94244 time= 0.11551\n",
      "训练次数: 8 Epoch: 0070 log_lik= 0.42691568 train_kl= 0.00871 train_loss= 0.43563 train_acc= 0.57893 val_roc= 0.94268 val_ap= 0.94291 time= 0.11252\n",
      "训练次数: 8 Epoch: 0071 log_lik= 0.4265807 train_kl= 0.00871 train_loss= 0.43529 train_acc= 0.57912 val_roc= 0.94292 val_ap= 0.94310 time= 0.12348\n",
      "训练次数: 8 Epoch: 0072 log_lik= 0.42625916 train_kl= 0.00872 train_loss= 0.43498 train_acc= 0.57923 val_roc= 0.94330 val_ap= 0.94336 time= 0.11452\n",
      "训练次数: 8 Epoch: 0073 log_lik= 0.42593566 train_kl= 0.00872 train_loss= 0.43465 train_acc= 0.57936 val_roc= 0.94380 val_ap= 0.94388 time= 0.12646\n",
      "训练次数: 8 Epoch: 0074 log_lik= 0.42557287 train_kl= 0.00872 train_loss= 0.43429 train_acc= 0.57937 val_roc= 0.94431 val_ap= 0.94430 time= 0.12746\n",
      "训练次数: 8 Epoch: 0075 log_lik= 0.42528176 train_kl= 0.00872 train_loss= 0.43400 train_acc= 0.57949 val_roc= 0.94476 val_ap= 0.94473 time= 0.11451\n",
      "训练次数: 8 Epoch: 0076 log_lik= 0.4249869 train_kl= 0.00872 train_loss= 0.43371 train_acc= 0.57939 val_roc= 0.94509 val_ap= 0.94511 time= 0.11252\n",
      "训练次数: 8 Epoch: 0077 log_lik= 0.42465988 train_kl= 0.00872 train_loss= 0.43338 train_acc= 0.57941 val_roc= 0.94528 val_ap= 0.94525 time= 0.12646\n",
      "训练次数: 8 Epoch: 0078 log_lik= 0.4243818 train_kl= 0.00872 train_loss= 0.43310 train_acc= 0.57953 val_roc= 0.94525 val_ap= 0.94517 time= 0.12547\n",
      "训练次数: 8 Epoch: 0079 log_lik= 0.4240454 train_kl= 0.00872 train_loss= 0.43276 train_acc= 0.57936 val_roc= 0.94531 val_ap= 0.94523 time= 0.12347\n",
      "训练次数: 8 Epoch: 0080 log_lik= 0.42373207 train_kl= 0.00872 train_loss= 0.43245 train_acc= 0.58034 val_roc= 0.94583 val_ap= 0.94572 time= 0.12248\n",
      "训练次数: 8 Epoch: 0081 log_lik= 0.42344466 train_kl= 0.00872 train_loss= 0.43216 train_acc= 0.58045 val_roc= 0.94641 val_ap= 0.94612 time= 0.12556\n",
      "训练次数: 8 Epoch: 0082 log_lik= 0.42319226 train_kl= 0.00872 train_loss= 0.43191 train_acc= 0.58034 val_roc= 0.94680 val_ap= 0.94631 time= 0.12447\n",
      "训练次数: 8 Epoch: 0083 log_lik= 0.42283088 train_kl= 0.00872 train_loss= 0.43155 train_acc= 0.58074 val_roc= 0.94683 val_ap= 0.94643 time= 0.11551\n",
      "训练次数: 8 Epoch: 0084 log_lik= 0.4226082 train_kl= 0.00872 train_loss= 0.43133 train_acc= 0.58084 val_roc= 0.94674 val_ap= 0.94644 time= 0.10953\n",
      "训练次数: 8 Epoch: 0085 log_lik= 0.4222595 train_kl= 0.00873 train_loss= 0.43099 train_acc= 0.58068 val_roc= 0.94680 val_ap= 0.94658 time= 0.11252\n",
      "训练次数: 8 Epoch: 0086 log_lik= 0.42196476 train_kl= 0.00873 train_loss= 0.43069 train_acc= 0.58089 val_roc= 0.94709 val_ap= 0.94682 time= 0.11551\n",
      "训练次数: 8 Epoch: 0087 log_lik= 0.42177385 train_kl= 0.00873 train_loss= 0.43050 train_acc= 0.58177 val_roc= 0.94748 val_ap= 0.94718 time= 0.11054\n",
      "训练次数: 8 Epoch: 0088 log_lik= 0.4214604 train_kl= 0.00873 train_loss= 0.43019 train_acc= 0.58165 val_roc= 0.94746 val_ap= 0.94731 time= 0.11052\n",
      "训练次数: 8 Epoch: 0089 log_lik= 0.42128772 train_kl= 0.00873 train_loss= 0.43002 train_acc= 0.58173 val_roc= 0.94732 val_ap= 0.94722 time= 0.11651\n",
      "训练次数: 8 Epoch: 0090 log_lik= 0.42093378 train_kl= 0.00873 train_loss= 0.42967 train_acc= 0.58205 val_roc= 0.94769 val_ap= 0.94764 time= 0.12646\n",
      "训练次数: 8 Epoch: 0091 log_lik= 0.42069572 train_kl= 0.00873 train_loss= 0.42943 train_acc= 0.58197 val_roc= 0.94781 val_ap= 0.94794 time= 0.10754\n",
      "训练次数: 8 Epoch: 0092 log_lik= 0.42050898 train_kl= 0.00873 train_loss= 0.42924 train_acc= 0.58253 val_roc= 0.94775 val_ap= 0.94792 time= 0.10854\n",
      "训练次数: 8 Epoch: 0093 log_lik= 0.42030916 train_kl= 0.00873 train_loss= 0.42904 train_acc= 0.58263 val_roc= 0.94758 val_ap= 0.94790 time= 0.12399\n",
      "训练次数: 8 Epoch: 0094 log_lik= 0.42007104 train_kl= 0.00873 train_loss= 0.42881 train_acc= 0.58306 val_roc= 0.94745 val_ap= 0.94797 time= 0.11352\n",
      "训练次数: 8 Epoch: 0095 log_lik= 0.41982922 train_kl= 0.00874 train_loss= 0.42856 train_acc= 0.58280 val_roc= 0.94768 val_ap= 0.94817 time= 0.11555\n",
      "训练次数: 8 Epoch: 0096 log_lik= 0.41960078 train_kl= 0.00874 train_loss= 0.42834 train_acc= 0.58298 val_roc= 0.94765 val_ap= 0.94822 time= 0.11451\n",
      "训练次数: 8 Epoch: 0097 log_lik= 0.4193915 train_kl= 0.00874 train_loss= 0.42813 train_acc= 0.58306 val_roc= 0.94769 val_ap= 0.94843 time= 0.11152\n",
      "训练次数: 8 Epoch: 0098 log_lik= 0.4192232 train_kl= 0.00874 train_loss= 0.42796 train_acc= 0.58368 val_roc= 0.94764 val_ap= 0.94865 time= 0.12449\n",
      "训练次数: 8 Epoch: 0099 log_lik= 0.4190585 train_kl= 0.00874 train_loss= 0.42780 train_acc= 0.58328 val_roc= 0.94751 val_ap= 0.94877 time= 0.11451\n",
      "训练次数: 8 Epoch: 0100 log_lik= 0.4188974 train_kl= 0.00874 train_loss= 0.42764 train_acc= 0.58410 val_roc= 0.94735 val_ap= 0.94875 time= 0.11207\n",
      "Optimization Finished!\n",
      "训练次数: 8 ROC score: 0.9233209351562133\n",
      "训练次数: 8 AP score: 0.936590209205589\n",
      "训练次数: 9 Epoch: 0001 log_lik= 0.78313625 train_kl= 0.00805 train_loss= 0.79119 train_acc= 0.11984 val_roc= 0.70273 val_ap= 0.73349 time= 9.25640\n",
      "训练次数: 9 Epoch: 0002 log_lik= 0.7684924 train_kl= 0.00827 train_loss= 0.77676 train_acc= 0.00161 val_roc= 0.71784 val_ap= 0.71952 time= 0.12547\n",
      "训练次数: 9 Epoch: 0003 log_lik= 0.7271632 train_kl= 0.00814 train_loss= 0.73530 train_acc= 0.02634 val_roc= 0.67776 val_ap= 0.69975 time= 0.11750\n",
      "训练次数: 9 Epoch: 0004 log_lik= 0.7759491 train_kl= 0.00828 train_loss= 0.78423 train_acc= 0.00688 val_roc= 0.72016 val_ap= 0.73095 time= 0.12148\n",
      "训练次数: 9 Epoch: 0005 log_lik= 0.7156984 train_kl= 0.00818 train_loss= 0.72388 train_acc= 0.05240 val_roc= 0.79012 val_ap= 0.78317 time= 0.11551\n",
      "训练次数: 9 Epoch: 0006 log_lik= 0.6998916 train_kl= 0.00814 train_loss= 0.70803 train_acc= 0.15983 val_roc= 0.81344 val_ap= 0.78620 time= 0.13454\n",
      "训练次数: 9 Epoch: 0007 log_lik= 0.6848286 train_kl= 0.00816 train_loss= 0.69299 train_acc= 0.24866 val_roc= 0.81120 val_ap= 0.77878 time= 0.12746\n",
      "训练次数: 9 Epoch: 0008 log_lik= 0.66451126 train_kl= 0.00821 train_loss= 0.67272 train_acc= 0.25615 val_roc= 0.82582 val_ap= 0.79656 time= 0.11650\n",
      "训练次数: 9 Epoch: 0009 log_lik= 0.6413976 train_kl= 0.00828 train_loss= 0.64967 train_acc= 0.26054 val_roc= 0.85081 val_ap= 0.82681 time= 0.11551\n",
      "训练次数: 9 Epoch: 0010 log_lik= 0.6131817 train_kl= 0.00833 train_loss= 0.62151 train_acc= 0.29640 val_roc= 0.85616 val_ap= 0.83760 time= 0.12746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0011 log_lik= 0.58848834 train_kl= 0.00839 train_loss= 0.59687 train_acc= 0.35480 val_roc= 0.85398 val_ap= 0.83931 time= 0.13045\n",
      "训练次数: 9 Epoch: 0012 log_lik= 0.5677435 train_kl= 0.00844 train_loss= 0.57618 train_acc= 0.41900 val_roc= 0.85893 val_ap= 0.84271 time= 0.11551\n",
      "训练次数: 9 Epoch: 0013 log_lik= 0.5488832 train_kl= 0.00850 train_loss= 0.55738 train_acc= 0.46788 val_roc= 0.86602 val_ap= 0.84627 time= 0.12746\n",
      "训练次数: 9 Epoch: 0014 log_lik= 0.5419957 train_kl= 0.00856 train_loss= 0.55056 train_acc= 0.48607 val_roc= 0.87010 val_ap= 0.85152 time= 0.12947\n",
      "训练次数: 9 Epoch: 0015 log_lik= 0.54525274 train_kl= 0.00863 train_loss= 0.55388 train_acc= 0.48385 val_roc= 0.87583 val_ap= 0.85882 time= 0.13542\n",
      "训练次数: 9 Epoch: 0016 log_lik= 0.5421386 train_kl= 0.00866 train_loss= 0.55080 train_acc= 0.48439 val_roc= 0.88011 val_ap= 0.86493 time= 0.11949\n",
      "训练次数: 9 Epoch: 0017 log_lik= 0.5320037 train_kl= 0.00866 train_loss= 0.54066 train_acc= 0.49377 val_roc= 0.88126 val_ap= 0.86664 time= 0.12401\n",
      "训练次数: 9 Epoch: 0018 log_lik= 0.52194935 train_kl= 0.00864 train_loss= 0.53059 train_acc= 0.50481 val_roc= 0.88149 val_ap= 0.86809 time= 0.12945\n",
      "训练次数: 9 Epoch: 0019 log_lik= 0.5149766 train_kl= 0.00862 train_loss= 0.52360 train_acc= 0.51450 val_roc= 0.88232 val_ap= 0.86911 time= 0.13145\n",
      "训练次数: 9 Epoch: 0020 log_lik= 0.5109863 train_kl= 0.00860 train_loss= 0.51959 train_acc= 0.52074 val_roc= 0.88278 val_ap= 0.86984 time= 0.12945\n",
      "训练次数: 9 Epoch: 0021 log_lik= 0.50817454 train_kl= 0.00858 train_loss= 0.51676 train_acc= 0.52527 val_roc= 0.88381 val_ap= 0.87113 time= 0.13144\n",
      "训练次数: 9 Epoch: 0022 log_lik= 0.50530374 train_kl= 0.00856 train_loss= 0.51387 train_acc= 0.53008 val_roc= 0.88527 val_ap= 0.87306 time= 0.12746\n",
      "训练次数: 9 Epoch: 0023 log_lik= 0.5017798 train_kl= 0.00855 train_loss= 0.51033 train_acc= 0.53324 val_roc= 0.88690 val_ap= 0.87610 time= 0.11651\n",
      "训练次数: 9 Epoch: 0024 log_lik= 0.49801475 train_kl= 0.00853 train_loss= 0.50655 train_acc= 0.53784 val_roc= 0.88898 val_ap= 0.88064 time= 0.11949\n",
      "训练次数: 9 Epoch: 0025 log_lik= 0.49393347 train_kl= 0.00853 train_loss= 0.50246 train_acc= 0.54075 val_roc= 0.89033 val_ap= 0.88541 time= 0.10953\n",
      "训练次数: 9 Epoch: 0026 log_lik= 0.4895316 train_kl= 0.00854 train_loss= 0.49807 train_acc= 0.54311 val_roc= 0.89060 val_ap= 0.88732 time= 0.12348\n",
      "训练次数: 9 Epoch: 0027 log_lik= 0.48576528 train_kl= 0.00855 train_loss= 0.49431 train_acc= 0.54528 val_roc= 0.89138 val_ap= 0.88892 time= 0.11551\n",
      "训练次数: 9 Epoch: 0028 log_lik= 0.4823151 train_kl= 0.00857 train_loss= 0.49088 train_acc= 0.54808 val_roc= 0.89199 val_ap= 0.88971 time= 0.11651\n",
      "训练次数: 9 Epoch: 0029 log_lik= 0.47986615 train_kl= 0.00859 train_loss= 0.48846 train_acc= 0.54932 val_roc= 0.89325 val_ap= 0.89083 time= 0.11053\n",
      "训练次数: 9 Epoch: 0030 log_lik= 0.4776156 train_kl= 0.00861 train_loss= 0.48623 train_acc= 0.54932 val_roc= 0.89479 val_ap= 0.89264 time= 0.11753\n",
      "训练次数: 9 Epoch: 0031 log_lik= 0.47565427 train_kl= 0.00863 train_loss= 0.48428 train_acc= 0.54959 val_roc= 0.89683 val_ap= 0.89484 time= 0.11551\n",
      "训练次数: 9 Epoch: 0032 log_lik= 0.4737168 train_kl= 0.00864 train_loss= 0.48236 train_acc= 0.54919 val_roc= 0.89782 val_ap= 0.89621 time= 0.11361\n",
      "训练次数: 9 Epoch: 0033 log_lik= 0.47126547 train_kl= 0.00864 train_loss= 0.47991 train_acc= 0.54936 val_roc= 0.89802 val_ap= 0.89683 time= 0.12148\n",
      "训练次数: 9 Epoch: 0034 log_lik= 0.4684117 train_kl= 0.00864 train_loss= 0.47705 train_acc= 0.55029 val_roc= 0.89849 val_ap= 0.89843 time= 0.11053\n",
      "训练次数: 9 Epoch: 0035 log_lik= 0.4653846 train_kl= 0.00863 train_loss= 0.47401 train_acc= 0.55068 val_roc= 0.89874 val_ap= 0.90103 time= 0.12646\n",
      "训练次数: 9 Epoch: 0036 log_lik= 0.46273527 train_kl= 0.00862 train_loss= 0.47135 train_acc= 0.55045 val_roc= 0.89893 val_ap= 0.90340 time= 0.12049\n",
      "训练次数: 9 Epoch: 0037 log_lik= 0.46087497 train_kl= 0.00861 train_loss= 0.46949 train_acc= 0.54974 val_roc= 0.89907 val_ap= 0.90494 time= 0.12667\n",
      "训练次数: 9 Epoch: 0038 log_lik= 0.4596811 train_kl= 0.00861 train_loss= 0.46829 train_acc= 0.54896 val_roc= 0.90017 val_ap= 0.90752 time= 0.10954\n",
      "训练次数: 9 Epoch: 0039 log_lik= 0.4580215 train_kl= 0.00860 train_loss= 0.46663 train_acc= 0.55006 val_roc= 0.90074 val_ap= 0.90870 time= 0.11053\n",
      "训练次数: 9 Epoch: 0040 log_lik= 0.45636156 train_kl= 0.00861 train_loss= 0.46497 train_acc= 0.55133 val_roc= 0.90183 val_ap= 0.91011 time= 0.11551\n",
      "训练次数: 9 Epoch: 0041 log_lik= 0.4546857 train_kl= 0.00862 train_loss= 0.46330 train_acc= 0.55212 val_roc= 0.90263 val_ap= 0.91089 time= 0.12447\n",
      "训练次数: 9 Epoch: 0042 log_lik= 0.4534931 train_kl= 0.00863 train_loss= 0.46212 train_acc= 0.55323 val_roc= 0.90342 val_ap= 0.91212 time= 0.12547\n",
      "训练次数: 9 Epoch: 0043 log_lik= 0.4523163 train_kl= 0.00864 train_loss= 0.46096 train_acc= 0.55375 val_roc= 0.90357 val_ap= 0.91272 time= 0.11205\n",
      "训练次数: 9 Epoch: 0044 log_lik= 0.45116308 train_kl= 0.00866 train_loss= 0.45982 train_acc= 0.55396 val_roc= 0.90393 val_ap= 0.91379 time= 0.11153\n",
      "训练次数: 9 Epoch: 0045 log_lik= 0.45018694 train_kl= 0.00867 train_loss= 0.45885 train_acc= 0.55481 val_roc= 0.90421 val_ap= 0.91506 time= 0.11452\n",
      "训练次数: 9 Epoch: 0046 log_lik= 0.4491082 train_kl= 0.00868 train_loss= 0.45778 train_acc= 0.55587 val_roc= 0.90389 val_ap= 0.91549 time= 0.12049\n",
      "训练次数: 9 Epoch: 0047 log_lik= 0.44812283 train_kl= 0.00868 train_loss= 0.45680 train_acc= 0.55649 val_roc= 0.90380 val_ap= 0.91615 time= 0.11053\n",
      "训练次数: 9 Epoch: 0048 log_lik= 0.44723877 train_kl= 0.00868 train_loss= 0.45592 train_acc= 0.55669 val_roc= 0.90360 val_ap= 0.91645 time= 0.11153\n",
      "训练次数: 9 Epoch: 0049 log_lik= 0.44607177 train_kl= 0.00868 train_loss= 0.45475 train_acc= 0.55694 val_roc= 0.90371 val_ap= 0.91682 time= 0.12143\n",
      "训练次数: 9 Epoch: 0050 log_lik= 0.44476804 train_kl= 0.00867 train_loss= 0.45344 train_acc= 0.55687 val_roc= 0.90392 val_ap= 0.91701 time= 0.12156\n",
      "训练次数: 9 Epoch: 0051 log_lik= 0.44380248 train_kl= 0.00867 train_loss= 0.45247 train_acc= 0.55768 val_roc= 0.90403 val_ap= 0.91740 time= 0.11277\n",
      "训练次数: 9 Epoch: 0052 log_lik= 0.4428585 train_kl= 0.00866 train_loss= 0.45152 train_acc= 0.55759 val_roc= 0.90390 val_ap= 0.91771 time= 0.11252\n",
      "训练次数: 9 Epoch: 0053 log_lik= 0.4418947 train_kl= 0.00866 train_loss= 0.45056 train_acc= 0.55812 val_roc= 0.90367 val_ap= 0.91798 time= 0.11451\n",
      "训练次数: 9 Epoch: 0054 log_lik= 0.44088298 train_kl= 0.00866 train_loss= 0.44954 train_acc= 0.55830 val_roc= 0.90325 val_ap= 0.91802 time= 0.11602\n",
      "训练次数: 9 Epoch: 0055 log_lik= 0.4401405 train_kl= 0.00866 train_loss= 0.44880 train_acc= 0.55839 val_roc= 0.90228 val_ap= 0.91746 time= 0.12845\n",
      "训练次数: 9 Epoch: 0056 log_lik= 0.43934557 train_kl= 0.00866 train_loss= 0.44801 train_acc= 0.55857 val_roc= 0.90189 val_ap= 0.91704 time= 0.11650\n",
      "训练次数: 9 Epoch: 0057 log_lik= 0.43865588 train_kl= 0.00867 train_loss= 0.44732 train_acc= 0.55887 val_roc= 0.90182 val_ap= 0.91681 time= 0.12252\n",
      "训练次数: 9 Epoch: 0058 log_lik= 0.43781385 train_kl= 0.00867 train_loss= 0.44649 train_acc= 0.55981 val_roc= 0.90126 val_ap= 0.91626 time= 0.12447\n",
      "训练次数: 9 Epoch: 0059 log_lik= 0.43711874 train_kl= 0.00868 train_loss= 0.44580 train_acc= 0.56055 val_roc= 0.90117 val_ap= 0.91606 time= 0.12945\n",
      "训练次数: 9 Epoch: 0060 log_lik= 0.4364194 train_kl= 0.00869 train_loss= 0.44511 train_acc= 0.56064 val_roc= 0.90053 val_ap= 0.91554 time= 0.12646\n",
      "训练次数: 9 Epoch: 0061 log_lik= 0.43563217 train_kl= 0.00869 train_loss= 0.44432 train_acc= 0.56160 val_roc= 0.90011 val_ap= 0.91562 time= 0.10769\n",
      "训练次数: 9 Epoch: 0062 log_lik= 0.43496084 train_kl= 0.00869 train_loss= 0.44365 train_acc= 0.56227 val_roc= 0.89941 val_ap= 0.91543 time= 0.13741\n",
      "训练次数: 9 Epoch: 0063 log_lik= 0.4343881 train_kl= 0.00870 train_loss= 0.44308 train_acc= 0.56248 val_roc= 0.89860 val_ap= 0.91506 time= 0.11546\n",
      "训练次数: 9 Epoch: 0064 log_lik= 0.43375748 train_kl= 0.00870 train_loss= 0.44245 train_acc= 0.56360 val_roc= 0.89766 val_ap= 0.91466 time= 0.12167\n",
      "训练次数: 9 Epoch: 0065 log_lik= 0.43314883 train_kl= 0.00870 train_loss= 0.44185 train_acc= 0.56432 val_roc= 0.89714 val_ap= 0.91453 time= 0.12248\n",
      "训练次数: 9 Epoch: 0066 log_lik= 0.43255535 train_kl= 0.00870 train_loss= 0.44125 train_acc= 0.56503 val_roc= 0.89663 val_ap= 0.91438 time= 0.11153\n",
      "训练次数: 9 Epoch: 0067 log_lik= 0.4320096 train_kl= 0.00870 train_loss= 0.44071 train_acc= 0.56547 val_roc= 0.89634 val_ap= 0.91412 time= 0.11651\n",
      "训练次数: 9 Epoch: 0068 log_lik= 0.43151656 train_kl= 0.00870 train_loss= 0.44022 train_acc= 0.56587 val_roc= 0.89604 val_ap= 0.91408 time= 0.10913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0069 log_lik= 0.4309431 train_kl= 0.00870 train_loss= 0.43964 train_acc= 0.56681 val_roc= 0.89605 val_ap= 0.91449 time= 0.12746\n",
      "训练次数: 9 Epoch: 0070 log_lik= 0.43053597 train_kl= 0.00870 train_loss= 0.43924 train_acc= 0.56647 val_roc= 0.89559 val_ap= 0.91463 time= 0.11054\n",
      "训练次数: 9 Epoch: 0071 log_lik= 0.42998198 train_kl= 0.00870 train_loss= 0.43868 train_acc= 0.56677 val_roc= 0.89514 val_ap= 0.91452 time= 0.11410\n",
      "训练次数: 9 Epoch: 0072 log_lik= 0.42958426 train_kl= 0.00870 train_loss= 0.43829 train_acc= 0.56729 val_roc= 0.89474 val_ap= 0.91454 time= 0.11750\n",
      "训练次数: 9 Epoch: 0073 log_lik= 0.4290178 train_kl= 0.00871 train_loss= 0.43772 train_acc= 0.56783 val_roc= 0.89414 val_ap= 0.91421 time= 0.12447\n",
      "训练次数: 9 Epoch: 0074 log_lik= 0.42856166 train_kl= 0.00871 train_loss= 0.43727 train_acc= 0.56756 val_roc= 0.89367 val_ap= 0.91395 time= 0.11850\n",
      "训练次数: 9 Epoch: 0075 log_lik= 0.42813662 train_kl= 0.00871 train_loss= 0.43685 train_acc= 0.56762 val_roc= 0.89326 val_ap= 0.91358 time= 0.10854\n",
      "训练次数: 9 Epoch: 0076 log_lik= 0.42770064 train_kl= 0.00871 train_loss= 0.43641 train_acc= 0.56789 val_roc= 0.89276 val_ap= 0.91355 time= 0.11252\n",
      "训练次数: 9 Epoch: 0077 log_lik= 0.4273268 train_kl= 0.00872 train_loss= 0.43604 train_acc= 0.56837 val_roc= 0.89229 val_ap= 0.91326 time= 0.12546\n",
      "训练次数: 9 Epoch: 0078 log_lik= 0.42686117 train_kl= 0.00872 train_loss= 0.43558 train_acc= 0.56841 val_roc= 0.89170 val_ap= 0.91298 time= 0.11317\n",
      "训练次数: 9 Epoch: 0079 log_lik= 0.42646208 train_kl= 0.00872 train_loss= 0.43518 train_acc= 0.56826 val_roc= 0.89119 val_ap= 0.91266 time= 0.11351\n",
      "训练次数: 9 Epoch: 0080 log_lik= 0.42606184 train_kl= 0.00872 train_loss= 0.43478 train_acc= 0.56899 val_roc= 0.89076 val_ap= 0.91226 time= 0.12248\n",
      "训练次数: 9 Epoch: 0081 log_lik= 0.42574742 train_kl= 0.00872 train_loss= 0.43447 train_acc= 0.56889 val_roc= 0.89036 val_ap= 0.91208 time= 0.12646\n",
      "训练次数: 9 Epoch: 0082 log_lik= 0.4253504 train_kl= 0.00872 train_loss= 0.43407 train_acc= 0.56876 val_roc= 0.88997 val_ap= 0.91187 time= 0.12746\n",
      "训练次数: 9 Epoch: 0083 log_lik= 0.42498964 train_kl= 0.00872 train_loss= 0.43371 train_acc= 0.56847 val_roc= 0.88907 val_ap= 0.91127 time= 0.12746\n",
      "训练次数: 9 Epoch: 0084 log_lik= 0.42474598 train_kl= 0.00872 train_loss= 0.43347 train_acc= 0.56891 val_roc= 0.88881 val_ap= 0.91120 time= 0.11451\n",
      "训练次数: 9 Epoch: 0085 log_lik= 0.42440906 train_kl= 0.00872 train_loss= 0.43313 train_acc= 0.56937 val_roc= 0.88837 val_ap= 0.91097 time= 0.12348\n",
      "训练次数: 9 Epoch: 0086 log_lik= 0.4240633 train_kl= 0.00873 train_loss= 0.43279 train_acc= 0.56956 val_roc= 0.88813 val_ap= 0.91092 time= 0.12447\n",
      "训练次数: 9 Epoch: 0087 log_lik= 0.42379424 train_kl= 0.00873 train_loss= 0.43252 train_acc= 0.56935 val_roc= 0.88793 val_ap= 0.91085 time= 0.11650\n",
      "训练次数: 9 Epoch: 0088 log_lik= 0.4234107 train_kl= 0.00873 train_loss= 0.43214 train_acc= 0.56965 val_roc= 0.88765 val_ap= 0.91073 time= 0.11650\n",
      "训练次数: 9 Epoch: 0089 log_lik= 0.423246 train_kl= 0.00873 train_loss= 0.43197 train_acc= 0.56995 val_roc= 0.88726 val_ap= 0.91065 time= 0.11850\n",
      "训练次数: 9 Epoch: 0090 log_lik= 0.4229245 train_kl= 0.00873 train_loss= 0.43165 train_acc= 0.56991 val_roc= 0.88706 val_ap= 0.91063 time= 0.11352\n",
      "训练次数: 9 Epoch: 0091 log_lik= 0.42274275 train_kl= 0.00873 train_loss= 0.43147 train_acc= 0.57046 val_roc= 0.88684 val_ap= 0.91081 time= 0.12148\n",
      "训练次数: 9 Epoch: 0092 log_lik= 0.42245892 train_kl= 0.00873 train_loss= 0.43119 train_acc= 0.57071 val_roc= 0.88665 val_ap= 0.91082 time= 0.11977\n",
      "训练次数: 9 Epoch: 0093 log_lik= 0.4222331 train_kl= 0.00873 train_loss= 0.43097 train_acc= 0.57048 val_roc= 0.88624 val_ap= 0.91062 time= 0.11551\n",
      "训练次数: 9 Epoch: 0094 log_lik= 0.42204878 train_kl= 0.00873 train_loss= 0.43078 train_acc= 0.57033 val_roc= 0.88587 val_ap= 0.91048 time= 0.12846\n",
      "训练次数: 9 Epoch: 0095 log_lik= 0.4217785 train_kl= 0.00873 train_loss= 0.43051 train_acc= 0.57066 val_roc= 0.88589 val_ap= 0.91045 time= 0.12447\n",
      "训练次数: 9 Epoch: 0096 log_lik= 0.4215792 train_kl= 0.00873 train_loss= 0.43031 train_acc= 0.57071 val_roc= 0.88573 val_ap= 0.91041 time= 0.11750\n",
      "训练次数: 9 Epoch: 0097 log_lik= 0.42135537 train_kl= 0.00873 train_loss= 0.43009 train_acc= 0.57074 val_roc= 0.88590 val_ap= 0.91055 time= 0.11352\n",
      "训练次数: 9 Epoch: 0098 log_lik= 0.4211494 train_kl= 0.00874 train_loss= 0.42988 train_acc= 0.57082 val_roc= 0.88571 val_ap= 0.91049 time= 0.12447\n",
      "训练次数: 9 Epoch: 0099 log_lik= 0.4210124 train_kl= 0.00873 train_loss= 0.42975 train_acc= 0.57009 val_roc= 0.88550 val_ap= 0.91037 time= 0.11352\n",
      "训练次数: 9 Epoch: 0100 log_lik= 0.42077756 train_kl= 0.00873 train_loss= 0.42951 train_acc= 0.57060 val_roc= 0.88499 val_ap= 0.91011 time= 0.13163\n",
      "Optimization Finished!\n",
      "训练次数: 9 ROC score: 0.9237890173514469\n",
      "训练次数: 9 AP score: 0.9379297047660244\n",
      "训练次数: 10 Epoch: 0001 log_lik= 0.7817611 train_kl= 0.00805 train_loss= 0.78982 train_acc= 0.05415 val_roc= 0.70198 val_ap= 0.73869 time= 9.43288\n",
      "训练次数: 10 Epoch: 0002 log_lik= 0.83380204 train_kl= 0.00834 train_loss= 0.84214 train_acc= 0.00159 val_roc= 0.78143 val_ap= 0.78333 time= 0.12945\n",
      "训练次数: 10 Epoch: 0003 log_lik= 0.74592936 train_kl= 0.00810 train_loss= 0.75403 train_acc= 0.02555 val_roc= 0.72359 val_ap= 0.73636 time= 0.11750\n",
      "训练次数: 10 Epoch: 0004 log_lik= 0.74021906 train_kl= 0.00811 train_loss= 0.74833 train_acc= 0.03781 val_roc= 0.70854 val_ap= 0.74023 time= 0.13045\n",
      "训练次数: 10 Epoch: 0005 log_lik= 0.73609585 train_kl= 0.00820 train_loss= 0.74429 train_acc= 0.00975 val_roc= 0.73352 val_ap= 0.76351 time= 0.12248\n",
      "训练次数: 10 Epoch: 0006 log_lik= 0.7309753 train_kl= 0.00822 train_loss= 0.73919 train_acc= 0.01366 val_roc= 0.77715 val_ap= 0.79419 time= 0.13542\n",
      "训练次数: 10 Epoch: 0007 log_lik= 0.7091795 train_kl= 0.00820 train_loss= 0.71738 train_acc= 0.03499 val_roc= 0.82259 val_ap= 0.82652 time= 0.13244\n",
      "训练次数: 10 Epoch: 0008 log_lik= 0.6908261 train_kl= 0.00819 train_loss= 0.69901 train_acc= 0.08035 val_roc= 0.84121 val_ap= 0.83927 time= 0.11949\n",
      "训练次数: 10 Epoch: 0009 log_lik= 0.67358506 train_kl= 0.00820 train_loss= 0.68179 train_acc= 0.13870 val_roc= 0.84162 val_ap= 0.83882 time= 0.11750\n",
      "训练次数: 10 Epoch: 0010 log_lik= 0.6551145 train_kl= 0.00824 train_loss= 0.66335 train_acc= 0.18933 val_roc= 0.84515 val_ap= 0.84471 time= 0.11352\n",
      "训练次数: 10 Epoch: 0011 log_lik= 0.63511366 train_kl= 0.00829 train_loss= 0.64340 train_acc= 0.24178 val_roc= 0.85509 val_ap= 0.85412 time= 0.11252\n",
      "训练次数: 10 Epoch: 0012 log_lik= 0.6134125 train_kl= 0.00834 train_loss= 0.62175 train_acc= 0.30124 val_roc= 0.86565 val_ap= 0.86544 time= 0.11355\n",
      "训练次数: 10 Epoch: 0013 log_lik= 0.59119695 train_kl= 0.00838 train_loss= 0.59958 train_acc= 0.36296 val_roc= 0.87126 val_ap= 0.87423 time= 0.11650\n",
      "训练次数: 10 Epoch: 0014 log_lik= 0.57514364 train_kl= 0.00843 train_loss= 0.58358 train_acc= 0.40378 val_roc= 0.87723 val_ap= 0.88146 time= 0.13144\n",
      "训练次数: 10 Epoch: 0015 log_lik= 0.5645406 train_kl= 0.00848 train_loss= 0.57302 train_acc= 0.43158 val_roc= 0.88522 val_ap= 0.88952 time= 0.12148\n",
      "训练次数: 10 Epoch: 0016 log_lik= 0.5496858 train_kl= 0.00850 train_loss= 0.55819 train_acc= 0.45743 val_roc= 0.89193 val_ap= 0.89634 time= 0.12746\n",
      "训练次数: 10 Epoch: 0017 log_lik= 0.5355293 train_kl= 0.00852 train_loss= 0.54405 train_acc= 0.47452 val_roc= 0.89251 val_ap= 0.89737 time= 0.11153\n",
      "训练次数: 10 Epoch: 0018 log_lik= 0.5305215 train_kl= 0.00853 train_loss= 0.53905 train_acc= 0.48249 val_roc= 0.89397 val_ap= 0.89873 time= 0.11252\n",
      "训练次数: 10 Epoch: 0019 log_lik= 0.5249403 train_kl= 0.00854 train_loss= 0.53348 train_acc= 0.49080 val_roc= 0.89874 val_ap= 0.90360 time= 0.11252\n",
      "训练次数: 10 Epoch: 0020 log_lik= 0.5169996 train_kl= 0.00854 train_loss= 0.52554 train_acc= 0.50100 val_roc= 0.90108 val_ap= 0.90555 time= 0.12746\n",
      "训练次数: 10 Epoch: 0021 log_lik= 0.5128309 train_kl= 0.00855 train_loss= 0.52138 train_acc= 0.50641 val_roc= 0.90146 val_ap= 0.90582 time= 0.12547\n",
      "训练次数: 10 Epoch: 0022 log_lik= 0.5098007 train_kl= 0.00855 train_loss= 0.51835 train_acc= 0.50834 val_roc= 0.90194 val_ap= 0.90767 time= 0.11252\n",
      "训练次数: 10 Epoch: 0023 log_lik= 0.50499976 train_kl= 0.00855 train_loss= 0.51355 train_acc= 0.51005 val_roc= 0.90162 val_ap= 0.90908 time= 0.11453\n",
      "训练次数: 10 Epoch: 0024 log_lik= 0.501199 train_kl= 0.00855 train_loss= 0.50975 train_acc= 0.51138 val_roc= 0.89977 val_ap= 0.90868 time= 0.12447\n",
      "训练次数: 10 Epoch: 0025 log_lik= 0.49980506 train_kl= 0.00855 train_loss= 0.50835 train_acc= 0.51212 val_roc= 0.89825 val_ap= 0.90789 time= 0.12598\n",
      "训练次数: 10 Epoch: 0026 log_lik= 0.4982096 train_kl= 0.00855 train_loss= 0.50676 train_acc= 0.51362 val_roc= 0.89816 val_ap= 0.90788 time= 0.12897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0027 log_lik= 0.49585164 train_kl= 0.00855 train_loss= 0.50440 train_acc= 0.51593 val_roc= 0.89776 val_ap= 0.90782 time= 0.11252\n",
      "训练次数: 10 Epoch: 0028 log_lik= 0.49326277 train_kl= 0.00855 train_loss= 0.50181 train_acc= 0.51814 val_roc= 0.89799 val_ap= 0.90821 time= 0.12738\n",
      "训练次数: 10 Epoch: 0029 log_lik= 0.49133614 train_kl= 0.00855 train_loss= 0.49989 train_acc= 0.52053 val_roc= 0.89819 val_ap= 0.90884 time= 0.12252\n",
      "训练次数: 10 Epoch: 0030 log_lik= 0.48895323 train_kl= 0.00856 train_loss= 0.49751 train_acc= 0.52383 val_roc= 0.89838 val_ap= 0.90929 time= 0.11153\n",
      "训练次数: 10 Epoch: 0031 log_lik= 0.48613766 train_kl= 0.00856 train_loss= 0.49469 train_acc= 0.52681 val_roc= 0.89943 val_ap= 0.91051 time= 0.11551\n",
      "训练次数: 10 Epoch: 0032 log_lik= 0.48351857 train_kl= 0.00856 train_loss= 0.49208 train_acc= 0.52959 val_roc= 0.89925 val_ap= 0.91044 time= 0.12646\n",
      "训练次数: 10 Epoch: 0033 log_lik= 0.4810458 train_kl= 0.00856 train_loss= 0.48961 train_acc= 0.53114 val_roc= 0.89941 val_ap= 0.91077 time= 0.11252\n",
      "训练次数: 10 Epoch: 0034 log_lik= 0.47904634 train_kl= 0.00857 train_loss= 0.48761 train_acc= 0.53195 val_roc= 0.89848 val_ap= 0.90927 time= 0.12049\n",
      "训练次数: 10 Epoch: 0035 log_lik= 0.4775735 train_kl= 0.00857 train_loss= 0.48615 train_acc= 0.53276 val_roc= 0.89753 val_ap= 0.90780 time= 0.11651\n",
      "训练次数: 10 Epoch: 0036 log_lik= 0.47610524 train_kl= 0.00858 train_loss= 0.48469 train_acc= 0.53397 val_roc= 0.89666 val_ap= 0.90635 time= 0.10953\n",
      "训练次数: 10 Epoch: 0037 log_lik= 0.4745715 train_kl= 0.00859 train_loss= 0.48316 train_acc= 0.53572 val_roc= 0.89620 val_ap= 0.90502 time= 0.11153\n",
      "训练次数: 10 Epoch: 0038 log_lik= 0.4732765 train_kl= 0.00859 train_loss= 0.48187 train_acc= 0.53737 val_roc= 0.89610 val_ap= 0.90478 time= 0.11651\n",
      "训练次数: 10 Epoch: 0039 log_lik= 0.47249332 train_kl= 0.00860 train_loss= 0.48109 train_acc= 0.53853 val_roc= 0.89588 val_ap= 0.90472 time= 0.11153\n",
      "训练次数: 10 Epoch: 0040 log_lik= 0.47128212 train_kl= 0.00860 train_loss= 0.47988 train_acc= 0.53865 val_roc= 0.89659 val_ap= 0.90570 time= 0.11352\n",
      "训练次数: 10 Epoch: 0041 log_lik= 0.4701342 train_kl= 0.00859 train_loss= 0.47873 train_acc= 0.53873 val_roc= 0.89690 val_ap= 0.90655 time= 0.11949\n",
      "训练次数: 10 Epoch: 0042 log_lik= 0.46890444 train_kl= 0.00859 train_loss= 0.47749 train_acc= 0.53901 val_roc= 0.89695 val_ap= 0.90689 time= 0.12149\n",
      "训练次数: 10 Epoch: 0043 log_lik= 0.46780655 train_kl= 0.00859 train_loss= 0.47639 train_acc= 0.53970 val_roc= 0.89787 val_ap= 0.90752 time= 0.12745\n",
      "训练次数: 10 Epoch: 0044 log_lik= 0.4662599 train_kl= 0.00859 train_loss= 0.47485 train_acc= 0.54097 val_roc= 0.89873 val_ap= 0.90811 time= 0.12945\n",
      "训练次数: 10 Epoch: 0045 log_lik= 0.4649438 train_kl= 0.00859 train_loss= 0.47353 train_acc= 0.54152 val_roc= 0.89977 val_ap= 0.90896 time= 0.13343\n",
      "训练次数: 10 Epoch: 0046 log_lik= 0.46355566 train_kl= 0.00859 train_loss= 0.47215 train_acc= 0.54216 val_roc= 0.90117 val_ap= 0.91054 time= 0.11850\n",
      "训练次数: 10 Epoch: 0047 log_lik= 0.4624417 train_kl= 0.00860 train_loss= 0.47104 train_acc= 0.54251 val_roc= 0.90264 val_ap= 0.91254 time= 0.12200\n",
      "训练次数: 10 Epoch: 0048 log_lik= 0.46139383 train_kl= 0.00860 train_loss= 0.47000 train_acc= 0.54273 val_roc= 0.90422 val_ap= 0.91466 time= 0.12746\n",
      "训练次数: 10 Epoch: 0049 log_lik= 0.4604148 train_kl= 0.00861 train_loss= 0.46902 train_acc= 0.54219 val_roc= 0.90502 val_ap= 0.91569 time= 0.12706\n",
      "训练次数: 10 Epoch: 0050 log_lik= 0.45958337 train_kl= 0.00861 train_loss= 0.46820 train_acc= 0.54211 val_roc= 0.90600 val_ap= 0.91667 time= 0.11551\n",
      "训练次数: 10 Epoch: 0051 log_lik= 0.45884183 train_kl= 0.00862 train_loss= 0.46746 train_acc= 0.54242 val_roc= 0.90653 val_ap= 0.91726 time= 0.11949\n",
      "训练次数: 10 Epoch: 0052 log_lik= 0.45813993 train_kl= 0.00862 train_loss= 0.46676 train_acc= 0.54286 val_roc= 0.90715 val_ap= 0.91783 time= 0.13343\n",
      "训练次数: 10 Epoch: 0053 log_lik= 0.457622 train_kl= 0.00862 train_loss= 0.46624 train_acc= 0.54300 val_roc= 0.90795 val_ap= 0.91858 time= 0.11850\n",
      "训练次数: 10 Epoch: 0054 log_lik= 0.45677188 train_kl= 0.00862 train_loss= 0.46539 train_acc= 0.54295 val_roc= 0.90883 val_ap= 0.91940 time= 0.12547\n",
      "训练次数: 10 Epoch: 0055 log_lik= 0.4560576 train_kl= 0.00862 train_loss= 0.46467 train_acc= 0.54400 val_roc= 0.90996 val_ap= 0.92037 time= 0.11252\n",
      "训练次数: 10 Epoch: 0056 log_lik= 0.4554394 train_kl= 0.00861 train_loss= 0.46405 train_acc= 0.54389 val_roc= 0.91077 val_ap= 0.92110 time= 0.12746\n",
      "训练次数: 10 Epoch: 0057 log_lik= 0.45473275 train_kl= 0.00861 train_loss= 0.46335 train_acc= 0.54456 val_roc= 0.91130 val_ap= 0.92129 time= 0.11406\n",
      "训练次数: 10 Epoch: 0058 log_lik= 0.45380172 train_kl= 0.00861 train_loss= 0.46242 train_acc= 0.54490 val_roc= 0.91142 val_ap= 0.92135 time= 0.11153\n",
      "训练次数: 10 Epoch: 0059 log_lik= 0.45301387 train_kl= 0.00862 train_loss= 0.46163 train_acc= 0.54539 val_roc= 0.91162 val_ap= 0.92153 time= 0.12447\n",
      "训练次数: 10 Epoch: 0060 log_lik= 0.45225823 train_kl= 0.00862 train_loss= 0.46088 train_acc= 0.54585 val_roc= 0.91242 val_ap= 0.92246 time= 0.12748\n",
      "训练次数: 10 Epoch: 0061 log_lik= 0.45150033 train_kl= 0.00862 train_loss= 0.46012 train_acc= 0.54630 val_roc= 0.91318 val_ap= 0.92314 time= 0.11452\n",
      "训练次数: 10 Epoch: 0062 log_lik= 0.45071226 train_kl= 0.00863 train_loss= 0.45934 train_acc= 0.54702 val_roc= 0.91370 val_ap= 0.92355 time= 0.11755\n",
      "训练次数: 10 Epoch: 0063 log_lik= 0.45000827 train_kl= 0.00863 train_loss= 0.45864 train_acc= 0.54788 val_roc= 0.91428 val_ap= 0.92388 time= 0.11750\n",
      "训练次数: 10 Epoch: 0064 log_lik= 0.44928244 train_kl= 0.00863 train_loss= 0.45792 train_acc= 0.54862 val_roc= 0.91470 val_ap= 0.92441 time= 0.11949\n",
      "训练次数: 10 Epoch: 0065 log_lik= 0.44855824 train_kl= 0.00864 train_loss= 0.45719 train_acc= 0.54903 val_roc= 0.91525 val_ap= 0.92496 time= 0.13144\n",
      "训练次数: 10 Epoch: 0066 log_lik= 0.44778806 train_kl= 0.00864 train_loss= 0.45642 train_acc= 0.54932 val_roc= 0.91516 val_ap= 0.92514 time= 0.11356\n",
      "训练次数: 10 Epoch: 0067 log_lik= 0.4471438 train_kl= 0.00864 train_loss= 0.45578 train_acc= 0.54970 val_roc= 0.91545 val_ap= 0.92581 time= 0.11551\n",
      "训练次数: 10 Epoch: 0068 log_lik= 0.44636142 train_kl= 0.00864 train_loss= 0.45500 train_acc= 0.55077 val_roc= 0.91580 val_ap= 0.92626 time= 0.11505\n",
      "训练次数: 10 Epoch: 0069 log_lik= 0.44570062 train_kl= 0.00864 train_loss= 0.45434 train_acc= 0.55159 val_roc= 0.91625 val_ap= 0.92688 time= 0.11451\n",
      "训练次数: 10 Epoch: 0070 log_lik= 0.44507715 train_kl= 0.00864 train_loss= 0.45371 train_acc= 0.55229 val_roc= 0.91686 val_ap= 0.92743 time= 0.11252\n",
      "训练次数: 10 Epoch: 0071 log_lik= 0.4443517 train_kl= 0.00864 train_loss= 0.45299 train_acc= 0.55273 val_roc= 0.91745 val_ap= 0.92818 time= 0.11405\n",
      "训练次数: 10 Epoch: 0072 log_lik= 0.44356802 train_kl= 0.00864 train_loss= 0.45221 train_acc= 0.55345 val_roc= 0.91811 val_ap= 0.92914 time= 0.12547\n",
      "训练次数: 10 Epoch: 0073 log_lik= 0.4428468 train_kl= 0.00864 train_loss= 0.45149 train_acc= 0.55392 val_roc= 0.91879 val_ap= 0.93010 time= 0.11352\n",
      "训练次数: 10 Epoch: 0074 log_lik= 0.4421685 train_kl= 0.00865 train_loss= 0.45082 train_acc= 0.55469 val_roc= 0.91905 val_ap= 0.93071 time= 0.12049\n",
      "训练次数: 10 Epoch: 0075 log_lik= 0.441577 train_kl= 0.00865 train_loss= 0.45023 train_acc= 0.55527 val_roc= 0.91953 val_ap= 0.93134 time= 0.12646\n",
      "训练次数: 10 Epoch: 0076 log_lik= 0.440891 train_kl= 0.00865 train_loss= 0.44954 train_acc= 0.55601 val_roc= 0.91991 val_ap= 0.93171 time= 0.11703\n",
      "训练次数: 10 Epoch: 0077 log_lik= 0.4402249 train_kl= 0.00866 train_loss= 0.44888 train_acc= 0.55628 val_roc= 0.92009 val_ap= 0.93198 time= 0.11352\n",
      "训练次数: 10 Epoch: 0078 log_lik= 0.43959638 train_kl= 0.00866 train_loss= 0.44826 train_acc= 0.55649 val_roc= 0.92025 val_ap= 0.93220 time= 0.11061\n",
      "训练次数: 10 Epoch: 0079 log_lik= 0.43888506 train_kl= 0.00866 train_loss= 0.44755 train_acc= 0.55702 val_roc= 0.92005 val_ap= 0.93227 time= 0.11004\n",
      "训练次数: 10 Epoch: 0080 log_lik= 0.4383482 train_kl= 0.00866 train_loss= 0.44701 train_acc= 0.55738 val_roc= 0.91996 val_ap= 0.93234 time= 0.11152\n",
      "训练次数: 10 Epoch: 0081 log_lik= 0.43771458 train_kl= 0.00867 train_loss= 0.44638 train_acc= 0.55810 val_roc= 0.91981 val_ap= 0.93232 time= 0.12547\n",
      "训练次数: 10 Epoch: 0082 log_lik= 0.43710762 train_kl= 0.00867 train_loss= 0.44578 train_acc= 0.55865 val_roc= 0.91939 val_ap= 0.93197 time= 0.11551\n",
      "训练次数: 10 Epoch: 0083 log_lik= 0.43655604 train_kl= 0.00867 train_loss= 0.44523 train_acc= 0.55870 val_roc= 0.91976 val_ap= 0.93236 time= 0.11451\n",
      "训练次数: 10 Epoch: 0084 log_lik= 0.43602386 train_kl= 0.00867 train_loss= 0.44470 train_acc= 0.55894 val_roc= 0.91983 val_ap= 0.93268 time= 0.11153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0085 log_lik= 0.4354421 train_kl= 0.00868 train_loss= 0.44412 train_acc= 0.55944 val_roc= 0.91937 val_ap= 0.93232 time= 0.11750\n",
      "训练次数: 10 Epoch: 0086 log_lik= 0.43490103 train_kl= 0.00868 train_loss= 0.44358 train_acc= 0.55995 val_roc= 0.91914 val_ap= 0.93206 time= 0.11451\n",
      "训练次数: 10 Epoch: 0087 log_lik= 0.434411 train_kl= 0.00868 train_loss= 0.44309 train_acc= 0.56016 val_roc= 0.91872 val_ap= 0.93181 time= 0.11352\n",
      "训练次数: 10 Epoch: 0088 log_lik= 0.43393204 train_kl= 0.00868 train_loss= 0.44262 train_acc= 0.56062 val_roc= 0.91810 val_ap= 0.93139 time= 0.13045\n",
      "训练次数: 10 Epoch: 0089 log_lik= 0.43345147 train_kl= 0.00869 train_loss= 0.44214 train_acc= 0.56055 val_roc= 0.91762 val_ap= 0.93119 time= 0.11153\n",
      "训练次数: 10 Epoch: 0090 log_lik= 0.43302605 train_kl= 0.00869 train_loss= 0.44171 train_acc= 0.56077 val_roc= 0.91719 val_ap= 0.93104 time= 0.12049\n",
      "训练次数: 10 Epoch: 0091 log_lik= 0.4325616 train_kl= 0.00869 train_loss= 0.44125 train_acc= 0.56113 val_roc= 0.91703 val_ap= 0.93097 time= 0.13096\n",
      "训练次数: 10 Epoch: 0092 log_lik= 0.43209454 train_kl= 0.00869 train_loss= 0.44079 train_acc= 0.56113 val_roc= 0.91683 val_ap= 0.93078 time= 0.11850\n",
      "训练次数: 10 Epoch: 0093 log_lik= 0.43163002 train_kl= 0.00869 train_loss= 0.44032 train_acc= 0.56143 val_roc= 0.91671 val_ap= 0.93076 time= 0.12447\n",
      "训练次数: 10 Epoch: 0094 log_lik= 0.4312556 train_kl= 0.00870 train_loss= 0.43995 train_acc= 0.56156 val_roc= 0.91651 val_ap= 0.93085 time= 0.11591\n",
      "训练次数: 10 Epoch: 0095 log_lik= 0.4309087 train_kl= 0.00870 train_loss= 0.43961 train_acc= 0.56169 val_roc= 0.91662 val_ap= 0.93102 time= 0.12149\n",
      "训练次数: 10 Epoch: 0096 log_lik= 0.430501 train_kl= 0.00870 train_loss= 0.43920 train_acc= 0.56180 val_roc= 0.91661 val_ap= 0.93111 time= 0.11156\n",
      "训练次数: 10 Epoch: 0097 log_lik= 0.4300911 train_kl= 0.00870 train_loss= 0.43879 train_acc= 0.56213 val_roc= 0.91665 val_ap= 0.93131 time= 0.12348\n",
      "训练次数: 10 Epoch: 0098 log_lik= 0.4297629 train_kl= 0.00870 train_loss= 0.43846 train_acc= 0.56238 val_roc= 0.91671 val_ap= 0.93148 time= 0.11452\n",
      "训练次数: 10 Epoch: 0099 log_lik= 0.42933303 train_kl= 0.00870 train_loss= 0.43803 train_acc= 0.56286 val_roc= 0.91675 val_ap= 0.93161 time= 0.11053\n",
      "训练次数: 10 Epoch: 0100 log_lik= 0.42898637 train_kl= 0.00870 train_loss= 0.43769 train_acc= 0.56300 val_roc= 0.91681 val_ap= 0.93185 time= 0.13343\n",
      "Optimization Finished!\n",
      "训练次数: 10 ROC score: 0.9262950574120816\n",
      "训练次数: 10 AP score: 0.9355705612490571\n",
      "Average Test ROC score: 0.9144939851437913\n",
      "Average Test AP score: 0.9215060705438969\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n实验效果\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动训练10次 CMVAE结果\n",
    "# 1.多次采样（模型内部采样1000次），求均值\n",
    "\n",
    "\n",
    "# 2.计算KL散度，预设 Ex=0, En=1.05, He=0.1\n",
    "%run train_debug.py\n",
    "\n",
    "'''\n",
    "实验效果\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1576b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "训练次数: 1 Epoch: 0001 log_lik= 0.78237236 train_kl= 0.00805 train_loss= 0.79043 train_acc= 0.08290 val_roc= 0.65116 val_ap= 0.69190 time= 7.54495\n",
      "训练次数: 1 Epoch: 0002 log_lik= 0.83527195 train_kl= 0.00834 train_loss= 0.84361 train_acc= 0.00161 val_roc= 0.68013 val_ap= 0.70507 time= 0.10954\n",
      "训练次数: 1 Epoch: 0003 log_lik= 0.7362416 train_kl= 0.00813 train_loss= 0.74437 train_acc= 0.01193 val_roc= 0.61721 val_ap= 0.66665 time= 0.12248\n",
      "训练次数: 1 Epoch: 0004 log_lik= 0.7430345 train_kl= 0.00822 train_loss= 0.75125 train_acc= 0.00455 val_roc= 0.66588 val_ap= 0.70694 time= 0.11146\n",
      "训练次数: 1 Epoch: 0005 log_lik= 0.72561955 train_kl= 0.00820 train_loss= 0.73382 train_acc= 0.00579 val_roc= 0.71980 val_ap= 0.74044 time= 0.10853\n",
      "训练次数: 1 Epoch: 0006 log_lik= 0.7093851 train_kl= 0.00818 train_loss= 0.71757 train_acc= 0.01776 val_roc= 0.74614 val_ap= 0.74935 time= 0.11551\n",
      "训练次数: 1 Epoch: 0007 log_lik= 0.69298226 train_kl= 0.00819 train_loss= 0.70117 train_acc= 0.06422 val_roc= 0.76167 val_ap= 0.75722 time= 0.10754\n",
      "训练次数: 1 Epoch: 0008 log_lik= 0.6720759 train_kl= 0.00821 train_loss= 0.68029 train_acc= 0.13729 val_roc= 0.77656 val_ap= 0.77001 time= 0.10954\n",
      "训练次数: 1 Epoch: 0009 log_lik= 0.64767206 train_kl= 0.00825 train_loss= 0.65592 train_acc= 0.21198 val_roc= 0.79309 val_ap= 0.78563 time= 0.11053\n",
      "训练次数: 1 Epoch: 0010 log_lik= 0.6235253 train_kl= 0.00830 train_loss= 0.63182 train_acc= 0.27529 val_roc= 0.80211 val_ap= 0.79560 time= 0.10754\n",
      "训练次数: 1 Epoch: 0011 log_lik= 0.6065866 train_kl= 0.00835 train_loss= 0.61494 train_acc= 0.33181 val_roc= 0.80504 val_ap= 0.79815 time= 0.10655\n",
      "训练次数: 1 Epoch: 0012 log_lik= 0.5953935 train_kl= 0.00841 train_loss= 0.60380 train_acc= 0.38276 val_roc= 0.81094 val_ap= 0.80527 time= 0.11451\n",
      "训练次数: 1 Epoch: 0013 log_lik= 0.57961255 train_kl= 0.00845 train_loss= 0.58806 train_acc= 0.42692 val_roc= 0.81548 val_ap= 0.81162 time= 0.11551\n",
      "训练次数: 1 Epoch: 0014 log_lik= 0.5665832 train_kl= 0.00847 train_loss= 0.57506 train_acc= 0.45676 val_roc= 0.82003 val_ap= 0.81822 time= 0.11372\n",
      "训练次数: 1 Epoch: 0015 log_lik= 0.5560043 train_kl= 0.00849 train_loss= 0.56449 train_acc= 0.47759 val_roc= 0.82543 val_ap= 0.82651 time= 0.11352\n",
      "训练次数: 1 Epoch: 0016 log_lik= 0.5454663 train_kl= 0.00849 train_loss= 0.55395 train_acc= 0.49425 val_roc= 0.83089 val_ap= 0.83231 time= 0.11452\n",
      "训练次数: 1 Epoch: 0017 log_lik= 0.5362439 train_kl= 0.00848 train_loss= 0.54473 train_acc= 0.50873 val_roc= 0.83633 val_ap= 0.83731 time= 0.11352\n",
      "训练次数: 1 Epoch: 0018 log_lik= 0.5298817 train_kl= 0.00848 train_loss= 0.53836 train_acc= 0.51865 val_roc= 0.84135 val_ap= 0.84130 time= 0.11451\n",
      "训练次数: 1 Epoch: 0019 log_lik= 0.5263186 train_kl= 0.00848 train_loss= 0.53480 train_acc= 0.52366 val_roc= 0.84493 val_ap= 0.84552 time= 0.11451\n",
      "训练次数: 1 Epoch: 0020 log_lik= 0.5230345 train_kl= 0.00848 train_loss= 0.53152 train_acc= 0.52668 val_roc= 0.84823 val_ap= 0.85061 time= 0.11451\n",
      "训练次数: 1 Epoch: 0021 log_lik= 0.51978314 train_kl= 0.00849 train_loss= 0.52827 train_acc= 0.52726 val_roc= 0.85099 val_ap= 0.85449 time= 0.10654\n",
      "训练次数: 1 Epoch: 0022 log_lik= 0.5173902 train_kl= 0.00850 train_loss= 0.52589 train_acc= 0.52711 val_roc= 0.85378 val_ap= 0.85866 time= 0.10654\n",
      "训练次数: 1 Epoch: 0023 log_lik= 0.51556444 train_kl= 0.00852 train_loss= 0.52408 train_acc= 0.52849 val_roc= 0.85595 val_ap= 0.86170 time= 0.10754\n",
      "训练次数: 1 Epoch: 0024 log_lik= 0.5134649 train_kl= 0.00853 train_loss= 0.52199 train_acc= 0.53042 val_roc= 0.85771 val_ap= 0.86418 time= 0.11551\n",
      "训练次数: 1 Epoch: 0025 log_lik= 0.5103216 train_kl= 0.00854 train_loss= 0.51886 train_acc= 0.53224 val_roc= 0.85927 val_ap= 0.86678 time= 0.10754\n",
      "训练次数: 1 Epoch: 0026 log_lik= 0.5076328 train_kl= 0.00855 train_loss= 0.51618 train_acc= 0.53361 val_roc= 0.85994 val_ap= 0.86816 time= 0.10854\n",
      "训练次数: 1 Epoch: 0027 log_lik= 0.50497615 train_kl= 0.00855 train_loss= 0.51353 train_acc= 0.53526 val_roc= 0.86086 val_ap= 0.86988 time= 0.10854\n",
      "训练次数: 1 Epoch: 0028 log_lik= 0.5020525 train_kl= 0.00855 train_loss= 0.51060 train_acc= 0.53661 val_roc= 0.86211 val_ap= 0.87292 time= 0.10854\n",
      "训练次数: 1 Epoch: 0029 log_lik= 0.49876273 train_kl= 0.00854 train_loss= 0.50730 train_acc= 0.53876 val_roc= 0.86284 val_ap= 0.87429 time= 0.10555\n",
      "训练次数: 1 Epoch: 0030 log_lik= 0.4957764 train_kl= 0.00853 train_loss= 0.50431 train_acc= 0.54099 val_roc= 0.86391 val_ap= 0.87568 time= 0.10655\n",
      "训练次数: 1 Epoch: 0031 log_lik= 0.49334532 train_kl= 0.00853 train_loss= 0.50187 train_acc= 0.54328 val_roc= 0.86468 val_ap= 0.87646 time= 0.10854\n",
      "训练次数: 1 Epoch: 0032 log_lik= 0.49088824 train_kl= 0.00852 train_loss= 0.49941 train_acc= 0.54527 val_roc= 0.86627 val_ap= 0.87785 time= 0.11253\n",
      "训练次数: 1 Epoch: 0033 log_lik= 0.48818132 train_kl= 0.00852 train_loss= 0.49670 train_acc= 0.54672 val_roc= 0.86715 val_ap= 0.87889 time= 0.10555\n",
      "训练次数: 1 Epoch: 0034 log_lik= 0.4859973 train_kl= 0.00853 train_loss= 0.49452 train_acc= 0.54858 val_roc= 0.86764 val_ap= 0.88011 time= 0.10780\n",
      "训练次数: 1 Epoch: 0035 log_lik= 0.4834102 train_kl= 0.00853 train_loss= 0.49194 train_acc= 0.54961 val_roc= 0.86837 val_ap= 0.88098 time= 0.10555\n",
      "训练次数: 1 Epoch: 0036 log_lik= 0.48148614 train_kl= 0.00855 train_loss= 0.49003 train_acc= 0.55137 val_roc= 0.86939 val_ap= 0.88229 time= 0.10854\n",
      "训练次数: 1 Epoch: 0037 log_lik= 0.47949544 train_kl= 0.00856 train_loss= 0.48805 train_acc= 0.55255 val_roc= 0.87058 val_ap= 0.88340 time= 0.10655\n",
      "训练次数: 1 Epoch: 0038 log_lik= 0.4776946 train_kl= 0.00857 train_loss= 0.48626 train_acc= 0.55406 val_roc= 0.87139 val_ap= 0.88360 time= 0.11551\n",
      "训练次数: 1 Epoch: 0039 log_lik= 0.47614044 train_kl= 0.00858 train_loss= 0.48472 train_acc= 0.55475 val_roc= 0.87225 val_ap= 0.88382 time= 0.10555\n",
      "训练次数: 1 Epoch: 0040 log_lik= 0.4746614 train_kl= 0.00858 train_loss= 0.48324 train_acc= 0.55555 val_roc= 0.87227 val_ap= 0.88294 time= 0.11053\n",
      "训练次数: 1 Epoch: 0041 log_lik= 0.47326759 train_kl= 0.00858 train_loss= 0.48185 train_acc= 0.55681 val_roc= 0.87285 val_ap= 0.88352 time= 0.10655\n",
      "训练次数: 1 Epoch: 0042 log_lik= 0.47183815 train_kl= 0.00858 train_loss= 0.48042 train_acc= 0.55850 val_roc= 0.87338 val_ap= 0.88408 time= 0.11053\n",
      "训练次数: 1 Epoch: 0043 log_lik= 0.4705078 train_kl= 0.00858 train_loss= 0.47909 train_acc= 0.56022 val_roc= 0.87370 val_ap= 0.88479 time= 0.11352\n",
      "训练次数: 1 Epoch: 0044 log_lik= 0.4690127 train_kl= 0.00858 train_loss= 0.47759 train_acc= 0.56162 val_roc= 0.87406 val_ap= 0.88531 time= 0.10754\n",
      "训练次数: 1 Epoch: 0045 log_lik= 0.46762484 train_kl= 0.00858 train_loss= 0.47621 train_acc= 0.56261 val_roc= 0.87425 val_ap= 0.88581 time= 0.10555\n",
      "训练次数: 1 Epoch: 0046 log_lik= 0.46615055 train_kl= 0.00858 train_loss= 0.47474 train_acc= 0.56264 val_roc= 0.87425 val_ap= 0.88644 time= 0.10854\n",
      "训练次数: 1 Epoch: 0047 log_lik= 0.46485043 train_kl= 0.00859 train_loss= 0.47344 train_acc= 0.56315 val_roc= 0.87386 val_ap= 0.88616 time= 0.11352\n",
      "训练次数: 1 Epoch: 0048 log_lik= 0.46369216 train_kl= 0.00859 train_loss= 0.47229 train_acc= 0.56384 val_roc= 0.87393 val_ap= 0.88651 time= 0.10754\n",
      "训练次数: 1 Epoch: 0049 log_lik= 0.46235153 train_kl= 0.00860 train_loss= 0.47095 train_acc= 0.56485 val_roc= 0.87396 val_ap= 0.88644 time= 0.12148\n",
      "训练次数: 1 Epoch: 0050 log_lik= 0.46126682 train_kl= 0.00861 train_loss= 0.46987 train_acc= 0.56528 val_roc= 0.87367 val_ap= 0.88632 time= 0.10655\n",
      "训练次数: 1 Epoch: 0051 log_lik= 0.46017304 train_kl= 0.00861 train_loss= 0.46878 train_acc= 0.56606 val_roc= 0.87367 val_ap= 0.88627 time= 0.10962\n",
      "训练次数: 1 Epoch: 0052 log_lik= 0.45909032 train_kl= 0.00862 train_loss= 0.46771 train_acc= 0.56701 val_roc= 0.87438 val_ap= 0.88703 time= 0.10854\n",
      "训练次数: 1 Epoch: 0053 log_lik= 0.45814523 train_kl= 0.00862 train_loss= 0.46676 train_acc= 0.56766 val_roc= 0.87486 val_ap= 0.88725 time= 0.10754\n",
      "训练次数: 1 Epoch: 0054 log_lik= 0.4571118 train_kl= 0.00862 train_loss= 0.46573 train_acc= 0.56812 val_roc= 0.87516 val_ap= 0.88705 time= 0.10456\n",
      "训练次数: 1 Epoch: 0055 log_lik= 0.456199 train_kl= 0.00862 train_loss= 0.46482 train_acc= 0.56827 val_roc= 0.87551 val_ap= 0.88720 time= 0.10754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 1 Epoch: 0056 log_lik= 0.45527595 train_kl= 0.00862 train_loss= 0.46390 train_acc= 0.56893 val_roc= 0.87584 val_ap= 0.88718 time= 0.11451\n",
      "训练次数: 1 Epoch: 0057 log_lik= 0.45448884 train_kl= 0.00862 train_loss= 0.46311 train_acc= 0.56889 val_roc= 0.87620 val_ap= 0.88724 time= 0.10555\n",
      "训练次数: 1 Epoch: 0058 log_lik= 0.45358568 train_kl= 0.00862 train_loss= 0.46221 train_acc= 0.56942 val_roc= 0.87707 val_ap= 0.88766 time= 0.11551\n",
      "训练次数: 1 Epoch: 0059 log_lik= 0.45265043 train_kl= 0.00862 train_loss= 0.46127 train_acc= 0.56953 val_roc= 0.87791 val_ap= 0.88837 time= 0.11352\n",
      "训练次数: 1 Epoch: 0060 log_lik= 0.4518377 train_kl= 0.00862 train_loss= 0.46046 train_acc= 0.57011 val_roc= 0.87879 val_ap= 0.88933 time= 0.11352\n",
      "训练次数: 1 Epoch: 0061 log_lik= 0.45109922 train_kl= 0.00863 train_loss= 0.45973 train_acc= 0.57036 val_roc= 0.87935 val_ap= 0.88972 time= 0.10754\n",
      "训练次数: 1 Epoch: 0062 log_lik= 0.45024088 train_kl= 0.00863 train_loss= 0.45887 train_acc= 0.57159 val_roc= 0.88013 val_ap= 0.88993 time= 0.10456\n",
      "训练次数: 1 Epoch: 0063 log_lik= 0.44944608 train_kl= 0.00863 train_loss= 0.45808 train_acc= 0.57179 val_roc= 0.88106 val_ap= 0.89058 time= 0.10754\n",
      "训练次数: 1 Epoch: 0064 log_lik= 0.44860616 train_kl= 0.00864 train_loss= 0.45724 train_acc= 0.57275 val_roc= 0.88194 val_ap= 0.89096 time= 0.10555\n",
      "训练次数: 1 Epoch: 0065 log_lik= 0.44786078 train_kl= 0.00864 train_loss= 0.45650 train_acc= 0.57355 val_roc= 0.88323 val_ap= 0.89173 time= 0.10754\n",
      "训练次数: 1 Epoch: 0066 log_lik= 0.4471508 train_kl= 0.00864 train_loss= 0.45579 train_acc= 0.57413 val_roc= 0.88389 val_ap= 0.89194 time= 0.10555\n",
      "训练次数: 1 Epoch: 0067 log_lik= 0.446463 train_kl= 0.00865 train_loss= 0.45511 train_acc= 0.57476 val_roc= 0.88508 val_ap= 0.89289 time= 0.11153\n",
      "训练次数: 1 Epoch: 0068 log_lik= 0.44589874 train_kl= 0.00865 train_loss= 0.45455 train_acc= 0.57569 val_roc= 0.88622 val_ap= 0.89390 time= 0.11352\n",
      "训练次数: 1 Epoch: 0069 log_lik= 0.4451928 train_kl= 0.00865 train_loss= 0.45384 train_acc= 0.57657 val_roc= 0.88716 val_ap= 0.89460 time= 0.10755\n",
      "训练次数: 1 Epoch: 0070 log_lik= 0.4445346 train_kl= 0.00865 train_loss= 0.45318 train_acc= 0.57734 val_roc= 0.88797 val_ap= 0.89519 time= 0.11551\n",
      "训练次数: 1 Epoch: 0071 log_lik= 0.4440231 train_kl= 0.00865 train_loss= 0.45267 train_acc= 0.57741 val_roc= 0.88840 val_ap= 0.89548 time= 0.10854\n",
      "训练次数: 1 Epoch: 0072 log_lik= 0.44327286 train_kl= 0.00865 train_loss= 0.45192 train_acc= 0.57831 val_roc= 0.88926 val_ap= 0.89632 time= 0.10655\n",
      "训练次数: 1 Epoch: 0073 log_lik= 0.442729 train_kl= 0.00865 train_loss= 0.45138 train_acc= 0.57856 val_roc= 0.89007 val_ap= 0.89732 time= 0.10456\n",
      "训练次数: 1 Epoch: 0074 log_lik= 0.4423052 train_kl= 0.00865 train_loss= 0.45096 train_acc= 0.57902 val_roc= 0.89051 val_ap= 0.89762 time= 0.10754\n",
      "训练次数: 1 Epoch: 0075 log_lik= 0.44178697 train_kl= 0.00866 train_loss= 0.45044 train_acc= 0.57926 val_roc= 0.89109 val_ap= 0.89847 time= 0.10555\n",
      "训练次数: 1 Epoch: 0076 log_lik= 0.44129646 train_kl= 0.00866 train_loss= 0.44996 train_acc= 0.57983 val_roc= 0.89190 val_ap= 0.89958 time= 0.10754\n",
      "训练次数: 1 Epoch: 0077 log_lik= 0.44086775 train_kl= 0.00866 train_loss= 0.44953 train_acc= 0.57962 val_roc= 0.89229 val_ap= 0.90015 time= 0.10456\n",
      "训练次数: 1 Epoch: 0078 log_lik= 0.44042894 train_kl= 0.00866 train_loss= 0.44909 train_acc= 0.57983 val_roc= 0.89283 val_ap= 0.90090 time= 0.10854\n",
      "训练次数: 1 Epoch: 0079 log_lik= 0.43996882 train_kl= 0.00867 train_loss= 0.44863 train_acc= 0.58029 val_roc= 0.89338 val_ap= 0.90137 time= 0.10655\n",
      "训练次数: 1 Epoch: 0080 log_lik= 0.43955886 train_kl= 0.00867 train_loss= 0.44822 train_acc= 0.58019 val_roc= 0.89416 val_ap= 0.90215 time= 0.10577\n",
      "训练次数: 1 Epoch: 0081 log_lik= 0.43920714 train_kl= 0.00867 train_loss= 0.44787 train_acc= 0.58064 val_roc= 0.89487 val_ap= 0.90268 time= 0.11053\n",
      "训练次数: 1 Epoch: 0082 log_lik= 0.43884745 train_kl= 0.00867 train_loss= 0.44751 train_acc= 0.58044 val_roc= 0.89531 val_ap= 0.90303 time= 0.11053\n",
      "训练次数: 1 Epoch: 0083 log_lik= 0.43842798 train_kl= 0.00867 train_loss= 0.44709 train_acc= 0.58067 val_roc= 0.89569 val_ap= 0.90318 time= 0.10655\n",
      "训练次数: 1 Epoch: 0084 log_lik= 0.43794903 train_kl= 0.00867 train_loss= 0.44661 train_acc= 0.58044 val_roc= 0.89640 val_ap= 0.90392 time= 0.10555\n",
      "训练次数: 1 Epoch: 0085 log_lik= 0.4376839 train_kl= 0.00866 train_loss= 0.44635 train_acc= 0.58099 val_roc= 0.89712 val_ap= 0.90473 time= 0.10456\n",
      "训练次数: 1 Epoch: 0086 log_lik= 0.43730596 train_kl= 0.00867 train_loss= 0.44597 train_acc= 0.58076 val_roc= 0.89727 val_ap= 0.90515 time= 0.11451\n",
      "训练次数: 1 Epoch: 0087 log_lik= 0.4369059 train_kl= 0.00867 train_loss= 0.44557 train_acc= 0.58101 val_roc= 0.89782 val_ap= 0.90560 time= 0.10555\n",
      "训练次数: 1 Epoch: 0088 log_lik= 0.43660548 train_kl= 0.00867 train_loss= 0.44527 train_acc= 0.58155 val_roc= 0.89831 val_ap= 0.90606 time= 0.11261\n",
      "训练次数: 1 Epoch: 0089 log_lik= 0.43618703 train_kl= 0.00867 train_loss= 0.44486 train_acc= 0.58098 val_roc= 0.89887 val_ap= 0.90659 time= 0.11352\n",
      "训练次数: 1 Epoch: 0090 log_lik= 0.43579453 train_kl= 0.00867 train_loss= 0.44447 train_acc= 0.58109 val_roc= 0.89945 val_ap= 0.90724 time= 0.10754\n",
      "训练次数: 1 Epoch: 0091 log_lik= 0.43549687 train_kl= 0.00867 train_loss= 0.44417 train_acc= 0.58119 val_roc= 0.89972 val_ap= 0.90745 time= 0.11352\n",
      "训练次数: 1 Epoch: 0092 log_lik= 0.4351483 train_kl= 0.00867 train_loss= 0.44382 train_acc= 0.58125 val_roc= 0.90006 val_ap= 0.90757 time= 0.10455\n",
      "训练次数: 1 Epoch: 0093 log_lik= 0.4348263 train_kl= 0.00867 train_loss= 0.44350 train_acc= 0.58112 val_roc= 0.90023 val_ap= 0.90785 time= 0.11252\n",
      "训练次数: 1 Epoch: 0094 log_lik= 0.43450505 train_kl= 0.00868 train_loss= 0.44318 train_acc= 0.58112 val_roc= 0.90059 val_ap= 0.90851 time= 0.10456\n",
      "训练次数: 1 Epoch: 0095 log_lik= 0.43413123 train_kl= 0.00868 train_loss= 0.44281 train_acc= 0.58163 val_roc= 0.90105 val_ap= 0.90916 time= 0.11352\n",
      "训练次数: 1 Epoch: 0096 log_lik= 0.43383884 train_kl= 0.00868 train_loss= 0.44252 train_acc= 0.58165 val_roc= 0.90117 val_ap= 0.90932 time= 0.11252\n",
      "训练次数: 1 Epoch: 0097 log_lik= 0.43346137 train_kl= 0.00868 train_loss= 0.44214 train_acc= 0.58144 val_roc= 0.90156 val_ap= 0.90963 time= 0.12152\n",
      "训练次数: 1 Epoch: 0098 log_lik= 0.4330874 train_kl= 0.00868 train_loss= 0.44177 train_acc= 0.58134 val_roc= 0.90179 val_ap= 0.90997 time= 0.11750\n",
      "训练次数: 1 Epoch: 0099 log_lik= 0.4328282 train_kl= 0.00868 train_loss= 0.44151 train_acc= 0.58133 val_roc= 0.90181 val_ap= 0.91003 time= 0.10788\n",
      "训练次数: 1 Epoch: 0100 log_lik= 0.43250278 train_kl= 0.00868 train_loss= 0.44118 train_acc= 0.58145 val_roc= 0.90199 val_ap= 0.91026 time= 0.10754\n",
      "Optimization Finished!\n",
      "训练次数: 1 ROC score: 0.9020880066539684\n",
      "训练次数: 1 AP score: 0.911596122865913\n",
      "训练次数: 2 Epoch: 0001 log_lik= 0.7789989 train_kl= 0.00806 train_loss= 0.78705 train_acc= 0.08888 val_roc= 0.73251 val_ap= 0.75216 time= 7.22669\n",
      "训练次数: 2 Epoch: 0002 log_lik= 0.7532419 train_kl= 0.00825 train_loss= 0.76149 train_acc= 0.00178 val_roc= 0.80441 val_ap= 0.80761 time= 0.11845\n",
      "训练次数: 2 Epoch: 0003 log_lik= 0.7087118 train_kl= 0.00814 train_loss= 0.71685 train_acc= 0.06194 val_roc= 0.76539 val_ap= 0.76541 time= 0.11152\n",
      "训练次数: 2 Epoch: 0004 log_lik= 0.68395245 train_kl= 0.00821 train_loss= 0.69216 train_acc= 0.09399 val_roc= 0.79543 val_ap= 0.80874 time= 0.11650\n",
      "训练次数: 2 Epoch: 0005 log_lik= 0.6438023 train_kl= 0.00824 train_loss= 0.65204 train_acc= 0.21670 val_roc= 0.81554 val_ap= 0.81879 time= 0.11153\n",
      "训练次数: 2 Epoch: 0006 log_lik= 0.60064954 train_kl= 0.00829 train_loss= 0.60894 train_acc= 0.36012 val_roc= 0.83328 val_ap= 0.83375 time= 0.11252\n",
      "训练次数: 2 Epoch: 0007 log_lik= 0.5613289 train_kl= 0.00838 train_loss= 0.56971 train_acc= 0.42595 val_roc= 0.84892 val_ap= 0.85048 time= 0.11319\n",
      "训练次数: 2 Epoch: 0008 log_lik= 0.5391109 train_kl= 0.00848 train_loss= 0.54759 train_acc= 0.45062 val_roc= 0.85768 val_ap= 0.85962 time= 0.11551\n",
      "训练次数: 2 Epoch: 0009 log_lik= 0.52883273 train_kl= 0.00856 train_loss= 0.53740 train_acc= 0.46854 val_roc= 0.86560 val_ap= 0.86927 time= 0.10754\n",
      "训练次数: 2 Epoch: 0010 log_lik= 0.52401155 train_kl= 0.00863 train_loss= 0.53265 train_acc= 0.47833 val_roc= 0.87464 val_ap= 0.87899 time= 0.12049\n",
      "训练次数: 2 Epoch: 0011 log_lik= 0.51635045 train_kl= 0.00868 train_loss= 0.52503 train_acc= 0.48942 val_roc= 0.88067 val_ap= 0.88444 time= 0.11153\n",
      "训练次数: 2 Epoch: 0012 log_lik= 0.50724655 train_kl= 0.00870 train_loss= 0.51595 train_acc= 0.50286 val_roc= 0.88123 val_ap= 0.88568 time= 0.10555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0013 log_lik= 0.498584 train_kl= 0.00869 train_loss= 0.50728 train_acc= 0.51526 val_roc= 0.87912 val_ap= 0.88343 time= 0.11809\n",
      "训练次数: 2 Epoch: 0014 log_lik= 0.49182436 train_kl= 0.00866 train_loss= 0.50049 train_acc= 0.52521 val_roc= 0.87908 val_ap= 0.88130 time= 0.11352\n",
      "训练次数: 2 Epoch: 0015 log_lik= 0.4871783 train_kl= 0.00862 train_loss= 0.49580 train_acc= 0.53070 val_roc= 0.88054 val_ap= 0.88074 time= 0.10556\n",
      "训练次数: 2 Epoch: 0016 log_lik= 0.48579934 train_kl= 0.00858 train_loss= 0.49438 train_acc= 0.53119 val_roc= 0.88184 val_ap= 0.88113 time= 0.10356\n",
      "训练次数: 2 Epoch: 0017 log_lik= 0.4850248 train_kl= 0.00855 train_loss= 0.49358 train_acc= 0.53004 val_roc= 0.88295 val_ap= 0.88274 time= 0.10655\n",
      "训练次数: 2 Epoch: 0018 log_lik= 0.48308116 train_kl= 0.00854 train_loss= 0.49162 train_acc= 0.53055 val_roc= 0.88440 val_ap= 0.88481 time= 0.10755\n",
      "训练次数: 2 Epoch: 0019 log_lik= 0.47976613 train_kl= 0.00853 train_loss= 0.48830 train_acc= 0.53151 val_roc= 0.88600 val_ap= 0.88764 time= 0.10854\n",
      "训练次数: 2 Epoch: 0020 log_lik= 0.47503844 train_kl= 0.00854 train_loss= 0.48358 train_acc= 0.53449 val_roc= 0.88785 val_ap= 0.89026 time= 0.10654\n",
      "训练次数: 2 Epoch: 0021 log_lik= 0.46963203 train_kl= 0.00856 train_loss= 0.47819 train_acc= 0.53790 val_roc= 0.89008 val_ap= 0.89333 time= 0.10455\n",
      "训练次数: 2 Epoch: 0022 log_lik= 0.46545416 train_kl= 0.00859 train_loss= 0.47404 train_acc= 0.53905 val_roc= 0.89040 val_ap= 0.89497 time= 0.10854\n",
      "训练次数: 2 Epoch: 0023 log_lik= 0.46257886 train_kl= 0.00862 train_loss= 0.47120 train_acc= 0.53993 val_roc= 0.88991 val_ap= 0.89541 time= 0.10356\n",
      "训练次数: 2 Epoch: 0024 log_lik= 0.46123382 train_kl= 0.00866 train_loss= 0.46989 train_acc= 0.54020 val_roc= 0.89053 val_ap= 0.89652 time= 0.11104\n",
      "训练次数: 2 Epoch: 0025 log_lik= 0.4600778 train_kl= 0.00868 train_loss= 0.46876 train_acc= 0.54057 val_roc= 0.89070 val_ap= 0.89731 time= 0.10754\n",
      "训练次数: 2 Epoch: 0026 log_lik= 0.45883283 train_kl= 0.00869 train_loss= 0.46752 train_acc= 0.54067 val_roc= 0.89119 val_ap= 0.89766 time= 0.11053\n",
      "训练次数: 2 Epoch: 0027 log_lik= 0.45802772 train_kl= 0.00869 train_loss= 0.46671 train_acc= 0.53926 val_roc= 0.89086 val_ap= 0.89808 time= 0.11452\n",
      "训练次数: 2 Epoch: 0028 log_lik= 0.4567981 train_kl= 0.00867 train_loss= 0.46547 train_acc= 0.53713 val_roc= 0.88991 val_ap= 0.89739 time= 0.10555\n",
      "训练次数: 2 Epoch: 0029 log_lik= 0.45521706 train_kl= 0.00865 train_loss= 0.46387 train_acc= 0.53596 val_roc= 0.88916 val_ap= 0.89765 time= 0.12248\n",
      "训练次数: 2 Epoch: 0030 log_lik= 0.4534013 train_kl= 0.00863 train_loss= 0.46203 train_acc= 0.53593 val_roc= 0.88930 val_ap= 0.89860 time= 0.10854\n",
      "训练次数: 2 Epoch: 0031 log_lik= 0.45205048 train_kl= 0.00862 train_loss= 0.46067 train_acc= 0.53752 val_roc= 0.88930 val_ap= 0.89903 time= 0.10953\n",
      "训练次数: 2 Epoch: 0032 log_lik= 0.4509587 train_kl= 0.00861 train_loss= 0.45957 train_acc= 0.53835 val_roc= 0.88911 val_ap= 0.89859 time= 0.11551\n",
      "训练次数: 2 Epoch: 0033 log_lik= 0.4498009 train_kl= 0.00861 train_loss= 0.45841 train_acc= 0.53951 val_roc= 0.88923 val_ap= 0.89819 time= 0.12148\n",
      "训练次数: 2 Epoch: 0034 log_lik= 0.44822663 train_kl= 0.00862 train_loss= 0.45684 train_acc= 0.54162 val_roc= 0.88913 val_ap= 0.89811 time= 0.10754\n",
      "训练次数: 2 Epoch: 0035 log_lik= 0.44632754 train_kl= 0.00863 train_loss= 0.45495 train_acc= 0.54291 val_roc= 0.88879 val_ap= 0.89749 time= 0.11053\n",
      "训练次数: 2 Epoch: 0036 log_lik= 0.4449406 train_kl= 0.00864 train_loss= 0.45358 train_acc= 0.54388 val_roc= 0.88817 val_ap= 0.89645 time= 0.11218\n",
      "训练次数: 2 Epoch: 0037 log_lik= 0.44401613 train_kl= 0.00866 train_loss= 0.45268 train_acc= 0.54484 val_roc= 0.88804 val_ap= 0.89628 time= 0.12099\n",
      "训练次数: 2 Epoch: 0038 log_lik= 0.44318497 train_kl= 0.00868 train_loss= 0.45186 train_acc= 0.54547 val_roc= 0.88794 val_ap= 0.89603 time= 0.10655\n",
      "训练次数: 2 Epoch: 0039 log_lik= 0.44256905 train_kl= 0.00869 train_loss= 0.45126 train_acc= 0.54657 val_roc= 0.88751 val_ap= 0.89571 time= 0.10655\n",
      "训练次数: 2 Epoch: 0040 log_lik= 0.4419978 train_kl= 0.00870 train_loss= 0.45070 train_acc= 0.54716 val_roc= 0.88717 val_ap= 0.89576 time= 0.11352\n",
      "训练次数: 2 Epoch: 0041 log_lik= 0.4413161 train_kl= 0.00870 train_loss= 0.45001 train_acc= 0.54744 val_roc= 0.88681 val_ap= 0.89545 time= 0.11850\n",
      "训练次数: 2 Epoch: 0042 log_lik= 0.4405035 train_kl= 0.00869 train_loss= 0.44919 train_acc= 0.54771 val_roc= 0.88668 val_ap= 0.89550 time= 0.10655\n",
      "训练次数: 2 Epoch: 0043 log_lik= 0.43953022 train_kl= 0.00868 train_loss= 0.44821 train_acc= 0.54812 val_roc= 0.88631 val_ap= 0.89469 time= 0.10456\n",
      "训练次数: 2 Epoch: 0044 log_lik= 0.4386766 train_kl= 0.00867 train_loss= 0.44735 train_acc= 0.54858 val_roc= 0.88534 val_ap= 0.89365 time= 0.11252\n",
      "训练次数: 2 Epoch: 0045 log_lik= 0.43799305 train_kl= 0.00866 train_loss= 0.44666 train_acc= 0.54909 val_roc= 0.88524 val_ap= 0.89398 time= 0.10655\n",
      "训练次数: 2 Epoch: 0046 log_lik= 0.43740544 train_kl= 0.00866 train_loss= 0.44607 train_acc= 0.54976 val_roc= 0.88573 val_ap= 0.89477 time= 0.10705\n",
      "训练次数: 2 Epoch: 0047 log_lik= 0.43671665 train_kl= 0.00866 train_loss= 0.44538 train_acc= 0.55016 val_roc= 0.88602 val_ap= 0.89549 time= 0.10655\n",
      "训练次数: 2 Epoch: 0048 log_lik= 0.43578222 train_kl= 0.00867 train_loss= 0.44445 train_acc= 0.55066 val_roc= 0.88704 val_ap= 0.89727 time= 0.10556\n",
      "训练次数: 2 Epoch: 0049 log_lik= 0.43502134 train_kl= 0.00867 train_loss= 0.44369 train_acc= 0.55084 val_roc= 0.88824 val_ap= 0.89903 time= 0.10256\n",
      "训练次数: 2 Epoch: 0050 log_lik= 0.4343616 train_kl= 0.00868 train_loss= 0.44304 train_acc= 0.55109 val_roc= 0.88917 val_ap= 0.90013 time= 0.10655\n",
      "训练次数: 2 Epoch: 0051 log_lik= 0.43366888 train_kl= 0.00869 train_loss= 0.44236 train_acc= 0.55137 val_roc= 0.89018 val_ap= 0.90145 time= 0.10854\n",
      "训练次数: 2 Epoch: 0052 log_lik= 0.4330394 train_kl= 0.00870 train_loss= 0.44174 train_acc= 0.55165 val_roc= 0.89059 val_ap= 0.90252 time= 0.10754\n",
      "训练次数: 2 Epoch: 0053 log_lik= 0.4325424 train_kl= 0.00871 train_loss= 0.44125 train_acc= 0.55230 val_roc= 0.89088 val_ap= 0.90351 time= 0.10754\n",
      "训练次数: 2 Epoch: 0054 log_lik= 0.432015 train_kl= 0.00871 train_loss= 0.44072 train_acc= 0.55273 val_roc= 0.89131 val_ap= 0.90435 time= 0.10456\n",
      "训练次数: 2 Epoch: 0055 log_lik= 0.43139756 train_kl= 0.00871 train_loss= 0.44010 train_acc= 0.55303 val_roc= 0.89170 val_ap= 0.90515 time= 0.11750\n",
      "训练次数: 2 Epoch: 0056 log_lik= 0.4307538 train_kl= 0.00870 train_loss= 0.43946 train_acc= 0.55337 val_roc= 0.89213 val_ap= 0.90603 time= 0.11352\n",
      "训练次数: 2 Epoch: 0057 log_lik= 0.43035385 train_kl= 0.00870 train_loss= 0.43905 train_acc= 0.55370 val_roc= 0.89255 val_ap= 0.90677 time= 0.10854\n",
      "训练次数: 2 Epoch: 0058 log_lik= 0.42965952 train_kl= 0.00869 train_loss= 0.43835 train_acc= 0.55407 val_roc= 0.89258 val_ap= 0.90690 time= 0.10754\n",
      "训练次数: 2 Epoch: 0059 log_lik= 0.42909747 train_kl= 0.00869 train_loss= 0.43779 train_acc= 0.55489 val_roc= 0.89306 val_ap= 0.90692 time= 0.11551\n",
      "训练次数: 2 Epoch: 0060 log_lik= 0.42867362 train_kl= 0.00869 train_loss= 0.43736 train_acc= 0.55536 val_roc= 0.89345 val_ap= 0.90720 time= 0.11255\n",
      "训练次数: 2 Epoch: 0061 log_lik= 0.42811766 train_kl= 0.00869 train_loss= 0.43681 train_acc= 0.55587 val_roc= 0.89410 val_ap= 0.90761 time= 0.10655\n",
      "训练次数: 2 Epoch: 0062 log_lik= 0.42755368 train_kl= 0.00870 train_loss= 0.43625 train_acc= 0.55612 val_roc= 0.89445 val_ap= 0.90792 time= 0.10906\n",
      "训练次数: 2 Epoch: 0063 log_lik= 0.4269542 train_kl= 0.00871 train_loss= 0.43566 train_acc= 0.55700 val_roc= 0.89507 val_ap= 0.90832 time= 0.10655\n",
      "训练次数: 2 Epoch: 0064 log_lik= 0.42638913 train_kl= 0.00871 train_loss= 0.43510 train_acc= 0.55802 val_roc= 0.89514 val_ap= 0.90824 time= 0.10953\n",
      "训练次数: 2 Epoch: 0065 log_lik= 0.4259119 train_kl= 0.00872 train_loss= 0.43463 train_acc= 0.55840 val_roc= 0.89544 val_ap= 0.90825 time= 0.10655\n",
      "训练次数: 2 Epoch: 0066 log_lik= 0.42535642 train_kl= 0.00872 train_loss= 0.43408 train_acc= 0.55944 val_roc= 0.89618 val_ap= 0.90823 time= 0.12248\n",
      "训练次数: 2 Epoch: 0067 log_lik= 0.42489392 train_kl= 0.00873 train_loss= 0.43362 train_acc= 0.55988 val_roc= 0.89659 val_ap= 0.90801 time= 0.11551\n",
      "训练次数: 2 Epoch: 0068 log_lik= 0.42442486 train_kl= 0.00873 train_loss= 0.43315 train_acc= 0.56086 val_roc= 0.89672 val_ap= 0.90793 time= 0.10754\n",
      "训练次数: 2 Epoch: 0069 log_lik= 0.4238483 train_kl= 0.00873 train_loss= 0.43258 train_acc= 0.56119 val_roc= 0.89643 val_ap= 0.90738 time= 0.11750\n",
      "训练次数: 2 Epoch: 0070 log_lik= 0.42338023 train_kl= 0.00873 train_loss= 0.43211 train_acc= 0.56232 val_roc= 0.89653 val_ap= 0.90731 time= 0.12708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0071 log_lik= 0.42296046 train_kl= 0.00872 train_loss= 0.43169 train_acc= 0.56260 val_roc= 0.89724 val_ap= 0.90765 time= 0.11650\n",
      "训练次数: 2 Epoch: 0072 log_lik= 0.42252472 train_kl= 0.00872 train_loss= 0.43125 train_acc= 0.56292 val_roc= 0.89758 val_ap= 0.90793 time= 0.11551\n",
      "训练次数: 2 Epoch: 0073 log_lik= 0.42213434 train_kl= 0.00872 train_loss= 0.43086 train_acc= 0.56356 val_roc= 0.89797 val_ap= 0.90818 time= 0.10854\n",
      "训练次数: 2 Epoch: 0074 log_lik= 0.4217428 train_kl= 0.00873 train_loss= 0.43047 train_acc= 0.56359 val_roc= 0.89806 val_ap= 0.90850 time= 0.10981\n",
      "训练次数: 2 Epoch: 0075 log_lik= 0.42131042 train_kl= 0.00873 train_loss= 0.43004 train_acc= 0.56407 val_roc= 0.89841 val_ap= 0.90875 time= 0.10854\n",
      "训练次数: 2 Epoch: 0076 log_lik= 0.4209856 train_kl= 0.00873 train_loss= 0.42972 train_acc= 0.56448 val_roc= 0.89883 val_ap= 0.90916 time= 0.11651\n",
      "训练次数: 2 Epoch: 0077 log_lik= 0.42062366 train_kl= 0.00874 train_loss= 0.42936 train_acc= 0.56466 val_roc= 0.89919 val_ap= 0.90956 time= 0.11352\n",
      "训练次数: 2 Epoch: 0078 log_lik= 0.42028058 train_kl= 0.00874 train_loss= 0.42902 train_acc= 0.56497 val_roc= 0.89939 val_ap= 0.90990 time= 0.11053\n",
      "训练次数: 2 Epoch: 0079 log_lik= 0.419948 train_kl= 0.00874 train_loss= 0.42869 train_acc= 0.56502 val_roc= 0.89980 val_ap= 0.91031 time= 0.11252\n",
      "训练次数: 2 Epoch: 0080 log_lik= 0.41966864 train_kl= 0.00874 train_loss= 0.42841 train_acc= 0.56498 val_roc= 0.90024 val_ap= 0.91075 time= 0.10655\n",
      "训练次数: 2 Epoch: 0081 log_lik= 0.41929907 train_kl= 0.00874 train_loss= 0.42804 train_acc= 0.56497 val_roc= 0.90049 val_ap= 0.91135 time= 0.10805\n",
      "训练次数: 2 Epoch: 0082 log_lik= 0.4190183 train_kl= 0.00874 train_loss= 0.42776 train_acc= 0.56518 val_roc= 0.90040 val_ap= 0.91246 time= 0.10754\n",
      "训练次数: 2 Epoch: 0083 log_lik= 0.41874272 train_kl= 0.00874 train_loss= 0.42749 train_acc= 0.56504 val_roc= 0.90056 val_ap= 0.91257 time= 0.11153\n",
      "训练次数: 2 Epoch: 0084 log_lik= 0.41839486 train_kl= 0.00874 train_loss= 0.42714 train_acc= 0.56504 val_roc= 0.90053 val_ap= 0.91258 time= 0.10754\n",
      "训练次数: 2 Epoch: 0085 log_lik= 0.4181558 train_kl= 0.00874 train_loss= 0.42690 train_acc= 0.56458 val_roc= 0.90088 val_ap= 0.91291 time= 0.10455\n",
      "训练次数: 2 Epoch: 0086 log_lik= 0.4179394 train_kl= 0.00874 train_loss= 0.42668 train_acc= 0.56457 val_roc= 0.90092 val_ap= 0.91330 time= 0.10655\n",
      "训练次数: 2 Epoch: 0087 log_lik= 0.4176538 train_kl= 0.00874 train_loss= 0.42640 train_acc= 0.56480 val_roc= 0.90111 val_ap= 0.91385 time= 0.11053\n",
      "训练次数: 2 Epoch: 0088 log_lik= 0.41735232 train_kl= 0.00875 train_loss= 0.42610 train_acc= 0.56451 val_roc= 0.90121 val_ap= 0.91419 time= 0.10854\n",
      "训练次数: 2 Epoch: 0089 log_lik= 0.4171396 train_kl= 0.00875 train_loss= 0.42589 train_acc= 0.56471 val_roc= 0.90144 val_ap= 0.91448 time= 0.10854\n",
      "训练次数: 2 Epoch: 0090 log_lik= 0.4168947 train_kl= 0.00875 train_loss= 0.42564 train_acc= 0.56477 val_roc= 0.90118 val_ap= 0.91420 time= 0.11650\n",
      "训练次数: 2 Epoch: 0091 log_lik= 0.41667306 train_kl= 0.00875 train_loss= 0.42543 train_acc= 0.56450 val_roc= 0.90097 val_ap= 0.91402 time= 0.10657\n",
      "训练次数: 2 Epoch: 0092 log_lik= 0.4164211 train_kl= 0.00876 train_loss= 0.42518 train_acc= 0.56457 val_roc= 0.90100 val_ap= 0.91436 time= 0.12447\n",
      "训练次数: 2 Epoch: 0093 log_lik= 0.41621783 train_kl= 0.00876 train_loss= 0.42497 train_acc= 0.56463 val_roc= 0.90104 val_ap= 0.91464 time= 0.10854\n",
      "训练次数: 2 Epoch: 0094 log_lik= 0.41601545 train_kl= 0.00876 train_loss= 0.42477 train_acc= 0.56478 val_roc= 0.90114 val_ap= 0.91489 time= 0.10555\n",
      "训练次数: 2 Epoch: 0095 log_lik= 0.41577393 train_kl= 0.00876 train_loss= 0.42453 train_acc= 0.56487 val_roc= 0.90085 val_ap= 0.91448 time= 0.10655\n",
      "训练次数: 2 Epoch: 0096 log_lik= 0.41553995 train_kl= 0.00876 train_loss= 0.42430 train_acc= 0.56461 val_roc= 0.90084 val_ap= 0.91440 time= 0.11352\n",
      "训练次数: 2 Epoch: 0097 log_lik= 0.41535056 train_kl= 0.00876 train_loss= 0.42411 train_acc= 0.56472 val_roc= 0.90103 val_ap= 0.91461 time= 0.10754\n",
      "训练次数: 2 Epoch: 0098 log_lik= 0.41512743 train_kl= 0.00876 train_loss= 0.42388 train_acc= 0.56466 val_roc= 0.90095 val_ap= 0.91473 time= 0.10754\n",
      "训练次数: 2 Epoch: 0099 log_lik= 0.41493082 train_kl= 0.00876 train_loss= 0.42369 train_acc= 0.56485 val_roc= 0.90085 val_ap= 0.91452 time= 0.10555\n",
      "训练次数: 2 Epoch: 0100 log_lik= 0.41477835 train_kl= 0.00876 train_loss= 0.42353 train_acc= 0.56469 val_roc= 0.90069 val_ap= 0.91416 time= 0.10754\n",
      "Optimization Finished!\n",
      "训练次数: 2 ROC score: 0.9144705810340297\n",
      "训练次数: 2 AP score: 0.9279877117417522\n",
      "训练次数: 3 Epoch: 0001 log_lik= 0.7869341 train_kl= 0.00805 train_loss= 0.79498 train_acc= 0.19975 val_roc= 0.71392 val_ap= 0.75026 time= 7.70935\n",
      "训练次数: 3 Epoch: 0002 log_lik= 0.7412798 train_kl= 0.00821 train_loss= 0.74949 train_acc= 0.00184 val_roc= 0.77564 val_ap= 0.78597 time= 0.12911\n",
      "训练次数: 3 Epoch: 0003 log_lik= 0.718441 train_kl= 0.00815 train_loss= 0.72659 train_acc= 0.02082 val_roc= 0.77614 val_ap= 0.77924 time= 0.12447\n",
      "训练次数: 3 Epoch: 0004 log_lik= 0.70141375 train_kl= 0.00824 train_loss= 0.70965 train_acc= 0.03893 val_roc= 0.85040 val_ap= 0.85016 time= 0.11602\n",
      "训练次数: 3 Epoch: 0005 log_lik= 0.6421855 train_kl= 0.00821 train_loss= 0.65039 train_acc= 0.24736 val_roc= 0.81784 val_ap= 0.81213 time= 0.11053\n",
      "训练次数: 3 Epoch: 0006 log_lik= 0.61179996 train_kl= 0.00828 train_loss= 0.62008 train_acc= 0.35490 val_roc= 0.84638 val_ap= 0.84481 time= 0.11451\n",
      "训练次数: 3 Epoch: 0007 log_lik= 0.56535137 train_kl= 0.00834 train_loss= 0.57369 train_acc= 0.44636 val_roc= 0.86212 val_ap= 0.85583 time= 0.11053\n",
      "训练次数: 3 Epoch: 0008 log_lik= 0.54210013 train_kl= 0.00844 train_loss= 0.55054 train_acc= 0.47846 val_roc= 0.87039 val_ap= 0.86728 time= 0.11949\n",
      "训练次数: 3 Epoch: 0009 log_lik= 0.5263733 train_kl= 0.00854 train_loss= 0.53492 train_acc= 0.50489 val_roc= 0.87927 val_ap= 0.88015 time= 0.11252\n",
      "训练次数: 3 Epoch: 0010 log_lik= 0.5178597 train_kl= 0.00864 train_loss= 0.52650 train_acc= 0.51600 val_roc= 0.88790 val_ap= 0.88787 time= 0.12348\n",
      "训练次数: 3 Epoch: 0011 log_lik= 0.5246095 train_kl= 0.00872 train_loss= 0.53333 train_acc= 0.51422 val_roc= 0.89838 val_ap= 0.89901 time= 0.10953\n",
      "训练次数: 3 Epoch: 0012 log_lik= 0.5132891 train_kl= 0.00874 train_loss= 0.52203 train_acc= 0.52014 val_roc= 0.90523 val_ap= 0.90591 time= 0.11352\n",
      "训练次数: 3 Epoch: 0013 log_lik= 0.50132865 train_kl= 0.00872 train_loss= 0.51005 train_acc= 0.52131 val_roc= 0.90934 val_ap= 0.91046 time= 0.11252\n",
      "训练次数: 3 Epoch: 0014 log_lik= 0.48830307 train_kl= 0.00869 train_loss= 0.49699 train_acc= 0.52434 val_roc= 0.91067 val_ap= 0.91136 time= 0.11553\n",
      "训练次数: 3 Epoch: 0015 log_lik= 0.48014653 train_kl= 0.00865 train_loss= 0.48879 train_acc= 0.52771 val_roc= 0.91078 val_ap= 0.91175 time= 0.10754\n",
      "训练次数: 3 Epoch: 0016 log_lik= 0.47768483 train_kl= 0.00861 train_loss= 0.48630 train_acc= 0.52909 val_roc= 0.91122 val_ap= 0.91246 time= 0.10555\n",
      "训练次数: 3 Epoch: 0017 log_lik= 0.47799054 train_kl= 0.00858 train_loss= 0.48658 train_acc= 0.52816 val_roc= 0.91207 val_ap= 0.91383 time= 0.11152\n",
      "训练次数: 3 Epoch: 0018 log_lik= 0.47657916 train_kl= 0.00857 train_loss= 0.48515 train_acc= 0.52857 val_roc= 0.91381 val_ap= 0.91714 time= 0.10555\n",
      "训练次数: 3 Epoch: 0019 log_lik= 0.4730952 train_kl= 0.00856 train_loss= 0.48166 train_acc= 0.53200 val_roc= 0.91674 val_ap= 0.92055 time= 0.10555\n",
      "训练次数: 3 Epoch: 0020 log_lik= 0.47003645 train_kl= 0.00857 train_loss= 0.47861 train_acc= 0.53262 val_roc= 0.91921 val_ap= 0.92363 time= 0.11551\n",
      "训练次数: 3 Epoch: 0021 log_lik= 0.46727347 train_kl= 0.00858 train_loss= 0.47586 train_acc= 0.53257 val_roc= 0.92197 val_ap= 0.92717 time= 0.10456\n",
      "训练次数: 3 Epoch: 0022 log_lik= 0.4635156 train_kl= 0.00860 train_loss= 0.47212 train_acc= 0.53450 val_roc= 0.92468 val_ap= 0.92995 time= 0.10655\n",
      "训练次数: 3 Epoch: 0023 log_lik= 0.46017843 train_kl= 0.00862 train_loss= 0.46880 train_acc= 0.53723 val_roc= 0.92631 val_ap= 0.93121 time= 0.11252\n",
      "训练次数: 3 Epoch: 0024 log_lik= 0.4578889 train_kl= 0.00865 train_loss= 0.46654 train_acc= 0.53873 val_roc= 0.92771 val_ap= 0.93235 time= 0.10555\n",
      "训练次数: 3 Epoch: 0025 log_lik= 0.4563383 train_kl= 0.00867 train_loss= 0.46501 train_acc= 0.54086 val_roc= 0.92783 val_ap= 0.93232 time= 0.10655\n",
      "训练次数: 3 Epoch: 0026 log_lik= 0.45433554 train_kl= 0.00869 train_loss= 0.46303 train_acc= 0.54386 val_roc= 0.92721 val_ap= 0.93195 time= 0.11153\n",
      "训练次数: 3 Epoch: 0027 log_lik= 0.45282832 train_kl= 0.00871 train_loss= 0.46153 train_acc= 0.54600 val_roc= 0.92625 val_ap= 0.93143 time= 0.10854\n",
      "训练次数: 3 Epoch: 0028 log_lik= 0.45162868 train_kl= 0.00871 train_loss= 0.46034 train_acc= 0.54696 val_roc= 0.92531 val_ap= 0.93123 time= 0.10558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0029 log_lik= 0.45001712 train_kl= 0.00870 train_loss= 0.45872 train_acc= 0.54799 val_roc= 0.92407 val_ap= 0.93010 time= 0.11650\n",
      "训练次数: 3 Epoch: 0030 log_lik= 0.4481363 train_kl= 0.00869 train_loss= 0.45683 train_acc= 0.54949 val_roc= 0.92304 val_ap= 0.92934 time= 0.10854\n",
      "训练次数: 3 Epoch: 0031 log_lik= 0.4464003 train_kl= 0.00868 train_loss= 0.45508 train_acc= 0.55071 val_roc= 0.92221 val_ap= 0.92874 time= 0.10954\n",
      "训练次数: 3 Epoch: 0032 log_lik= 0.44510046 train_kl= 0.00867 train_loss= 0.45377 train_acc= 0.55102 val_roc= 0.92125 val_ap= 0.92819 time= 0.11750\n",
      "训练次数: 3 Epoch: 0033 log_lik= 0.44393045 train_kl= 0.00866 train_loss= 0.45259 train_acc= 0.55147 val_roc= 0.92020 val_ap= 0.92749 time= 0.10853\n",
      "训练次数: 3 Epoch: 0034 log_lik= 0.4428128 train_kl= 0.00866 train_loss= 0.45147 train_acc= 0.55194 val_roc= 0.91908 val_ap= 0.92678 time= 0.10954\n",
      "训练次数: 3 Epoch: 0035 log_lik= 0.4416625 train_kl= 0.00866 train_loss= 0.45032 train_acc= 0.55278 val_roc= 0.91855 val_ap= 0.92642 time= 0.11252\n",
      "训练次数: 3 Epoch: 0036 log_lik= 0.44057143 train_kl= 0.00866 train_loss= 0.44923 train_acc= 0.55320 val_roc= 0.91839 val_ap= 0.92622 time= 0.11451\n",
      "训练次数: 3 Epoch: 0037 log_lik= 0.43983948 train_kl= 0.00867 train_loss= 0.44851 train_acc= 0.55342 val_roc= 0.91863 val_ap= 0.92626 time= 0.11352\n",
      "训练次数: 3 Epoch: 0038 log_lik= 0.43885845 train_kl= 0.00868 train_loss= 0.44754 train_acc= 0.55474 val_roc= 0.91871 val_ap= 0.92620 time= 0.10777\n",
      "训练次数: 3 Epoch: 0039 log_lik= 0.4379661 train_kl= 0.00869 train_loss= 0.44666 train_acc= 0.55557 val_roc= 0.91881 val_ap= 0.92651 time= 0.11551\n",
      "训练次数: 3 Epoch: 0040 log_lik= 0.43702888 train_kl= 0.00870 train_loss= 0.44573 train_acc= 0.55692 val_roc= 0.91904 val_ap= 0.92680 time= 0.10754\n",
      "训练次数: 3 Epoch: 0041 log_lik= 0.4361689 train_kl= 0.00871 train_loss= 0.44488 train_acc= 0.55831 val_roc= 0.91939 val_ap= 0.92724 time= 0.11451\n",
      "训练次数: 3 Epoch: 0042 log_lik= 0.43546754 train_kl= 0.00871 train_loss= 0.44418 train_acc= 0.55860 val_roc= 0.92007 val_ap= 0.92805 time= 0.10854\n",
      "训练次数: 3 Epoch: 0043 log_lik= 0.43472752 train_kl= 0.00871 train_loss= 0.44344 train_acc= 0.55924 val_roc= 0.92060 val_ap= 0.92873 time= 0.10754\n",
      "训练次数: 3 Epoch: 0044 log_lik= 0.43373775 train_kl= 0.00871 train_loss= 0.44245 train_acc= 0.55994 val_roc= 0.92150 val_ap= 0.92950 time= 0.11153\n",
      "训练次数: 3 Epoch: 0045 log_lik= 0.43303573 train_kl= 0.00870 train_loss= 0.44174 train_acc= 0.56086 val_roc= 0.92205 val_ap= 0.93003 time= 0.11252\n",
      "训练次数: 3 Epoch: 0046 log_lik= 0.43233293 train_kl= 0.00870 train_loss= 0.44103 train_acc= 0.56154 val_roc= 0.92242 val_ap= 0.93080 time= 0.10953\n",
      "训练次数: 3 Epoch: 0047 log_lik= 0.43151706 train_kl= 0.00870 train_loss= 0.44021 train_acc= 0.56243 val_roc= 0.92300 val_ap= 0.93187 time= 0.11451\n",
      "训练次数: 3 Epoch: 0048 log_lik= 0.43096426 train_kl= 0.00869 train_loss= 0.43966 train_acc= 0.56301 val_roc= 0.92330 val_ap= 0.93254 time= 0.11750\n",
      "训练次数: 3 Epoch: 0049 log_lik= 0.4304647 train_kl= 0.00870 train_loss= 0.43916 train_acc= 0.56356 val_roc= 0.92398 val_ap= 0.93308 time= 0.10606\n",
      "训练次数: 3 Epoch: 0050 log_lik= 0.42983866 train_kl= 0.00870 train_loss= 0.43854 train_acc= 0.56386 val_roc= 0.92498 val_ap= 0.93406 time= 0.11551\n",
      "训练次数: 3 Epoch: 0051 log_lik= 0.4293661 train_kl= 0.00870 train_loss= 0.43807 train_acc= 0.56402 val_roc= 0.92527 val_ap= 0.93456 time= 0.12746\n",
      "训练次数: 3 Epoch: 0052 log_lik= 0.42894664 train_kl= 0.00870 train_loss= 0.43765 train_acc= 0.56375 val_roc= 0.92472 val_ap= 0.93444 time= 0.12797\n",
      "训练次数: 3 Epoch: 0053 log_lik= 0.42841733 train_kl= 0.00871 train_loss= 0.43713 train_acc= 0.56386 val_roc= 0.92430 val_ap= 0.93414 time= 0.11252\n",
      "训练次数: 3 Epoch: 0054 log_lik= 0.4279705 train_kl= 0.00872 train_loss= 0.43669 train_acc= 0.56453 val_roc= 0.92356 val_ap= 0.93393 time= 0.10978\n",
      "训练次数: 3 Epoch: 0055 log_lik= 0.42755643 train_kl= 0.00872 train_loss= 0.43628 train_acc= 0.56365 val_roc= 0.92348 val_ap= 0.93386 time= 0.13742\n",
      "训练次数: 3 Epoch: 0056 log_lik= 0.4271894 train_kl= 0.00873 train_loss= 0.43591 train_acc= 0.56410 val_roc= 0.92288 val_ap= 0.93332 time= 0.11551\n",
      "训练次数: 3 Epoch: 0057 log_lik= 0.42672986 train_kl= 0.00873 train_loss= 0.43546 train_acc= 0.56396 val_roc= 0.92273 val_ap= 0.93291 time= 0.10754\n",
      "训练次数: 3 Epoch: 0058 log_lik= 0.42633992 train_kl= 0.00873 train_loss= 0.43507 train_acc= 0.56383 val_roc= 0.92212 val_ap= 0.93234 time= 0.10854\n",
      "训练次数: 3 Epoch: 0059 log_lik= 0.4259847 train_kl= 0.00873 train_loss= 0.43471 train_acc= 0.56332 val_roc= 0.92167 val_ap= 0.93178 time= 0.11153\n",
      "训练次数: 3 Epoch: 0060 log_lik= 0.4255933 train_kl= 0.00872 train_loss= 0.43432 train_acc= 0.56355 val_roc= 0.92114 val_ap= 0.93124 time= 0.10655\n",
      "训练次数: 3 Epoch: 0061 log_lik= 0.4252211 train_kl= 0.00872 train_loss= 0.43394 train_acc= 0.56346 val_roc= 0.92020 val_ap= 0.93046 time= 0.10655\n",
      "训练次数: 3 Epoch: 0062 log_lik= 0.4249394 train_kl= 0.00872 train_loss= 0.43366 train_acc= 0.56371 val_roc= 0.91985 val_ap= 0.93014 time= 0.12251\n",
      "训练次数: 3 Epoch: 0063 log_lik= 0.42457548 train_kl= 0.00872 train_loss= 0.43330 train_acc= 0.56368 val_roc= 0.91953 val_ap= 0.92970 time= 0.11152\n",
      "训练次数: 3 Epoch: 0064 log_lik= 0.42430753 train_kl= 0.00872 train_loss= 0.43303 train_acc= 0.56368 val_roc= 0.91969 val_ap= 0.92986 time= 0.13941\n",
      "训练次数: 3 Epoch: 0065 log_lik= 0.4240388 train_kl= 0.00872 train_loss= 0.43276 train_acc= 0.56396 val_roc= 0.91963 val_ap= 0.92974 time= 0.11650\n",
      "训练次数: 3 Epoch: 0066 log_lik= 0.42373252 train_kl= 0.00872 train_loss= 0.43246 train_acc= 0.56383 val_roc= 0.91946 val_ap= 0.92951 time= 0.10954\n",
      "训练次数: 3 Epoch: 0067 log_lik= 0.4233388 train_kl= 0.00873 train_loss= 0.43206 train_acc= 0.56394 val_roc= 0.91933 val_ap= 0.92949 time= 0.11451\n",
      "训练次数: 3 Epoch: 0068 log_lik= 0.42302522 train_kl= 0.00873 train_loss= 0.43175 train_acc= 0.56413 val_roc= 0.91956 val_ap= 0.92972 time= 0.12447\n",
      "训练次数: 3 Epoch: 0069 log_lik= 0.42271736 train_kl= 0.00873 train_loss= 0.43145 train_acc= 0.56443 val_roc= 0.92002 val_ap= 0.93018 time= 0.11551\n",
      "训练次数: 3 Epoch: 0070 log_lik= 0.4224521 train_kl= 0.00873 train_loss= 0.43119 train_acc= 0.56503 val_roc= 0.92024 val_ap= 0.93049 time= 0.11850\n",
      "训练次数: 3 Epoch: 0071 log_lik= 0.42212993 train_kl= 0.00873 train_loss= 0.43086 train_acc= 0.56472 val_roc= 0.92035 val_ap= 0.93078 time= 0.11651\n",
      "训练次数: 3 Epoch: 0072 log_lik= 0.4218671 train_kl= 0.00874 train_loss= 0.43060 train_acc= 0.56503 val_roc= 0.92043 val_ap= 0.93106 time= 0.10953\n",
      "训练次数: 3 Epoch: 0073 log_lik= 0.42155197 train_kl= 0.00874 train_loss= 0.43029 train_acc= 0.56540 val_roc= 0.92048 val_ap= 0.93138 time= 0.10754\n",
      "训练次数: 3 Epoch: 0074 log_lik= 0.42126808 train_kl= 0.00874 train_loss= 0.43000 train_acc= 0.56558 val_roc= 0.92030 val_ap= 0.93139 time= 0.11053\n",
      "训练次数: 3 Epoch: 0075 log_lik= 0.42096978 train_kl= 0.00874 train_loss= 0.42971 train_acc= 0.56590 val_roc= 0.92035 val_ap= 0.93158 time= 0.12199\n",
      "训练次数: 3 Epoch: 0076 log_lik= 0.420764 train_kl= 0.00873 train_loss= 0.42950 train_acc= 0.56606 val_roc= 0.92033 val_ap= 0.93172 time= 0.10854\n",
      "训练次数: 3 Epoch: 0077 log_lik= 0.42043185 train_kl= 0.00873 train_loss= 0.42917 train_acc= 0.56616 val_roc= 0.92008 val_ap= 0.93169 time= 0.10953\n",
      "训练次数: 3 Epoch: 0078 log_lik= 0.4201942 train_kl= 0.00873 train_loss= 0.42893 train_acc= 0.56616 val_roc= 0.92004 val_ap= 0.93180 time= 0.12945\n",
      "训练次数: 3 Epoch: 0079 log_lik= 0.4198907 train_kl= 0.00873 train_loss= 0.42863 train_acc= 0.56615 val_roc= 0.92002 val_ap= 0.93197 time= 0.11850\n",
      "训练次数: 3 Epoch: 0080 log_lik= 0.41957012 train_kl= 0.00874 train_loss= 0.42831 train_acc= 0.56650 val_roc= 0.91976 val_ap= 0.93189 time= 0.10655\n",
      "训练次数: 3 Epoch: 0081 log_lik= 0.4194105 train_kl= 0.00874 train_loss= 0.42815 train_acc= 0.56647 val_roc= 0.91947 val_ap= 0.93176 time= 0.12506\n",
      "训练次数: 3 Epoch: 0082 log_lik= 0.4190833 train_kl= 0.00874 train_loss= 0.42782 train_acc= 0.56677 val_roc= 0.91913 val_ap= 0.93140 time= 0.11053\n",
      "训练次数: 3 Epoch: 0083 log_lik= 0.4188602 train_kl= 0.00874 train_loss= 0.42760 train_acc= 0.56700 val_roc= 0.91897 val_ap= 0.93131 time= 0.11352\n",
      "训练次数: 3 Epoch: 0084 log_lik= 0.41856542 train_kl= 0.00874 train_loss= 0.42731 train_acc= 0.56675 val_roc= 0.91892 val_ap= 0.93143 time= 0.13343\n",
      "训练次数: 3 Epoch: 0085 log_lik= 0.41825256 train_kl= 0.00875 train_loss= 0.42700 train_acc= 0.56675 val_roc= 0.91855 val_ap= 0.93121 time= 0.12149\n",
      "训练次数: 3 Epoch: 0086 log_lik= 0.418033 train_kl= 0.00875 train_loss= 0.42678 train_acc= 0.56693 val_roc= 0.91848 val_ap= 0.93107 time= 0.12547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0087 log_lik= 0.41779375 train_kl= 0.00875 train_loss= 0.42654 train_acc= 0.56687 val_roc= 0.91827 val_ap= 0.93107 time= 0.11056\n",
      "训练次数: 3 Epoch: 0088 log_lik= 0.417514 train_kl= 0.00875 train_loss= 0.42626 train_acc= 0.56703 val_roc= 0.91793 val_ap= 0.93084 time= 0.12148\n",
      "训练次数: 3 Epoch: 0089 log_lik= 0.41726032 train_kl= 0.00875 train_loss= 0.42601 train_acc= 0.56705 val_roc= 0.91778 val_ap= 0.93080 time= 0.11750\n",
      "训练次数: 3 Epoch: 0090 log_lik= 0.4169537 train_kl= 0.00875 train_loss= 0.42570 train_acc= 0.56718 val_roc= 0.91746 val_ap= 0.93068 time= 0.11850\n",
      "训练次数: 3 Epoch: 0091 log_lik= 0.41674548 train_kl= 0.00875 train_loss= 0.42550 train_acc= 0.56711 val_roc= 0.91722 val_ap= 0.93047 time= 0.13741\n",
      "训练次数: 3 Epoch: 0092 log_lik= 0.41646114 train_kl= 0.00875 train_loss= 0.42521 train_acc= 0.56701 val_roc= 0.91706 val_ap= 0.93039 time= 0.11610\n",
      "训练次数: 3 Epoch: 0093 log_lik= 0.41623178 train_kl= 0.00875 train_loss= 0.42498 train_acc= 0.56728 val_roc= 0.91699 val_ap= 0.93050 time= 0.11053\n",
      "训练次数: 3 Epoch: 0094 log_lik= 0.4159889 train_kl= 0.00875 train_loss= 0.42474 train_acc= 0.56718 val_roc= 0.91675 val_ap= 0.93041 time= 0.12049\n",
      "训练次数: 3 Epoch: 0095 log_lik= 0.41566968 train_kl= 0.00876 train_loss= 0.42442 train_acc= 0.56753 val_roc= 0.91674 val_ap= 0.93043 time= 0.11451\n",
      "训练次数: 3 Epoch: 0096 log_lik= 0.41549972 train_kl= 0.00876 train_loss= 0.42426 train_acc= 0.56727 val_roc= 0.91668 val_ap= 0.93039 time= 0.10754\n",
      "训练次数: 3 Epoch: 0097 log_lik= 0.41524577 train_kl= 0.00876 train_loss= 0.42400 train_acc= 0.56709 val_roc= 0.91651 val_ap= 0.93040 time= 0.12646\n",
      "训练次数: 3 Epoch: 0098 log_lik= 0.41507813 train_kl= 0.00876 train_loss= 0.42384 train_acc= 0.56745 val_roc= 0.91615 val_ap= 0.93014 time= 0.11081\n",
      "训练次数: 3 Epoch: 0099 log_lik= 0.41484198 train_kl= 0.00876 train_loss= 0.42360 train_acc= 0.56774 val_roc= 0.91615 val_ap= 0.93031 time= 0.10954\n",
      "训练次数: 3 Epoch: 0100 log_lik= 0.41461802 train_kl= 0.00876 train_loss= 0.42338 train_acc= 0.56785 val_roc= 0.91606 val_ap= 0.93031 time= 0.10854\n",
      "Optimization Finished!\n",
      "训练次数: 3 ROC score: 0.9237098034414843\n",
      "训练次数: 3 AP score: 0.9342487738136236\n",
      "训练次数: 4 Epoch: 0001 log_lik= 0.78164756 train_kl= 0.00806 train_loss= 0.78970 train_acc= 0.01823 val_roc= 0.68921 val_ap= 0.70934 time= 9.67070\n",
      "训练次数: 4 Epoch: 0002 log_lik= 0.9293088 train_kl= 0.00841 train_loss= 0.93772 train_acc= 0.00159 val_roc= 0.74071 val_ap= 0.75084 time= 0.13941\n",
      "训练次数: 4 Epoch: 0003 log_lik= 0.7517034 train_kl= 0.00810 train_loss= 0.75980 train_acc= 0.00440 val_roc= 0.67081 val_ap= 0.68957 time= 0.11949\n",
      "训练次数: 4 Epoch: 0004 log_lik= 0.7575782 train_kl= 0.00809 train_loss= 0.76567 train_acc= 0.01551 val_roc= 0.65366 val_ap= 0.67600 time= 0.11453\n",
      "训练次数: 4 Epoch: 0005 log_lik= 0.7488657 train_kl= 0.00816 train_loss= 0.75702 train_acc= 0.00327 val_roc= 0.65694 val_ap= 0.68021 time= 0.11252\n",
      "训练次数: 4 Epoch: 0006 log_lik= 0.760717 train_kl= 0.00821 train_loss= 0.76892 train_acc= 0.00202 val_roc= 0.66074 val_ap= 0.68648 time= 0.11153\n",
      "训练次数: 4 Epoch: 0007 log_lik= 0.7464636 train_kl= 0.00817 train_loss= 0.75464 train_acc= 0.00470 val_roc= 0.67160 val_ap= 0.69653 time= 0.11252\n",
      "训练次数: 4 Epoch: 0008 log_lik= 0.739258 train_kl= 0.00814 train_loss= 0.74739 train_acc= 0.00943 val_roc= 0.69295 val_ap= 0.71674 time= 0.11949\n",
      "训练次数: 4 Epoch: 0009 log_lik= 0.7367315 train_kl= 0.00812 train_loss= 0.74485 train_acc= 0.01499 val_roc= 0.71849 val_ap= 0.73964 time= 0.11850\n",
      "训练次数: 4 Epoch: 0010 log_lik= 0.7306461 train_kl= 0.00812 train_loss= 0.73877 train_acc= 0.01526 val_roc= 0.73273 val_ap= 0.74961 time= 0.10754\n",
      "训练次数: 4 Epoch: 0011 log_lik= 0.7201488 train_kl= 0.00814 train_loss= 0.72829 train_acc= 0.01423 val_roc= 0.73952 val_ap= 0.75351 time= 0.11551\n",
      "训练次数: 4 Epoch: 0012 log_lik= 0.7105949 train_kl= 0.00817 train_loss= 0.71877 train_acc= 0.01299 val_roc= 0.75207 val_ap= 0.76219 time= 0.10854\n",
      "训练次数: 4 Epoch: 0013 log_lik= 0.7033413 train_kl= 0.00820 train_loss= 0.71154 train_acc= 0.01684 val_roc= 0.77487 val_ap= 0.78139 time= 0.12547\n",
      "训练次数: 4 Epoch: 0014 log_lik= 0.6917981 train_kl= 0.00822 train_loss= 0.70001 train_acc= 0.03276 val_roc= 0.80238 val_ap= 0.80590 time= 0.11750\n",
      "训练次数: 4 Epoch: 0015 log_lik= 0.67653286 train_kl= 0.00822 train_loss= 0.68475 train_acc= 0.06832 val_roc= 0.82362 val_ap= 0.82733 time= 0.11750\n",
      "训练次数: 4 Epoch: 0016 log_lik= 0.6615369 train_kl= 0.00822 train_loss= 0.66975 train_acc= 0.12481 val_roc= 0.83373 val_ap= 0.83901 time= 0.11670\n",
      "训练次数: 4 Epoch: 0017 log_lik= 0.6464452 train_kl= 0.00823 train_loss= 0.65467 train_acc= 0.19872 val_roc= 0.83517 val_ap= 0.83914 time= 0.11004\n",
      "训练次数: 4 Epoch: 0018 log_lik= 0.6309146 train_kl= 0.00825 train_loss= 0.63916 train_acc= 0.27413 val_roc= 0.83397 val_ap= 0.83700 time= 0.11153\n",
      "训练次数: 4 Epoch: 0019 log_lik= 0.6151525 train_kl= 0.00828 train_loss= 0.62343 train_acc= 0.33681 val_roc= 0.83292 val_ap= 0.83494 time= 0.11751\n",
      "训练次数: 4 Epoch: 0020 log_lik= 0.60179484 train_kl= 0.00832 train_loss= 0.61012 train_acc= 0.38101 val_roc= 0.83533 val_ap= 0.83713 time= 0.11650\n",
      "训练次数: 4 Epoch: 0021 log_lik= 0.5893974 train_kl= 0.00836 train_loss= 0.59776 train_acc= 0.41471 val_roc= 0.83964 val_ap= 0.84229 time= 0.11153\n",
      "训练次数: 4 Epoch: 0022 log_lik= 0.5777783 train_kl= 0.00840 train_loss= 0.58617 train_acc= 0.44226 val_roc= 0.84440 val_ap= 0.84873 time= 0.12447\n",
      "训练次数: 4 Epoch: 0023 log_lik= 0.56731427 train_kl= 0.00843 train_loss= 0.57574 train_acc= 0.46262 val_roc= 0.84875 val_ap= 0.85320 time= 0.10854\n",
      "训练次数: 4 Epoch: 0024 log_lik= 0.5574928 train_kl= 0.00845 train_loss= 0.56595 train_acc= 0.48035 val_roc= 0.85239 val_ap= 0.85757 time= 0.11651\n",
      "训练次数: 4 Epoch: 0025 log_lik= 0.5483521 train_kl= 0.00848 train_loss= 0.55683 train_acc= 0.49472 val_roc= 0.85596 val_ap= 0.86014 time= 0.11352\n",
      "训练次数: 4 Epoch: 0026 log_lik= 0.53977567 train_kl= 0.00849 train_loss= 0.54827 train_acc= 0.50741 val_roc= 0.85888 val_ap= 0.86195 time= 0.11153\n",
      "训练次数: 4 Epoch: 0027 log_lik= 0.53260565 train_kl= 0.00851 train_loss= 0.54111 train_acc= 0.51762 val_roc= 0.86130 val_ap= 0.86357 time= 0.10754\n",
      "训练次数: 4 Epoch: 0028 log_lik= 0.5255951 train_kl= 0.00851 train_loss= 0.53411 train_acc= 0.52773 val_roc= 0.86517 val_ap= 0.86702 time= 0.10854\n",
      "训练次数: 4 Epoch: 0029 log_lik= 0.5200928 train_kl= 0.00851 train_loss= 0.52861 train_acc= 0.53359 val_roc= 0.86839 val_ap= 0.87006 time= 0.11774\n",
      "训练次数: 4 Epoch: 0030 log_lik= 0.51664215 train_kl= 0.00852 train_loss= 0.52516 train_acc= 0.53708 val_roc= 0.87030 val_ap= 0.87229 time= 0.11153\n",
      "训练次数: 4 Epoch: 0031 log_lik= 0.51512724 train_kl= 0.00852 train_loss= 0.52365 train_acc= 0.53769 val_roc= 0.87250 val_ap= 0.87501 time= 0.11253\n",
      "训练次数: 4 Epoch: 0032 log_lik= 0.5136985 train_kl= 0.00853 train_loss= 0.52222 train_acc= 0.53757 val_roc= 0.87425 val_ap= 0.87692 time= 0.11750\n",
      "训练次数: 4 Epoch: 0033 log_lik= 0.5119346 train_kl= 0.00853 train_loss= 0.52046 train_acc= 0.53874 val_roc= 0.87590 val_ap= 0.87786 time= 0.11750\n",
      "训练次数: 4 Epoch: 0034 log_lik= 0.50923616 train_kl= 0.00853 train_loss= 0.51777 train_acc= 0.54039 val_roc= 0.87731 val_ap= 0.87849 time= 0.11053\n",
      "训练次数: 4 Epoch: 0035 log_lik= 0.5064346 train_kl= 0.00853 train_loss= 0.51496 train_acc= 0.54221 val_roc= 0.87896 val_ap= 0.87957 time= 0.11352\n",
      "训练次数: 4 Epoch: 0036 log_lik= 0.50422615 train_kl= 0.00853 train_loss= 0.51276 train_acc= 0.54418 val_roc= 0.88116 val_ap= 0.88196 time= 0.11651\n",
      "训练次数: 4 Epoch: 0037 log_lik= 0.50172144 train_kl= 0.00853 train_loss= 0.51025 train_acc= 0.54638 val_roc= 0.88486 val_ap= 0.88557 time= 0.11352\n",
      "训练次数: 4 Epoch: 0038 log_lik= 0.4987471 train_kl= 0.00853 train_loss= 0.50727 train_acc= 0.54910 val_roc= 0.88853 val_ap= 0.88968 time= 0.11452\n",
      "训练次数: 4 Epoch: 0039 log_lik= 0.4957627 train_kl= 0.00852 train_loss= 0.50429 train_acc= 0.55171 val_roc= 0.89280 val_ap= 0.89446 time= 0.10555\n",
      "训练次数: 4 Epoch: 0040 log_lik= 0.4926537 train_kl= 0.00853 train_loss= 0.50118 train_acc= 0.55385 val_roc= 0.89627 val_ap= 0.89816 time= 0.11551\n",
      "训练次数: 4 Epoch: 0041 log_lik= 0.489835 train_kl= 0.00853 train_loss= 0.49836 train_acc= 0.55661 val_roc= 0.89897 val_ap= 0.90150 time= 0.12270\n",
      "训练次数: 4 Epoch: 0042 log_lik= 0.48712143 train_kl= 0.00854 train_loss= 0.49566 train_acc= 0.55986 val_roc= 0.90118 val_ap= 0.90390 time= 0.10754\n",
      "训练次数: 4 Epoch: 0043 log_lik= 0.4846863 train_kl= 0.00855 train_loss= 0.49323 train_acc= 0.56391 val_roc= 0.90249 val_ap= 0.90482 time= 0.11352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 Epoch: 0044 log_lik= 0.48206612 train_kl= 0.00856 train_loss= 0.49062 train_acc= 0.56774 val_roc= 0.90329 val_ap= 0.90564 time= 0.10854\n",
      "训练次数: 4 Epoch: 0045 log_lik= 0.47997496 train_kl= 0.00857 train_loss= 0.48854 train_acc= 0.57173 val_roc= 0.90396 val_ap= 0.90628 time= 0.11352\n",
      "训练次数: 4 Epoch: 0046 log_lik= 0.47807556 train_kl= 0.00858 train_loss= 0.48665 train_acc= 0.57449 val_roc= 0.90516 val_ap= 0.90758 time= 0.11451\n",
      "训练次数: 4 Epoch: 0047 log_lik= 0.47612858 train_kl= 0.00858 train_loss= 0.48471 train_acc= 0.57677 val_roc= 0.90610 val_ap= 0.90832 time= 0.11153\n",
      "训练次数: 4 Epoch: 0048 log_lik= 0.47475675 train_kl= 0.00859 train_loss= 0.48335 train_acc= 0.57996 val_roc= 0.90742 val_ap= 0.90991 time= 0.11653\n",
      "训练次数: 4 Epoch: 0049 log_lik= 0.4733486 train_kl= 0.00859 train_loss= 0.48194 train_acc= 0.58235 val_roc= 0.90798 val_ap= 0.91025 time= 0.13543\n",
      "训练次数: 4 Epoch: 0050 log_lik= 0.4721557 train_kl= 0.00859 train_loss= 0.48075 train_acc= 0.58497 val_roc= 0.90834 val_ap= 0.91042 time= 0.11750\n",
      "训练次数: 4 Epoch: 0051 log_lik= 0.4713518 train_kl= 0.00859 train_loss= 0.47995 train_acc= 0.58774 val_roc= 0.90789 val_ap= 0.91039 time= 0.11153\n",
      "训练次数: 4 Epoch: 0052 log_lik= 0.47038603 train_kl= 0.00859 train_loss= 0.47898 train_acc= 0.58947 val_roc= 0.90752 val_ap= 0.91038 time= 0.11152\n",
      "训练次数: 4 Epoch: 0053 log_lik= 0.46926564 train_kl= 0.00859 train_loss= 0.47786 train_acc= 0.59293 val_roc= 0.90715 val_ap= 0.91030 time= 0.11865\n",
      "训练次数: 4 Epoch: 0054 log_lik= 0.468112 train_kl= 0.00859 train_loss= 0.47670 train_acc= 0.59432 val_roc= 0.90668 val_ap= 0.90997 time= 0.11551\n",
      "训练次数: 4 Epoch: 0055 log_lik= 0.46712422 train_kl= 0.00859 train_loss= 0.47572 train_acc= 0.59596 val_roc= 0.90610 val_ap= 0.90945 time= 0.12058\n",
      "训练次数: 4 Epoch: 0056 log_lik= 0.46627122 train_kl= 0.00859 train_loss= 0.47486 train_acc= 0.59659 val_roc= 0.90630 val_ap= 0.90981 time= 0.13343\n",
      "训练次数: 4 Epoch: 0057 log_lik= 0.4656088 train_kl= 0.00859 train_loss= 0.47420 train_acc= 0.59706 val_roc= 0.90701 val_ap= 0.91099 time= 0.12547\n",
      "训练次数: 4 Epoch: 0058 log_lik= 0.4646805 train_kl= 0.00859 train_loss= 0.47327 train_acc= 0.59665 val_roc= 0.90750 val_ap= 0.91178 time= 0.11352\n",
      "训练次数: 4 Epoch: 0059 log_lik= 0.46390527 train_kl= 0.00860 train_loss= 0.47250 train_acc= 0.59715 val_roc= 0.90768 val_ap= 0.91210 time= 0.11153\n",
      "训练次数: 4 Epoch: 0060 log_lik= 0.4631359 train_kl= 0.00860 train_loss= 0.47173 train_acc= 0.59673 val_roc= 0.90770 val_ap= 0.91246 time= 0.11816\n",
      "训练次数: 4 Epoch: 0061 log_lik= 0.4623713 train_kl= 0.00860 train_loss= 0.47097 train_acc= 0.59670 val_roc= 0.90792 val_ap= 0.91297 time= 0.13144\n",
      "训练次数: 4 Epoch: 0062 log_lik= 0.46170074 train_kl= 0.00860 train_loss= 0.47030 train_acc= 0.59661 val_roc= 0.90786 val_ap= 0.91316 time= 0.10961\n",
      "训练次数: 4 Epoch: 0063 log_lik= 0.46092895 train_kl= 0.00861 train_loss= 0.46953 train_acc= 0.59693 val_roc= 0.90817 val_ap= 0.91378 time= 0.10655\n",
      "训练次数: 4 Epoch: 0064 log_lik= 0.46018472 train_kl= 0.00861 train_loss= 0.46879 train_acc= 0.59642 val_roc= 0.90867 val_ap= 0.91401 time= 0.13175\n",
      "训练次数: 4 Epoch: 0065 log_lik= 0.45972046 train_kl= 0.00861 train_loss= 0.46833 train_acc= 0.59675 val_roc= 0.90967 val_ap= 0.91452 time= 0.11551\n",
      "训练次数: 4 Epoch: 0066 log_lik= 0.45905152 train_kl= 0.00861 train_loss= 0.46766 train_acc= 0.59685 val_roc= 0.91071 val_ap= 0.91555 time= 0.12347\n",
      "训练次数: 4 Epoch: 0067 log_lik= 0.4585679 train_kl= 0.00861 train_loss= 0.46718 train_acc= 0.59682 val_roc= 0.91180 val_ap= 0.91656 time= 0.11650\n",
      "训练次数: 4 Epoch: 0068 log_lik= 0.4580091 train_kl= 0.00861 train_loss= 0.46662 train_acc= 0.59668 val_roc= 0.91255 val_ap= 0.91731 time= 0.11451\n",
      "训练次数: 4 Epoch: 0069 log_lik= 0.45743173 train_kl= 0.00861 train_loss= 0.46604 train_acc= 0.59645 val_roc= 0.91318 val_ap= 0.91796 time= 0.11750\n",
      "训练次数: 4 Epoch: 0070 log_lik= 0.45704865 train_kl= 0.00861 train_loss= 0.46566 train_acc= 0.59621 val_roc= 0.91352 val_ap= 0.91814 time= 0.11651\n",
      "训练次数: 4 Epoch: 0071 log_lik= 0.45654637 train_kl= 0.00861 train_loss= 0.46515 train_acc= 0.59608 val_roc= 0.91389 val_ap= 0.91836 time= 0.11013\n",
      "训练次数: 4 Epoch: 0072 log_lik= 0.45615867 train_kl= 0.00861 train_loss= 0.46477 train_acc= 0.59635 val_roc= 0.91399 val_ap= 0.91804 time= 0.11650\n",
      "训练次数: 4 Epoch: 0073 log_lik= 0.4557641 train_kl= 0.00861 train_loss= 0.46437 train_acc= 0.59630 val_roc= 0.91412 val_ap= 0.91759 time= 0.10754\n",
      "训练次数: 4 Epoch: 0074 log_lik= 0.45528686 train_kl= 0.00861 train_loss= 0.46390 train_acc= 0.59626 val_roc= 0.91405 val_ap= 0.91710 time= 0.10655\n",
      "训练次数: 4 Epoch: 0075 log_lik= 0.4547869 train_kl= 0.00861 train_loss= 0.46340 train_acc= 0.59594 val_roc= 0.91428 val_ap= 0.91695 time= 0.11053\n",
      "训练次数: 4 Epoch: 0076 log_lik= 0.45449257 train_kl= 0.00861 train_loss= 0.46311 train_acc= 0.59571 val_roc= 0.91435 val_ap= 0.91698 time= 0.11153\n",
      "训练次数: 4 Epoch: 0077 log_lik= 0.4540913 train_kl= 0.00862 train_loss= 0.46271 train_acc= 0.59567 val_roc= 0.91433 val_ap= 0.91684 time= 0.10854\n",
      "训练次数: 4 Epoch: 0078 log_lik= 0.45381665 train_kl= 0.00862 train_loss= 0.46243 train_acc= 0.59476 val_roc= 0.91411 val_ap= 0.91704 time= 0.10754\n",
      "训练次数: 4 Epoch: 0079 log_lik= 0.45345393 train_kl= 0.00862 train_loss= 0.46207 train_acc= 0.59500 val_roc= 0.91392 val_ap= 0.91723 time= 0.10954\n",
      "训练次数: 4 Epoch: 0080 log_lik= 0.45306093 train_kl= 0.00862 train_loss= 0.46168 train_acc= 0.59487 val_roc= 0.91360 val_ap= 0.91711 time= 0.10854\n",
      "训练次数: 4 Epoch: 0081 log_lik= 0.45281598 train_kl= 0.00862 train_loss= 0.46143 train_acc= 0.59475 val_roc= 0.91350 val_ap= 0.91712 time= 0.10854\n",
      "训练次数: 4 Epoch: 0082 log_lik= 0.45249176 train_kl= 0.00862 train_loss= 0.46111 train_acc= 0.59576 val_roc= 0.91352 val_ap= 0.91715 time= 0.11153\n",
      "训练次数: 4 Epoch: 0083 log_lik= 0.45215917 train_kl= 0.00862 train_loss= 0.46078 train_acc= 0.59628 val_roc= 0.91368 val_ap= 0.91706 time= 0.12646\n",
      "训练次数: 4 Epoch: 0084 log_lik= 0.45187557 train_kl= 0.00862 train_loss= 0.46049 train_acc= 0.59560 val_roc= 0.91378 val_ap= 0.91717 time= 0.11551\n",
      "训练次数: 4 Epoch: 0085 log_lik= 0.45155653 train_kl= 0.00862 train_loss= 0.46018 train_acc= 0.59589 val_roc= 0.91422 val_ap= 0.91756 time= 0.10953\n",
      "训练次数: 4 Epoch: 0086 log_lik= 0.45131823 train_kl= 0.00862 train_loss= 0.45994 train_acc= 0.59528 val_roc= 0.91457 val_ap= 0.91823 time= 0.11650\n",
      "训练次数: 4 Epoch: 0087 log_lik= 0.4510761 train_kl= 0.00862 train_loss= 0.45970 train_acc= 0.59559 val_roc= 0.91473 val_ap= 0.91837 time= 0.11650\n",
      "训练次数: 4 Epoch: 0088 log_lik= 0.4508744 train_kl= 0.00862 train_loss= 0.45949 train_acc= 0.59545 val_roc= 0.91511 val_ap= 0.91889 time= 0.11005\n",
      "训练次数: 4 Epoch: 0089 log_lik= 0.45055446 train_kl= 0.00862 train_loss= 0.45918 train_acc= 0.59559 val_roc= 0.91518 val_ap= 0.91924 time= 0.11551\n",
      "训练次数: 4 Epoch: 0090 log_lik= 0.45025045 train_kl= 0.00862 train_loss= 0.45887 train_acc= 0.59533 val_roc= 0.91514 val_ap= 0.91928 time= 0.11053\n",
      "训练次数: 4 Epoch: 0091 log_lik= 0.45003796 train_kl= 0.00862 train_loss= 0.45866 train_acc= 0.59567 val_roc= 0.91514 val_ap= 0.91943 time= 0.11949\n",
      "训练次数: 4 Epoch: 0092 log_lik= 0.44981587 train_kl= 0.00862 train_loss= 0.45844 train_acc= 0.59578 val_roc= 0.91528 val_ap= 0.91949 time= 0.11750\n",
      "训练次数: 4 Epoch: 0093 log_lik= 0.44957232 train_kl= 0.00862 train_loss= 0.45820 train_acc= 0.59559 val_roc= 0.91555 val_ap= 0.91959 time= 0.11750\n",
      "训练次数: 4 Epoch: 0094 log_lik= 0.44934008 train_kl= 0.00862 train_loss= 0.45796 train_acc= 0.59542 val_roc= 0.91553 val_ap= 0.91965 time= 0.13244\n",
      "训练次数: 4 Epoch: 0095 log_lik= 0.44914168 train_kl= 0.00862 train_loss= 0.45777 train_acc= 0.59543 val_roc= 0.91557 val_ap= 0.91959 time= 0.11252\n",
      "训练次数: 4 Epoch: 0096 log_lik= 0.44894436 train_kl= 0.00862 train_loss= 0.45757 train_acc= 0.59504 val_roc= 0.91551 val_ap= 0.91968 time= 0.11153\n",
      "训练次数: 4 Epoch: 0097 log_lik= 0.44877356 train_kl= 0.00862 train_loss= 0.45740 train_acc= 0.59559 val_roc= 0.91535 val_ap= 0.91954 time= 0.10854\n",
      "训练次数: 4 Epoch: 0098 log_lik= 0.44845977 train_kl= 0.00862 train_loss= 0.45708 train_acc= 0.59539 val_roc= 0.91529 val_ap= 0.91950 time= 0.11950\n",
      "训练次数: 4 Epoch: 0099 log_lik= 0.4482577 train_kl= 0.00863 train_loss= 0.45688 train_acc= 0.59509 val_roc= 0.91525 val_ap= 0.91964 time= 0.12756\n",
      "训练次数: 4 Epoch: 0100 log_lik= 0.4480807 train_kl= 0.00863 train_loss= 0.45671 train_acc= 0.59546 val_roc= 0.91535 val_ap= 0.91957 time= 0.11245\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 ROC score: 0.9020772047571554\n",
      "训练次数: 4 AP score: 0.9045396078577442\n",
      "训练次数: 5 Epoch: 0001 log_lik= 0.7847893 train_kl= 0.00805 train_loss= 0.79284 train_acc= 0.11969 val_roc= 0.71682 val_ap= 0.74290 time= 8.00111\n",
      "训练次数: 5 Epoch: 0002 log_lik= 0.7781919 train_kl= 0.00828 train_loss= 0.78647 train_acc= 0.00160 val_roc= 0.82658 val_ap= 0.82176 time= 0.11949\n",
      "训练次数: 5 Epoch: 0003 log_lik= 0.72862315 train_kl= 0.00811 train_loss= 0.73673 train_acc= 0.07164 val_roc= 0.73309 val_ap= 0.75432 time= 0.11551\n",
      "训练次数: 5 Epoch: 0004 log_lik= 0.710653 train_kl= 0.00817 train_loss= 0.71883 train_acc= 0.03314 val_roc= 0.78274 val_ap= 0.79323 time= 0.10877\n",
      "训练次数: 5 Epoch: 0005 log_lik= 0.687375 train_kl= 0.00822 train_loss= 0.69560 train_acc= 0.07292 val_roc= 0.84327 val_ap= 0.83385 time= 0.11352\n",
      "训练次数: 5 Epoch: 0006 log_lik= 0.6468836 train_kl= 0.00824 train_loss= 0.65513 train_acc= 0.20591 val_roc= 0.83915 val_ap= 0.82556 time= 0.12746\n",
      "训练次数: 5 Epoch: 0007 log_lik= 0.62068987 train_kl= 0.00830 train_loss= 0.62899 train_acc= 0.30368 val_roc= 0.84253 val_ap= 0.82998 time= 0.11153\n",
      "训练次数: 5 Epoch: 0008 log_lik= 0.59879124 train_kl= 0.00835 train_loss= 0.60714 train_acc= 0.37561 val_roc= 0.84824 val_ap= 0.83320 time= 0.11053\n",
      "训练次数: 5 Epoch: 0009 log_lik= 0.5819554 train_kl= 0.00841 train_loss= 0.59036 train_acc= 0.43233 val_roc= 0.85470 val_ap= 0.83949 time= 0.11451\n",
      "训练次数: 5 Epoch: 0010 log_lik= 0.56574357 train_kl= 0.00845 train_loss= 0.57419 train_acc= 0.47447 val_roc= 0.86271 val_ap= 0.84926 time= 0.12646\n",
      "训练次数: 5 Epoch: 0011 log_lik= 0.5535965 train_kl= 0.00849 train_loss= 0.56209 train_acc= 0.49046 val_roc= 0.87108 val_ap= 0.86076 time= 0.10959\n",
      "训练次数: 5 Epoch: 0012 log_lik= 0.54392076 train_kl= 0.00851 train_loss= 0.55243 train_acc= 0.49363 val_roc= 0.88184 val_ap= 0.87608 time= 0.11053\n",
      "训练次数: 5 Epoch: 0013 log_lik= 0.52861935 train_kl= 0.00851 train_loss= 0.53713 train_acc= 0.50238 val_roc= 0.88879 val_ap= 0.88580 time= 0.10959\n",
      "训练次数: 5 Epoch: 0014 log_lik= 0.51863056 train_kl= 0.00850 train_loss= 0.52714 train_acc= 0.50872 val_roc= 0.88929 val_ap= 0.88671 time= 0.11651\n",
      "训练次数: 5 Epoch: 0015 log_lik= 0.51613766 train_kl= 0.00850 train_loss= 0.52464 train_acc= 0.51020 val_roc= 0.88992 val_ap= 0.88863 time= 0.10655\n",
      "训练次数: 5 Epoch: 0016 log_lik= 0.51231045 train_kl= 0.00850 train_loss= 0.52081 train_acc= 0.51272 val_roc= 0.89186 val_ap= 0.89062 time= 0.10757\n",
      "训练次数: 5 Epoch: 0017 log_lik= 0.5082148 train_kl= 0.00850 train_loss= 0.51671 train_acc= 0.51441 val_roc= 0.89433 val_ap= 0.89159 time= 0.11352\n",
      "训练次数: 5 Epoch: 0018 log_lik= 0.5061713 train_kl= 0.00850 train_loss= 0.51467 train_acc= 0.51441 val_roc= 0.89832 val_ap= 0.89546 time= 0.11352\n",
      "训练次数: 5 Epoch: 0019 log_lik= 0.50199723 train_kl= 0.00851 train_loss= 0.51051 train_acc= 0.51575 val_roc= 0.90436 val_ap= 0.90228 time= 0.10857\n",
      "训练次数: 5 Epoch: 0020 log_lik= 0.49595162 train_kl= 0.00852 train_loss= 0.50447 train_acc= 0.51703 val_roc= 0.91087 val_ap= 0.90972 time= 0.10854\n",
      "训练次数: 5 Epoch: 0021 log_lik= 0.49109486 train_kl= 0.00853 train_loss= 0.49963 train_acc= 0.51807 val_roc= 0.91525 val_ap= 0.91481 time= 0.10854\n",
      "训练次数: 5 Epoch: 0022 log_lik= 0.48763946 train_kl= 0.00856 train_loss= 0.49620 train_acc= 0.51877 val_roc= 0.91684 val_ap= 0.91637 time= 0.11252\n",
      "训练次数: 5 Epoch: 0023 log_lik= 0.48327005 train_kl= 0.00858 train_loss= 0.49185 train_acc= 0.52228 val_roc= 0.91738 val_ap= 0.91658 time= 0.11153\n",
      "训练次数: 5 Epoch: 0024 log_lik= 0.47962067 train_kl= 0.00860 train_loss= 0.48822 train_acc= 0.52581 val_roc= 0.91782 val_ap= 0.91726 time= 0.11053\n",
      "训练次数: 5 Epoch: 0025 log_lik= 0.47728175 train_kl= 0.00862 train_loss= 0.48591 train_acc= 0.52720 val_roc= 0.92008 val_ap= 0.92078 time= 0.10954\n",
      "训练次数: 5 Epoch: 0026 log_lik= 0.47526816 train_kl= 0.00864 train_loss= 0.48391 train_acc= 0.52723 val_roc= 0.92274 val_ap= 0.92438 time= 0.10953\n",
      "训练次数: 5 Epoch: 0027 log_lik= 0.47305107 train_kl= 0.00865 train_loss= 0.48170 train_acc= 0.52675 val_roc= 0.92543 val_ap= 0.92760 time= 0.11750\n",
      "训练次数: 5 Epoch: 0028 log_lik= 0.47117975 train_kl= 0.00865 train_loss= 0.47983 train_acc= 0.52694 val_roc= 0.92667 val_ap= 0.92931 time= 0.10957\n",
      "训练次数: 5 Epoch: 0029 log_lik= 0.46929163 train_kl= 0.00864 train_loss= 0.47793 train_acc= 0.53014 val_roc= 0.92640 val_ap= 0.92894 time= 0.11252\n",
      "训练次数: 5 Epoch: 0030 log_lik= 0.466825 train_kl= 0.00863 train_loss= 0.47546 train_acc= 0.53461 val_roc= 0.92664 val_ap= 0.92962 time= 0.10854\n",
      "训练次数: 5 Epoch: 0031 log_lik= 0.4646814 train_kl= 0.00862 train_loss= 0.47330 train_acc= 0.53789 val_roc= 0.92685 val_ap= 0.93054 time= 0.11053\n",
      "训练次数: 5 Epoch: 0032 log_lik= 0.46274605 train_kl= 0.00861 train_loss= 0.47136 train_acc= 0.53988 val_roc= 0.92692 val_ap= 0.93083 time= 0.11551\n",
      "训练次数: 5 Epoch: 0033 log_lik= 0.46130943 train_kl= 0.00861 train_loss= 0.46992 train_acc= 0.54193 val_roc= 0.92742 val_ap= 0.93116 time= 0.11352\n",
      "训练次数: 5 Epoch: 0034 log_lik= 0.45972592 train_kl= 0.00861 train_loss= 0.46833 train_acc= 0.54409 val_roc= 0.92767 val_ap= 0.93169 time= 0.10953\n",
      "训练次数: 5 Epoch: 0035 log_lik= 0.4580642 train_kl= 0.00861 train_loss= 0.46668 train_acc= 0.54593 val_roc= 0.92753 val_ap= 0.93199 time= 0.11352\n",
      "训练次数: 5 Epoch: 0036 log_lik= 0.4566745 train_kl= 0.00862 train_loss= 0.46530 train_acc= 0.54792 val_roc= 0.92774 val_ap= 0.93278 time= 0.11053\n",
      "训练次数: 5 Epoch: 0037 log_lik= 0.4553688 train_kl= 0.00863 train_loss= 0.46400 train_acc= 0.54911 val_roc= 0.92786 val_ap= 0.93331 time= 0.11252\n",
      "训练次数: 5 Epoch: 0038 log_lik= 0.4539179 train_kl= 0.00864 train_loss= 0.46256 train_acc= 0.55080 val_roc= 0.92818 val_ap= 0.93396 time= 0.10854\n",
      "训练次数: 5 Epoch: 0039 log_lik= 0.45267954 train_kl= 0.00865 train_loss= 0.46133 train_acc= 0.55227 val_roc= 0.92796 val_ap= 0.93416 time= 0.11252\n",
      "训练次数: 5 Epoch: 0040 log_lik= 0.45163518 train_kl= 0.00866 train_loss= 0.46029 train_acc= 0.55374 val_roc= 0.92760 val_ap= 0.93444 time= 0.10854\n",
      "训练次数: 5 Epoch: 0041 log_lik= 0.4503132 train_kl= 0.00866 train_loss= 0.45898 train_acc= 0.55545 val_roc= 0.92689 val_ap= 0.93444 time= 0.11611\n",
      "训练次数: 5 Epoch: 0042 log_lik= 0.44908762 train_kl= 0.00867 train_loss= 0.45775 train_acc= 0.55659 val_roc= 0.92651 val_ap= 0.93497 time= 0.10954\n",
      "训练次数: 5 Epoch: 0043 log_lik= 0.4481865 train_kl= 0.00866 train_loss= 0.45685 train_acc= 0.55693 val_roc= 0.92627 val_ap= 0.93542 time= 0.10862\n",
      "训练次数: 5 Epoch: 0044 log_lik= 0.44722417 train_kl= 0.00866 train_loss= 0.45589 train_acc= 0.55769 val_roc= 0.92566 val_ap= 0.93540 time= 0.10954\n",
      "训练次数: 5 Epoch: 0045 log_lik= 0.44618666 train_kl= 0.00866 train_loss= 0.45485 train_acc= 0.55808 val_roc= 0.92556 val_ap= 0.93568 time= 0.11651\n",
      "训练次数: 5 Epoch: 0046 log_lik= 0.44530782 train_kl= 0.00866 train_loss= 0.45396 train_acc= 0.55769 val_roc= 0.92550 val_ap= 0.93606 time= 0.10854\n",
      "训练次数: 5 Epoch: 0047 log_lik= 0.4444374 train_kl= 0.00865 train_loss= 0.45309 train_acc= 0.55824 val_roc= 0.92572 val_ap= 0.93691 time= 0.10954\n",
      "训练次数: 5 Epoch: 0048 log_lik= 0.44358328 train_kl= 0.00866 train_loss= 0.45224 train_acc= 0.55902 val_roc= 0.92608 val_ap= 0.93752 time= 0.11352\n",
      "训练次数: 5 Epoch: 0049 log_lik= 0.44278964 train_kl= 0.00866 train_loss= 0.45145 train_acc= 0.55954 val_roc= 0.92604 val_ap= 0.93765 time= 0.10754\n",
      "训练次数: 5 Epoch: 0050 log_lik= 0.4417464 train_kl= 0.00866 train_loss= 0.45041 train_acc= 0.56002 val_roc= 0.92648 val_ap= 0.93826 time= 0.11053\n",
      "训练次数: 5 Epoch: 0051 log_lik= 0.44093904 train_kl= 0.00867 train_loss= 0.44961 train_acc= 0.56004 val_roc= 0.92708 val_ap= 0.93909 time= 0.11153\n",
      "训练次数: 5 Epoch: 0052 log_lik= 0.43997532 train_kl= 0.00867 train_loss= 0.44865 train_acc= 0.56098 val_roc= 0.92767 val_ap= 0.93976 time= 0.10953\n",
      "训练次数: 5 Epoch: 0053 log_lik= 0.439134 train_kl= 0.00867 train_loss= 0.44781 train_acc= 0.56155 val_roc= 0.92784 val_ap= 0.94005 time= 0.11004\n",
      "训练次数: 5 Epoch: 0054 log_lik= 0.43849984 train_kl= 0.00868 train_loss= 0.44718 train_acc= 0.56179 val_roc= 0.92766 val_ap= 0.93991 time= 0.10655\n",
      "训练次数: 5 Epoch: 0055 log_lik= 0.43786475 train_kl= 0.00868 train_loss= 0.44655 train_acc= 0.56210 val_roc= 0.92768 val_ap= 0.94011 time= 0.11061\n",
      "训练次数: 5 Epoch: 0056 log_lik= 0.43722877 train_kl= 0.00868 train_loss= 0.44591 train_acc= 0.56268 val_roc= 0.92807 val_ap= 0.94062 time= 0.10960\n",
      "训练次数: 5 Epoch: 0057 log_lik= 0.43641412 train_kl= 0.00868 train_loss= 0.44510 train_acc= 0.56338 val_roc= 0.92862 val_ap= 0.94121 time= 0.10953\n",
      "训练次数: 5 Epoch: 0058 log_lik= 0.43570155 train_kl= 0.00868 train_loss= 0.44439 train_acc= 0.56370 val_roc= 0.92858 val_ap= 0.94108 time= 0.10854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0059 log_lik= 0.43495104 train_kl= 0.00868 train_loss= 0.44364 train_acc= 0.56453 val_roc= 0.92857 val_ap= 0.94116 time= 0.11352\n",
      "训练次数: 5 Epoch: 0060 log_lik= 0.43423873 train_kl= 0.00868 train_loss= 0.44292 train_acc= 0.56456 val_roc= 0.92841 val_ap= 0.94100 time= 0.10854\n",
      "训练次数: 5 Epoch: 0061 log_lik= 0.43355888 train_kl= 0.00869 train_loss= 0.44224 train_acc= 0.56480 val_roc= 0.92891 val_ap= 0.94132 time= 0.10757\n",
      "训练次数: 5 Epoch: 0062 log_lik= 0.4329126 train_kl= 0.00869 train_loss= 0.44160 train_acc= 0.56541 val_roc= 0.92893 val_ap= 0.94122 time= 0.11053\n",
      "训练次数: 5 Epoch: 0063 log_lik= 0.43230858 train_kl= 0.00869 train_loss= 0.44100 train_acc= 0.56564 val_roc= 0.92897 val_ap= 0.94107 time= 0.11750\n",
      "训练次数: 5 Epoch: 0064 log_lik= 0.4316783 train_kl= 0.00869 train_loss= 0.44037 train_acc= 0.56638 val_roc= 0.92867 val_ap= 0.94079 time= 0.11452\n",
      "训练次数: 5 Epoch: 0065 log_lik= 0.43099314 train_kl= 0.00870 train_loss= 0.43969 train_acc= 0.56670 val_roc= 0.92891 val_ap= 0.94107 time= 0.11056\n",
      "训练次数: 5 Epoch: 0066 log_lik= 0.4304335 train_kl= 0.00870 train_loss= 0.43914 train_acc= 0.56692 val_roc= 0.92925 val_ap= 0.94131 time= 0.11551\n",
      "训练次数: 5 Epoch: 0067 log_lik= 0.4297169 train_kl= 0.00871 train_loss= 0.43842 train_acc= 0.56740 val_roc= 0.92948 val_ap= 0.94157 time= 0.10854\n",
      "训练次数: 5 Epoch: 0068 log_lik= 0.42922273 train_kl= 0.00871 train_loss= 0.43793 train_acc= 0.56785 val_roc= 0.92936 val_ap= 0.94147 time= 0.11252\n",
      "训练次数: 5 Epoch: 0069 log_lik= 0.42848375 train_kl= 0.00871 train_loss= 0.43720 train_acc= 0.56854 val_roc= 0.92938 val_ap= 0.94168 time= 0.11152\n",
      "训练次数: 5 Epoch: 0070 log_lik= 0.4279312 train_kl= 0.00872 train_loss= 0.43665 train_acc= 0.56903 val_roc= 0.92980 val_ap= 0.94221 time= 0.10854\n",
      "训练次数: 5 Epoch: 0071 log_lik= 0.42745623 train_kl= 0.00872 train_loss= 0.43617 train_acc= 0.56931 val_roc= 0.93006 val_ap= 0.94242 time= 0.11152\n",
      "训练次数: 5 Epoch: 0072 log_lik= 0.42685327 train_kl= 0.00872 train_loss= 0.43557 train_acc= 0.57000 val_roc= 0.92982 val_ap= 0.94226 time= 0.11053\n",
      "训练次数: 5 Epoch: 0073 log_lik= 0.42635962 train_kl= 0.00872 train_loss= 0.43508 train_acc= 0.57010 val_roc= 0.92984 val_ap= 0.94224 time= 0.10557\n",
      "训练次数: 5 Epoch: 0074 log_lik= 0.4258548 train_kl= 0.00872 train_loss= 0.43457 train_acc= 0.57072 val_roc= 0.92972 val_ap= 0.94197 time= 0.11153\n",
      "训练次数: 5 Epoch: 0075 log_lik= 0.42532396 train_kl= 0.00872 train_loss= 0.43404 train_acc= 0.57106 val_roc= 0.92978 val_ap= 0.94194 time= 0.10655\n",
      "训练次数: 5 Epoch: 0076 log_lik= 0.4248542 train_kl= 0.00872 train_loss= 0.43358 train_acc= 0.57121 val_roc= 0.92998 val_ap= 0.94202 time= 0.11850\n",
      "训练次数: 5 Epoch: 0077 log_lik= 0.42451784 train_kl= 0.00872 train_loss= 0.43324 train_acc= 0.57166 val_roc= 0.93006 val_ap= 0.94221 time= 0.11354\n",
      "训练次数: 5 Epoch: 0078 log_lik= 0.42408782 train_kl= 0.00873 train_loss= 0.43281 train_acc= 0.57185 val_roc= 0.93020 val_ap= 0.94242 time= 0.10953\n",
      "训练次数: 5 Epoch: 0079 log_lik= 0.42366675 train_kl= 0.00873 train_loss= 0.43240 train_acc= 0.57210 val_roc= 0.93030 val_ap= 0.94253 time= 0.10854\n",
      "训练次数: 5 Epoch: 0080 log_lik= 0.4232065 train_kl= 0.00873 train_loss= 0.43194 train_acc= 0.57188 val_roc= 0.93040 val_ap= 0.94271 time= 0.10555\n",
      "训练次数: 5 Epoch: 0081 log_lik= 0.42289406 train_kl= 0.00873 train_loss= 0.43163 train_acc= 0.57212 val_roc= 0.93049 val_ap= 0.94272 time= 0.11257\n",
      "训练次数: 5 Epoch: 0082 log_lik= 0.42254403 train_kl= 0.00874 train_loss= 0.43128 train_acc= 0.57222 val_roc= 0.93020 val_ap= 0.94251 time= 0.12450\n",
      "训练次数: 5 Epoch: 0083 log_lik= 0.42224956 train_kl= 0.00874 train_loss= 0.43099 train_acc= 0.57191 val_roc= 0.93010 val_ap= 0.94243 time= 0.10953\n",
      "训练次数: 5 Epoch: 0084 log_lik= 0.42198482 train_kl= 0.00874 train_loss= 0.43073 train_acc= 0.57235 val_roc= 0.93003 val_ap= 0.94254 time= 0.10854\n",
      "训练次数: 5 Epoch: 0085 log_lik= 0.42165053 train_kl= 0.00874 train_loss= 0.43039 train_acc= 0.57204 val_roc= 0.93006 val_ap= 0.94263 time= 0.11252\n",
      "训练次数: 5 Epoch: 0086 log_lik= 0.42136225 train_kl= 0.00874 train_loss= 0.43010 train_acc= 0.57144 val_roc= 0.93001 val_ap= 0.94284 time= 0.11650\n",
      "训练次数: 5 Epoch: 0087 log_lik= 0.42116487 train_kl= 0.00874 train_loss= 0.42991 train_acc= 0.57178 val_roc= 0.92993 val_ap= 0.94267 time= 0.12447\n",
      "训练次数: 5 Epoch: 0088 log_lik= 0.42085525 train_kl= 0.00874 train_loss= 0.42960 train_acc= 0.57173 val_roc= 0.93003 val_ap= 0.94274 time= 0.11053\n",
      "训练次数: 5 Epoch: 0089 log_lik= 0.42057994 train_kl= 0.00874 train_loss= 0.42932 train_acc= 0.57166 val_roc= 0.92971 val_ap= 0.94261 time= 0.11355\n",
      "训练次数: 5 Epoch: 0090 log_lik= 0.4203627 train_kl= 0.00874 train_loss= 0.42910 train_acc= 0.57162 val_roc= 0.92961 val_ap= 0.94272 time= 0.11750\n",
      "训练次数: 5 Epoch: 0091 log_lik= 0.4201393 train_kl= 0.00874 train_loss= 0.42888 train_acc= 0.57146 val_roc= 0.92943 val_ap= 0.94267 time= 0.10606\n",
      "训练次数: 5 Epoch: 0092 log_lik= 0.41989765 train_kl= 0.00874 train_loss= 0.42864 train_acc= 0.57100 val_roc= 0.92926 val_ap= 0.94250 time= 0.11252\n",
      "训练次数: 5 Epoch: 0093 log_lik= 0.4196772 train_kl= 0.00874 train_loss= 0.42842 train_acc= 0.57115 val_roc= 0.92901 val_ap= 0.94238 time= 0.10854\n",
      "训练次数: 5 Epoch: 0094 log_lik= 0.4194864 train_kl= 0.00874 train_loss= 0.42823 train_acc= 0.57099 val_roc= 0.92890 val_ap= 0.94221 time= 0.10854\n",
      "训练次数: 5 Epoch: 0095 log_lik= 0.41924453 train_kl= 0.00874 train_loss= 0.42799 train_acc= 0.57112 val_roc= 0.92844 val_ap= 0.94182 time= 0.11053\n",
      "训练次数: 5 Epoch: 0096 log_lik= 0.41902265 train_kl= 0.00874 train_loss= 0.42777 train_acc= 0.57074 val_roc= 0.92777 val_ap= 0.94130 time= 0.10655\n",
      "训练次数: 5 Epoch: 0097 log_lik= 0.41892496 train_kl= 0.00875 train_loss= 0.42767 train_acc= 0.57069 val_roc= 0.92757 val_ap= 0.94110 time= 0.11551\n",
      "训练次数: 5 Epoch: 0098 log_lik= 0.41866702 train_kl= 0.00875 train_loss= 0.42741 train_acc= 0.57074 val_roc= 0.92738 val_ap= 0.94100 time= 0.11750\n",
      "训练次数: 5 Epoch: 0099 log_lik= 0.41842887 train_kl= 0.00875 train_loss= 0.42718 train_acc= 0.57064 val_roc= 0.92700 val_ap= 0.94075 time= 0.10854\n",
      "训练次数: 5 Epoch: 0100 log_lik= 0.4183106 train_kl= 0.00875 train_loss= 0.42706 train_acc= 0.57023 val_roc= 0.92650 val_ap= 0.94028 time= 0.10805\n",
      "Optimization Finished!\n",
      "训练次数: 5 ROC score: 0.9416517540480109\n",
      "训练次数: 5 AP score: 0.9501346736852996\n",
      "训练次数: 6 Epoch: 0001 log_lik= 0.7853857 train_kl= 0.00805 train_loss= 0.79344 train_acc= 0.06509 val_roc= 0.65756 val_ap= 0.69492 time= 8.34451\n",
      "训练次数: 6 Epoch: 0002 log_lik= 0.86241734 train_kl= 0.00836 train_loss= 0.87078 train_acc= 0.00160 val_roc= 0.70923 val_ap= 0.73177 time= 0.11949\n",
      "训练次数: 6 Epoch: 0003 log_lik= 0.7490735 train_kl= 0.00811 train_loss= 0.75718 train_acc= 0.00388 val_roc= 0.63228 val_ap= 0.68776 time= 0.11352\n",
      "训练次数: 6 Epoch: 0004 log_lik= 0.74695164 train_kl= 0.00816 train_loss= 0.75512 train_acc= 0.00293 val_roc= 0.63257 val_ap= 0.69027 time= 0.11949\n",
      "训练次数: 6 Epoch: 0005 log_lik= 0.75371224 train_kl= 0.00820 train_loss= 0.76191 train_acc= 0.00314 val_roc= 0.64234 val_ap= 0.69928 time= 0.11551\n",
      "训练次数: 6 Epoch: 0006 log_lik= 0.7417435 train_kl= 0.00815 train_loss= 0.74989 train_acc= 0.00420 val_roc= 0.66147 val_ap= 0.71552 time= 0.10854\n",
      "训练次数: 6 Epoch: 0007 log_lik= 0.73988605 train_kl= 0.00812 train_loss= 0.74801 train_acc= 0.00645 val_roc= 0.68058 val_ap= 0.73218 time= 0.10953\n",
      "训练次数: 6 Epoch: 0008 log_lik= 0.7348829 train_kl= 0.00812 train_loss= 0.74300 train_acc= 0.01230 val_roc= 0.69184 val_ap= 0.74048 time= 0.11969\n",
      "训练次数: 6 Epoch: 0009 log_lik= 0.72574335 train_kl= 0.00813 train_loss= 0.73388 train_acc= 0.01179 val_roc= 0.69492 val_ap= 0.73889 time= 0.11152\n",
      "训练次数: 6 Epoch: 0010 log_lik= 0.71805906 train_kl= 0.00817 train_loss= 0.72623 train_acc= 0.01191 val_roc= 0.70173 val_ap= 0.74094 time= 0.11252\n",
      "训练次数: 6 Epoch: 0011 log_lik= 0.71499765 train_kl= 0.00820 train_loss= 0.72319 train_acc= 0.01253 val_roc= 0.72186 val_ap= 0.75528 time= 0.11451\n",
      "训练次数: 6 Epoch: 0012 log_lik= 0.70663214 train_kl= 0.00820 train_loss= 0.71483 train_acc= 0.02087 val_roc= 0.74973 val_ap= 0.77585 time= 0.11152\n",
      "训练次数: 6 Epoch: 0013 log_lik= 0.695832 train_kl= 0.00819 train_loss= 0.70402 train_acc= 0.03859 val_roc= 0.77552 val_ap= 0.79534 time= 0.11053\n",
      "训练次数: 6 Epoch: 0014 log_lik= 0.6867772 train_kl= 0.00818 train_loss= 0.69496 train_acc= 0.06581 val_roc= 0.79729 val_ap= 0.81046 time= 0.11153\n",
      "训练次数: 6 Epoch: 0015 log_lik= 0.677337 train_kl= 0.00818 train_loss= 0.68552 train_acc= 0.09832 val_roc= 0.81255 val_ap= 0.81930 time= 0.10953\n",
      "训练次数: 6 Epoch: 0016 log_lik= 0.66630965 train_kl= 0.00820 train_loss= 0.67451 train_acc= 0.12794 val_roc= 0.82319 val_ap= 0.82390 time= 0.10752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0017 log_lik= 0.6531914 train_kl= 0.00822 train_loss= 0.66141 train_acc= 0.15708 val_roc= 0.83176 val_ap= 0.82771 time= 0.11252\n",
      "训练次数: 6 Epoch: 0018 log_lik= 0.6406309 train_kl= 0.00825 train_loss= 0.64888 train_acc= 0.18460 val_roc= 0.83973 val_ap= 0.83176 time= 0.11252\n",
      "训练次数: 6 Epoch: 0019 log_lik= 0.62776387 train_kl= 0.00829 train_loss= 0.63605 train_acc= 0.22092 val_roc= 0.84597 val_ap= 0.83564 time= 0.11152\n",
      "训练次数: 6 Epoch: 0020 log_lik= 0.6151668 train_kl= 0.00832 train_loss= 0.62348 train_acc= 0.26202 val_roc= 0.85116 val_ap= 0.83813 time= 0.11055\n",
      "训练次数: 6 Epoch: 0021 log_lik= 0.6021765 train_kl= 0.00834 train_loss= 0.61052 train_acc= 0.30745 val_roc= 0.85551 val_ap= 0.84110 time= 0.11153\n",
      "训练次数: 6 Epoch: 0022 log_lik= 0.5894007 train_kl= 0.00837 train_loss= 0.59777 train_acc= 0.35450 val_roc= 0.85800 val_ap= 0.84228 time= 0.11303\n",
      "训练次数: 6 Epoch: 0023 log_lik= 0.57755023 train_kl= 0.00839 train_loss= 0.58594 train_acc= 0.39592 val_roc= 0.86026 val_ap= 0.84496 time= 0.11153\n",
      "训练次数: 6 Epoch: 0024 log_lik= 0.56671953 train_kl= 0.00841 train_loss= 0.57513 train_acc= 0.43113 val_roc= 0.86297 val_ap= 0.84953 time= 0.10954\n",
      "训练次数: 6 Epoch: 0025 log_lik= 0.5570443 train_kl= 0.00843 train_loss= 0.56547 train_acc= 0.45720 val_roc= 0.86637 val_ap= 0.85279 time= 0.11153\n",
      "训练次数: 6 Epoch: 0026 log_lik= 0.5479158 train_kl= 0.00845 train_loss= 0.55636 train_acc= 0.48035 val_roc= 0.86991 val_ap= 0.85646 time= 0.10953\n",
      "训练次数: 6 Epoch: 0027 log_lik= 0.5392457 train_kl= 0.00847 train_loss= 0.54771 train_acc= 0.49850 val_roc= 0.87327 val_ap= 0.86061 time= 0.10754\n",
      "训练次数: 6 Epoch: 0028 log_lik= 0.53186893 train_kl= 0.00849 train_loss= 0.54036 train_acc= 0.51270 val_roc= 0.87519 val_ap= 0.86289 time= 0.10854\n",
      "训练次数: 6 Epoch: 0029 log_lik= 0.5259771 train_kl= 0.00851 train_loss= 0.53448 train_acc= 0.52184 val_roc= 0.87664 val_ap= 0.86427 time= 0.10754\n",
      "训练次数: 6 Epoch: 0030 log_lik= 0.52172035 train_kl= 0.00852 train_loss= 0.53024 train_acc= 0.52785 val_roc= 0.87656 val_ap= 0.86371 time= 0.11004\n",
      "训练次数: 6 Epoch: 0031 log_lik= 0.5188549 train_kl= 0.00854 train_loss= 0.52739 train_acc= 0.53195 val_roc= 0.87617 val_ap= 0.86285 time= 0.11053\n",
      "训练次数: 6 Epoch: 0032 log_lik= 0.5167933 train_kl= 0.00855 train_loss= 0.52534 train_acc= 0.53449 val_roc= 0.87656 val_ap= 0.86382 time= 0.11153\n",
      "训练次数: 6 Epoch: 0033 log_lik= 0.5148853 train_kl= 0.00855 train_loss= 0.52343 train_acc= 0.53628 val_roc= 0.87698 val_ap= 0.86520 time= 0.11153\n",
      "训练次数: 6 Epoch: 0034 log_lik= 0.51269233 train_kl= 0.00855 train_loss= 0.52124 train_acc= 0.53780 val_roc= 0.87795 val_ap= 0.86802 time= 0.10854\n",
      "训练次数: 6 Epoch: 0035 log_lik= 0.51068115 train_kl= 0.00854 train_loss= 0.51922 train_acc= 0.53849 val_roc= 0.87901 val_ap= 0.87016 time= 0.10854\n",
      "训练次数: 6 Epoch: 0036 log_lik= 0.50876683 train_kl= 0.00853 train_loss= 0.51730 train_acc= 0.53946 val_roc= 0.87989 val_ap= 0.87219 time= 0.11252\n",
      "训练次数: 6 Epoch: 0037 log_lik= 0.5067185 train_kl= 0.00853 train_loss= 0.51524 train_acc= 0.53901 val_roc= 0.88078 val_ap= 0.87438 time= 0.11849\n",
      "训练次数: 6 Epoch: 0038 log_lik= 0.5043116 train_kl= 0.00852 train_loss= 0.51283 train_acc= 0.53930 val_roc= 0.88158 val_ap= 0.87606 time= 0.11053\n",
      "训练次数: 6 Epoch: 0039 log_lik= 0.501906 train_kl= 0.00852 train_loss= 0.51042 train_acc= 0.53970 val_roc= 0.88259 val_ap= 0.87788 time= 0.11252\n",
      "训练次数: 6 Epoch: 0040 log_lik= 0.4990525 train_kl= 0.00852 train_loss= 0.50757 train_acc= 0.54105 val_roc= 0.88372 val_ap= 0.88004 time= 0.10954\n",
      "训练次数: 6 Epoch: 0041 log_lik= 0.49624768 train_kl= 0.00853 train_loss= 0.50477 train_acc= 0.54204 val_roc= 0.88466 val_ap= 0.88182 time= 0.11919\n",
      "训练次数: 6 Epoch: 0042 log_lik= 0.49306658 train_kl= 0.00854 train_loss= 0.50160 train_acc= 0.54220 val_roc= 0.88645 val_ap= 0.88484 time= 0.10953\n",
      "训练次数: 6 Epoch: 0043 log_lik= 0.49000496 train_kl= 0.00855 train_loss= 0.49855 train_acc= 0.54294 val_roc= 0.88833 val_ap= 0.88760 time= 0.12348\n",
      "训练次数: 6 Epoch: 0044 log_lik= 0.4875811 train_kl= 0.00856 train_loss= 0.49614 train_acc= 0.54266 val_roc= 0.88982 val_ap= 0.88956 time= 0.11053\n",
      "训练次数: 6 Epoch: 0045 log_lik= 0.4852007 train_kl= 0.00857 train_loss= 0.49377 train_acc= 0.54236 val_roc= 0.89164 val_ap= 0.89192 time= 0.11053\n",
      "训练次数: 6 Epoch: 0046 log_lik= 0.48343945 train_kl= 0.00858 train_loss= 0.49202 train_acc= 0.54126 val_roc= 0.89336 val_ap= 0.89394 time= 0.11750\n",
      "训练次数: 6 Epoch: 0047 log_lik= 0.48172128 train_kl= 0.00858 train_loss= 0.49030 train_acc= 0.54077 val_roc= 0.89500 val_ap= 0.89616 time= 0.11053\n",
      "训练次数: 6 Epoch: 0048 log_lik= 0.48027053 train_kl= 0.00858 train_loss= 0.48886 train_acc= 0.54023 val_roc= 0.89625 val_ap= 0.89822 time= 0.10953\n",
      "训练次数: 6 Epoch: 0049 log_lik= 0.47886443 train_kl= 0.00858 train_loss= 0.48745 train_acc= 0.54029 val_roc= 0.89748 val_ap= 0.90006 time= 0.10953\n",
      "训练次数: 6 Epoch: 0050 log_lik= 0.47734275 train_kl= 0.00858 train_loss= 0.48592 train_acc= 0.54008 val_roc= 0.89871 val_ap= 0.90209 time= 0.10854\n",
      "训练次数: 6 Epoch: 0051 log_lik= 0.47628456 train_kl= 0.00857 train_loss= 0.48486 train_acc= 0.54010 val_roc= 0.89983 val_ap= 0.90382 time= 0.12348\n",
      "训练次数: 6 Epoch: 0052 log_lik= 0.47506842 train_kl= 0.00857 train_loss= 0.48364 train_acc= 0.54033 val_roc= 0.90091 val_ap= 0.90544 time= 0.10754\n",
      "训练次数: 6 Epoch: 0053 log_lik= 0.47421113 train_kl= 0.00857 train_loss= 0.48278 train_acc= 0.53994 val_roc= 0.90214 val_ap= 0.90689 time= 0.11650\n",
      "训练次数: 6 Epoch: 0054 log_lik= 0.4733152 train_kl= 0.00857 train_loss= 0.48188 train_acc= 0.54068 val_roc= 0.90303 val_ap= 0.90773 time= 0.10954\n",
      "训练次数: 6 Epoch: 0055 log_lik= 0.4721395 train_kl= 0.00857 train_loss= 0.48071 train_acc= 0.54099 val_roc= 0.90366 val_ap= 0.90825 time= 0.11252\n",
      "训练次数: 6 Epoch: 0056 log_lik= 0.4710082 train_kl= 0.00857 train_loss= 0.47958 train_acc= 0.54195 val_roc= 0.90454 val_ap= 0.90891 time= 0.11252\n",
      "训练次数: 6 Epoch: 0057 log_lik= 0.46998876 train_kl= 0.00858 train_loss= 0.47857 train_acc= 0.54257 val_roc= 0.90545 val_ap= 0.90956 time= 0.11153\n",
      "训练次数: 6 Epoch: 0058 log_lik= 0.46900553 train_kl= 0.00859 train_loss= 0.47759 train_acc= 0.54275 val_roc= 0.90649 val_ap= 0.91051 time= 0.10953\n",
      "训练次数: 6 Epoch: 0059 log_lik= 0.4682863 train_kl= 0.00859 train_loss= 0.47688 train_acc= 0.54319 val_roc= 0.90692 val_ap= 0.91081 time= 0.11852\n",
      "训练次数: 6 Epoch: 0060 log_lik= 0.467294 train_kl= 0.00860 train_loss= 0.47589 train_acc= 0.54407 val_roc= 0.90723 val_ap= 0.91115 time= 0.11252\n",
      "训练次数: 6 Epoch: 0061 log_lik= 0.46648422 train_kl= 0.00860 train_loss= 0.47508 train_acc= 0.54460 val_roc= 0.90720 val_ap= 0.91066 time= 0.11153\n",
      "训练次数: 6 Epoch: 0062 log_lik= 0.4655649 train_kl= 0.00860 train_loss= 0.47416 train_acc= 0.54496 val_roc= 0.90750 val_ap= 0.91090 time= 0.10953\n",
      "训练次数: 6 Epoch: 0063 log_lik= 0.46490532 train_kl= 0.00860 train_loss= 0.47350 train_acc= 0.54533 val_roc= 0.90721 val_ap= 0.91079 time= 0.10754\n",
      "训练次数: 6 Epoch: 0064 log_lik= 0.4640474 train_kl= 0.00860 train_loss= 0.47264 train_acc= 0.54662 val_roc= 0.90744 val_ap= 0.91122 time= 0.10854\n",
      "训练次数: 6 Epoch: 0065 log_lik= 0.46326274 train_kl= 0.00859 train_loss= 0.47186 train_acc= 0.54713 val_roc= 0.90701 val_ap= 0.91068 time= 0.11451\n",
      "训练次数: 6 Epoch: 0066 log_lik= 0.46255153 train_kl= 0.00859 train_loss= 0.47114 train_acc= 0.54731 val_roc= 0.90698 val_ap= 0.91050 time= 0.10953\n",
      "训练次数: 6 Epoch: 0067 log_lik= 0.4619166 train_kl= 0.00859 train_loss= 0.47051 train_acc= 0.54725 val_roc= 0.90714 val_ap= 0.91029 time= 0.10954\n",
      "训练次数: 6 Epoch: 0068 log_lik= 0.4612288 train_kl= 0.00859 train_loss= 0.46982 train_acc= 0.54795 val_roc= 0.90733 val_ap= 0.91027 time= 0.10953\n",
      "训练次数: 6 Epoch: 0069 log_lik= 0.46060178 train_kl= 0.00860 train_loss= 0.46920 train_acc= 0.54808 val_roc= 0.90727 val_ap= 0.90974 time= 0.10953\n",
      "训练次数: 6 Epoch: 0070 log_lik= 0.45996866 train_kl= 0.00860 train_loss= 0.46857 train_acc= 0.54832 val_roc= 0.90755 val_ap= 0.91008 time= 0.11053\n",
      "训练次数: 6 Epoch: 0071 log_lik= 0.45942187 train_kl= 0.00860 train_loss= 0.46803 train_acc= 0.54880 val_roc= 0.90762 val_ap= 0.91003 time= 0.10953\n",
      "训练次数: 6 Epoch: 0072 log_lik= 0.45886764 train_kl= 0.00861 train_loss= 0.46748 train_acc= 0.54901 val_roc= 0.90779 val_ap= 0.91014 time= 0.11053\n",
      "训练次数: 6 Epoch: 0073 log_lik= 0.45820808 train_kl= 0.00861 train_loss= 0.46682 train_acc= 0.54941 val_roc= 0.90801 val_ap= 0.91009 time= 0.10754\n",
      "训练次数: 6 Epoch: 0074 log_lik= 0.45759237 train_kl= 0.00861 train_loss= 0.46620 train_acc= 0.54996 val_roc= 0.90841 val_ap= 0.91052 time= 0.11750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0075 log_lik= 0.45697513 train_kl= 0.00861 train_loss= 0.46559 train_acc= 0.55033 val_roc= 0.90899 val_ap= 0.91125 time= 0.11053\n",
      "训练次数: 6 Epoch: 0076 log_lik= 0.4563768 train_kl= 0.00861 train_loss= 0.46499 train_acc= 0.55130 val_roc= 0.90953 val_ap= 0.91171 time= 0.11252\n",
      "训练次数: 6 Epoch: 0077 log_lik= 0.45575932 train_kl= 0.00861 train_loss= 0.46437 train_acc= 0.55170 val_roc= 0.90961 val_ap= 0.91180 time= 0.10754\n",
      "训练次数: 6 Epoch: 0078 log_lik= 0.4551769 train_kl= 0.00861 train_loss= 0.46379 train_acc= 0.55326 val_roc= 0.90990 val_ap= 0.91216 time= 0.11551\n",
      "训练次数: 6 Epoch: 0079 log_lik= 0.45459965 train_kl= 0.00861 train_loss= 0.46321 train_acc= 0.55370 val_roc= 0.91044 val_ap= 0.91286 time= 0.11053\n",
      "训练次数: 6 Epoch: 0080 log_lik= 0.45393038 train_kl= 0.00861 train_loss= 0.46254 train_acc= 0.55428 val_roc= 0.91062 val_ap= 0.91318 time= 0.11153\n",
      "训练次数: 6 Epoch: 0081 log_lik= 0.45344827 train_kl= 0.00862 train_loss= 0.46206 train_acc= 0.55554 val_roc= 0.91088 val_ap= 0.91369 time= 0.11053\n",
      "训练次数: 6 Epoch: 0082 log_lik= 0.45285806 train_kl= 0.00862 train_loss= 0.46148 train_acc= 0.55620 val_roc= 0.91125 val_ap= 0.91414 time= 0.11352\n",
      "训练次数: 6 Epoch: 0083 log_lik= 0.45235556 train_kl= 0.00862 train_loss= 0.46098 train_acc= 0.55683 val_roc= 0.91167 val_ap= 0.91485 time= 0.11352\n",
      "训练次数: 6 Epoch: 0084 log_lik= 0.45176846 train_kl= 0.00862 train_loss= 0.46039 train_acc= 0.55797 val_roc= 0.91195 val_ap= 0.91550 time= 0.11053\n",
      "训练次数: 6 Epoch: 0085 log_lik= 0.45135465 train_kl= 0.00863 train_loss= 0.45998 train_acc= 0.55915 val_roc= 0.91195 val_ap= 0.91562 time= 0.12049\n",
      "训练次数: 6 Epoch: 0086 log_lik= 0.4507625 train_kl= 0.00863 train_loss= 0.45939 train_acc= 0.56067 val_roc= 0.91213 val_ap= 0.91615 time= 0.11153\n",
      "训练次数: 6 Epoch: 0087 log_lik= 0.45027438 train_kl= 0.00863 train_loss= 0.45891 train_acc= 0.56202 val_roc= 0.91217 val_ap= 0.91651 time= 0.11159\n",
      "训练次数: 6 Epoch: 0088 log_lik= 0.4497885 train_kl= 0.00863 train_loss= 0.45842 train_acc= 0.56345 val_roc= 0.91207 val_ap= 0.91663 time= 0.11252\n",
      "训练次数: 6 Epoch: 0089 log_lik= 0.44920257 train_kl= 0.00863 train_loss= 0.45784 train_acc= 0.56468 val_roc= 0.91178 val_ap= 0.91681 time= 0.12398\n",
      "训练次数: 6 Epoch: 0090 log_lik= 0.44881618 train_kl= 0.00863 train_loss= 0.45745 train_acc= 0.56594 val_roc= 0.91184 val_ap= 0.91720 time= 0.11850\n",
      "训练次数: 6 Epoch: 0091 log_lik= 0.44832903 train_kl= 0.00864 train_loss= 0.45697 train_acc= 0.56745 val_roc= 0.91194 val_ap= 0.91742 time= 0.11053\n",
      "训练次数: 6 Epoch: 0092 log_lik= 0.44790003 train_kl= 0.00864 train_loss= 0.45654 train_acc= 0.56826 val_roc= 0.91206 val_ap= 0.91797 time= 0.11252\n",
      "训练次数: 6 Epoch: 0093 log_lik= 0.44748813 train_kl= 0.00864 train_loss= 0.45613 train_acc= 0.56967 val_roc= 0.91223 val_ap= 0.91820 time= 0.10953\n",
      "训练次数: 6 Epoch: 0094 log_lik= 0.4470969 train_kl= 0.00864 train_loss= 0.45574 train_acc= 0.57032 val_roc= 0.91223 val_ap= 0.91806 time= 0.10854\n",
      "训练次数: 6 Epoch: 0095 log_lik= 0.44657728 train_kl= 0.00864 train_loss= 0.45522 train_acc= 0.57209 val_roc= 0.91224 val_ap= 0.91827 time= 0.11053\n",
      "训练次数: 6 Epoch: 0096 log_lik= 0.44614643 train_kl= 0.00864 train_loss= 0.45479 train_acc= 0.57286 val_roc= 0.91249 val_ap= 0.91883 time= 0.12049\n",
      "训练次数: 6 Epoch: 0097 log_lik= 0.44564754 train_kl= 0.00865 train_loss= 0.45429 train_acc= 0.57411 val_roc= 0.91263 val_ap= 0.91906 time= 0.10754\n",
      "训练次数: 6 Epoch: 0098 log_lik= 0.44539124 train_kl= 0.00865 train_loss= 0.45404 train_acc= 0.57504 val_roc= 0.91284 val_ap= 0.91950 time= 0.11153\n",
      "训练次数: 6 Epoch: 0099 log_lik= 0.44498035 train_kl= 0.00865 train_loss= 0.45363 train_acc= 0.57542 val_roc= 0.91307 val_ap= 0.91980 time= 0.11053\n",
      "训练次数: 6 Epoch: 0100 log_lik= 0.44455686 train_kl= 0.00865 train_loss= 0.45321 train_acc= 0.57620 val_roc= 0.91315 val_ap= 0.92005 time= 0.11451\n",
      "Optimization Finished!\n",
      "训练次数: 6 ROC score: 0.9163357085504212\n",
      "训练次数: 6 AP score: 0.9206658819679919\n",
      "训练次数: 7 Epoch: 0001 log_lik= 0.7812083 train_kl= 0.00806 train_loss= 0.78926 train_acc= 0.04786 val_roc= 0.67964 val_ap= 0.69967 time= 8.57946\n",
      "训练次数: 7 Epoch: 0002 log_lik= 0.8855056 train_kl= 0.00838 train_loss= 0.89389 train_acc= 0.00159 val_roc= 0.74555 val_ap= 0.75778 time= 0.12049\n",
      "训练次数: 7 Epoch: 0003 log_lik= 0.74561405 train_kl= 0.00810 train_loss= 0.75372 train_acc= 0.00591 val_roc= 0.70375 val_ap= 0.72487 time= 0.11949\n",
      "训练次数: 7 Epoch: 0004 log_lik= 0.74517304 train_kl= 0.00811 train_loss= 0.75328 train_acc= 0.00986 val_roc= 0.67832 val_ap= 0.70874 time= 0.11053\n",
      "训练次数: 7 Epoch: 0005 log_lik= 0.7466427 train_kl= 0.00818 train_loss= 0.75483 train_acc= 0.00454 val_roc= 0.68548 val_ap= 0.71620 time= 0.11551\n",
      "训练次数: 7 Epoch: 0006 log_lik= 0.74601436 train_kl= 0.00819 train_loss= 0.75421 train_acc= 0.00374 val_roc= 0.70167 val_ap= 0.73303 time= 0.11850\n",
      "训练次数: 7 Epoch: 0007 log_lik= 0.7346102 train_kl= 0.00817 train_loss= 0.74278 train_acc= 0.00529 val_roc= 0.72131 val_ap= 0.75236 time= 0.11252\n",
      "训练次数: 7 Epoch: 0008 log_lik= 0.7270838 train_kl= 0.00815 train_loss= 0.73523 train_acc= 0.01264 val_roc= 0.74020 val_ap= 0.76479 time= 0.11352\n",
      "训练次数: 7 Epoch: 0009 log_lik= 0.7199627 train_kl= 0.00814 train_loss= 0.72810 train_acc= 0.03365 val_roc= 0.75972 val_ap= 0.77387 time= 0.11551\n",
      "训练次数: 7 Epoch: 0010 log_lik= 0.7080385 train_kl= 0.00815 train_loss= 0.71619 train_acc= 0.06951 val_roc= 0.77879 val_ap= 0.78267 time= 0.12547\n",
      "训练次数: 7 Epoch: 0011 log_lik= 0.6923873 train_kl= 0.00818 train_loss= 0.70056 train_acc= 0.10684 val_roc= 0.79526 val_ap= 0.79489 time= 0.11053\n",
      "训练次数: 7 Epoch: 0012 log_lik= 0.6747063 train_kl= 0.00821 train_loss= 0.68292 train_acc= 0.14715 val_roc= 0.80350 val_ap= 0.79893 time= 0.10854\n",
      "训练次数: 7 Epoch: 0013 log_lik= 0.6605382 train_kl= 0.00826 train_loss= 0.66879 train_acc= 0.19336 val_roc= 0.81049 val_ap= 0.80063 time= 0.11850\n",
      "训练次数: 7 Epoch: 0014 log_lik= 0.6498587 train_kl= 0.00830 train_loss= 0.65816 train_acc= 0.24435 val_roc= 0.82228 val_ap= 0.80355 time= 0.11153\n",
      "训练次数: 7 Epoch: 0015 log_lik= 0.63567483 train_kl= 0.00832 train_loss= 0.64400 train_acc= 0.29993 val_roc= 0.82562 val_ap= 0.80198 time= 0.11053\n",
      "训练次数: 7 Epoch: 0016 log_lik= 0.62443316 train_kl= 0.00835 train_loss= 0.63278 train_acc= 0.35062 val_roc= 0.82375 val_ap= 0.79947 time= 0.11352\n",
      "训练次数: 7 Epoch: 0017 log_lik= 0.6181671 train_kl= 0.00837 train_loss= 0.62653 train_acc= 0.38851 val_roc= 0.82560 val_ap= 0.80250 time= 0.10953\n",
      "训练次数: 7 Epoch: 0018 log_lik= 0.6095452 train_kl= 0.00838 train_loss= 0.61793 train_acc= 0.41664 val_roc= 0.83060 val_ap= 0.80991 time= 0.11651\n",
      "训练次数: 7 Epoch: 0019 log_lik= 0.5981156 train_kl= 0.00839 train_loss= 0.60650 train_acc= 0.44084 val_roc= 0.83832 val_ap= 0.82230 time= 0.11356\n",
      "训练次数: 7 Epoch: 0020 log_lik= 0.586393 train_kl= 0.00839 train_loss= 0.59478 train_acc= 0.45434 val_roc= 0.84538 val_ap= 0.83386 time= 0.11692\n",
      "训练次数: 7 Epoch: 0021 log_lik= 0.57499766 train_kl= 0.00839 train_loss= 0.58339 train_acc= 0.46190 val_roc= 0.85298 val_ap= 0.84512 time= 0.10854\n",
      "训练次数: 7 Epoch: 0022 log_lik= 0.5652721 train_kl= 0.00839 train_loss= 0.57366 train_acc= 0.46514 val_roc= 0.86062 val_ap= 0.85450 time= 0.10953\n",
      "训练次数: 7 Epoch: 0023 log_lik= 0.5571442 train_kl= 0.00839 train_loss= 0.56554 train_acc= 0.46901 val_roc= 0.86568 val_ap= 0.85967 time= 0.11204\n",
      "训练次数: 7 Epoch: 0024 log_lik= 0.54985124 train_kl= 0.00840 train_loss= 0.55825 train_acc= 0.47584 val_roc= 0.87000 val_ap= 0.86326 time= 0.11153\n",
      "训练次数: 7 Epoch: 0025 log_lik= 0.54225755 train_kl= 0.00842 train_loss= 0.55067 train_acc= 0.48523 val_roc= 0.87379 val_ap= 0.86671 time= 0.10922\n",
      "训练次数: 7 Epoch: 0026 log_lik= 0.534538 train_kl= 0.00844 train_loss= 0.54298 train_acc= 0.49459 val_roc= 0.87539 val_ap= 0.86802 time= 0.11053\n",
      "训练次数: 7 Epoch: 0027 log_lik= 0.52883404 train_kl= 0.00847 train_loss= 0.53730 train_acc= 0.50098 val_roc= 0.87643 val_ap= 0.87069 time= 0.11750\n",
      "训练次数: 7 Epoch: 0028 log_lik= 0.5251493 train_kl= 0.00850 train_loss= 0.53365 train_acc= 0.50533 val_roc= 0.87701 val_ap= 0.87318 time= 0.11352\n",
      "训练次数: 7 Epoch: 0029 log_lik= 0.52214205 train_kl= 0.00853 train_loss= 0.53067 train_acc= 0.50831 val_roc= 0.87957 val_ap= 0.87743 time= 0.11153\n",
      "训练次数: 7 Epoch: 0030 log_lik= 0.5181635 train_kl= 0.00855 train_loss= 0.52672 train_acc= 0.51166 val_roc= 0.88269 val_ap= 0.88247 time= 0.11352\n",
      "训练次数: 7 Epoch: 0031 log_lik= 0.51359534 train_kl= 0.00857 train_loss= 0.52216 train_acc= 0.51477 val_roc= 0.88668 val_ap= 0.88698 time= 0.11053\n",
      "训练次数: 7 Epoch: 0032 log_lik= 0.50960237 train_kl= 0.00857 train_loss= 0.51818 train_acc= 0.51763 val_roc= 0.88982 val_ap= 0.89034 time= 0.10854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0033 log_lik= 0.50624454 train_kl= 0.00857 train_loss= 0.51482 train_acc= 0.52078 val_roc= 0.89223 val_ap= 0.89353 time= 0.11352\n",
      "训练次数: 7 Epoch: 0034 log_lik= 0.50282335 train_kl= 0.00857 train_loss= 0.51139 train_acc= 0.52405 val_roc= 0.89367 val_ap= 0.89573 time= 0.11252\n",
      "训练次数: 7 Epoch: 0035 log_lik= 0.4997607 train_kl= 0.00856 train_loss= 0.50832 train_acc= 0.52784 val_roc= 0.89469 val_ap= 0.89780 time= 0.10953\n",
      "训练次数: 7 Epoch: 0036 log_lik= 0.4968951 train_kl= 0.00856 train_loss= 0.50545 train_acc= 0.53127 val_roc= 0.89527 val_ap= 0.89942 time= 0.10953\n",
      "训练次数: 7 Epoch: 0037 log_lik= 0.49486357 train_kl= 0.00855 train_loss= 0.50341 train_acc= 0.53421 val_roc= 0.89544 val_ap= 0.90034 time= 0.11153\n",
      "训练次数: 7 Epoch: 0038 log_lik= 0.4930153 train_kl= 0.00855 train_loss= 0.50156 train_acc= 0.53660 val_roc= 0.89596 val_ap= 0.90083 time= 0.11156\n",
      "训练次数: 7 Epoch: 0039 log_lik= 0.49125648 train_kl= 0.00855 train_loss= 0.49980 train_acc= 0.53900 val_roc= 0.89679 val_ap= 0.90177 time= 0.11252\n",
      "训练次数: 7 Epoch: 0040 log_lik= 0.4891756 train_kl= 0.00855 train_loss= 0.49773 train_acc= 0.54192 val_roc= 0.89735 val_ap= 0.90206 time= 0.11451\n",
      "训练次数: 7 Epoch: 0041 log_lik= 0.48668033 train_kl= 0.00856 train_loss= 0.49524 train_acc= 0.54410 val_roc= 0.89826 val_ap= 0.90299 time= 0.11352\n",
      "训练次数: 7 Epoch: 0042 log_lik= 0.4844192 train_kl= 0.00857 train_loss= 0.49299 train_acc= 0.54653 val_roc= 0.89873 val_ap= 0.90372 time= 0.11252\n",
      "训练次数: 7 Epoch: 0043 log_lik= 0.4826024 train_kl= 0.00858 train_loss= 0.49118 train_acc= 0.54736 val_roc= 0.89881 val_ap= 0.90418 time= 0.10953\n",
      "训练次数: 7 Epoch: 0044 log_lik= 0.48067918 train_kl= 0.00859 train_loss= 0.48927 train_acc= 0.54871 val_roc= 0.89904 val_ap= 0.90466 time= 0.11153\n",
      "训练次数: 7 Epoch: 0045 log_lik= 0.47866884 train_kl= 0.00859 train_loss= 0.48726 train_acc= 0.54911 val_roc= 0.89899 val_ap= 0.90508 time= 0.11551\n",
      "训练次数: 7 Epoch: 0046 log_lik= 0.47704586 train_kl= 0.00860 train_loss= 0.48564 train_acc= 0.54949 val_roc= 0.89936 val_ap= 0.90569 time= 0.11352\n",
      "训练次数: 7 Epoch: 0047 log_lik= 0.47563082 train_kl= 0.00860 train_loss= 0.48423 train_acc= 0.54898 val_roc= 0.89967 val_ap= 0.90564 time= 0.11551\n",
      "训练次数: 7 Epoch: 0048 log_lik= 0.47396663 train_kl= 0.00859 train_loss= 0.48256 train_acc= 0.54897 val_roc= 0.89984 val_ap= 0.90570 time= 0.11152\n",
      "训练次数: 7 Epoch: 0049 log_lik= 0.47253183 train_kl= 0.00859 train_loss= 0.48112 train_acc= 0.54921 val_roc= 0.90042 val_ap= 0.90594 time= 0.11053\n",
      "训练次数: 7 Epoch: 0050 log_lik= 0.47084424 train_kl= 0.00859 train_loss= 0.47943 train_acc= 0.54999 val_roc= 0.90078 val_ap= 0.90601 time= 0.10854\n",
      "训练次数: 7 Epoch: 0051 log_lik= 0.4697516 train_kl= 0.00858 train_loss= 0.47834 train_acc= 0.55084 val_roc= 0.90147 val_ap= 0.90697 time= 0.11153\n",
      "训练次数: 7 Epoch: 0052 log_lik= 0.46857342 train_kl= 0.00858 train_loss= 0.47716 train_acc= 0.55091 val_roc= 0.90196 val_ap= 0.90764 time= 0.11053\n",
      "训练次数: 7 Epoch: 0053 log_lik= 0.4675697 train_kl= 0.00858 train_loss= 0.47615 train_acc= 0.55205 val_roc= 0.90262 val_ap= 0.90835 time= 0.11053\n",
      "训练次数: 7 Epoch: 0054 log_lik= 0.46661755 train_kl= 0.00858 train_loss= 0.47520 train_acc= 0.55184 val_roc= 0.90361 val_ap= 0.90963 time= 0.10953\n",
      "训练次数: 7 Epoch: 0055 log_lik= 0.46576357 train_kl= 0.00859 train_loss= 0.47435 train_acc= 0.55132 val_roc= 0.90429 val_ap= 0.91040 time= 0.11053\n",
      "训练次数: 7 Epoch: 0056 log_lik= 0.46469945 train_kl= 0.00859 train_loss= 0.47329 train_acc= 0.55086 val_roc= 0.90480 val_ap= 0.91073 time= 0.10954\n",
      "训练次数: 7 Epoch: 0057 log_lik= 0.46375924 train_kl= 0.00859 train_loss= 0.47235 train_acc= 0.55052 val_roc= 0.90499 val_ap= 0.91072 time= 0.10854\n",
      "训练次数: 7 Epoch: 0058 log_lik= 0.4625968 train_kl= 0.00860 train_loss= 0.47119 train_acc= 0.55067 val_roc= 0.90539 val_ap= 0.91086 time= 0.10953\n",
      "训练次数: 7 Epoch: 0059 log_lik= 0.46173617 train_kl= 0.00860 train_loss= 0.47034 train_acc= 0.55079 val_roc= 0.90603 val_ap= 0.91119 time= 0.10854\n",
      "训练次数: 7 Epoch: 0060 log_lik= 0.46079004 train_kl= 0.00861 train_loss= 0.46940 train_acc= 0.55094 val_roc= 0.90661 val_ap= 0.91180 time= 0.10953\n",
      "训练次数: 7 Epoch: 0061 log_lik= 0.46000546 train_kl= 0.00861 train_loss= 0.46862 train_acc= 0.55190 val_roc= 0.90734 val_ap= 0.91320 time= 0.10953\n",
      "训练次数: 7 Epoch: 0062 log_lik= 0.45912096 train_kl= 0.00861 train_loss= 0.46774 train_acc= 0.55275 val_roc= 0.90759 val_ap= 0.91365 time= 0.11949\n",
      "训练次数: 7 Epoch: 0063 log_lik= 0.45826268 train_kl= 0.00861 train_loss= 0.46688 train_acc= 0.55357 val_roc= 0.90770 val_ap= 0.91395 time= 0.11551\n",
      "训练次数: 7 Epoch: 0064 log_lik= 0.4572531 train_kl= 0.00861 train_loss= 0.46587 train_acc= 0.55407 val_roc= 0.90759 val_ap= 0.91412 time= 0.11551\n",
      "训练次数: 7 Epoch: 0065 log_lik= 0.4565397 train_kl= 0.00861 train_loss= 0.46515 train_acc= 0.55445 val_roc= 0.90715 val_ap= 0.91361 time= 0.10854\n",
      "训练次数: 7 Epoch: 0066 log_lik= 0.45584497 train_kl= 0.00861 train_loss= 0.46446 train_acc= 0.55560 val_roc= 0.90733 val_ap= 0.91344 time= 0.10953\n",
      "训练次数: 7 Epoch: 0067 log_lik= 0.4549213 train_kl= 0.00861 train_loss= 0.46353 train_acc= 0.55642 val_roc= 0.90789 val_ap= 0.91390 time= 0.11554\n",
      "训练次数: 7 Epoch: 0068 log_lik= 0.45411637 train_kl= 0.00861 train_loss= 0.46273 train_acc= 0.55782 val_roc= 0.90851 val_ap= 0.91456 time= 0.10953\n",
      "训练次数: 7 Epoch: 0069 log_lik= 0.4534035 train_kl= 0.00861 train_loss= 0.46202 train_acc= 0.55923 val_roc= 0.90903 val_ap= 0.91542 time= 0.11153\n",
      "训练次数: 7 Epoch: 0070 log_lik= 0.45270914 train_kl= 0.00861 train_loss= 0.46132 train_acc= 0.55960 val_roc= 0.90931 val_ap= 0.91590 time= 0.11352\n",
      "训练次数: 7 Epoch: 0071 log_lik= 0.4520003 train_kl= 0.00862 train_loss= 0.46062 train_acc= 0.56009 val_roc= 0.90938 val_ap= 0.91613 time= 0.11551\n",
      "训练次数: 7 Epoch: 0072 log_lik= 0.4513859 train_kl= 0.00862 train_loss= 0.46001 train_acc= 0.56037 val_roc= 0.90967 val_ap= 0.91659 time= 0.11651\n",
      "训练次数: 7 Epoch: 0073 log_lik= 0.45079342 train_kl= 0.00862 train_loss= 0.45942 train_acc= 0.56067 val_roc= 0.90976 val_ap= 0.91660 time= 0.11651\n",
      "训练次数: 7 Epoch: 0074 log_lik= 0.4502656 train_kl= 0.00863 train_loss= 0.45889 train_acc= 0.56093 val_roc= 0.91005 val_ap= 0.91661 time= 0.11451\n",
      "训练次数: 7 Epoch: 0075 log_lik= 0.4497154 train_kl= 0.00863 train_loss= 0.45835 train_acc= 0.56093 val_roc= 0.91045 val_ap= 0.91689 time= 0.11451\n",
      "训练次数: 7 Epoch: 0076 log_lik= 0.44915843 train_kl= 0.00863 train_loss= 0.45779 train_acc= 0.56109 val_roc= 0.91094 val_ap= 0.91739 time= 0.11451\n",
      "训练次数: 7 Epoch: 0077 log_lik= 0.4487899 train_kl= 0.00864 train_loss= 0.45743 train_acc= 0.56177 val_roc= 0.91133 val_ap= 0.91788 time= 0.11750\n",
      "训练次数: 7 Epoch: 0078 log_lik= 0.44826353 train_kl= 0.00864 train_loss= 0.45690 train_acc= 0.56170 val_roc= 0.91152 val_ap= 0.91786 time= 0.11053\n",
      "训练次数: 7 Epoch: 0079 log_lik= 0.44792607 train_kl= 0.00864 train_loss= 0.45656 train_acc= 0.56207 val_roc= 0.91198 val_ap= 0.91811 time= 0.10854\n",
      "训练次数: 7 Epoch: 0080 log_lik= 0.4473713 train_kl= 0.00864 train_loss= 0.45601 train_acc= 0.56198 val_roc= 0.91249 val_ap= 0.91878 time= 0.11352\n",
      "训练次数: 7 Epoch: 0081 log_lik= 0.44697922 train_kl= 0.00864 train_loss= 0.45562 train_acc= 0.56211 val_roc= 0.91301 val_ap= 0.91913 time= 0.11750\n",
      "训练次数: 7 Epoch: 0082 log_lik= 0.4466429 train_kl= 0.00864 train_loss= 0.45528 train_acc= 0.56228 val_roc= 0.91362 val_ap= 0.91942 time= 0.10953\n",
      "训练次数: 7 Epoch: 0083 log_lik= 0.44611335 train_kl= 0.00864 train_loss= 0.45475 train_acc= 0.56244 val_roc= 0.91427 val_ap= 0.91998 time= 0.11153\n",
      "训练次数: 7 Epoch: 0084 log_lik= 0.4457234 train_kl= 0.00864 train_loss= 0.45436 train_acc= 0.56304 val_roc= 0.91532 val_ap= 0.92116 time= 0.11651\n",
      "训练次数: 7 Epoch: 0085 log_lik= 0.44522008 train_kl= 0.00864 train_loss= 0.45386 train_acc= 0.56289 val_roc= 0.91603 val_ap= 0.92175 time= 0.11273\n",
      "训练次数: 7 Epoch: 0086 log_lik= 0.44487578 train_kl= 0.00864 train_loss= 0.45352 train_acc= 0.56293 val_roc= 0.91655 val_ap= 0.92249 time= 0.11053\n",
      "训练次数: 7 Epoch: 0087 log_lik= 0.44446808 train_kl= 0.00864 train_loss= 0.45311 train_acc= 0.56290 val_roc= 0.91703 val_ap= 0.92303 time= 0.10954\n",
      "训练次数: 7 Epoch: 0088 log_lik= 0.44411477 train_kl= 0.00864 train_loss= 0.45276 train_acc= 0.56304 val_roc= 0.91729 val_ap= 0.92333 time= 0.11053\n",
      "训练次数: 7 Epoch: 0089 log_lik= 0.4436251 train_kl= 0.00864 train_loss= 0.45227 train_acc= 0.56310 val_roc= 0.91824 val_ap= 0.92450 time= 0.11053\n",
      "训练次数: 7 Epoch: 0090 log_lik= 0.44324154 train_kl= 0.00864 train_loss= 0.45189 train_acc= 0.56392 val_roc= 0.91889 val_ap= 0.92519 time= 0.11949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0091 log_lik= 0.44288793 train_kl= 0.00865 train_loss= 0.45153 train_acc= 0.56419 val_roc= 0.91985 val_ap= 0.92604 time= 0.11451\n",
      "训练次数: 7 Epoch: 0092 log_lik= 0.44253376 train_kl= 0.00865 train_loss= 0.45118 train_acc= 0.56423 val_roc= 0.92034 val_ap= 0.92657 time= 0.10854\n",
      "训练次数: 7 Epoch: 0093 log_lik= 0.44216564 train_kl= 0.00865 train_loss= 0.45081 train_acc= 0.56432 val_roc= 0.92059 val_ap= 0.92691 time= 0.11451\n",
      "训练次数: 7 Epoch: 0094 log_lik= 0.44176975 train_kl= 0.00865 train_loss= 0.45042 train_acc= 0.56509 val_roc= 0.92105 val_ap= 0.92733 time= 0.10854\n",
      "训练次数: 7 Epoch: 0095 log_lik= 0.44138208 train_kl= 0.00865 train_loss= 0.45003 train_acc= 0.56530 val_roc= 0.92118 val_ap= 0.92744 time= 0.10954\n",
      "训练次数: 7 Epoch: 0096 log_lik= 0.44105965 train_kl= 0.00865 train_loss= 0.44971 train_acc= 0.56564 val_roc= 0.92142 val_ap= 0.92776 time= 0.11053\n",
      "训练次数: 7 Epoch: 0097 log_lik= 0.44067502 train_kl= 0.00865 train_loss= 0.44933 train_acc= 0.56568 val_roc= 0.92155 val_ap= 0.92771 time= 0.10953\n",
      "训练次数: 7 Epoch: 0098 log_lik= 0.44030756 train_kl= 0.00865 train_loss= 0.44896 train_acc= 0.56625 val_roc= 0.92163 val_ap= 0.92778 time= 0.11053\n",
      "训练次数: 7 Epoch: 0099 log_lik= 0.43995062 train_kl= 0.00865 train_loss= 0.44860 train_acc= 0.56661 val_roc= 0.92157 val_ap= 0.92780 time= 0.10754\n",
      "训练次数: 7 Epoch: 0100 log_lik= 0.4395664 train_kl= 0.00865 train_loss= 0.44822 train_acc= 0.56712 val_roc= 0.92173 val_ap= 0.92783 time= 0.10953\n",
      "Optimization Finished!\n",
      "训练次数: 7 ROC score: 0.9249340184136334\n",
      "训练次数: 7 AP score: 0.9346101376107507\n",
      "训练次数: 8 Epoch: 0001 log_lik= 0.7801986 train_kl= 0.00806 train_loss= 0.78825 train_acc= 0.07375 val_roc= 0.63488 val_ap= 0.66162 time= 8.89004\n",
      "训练次数: 8 Epoch: 0002 log_lik= 0.8700917 train_kl= 0.00837 train_loss= 0.87846 train_acc= 0.00159 val_roc= 0.72433 val_ap= 0.74015 time= 0.12248\n",
      "训练次数: 8 Epoch: 0003 log_lik= 0.7339238 train_kl= 0.00812 train_loss= 0.74205 train_acc= 0.00685 val_roc= 0.67222 val_ap= 0.70035 time= 0.11172\n",
      "训练次数: 8 Epoch: 0004 log_lik= 0.7465598 train_kl= 0.00822 train_loss= 0.75478 train_acc= 0.00316 val_roc= 0.70384 val_ap= 0.72490 time= 0.11551\n",
      "训练次数: 8 Epoch: 0005 log_lik= 0.72924364 train_kl= 0.00819 train_loss= 0.73744 train_acc= 0.00657 val_roc= 0.75515 val_ap= 0.76789 time= 0.12201\n",
      "训练次数: 8 Epoch: 0006 log_lik= 0.7117361 train_kl= 0.00816 train_loss= 0.71989 train_acc= 0.04692 val_roc= 0.78090 val_ap= 0.78424 time= 0.11252\n",
      "训练次数: 8 Epoch: 0007 log_lik= 0.6949785 train_kl= 0.00816 train_loss= 0.70314 train_acc= 0.16722 val_roc= 0.78894 val_ap= 0.78633 time= 0.11558\n",
      "训练次数: 8 Epoch: 0008 log_lik= 0.6719383 train_kl= 0.00819 train_loss= 0.68013 train_acc= 0.24864 val_roc= 0.79481 val_ap= 0.79170 time= 0.11551\n",
      "训练次数: 8 Epoch: 0009 log_lik= 0.6466613 train_kl= 0.00824 train_loss= 0.65490 train_acc= 0.29248 val_roc= 0.79942 val_ap= 0.79395 time= 0.11352\n",
      "训练次数: 8 Epoch: 0010 log_lik= 0.62604845 train_kl= 0.00830 train_loss= 0.63434 train_acc= 0.32571 val_roc= 0.79917 val_ap= 0.79257 time= 0.11252\n",
      "训练次数: 8 Epoch: 0011 log_lik= 0.6126335 train_kl= 0.00835 train_loss= 0.62099 train_acc= 0.36266 val_roc= 0.80305 val_ap= 0.79811 time= 0.11451\n",
      "训练次数: 8 Epoch: 0012 log_lik= 0.5999026 train_kl= 0.00840 train_loss= 0.60830 train_acc= 0.40169 val_roc= 0.80869 val_ap= 0.80430 time= 0.11053\n",
      "训练次数: 8 Epoch: 0013 log_lik= 0.5867038 train_kl= 0.00843 train_loss= 0.59513 train_acc= 0.43337 val_roc= 0.81589 val_ap= 0.81185 time= 0.11053\n",
      "训练次数: 8 Epoch: 0014 log_lik= 0.57348055 train_kl= 0.00844 train_loss= 0.58192 train_acc= 0.45479 val_roc= 0.82401 val_ap= 0.82014 time= 0.10854\n",
      "训练次数: 8 Epoch: 0015 log_lik= 0.56306607 train_kl= 0.00845 train_loss= 0.57151 train_acc= 0.46609 val_roc= 0.83046 val_ap= 0.82397 time= 0.11053\n",
      "训练次数: 8 Epoch: 0016 log_lik= 0.55566114 train_kl= 0.00845 train_loss= 0.56411 train_acc= 0.47131 val_roc= 0.83613 val_ap= 0.82990 time= 0.10854\n",
      "训练次数: 8 Epoch: 0017 log_lik= 0.55096275 train_kl= 0.00845 train_loss= 0.55942 train_acc= 0.47295 val_roc= 0.84081 val_ap= 0.83466 time= 0.11252\n",
      "训练次数: 8 Epoch: 0018 log_lik= 0.54716146 train_kl= 0.00846 train_loss= 0.55562 train_acc= 0.47385 val_roc= 0.84473 val_ap= 0.83849 time= 0.11153\n",
      "训练次数: 8 Epoch: 0019 log_lik= 0.54332423 train_kl= 0.00847 train_loss= 0.55179 train_acc= 0.47637 val_roc= 0.84755 val_ap= 0.84091 time= 0.11053\n",
      "训练次数: 8 Epoch: 0020 log_lik= 0.5382381 train_kl= 0.00847 train_loss= 0.54671 train_acc= 0.48149 val_roc= 0.84987 val_ap= 0.84372 time= 0.11850\n",
      "训练次数: 8 Epoch: 0021 log_lik= 0.5327282 train_kl= 0.00848 train_loss= 0.54121 train_acc= 0.48800 val_roc= 0.85162 val_ap= 0.84535 time= 0.11153\n",
      "训练次数: 8 Epoch: 0022 log_lik= 0.52745205 train_kl= 0.00849 train_loss= 0.53594 train_acc= 0.49548 val_roc= 0.85227 val_ap= 0.84660 time= 0.11650\n",
      "训练次数: 8 Epoch: 0023 log_lik= 0.5218199 train_kl= 0.00850 train_loss= 0.53032 train_acc= 0.50099 val_roc= 0.85450 val_ap= 0.85040 time= 0.11551\n",
      "训练次数: 8 Epoch: 0024 log_lik= 0.51699626 train_kl= 0.00850 train_loss= 0.52550 train_acc= 0.50623 val_roc= 0.85595 val_ap= 0.85274 time= 0.11252\n",
      "训练次数: 8 Epoch: 0025 log_lik= 0.5128268 train_kl= 0.00851 train_loss= 0.52134 train_acc= 0.51001 val_roc= 0.85755 val_ap= 0.85526 time= 0.11551\n",
      "训练次数: 8 Epoch: 0026 log_lik= 0.5088403 train_kl= 0.00852 train_loss= 0.51736 train_acc= 0.51540 val_roc= 0.85881 val_ap= 0.85736 time= 0.11053\n",
      "训练次数: 8 Epoch: 0027 log_lik= 0.5050677 train_kl= 0.00853 train_loss= 0.51359 train_acc= 0.52002 val_roc= 0.86131 val_ap= 0.85981 time= 0.10954\n",
      "训练次数: 8 Epoch: 0028 log_lik= 0.50115913 train_kl= 0.00853 train_loss= 0.50969 train_acc= 0.52521 val_roc= 0.86416 val_ap= 0.86256 time= 0.11252\n",
      "训练次数: 8 Epoch: 0029 log_lik= 0.49703163 train_kl= 0.00853 train_loss= 0.50556 train_acc= 0.52953 val_roc= 0.86673 val_ap= 0.86483 time= 0.11252\n",
      "训练次数: 8 Epoch: 0030 log_lik= 0.49328417 train_kl= 0.00853 train_loss= 0.50182 train_acc= 0.53395 val_roc= 0.86987 val_ap= 0.86795 time= 0.10953\n",
      "训练次数: 8 Epoch: 0031 log_lik= 0.48974866 train_kl= 0.00854 train_loss= 0.49829 train_acc= 0.53654 val_roc= 0.87181 val_ap= 0.86924 time= 0.11551\n",
      "训练次数: 8 Epoch: 0032 log_lik= 0.48605305 train_kl= 0.00855 train_loss= 0.49460 train_acc= 0.53900 val_roc= 0.87387 val_ap= 0.87001 time= 0.10953\n",
      "训练次数: 8 Epoch: 0033 log_lik= 0.4826943 train_kl= 0.00856 train_loss= 0.49125 train_acc= 0.54036 val_roc= 0.87588 val_ap= 0.87023 time= 0.10954\n",
      "训练次数: 8 Epoch: 0034 log_lik= 0.4796033 train_kl= 0.00857 train_loss= 0.48817 train_acc= 0.54340 val_roc= 0.87745 val_ap= 0.87075 time= 0.10954\n",
      "训练次数: 8 Epoch: 0035 log_lik= 0.4771925 train_kl= 0.00858 train_loss= 0.48578 train_acc= 0.54583 val_roc= 0.87865 val_ap= 0.87164 time= 0.11352\n",
      "训练次数: 8 Epoch: 0036 log_lik= 0.47528237 train_kl= 0.00859 train_loss= 0.48388 train_acc= 0.54814 val_roc= 0.87937 val_ap= 0.87241 time= 0.11153\n",
      "训练次数: 8 Epoch: 0037 log_lik= 0.4739995 train_kl= 0.00860 train_loss= 0.48260 train_acc= 0.54828 val_roc= 0.88013 val_ap= 0.87447 time= 0.10953\n",
      "训练次数: 8 Epoch: 0038 log_lik= 0.47288838 train_kl= 0.00860 train_loss= 0.48149 train_acc= 0.54804 val_roc= 0.88093 val_ap= 0.87675 time= 0.11850\n",
      "训练次数: 8 Epoch: 0039 log_lik= 0.47170222 train_kl= 0.00860 train_loss= 0.48030 train_acc= 0.54789 val_roc= 0.88180 val_ap= 0.87929 time= 0.11053\n",
      "训练次数: 8 Epoch: 0040 log_lik= 0.4705881 train_kl= 0.00860 train_loss= 0.47919 train_acc= 0.54925 val_roc= 0.88191 val_ap= 0.88011 time= 0.10854\n",
      "训练次数: 8 Epoch: 0041 log_lik= 0.46905872 train_kl= 0.00860 train_loss= 0.47766 train_acc= 0.55117 val_roc= 0.88201 val_ap= 0.88060 time= 0.11204\n",
      "训练次数: 8 Epoch: 0042 log_lik= 0.46764022 train_kl= 0.00859 train_loss= 0.47623 train_acc= 0.55376 val_roc= 0.88198 val_ap= 0.88107 time= 0.11850\n",
      "训练次数: 8 Epoch: 0043 log_lik= 0.4659734 train_kl= 0.00859 train_loss= 0.47457 train_acc= 0.55607 val_roc= 0.88259 val_ap= 0.88216 time= 0.11352\n",
      "训练次数: 8 Epoch: 0044 log_lik= 0.46431765 train_kl= 0.00860 train_loss= 0.47291 train_acc= 0.55804 val_roc= 0.88314 val_ap= 0.88385 time= 0.11252\n",
      "训练次数: 8 Epoch: 0045 log_lik= 0.46259946 train_kl= 0.00860 train_loss= 0.47120 train_acc= 0.55975 val_roc= 0.88431 val_ap= 0.88583 time= 0.11352\n",
      "训练次数: 8 Epoch: 0046 log_lik= 0.4612769 train_kl= 0.00860 train_loss= 0.46988 train_acc= 0.56042 val_roc= 0.88504 val_ap= 0.88707 time= 0.11053\n",
      "训练次数: 8 Epoch: 0047 log_lik= 0.46006656 train_kl= 0.00861 train_loss= 0.46868 train_acc= 0.56010 val_roc= 0.88628 val_ap= 0.88938 time= 0.11252\n",
      "训练次数: 8 Epoch: 0048 log_lik= 0.45923206 train_kl= 0.00862 train_loss= 0.46785 train_acc= 0.56059 val_roc= 0.88663 val_ap= 0.88997 time= 0.11352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 8 Epoch: 0049 log_lik= 0.45851895 train_kl= 0.00862 train_loss= 0.46714 train_acc= 0.56140 val_roc= 0.88696 val_ap= 0.89053 time= 0.11451\n",
      "训练次数: 8 Epoch: 0050 log_lik= 0.45775104 train_kl= 0.00863 train_loss= 0.46638 train_acc= 0.56171 val_roc= 0.88783 val_ap= 0.89098 time= 0.11352\n",
      "训练次数: 8 Epoch: 0051 log_lik= 0.45712847 train_kl= 0.00863 train_loss= 0.46576 train_acc= 0.56299 val_roc= 0.88790 val_ap= 0.89049 time= 0.11949\n",
      "训练次数: 8 Epoch: 0052 log_lik= 0.4562765 train_kl= 0.00863 train_loss= 0.46490 train_acc= 0.56418 val_roc= 0.88869 val_ap= 0.89079 time= 0.10854\n",
      "训练次数: 8 Epoch: 0053 log_lik= 0.45537186 train_kl= 0.00862 train_loss= 0.46399 train_acc= 0.56612 val_roc= 0.88943 val_ap= 0.89184 time= 0.10954\n",
      "训练次数: 8 Epoch: 0054 log_lik= 0.45465335 train_kl= 0.00862 train_loss= 0.46327 train_acc= 0.56733 val_roc= 0.89041 val_ap= 0.89292 time= 0.11053\n",
      "训练次数: 8 Epoch: 0055 log_lik= 0.4537912 train_kl= 0.00861 train_loss= 0.46240 train_acc= 0.56812 val_roc= 0.89130 val_ap= 0.89398 time= 0.10954\n",
      "训练次数: 8 Epoch: 0056 log_lik= 0.4529659 train_kl= 0.00861 train_loss= 0.46158 train_acc= 0.56946 val_roc= 0.89213 val_ap= 0.89502 time= 0.11153\n",
      "训练次数: 8 Epoch: 0057 log_lik= 0.45215318 train_kl= 0.00861 train_loss= 0.46077 train_acc= 0.57021 val_roc= 0.89319 val_ap= 0.89612 time= 0.11849\n",
      "训练次数: 8 Epoch: 0058 log_lik= 0.45143345 train_kl= 0.00862 train_loss= 0.46005 train_acc= 0.57123 val_roc= 0.89359 val_ap= 0.89631 time= 0.10954\n",
      "训练次数: 8 Epoch: 0059 log_lik= 0.4506259 train_kl= 0.00863 train_loss= 0.45925 train_acc= 0.57325 val_roc= 0.89437 val_ap= 0.89681 time= 0.11053\n",
      "训练次数: 8 Epoch: 0060 log_lik= 0.44985402 train_kl= 0.00863 train_loss= 0.45849 train_acc= 0.57399 val_roc= 0.89497 val_ap= 0.89779 time= 0.11451\n",
      "训练次数: 8 Epoch: 0061 log_lik= 0.44929594 train_kl= 0.00864 train_loss= 0.45794 train_acc= 0.57473 val_roc= 0.89581 val_ap= 0.89907 time= 0.10854\n",
      "训练次数: 8 Epoch: 0062 log_lik= 0.44875202 train_kl= 0.00864 train_loss= 0.45740 train_acc= 0.57508 val_roc= 0.89657 val_ap= 0.90079 time= 0.11053\n",
      "训练次数: 8 Epoch: 0063 log_lik= 0.44810247 train_kl= 0.00865 train_loss= 0.45675 train_acc= 0.57509 val_roc= 0.89712 val_ap= 0.90184 time= 0.11053\n",
      "训练次数: 8 Epoch: 0064 log_lik= 0.44751787 train_kl= 0.00865 train_loss= 0.45616 train_acc= 0.57560 val_roc= 0.89774 val_ap= 0.90263 time= 0.10953\n",
      "训练次数: 8 Epoch: 0065 log_lik= 0.44701773 train_kl= 0.00864 train_loss= 0.45566 train_acc= 0.57660 val_roc= 0.89787 val_ap= 0.90282 time= 0.11053\n",
      "训练次数: 8 Epoch: 0066 log_lik= 0.44642687 train_kl= 0.00864 train_loss= 0.45507 train_acc= 0.57715 val_roc= 0.89784 val_ap= 0.90263 time= 0.10854\n",
      "训练次数: 8 Epoch: 0067 log_lik= 0.44588232 train_kl= 0.00864 train_loss= 0.45452 train_acc= 0.57787 val_roc= 0.89803 val_ap= 0.90233 time= 0.11053\n",
      "训练次数: 8 Epoch: 0068 log_lik= 0.44534364 train_kl= 0.00864 train_loss= 0.45398 train_acc= 0.57858 val_roc= 0.89803 val_ap= 0.90232 time= 0.11451\n",
      "训练次数: 8 Epoch: 0069 log_lik= 0.4448204 train_kl= 0.00864 train_loss= 0.45346 train_acc= 0.57962 val_roc= 0.89821 val_ap= 0.90242 time= 0.11053\n",
      "训练次数: 8 Epoch: 0070 log_lik= 0.444384 train_kl= 0.00864 train_loss= 0.45303 train_acc= 0.57994 val_roc= 0.89891 val_ap= 0.90306 time= 0.10953\n",
      "训练次数: 8 Epoch: 0071 log_lik= 0.44390786 train_kl= 0.00864 train_loss= 0.45255 train_acc= 0.58077 val_roc= 0.89929 val_ap= 0.90342 time= 0.11252\n",
      "训练次数: 8 Epoch: 0072 log_lik= 0.44342938 train_kl= 0.00865 train_loss= 0.45208 train_acc= 0.58086 val_roc= 0.89942 val_ap= 0.90374 time= 0.11750\n",
      "训练次数: 8 Epoch: 0073 log_lik= 0.44294086 train_kl= 0.00865 train_loss= 0.45159 train_acc= 0.58171 val_roc= 0.89952 val_ap= 0.90344 time= 0.11850\n",
      "训练次数: 8 Epoch: 0074 log_lik= 0.44254097 train_kl= 0.00865 train_loss= 0.45120 train_acc= 0.58157 val_roc= 0.89975 val_ap= 0.90321 time= 0.11451\n",
      "训练次数: 8 Epoch: 0075 log_lik= 0.44208062 train_kl= 0.00866 train_loss= 0.45074 train_acc= 0.58229 val_roc= 0.89991 val_ap= 0.90315 time= 0.11352\n",
      "训练次数: 8 Epoch: 0076 log_lik= 0.44176435 train_kl= 0.00866 train_loss= 0.45042 train_acc= 0.58272 val_roc= 0.90000 val_ap= 0.90328 time= 0.11352\n",
      "训练次数: 8 Epoch: 0077 log_lik= 0.4413902 train_kl= 0.00866 train_loss= 0.45005 train_acc= 0.58288 val_roc= 0.90027 val_ap= 0.90360 time= 0.11452\n",
      "训练次数: 8 Epoch: 0078 log_lik= 0.44090825 train_kl= 0.00866 train_loss= 0.44957 train_acc= 0.58299 val_roc= 0.90030 val_ap= 0.90369 time= 0.11252\n",
      "训练次数: 8 Epoch: 0079 log_lik= 0.4404991 train_kl= 0.00866 train_loss= 0.44916 train_acc= 0.58364 val_roc= 0.90025 val_ap= 0.90352 time= 0.11452\n",
      "训练次数: 8 Epoch: 0080 log_lik= 0.4400733 train_kl= 0.00866 train_loss= 0.44873 train_acc= 0.58380 val_roc= 0.90019 val_ap= 0.90339 time= 0.11152\n",
      "训练次数: 8 Epoch: 0081 log_lik= 0.4397538 train_kl= 0.00866 train_loss= 0.44841 train_acc= 0.58383 val_roc= 0.89981 val_ap= 0.90280 time= 0.11352\n",
      "训练次数: 8 Epoch: 0082 log_lik= 0.43950188 train_kl= 0.00866 train_loss= 0.44816 train_acc= 0.58404 val_roc= 0.89962 val_ap= 0.90219 time= 0.12447\n",
      "训练次数: 8 Epoch: 0083 log_lik= 0.4390346 train_kl= 0.00866 train_loss= 0.44769 train_acc= 0.58375 val_roc= 0.89983 val_ap= 0.90244 time= 0.11551\n",
      "训练次数: 8 Epoch: 0084 log_lik= 0.43878052 train_kl= 0.00866 train_loss= 0.44744 train_acc= 0.58379 val_roc= 0.90048 val_ap= 0.90299 time= 0.11152\n",
      "训练次数: 8 Epoch: 0085 log_lik= 0.4384244 train_kl= 0.00866 train_loss= 0.44709 train_acc= 0.58341 val_roc= 0.90063 val_ap= 0.90310 time= 0.11378\n",
      "训练次数: 8 Epoch: 0086 log_lik= 0.43802017 train_kl= 0.00867 train_loss= 0.44669 train_acc= 0.58324 val_roc= 0.90052 val_ap= 0.90290 time= 0.11153\n",
      "训练次数: 8 Epoch: 0087 log_lik= 0.43763384 train_kl= 0.00867 train_loss= 0.44630 train_acc= 0.58313 val_roc= 0.90058 val_ap= 0.90269 time= 0.11053\n",
      "训练次数: 8 Epoch: 0088 log_lik= 0.43723682 train_kl= 0.00867 train_loss= 0.44590 train_acc= 0.58282 val_roc= 0.90019 val_ap= 0.90209 time= 0.12049\n",
      "训练次数: 8 Epoch: 0089 log_lik= 0.43692636 train_kl= 0.00867 train_loss= 0.44560 train_acc= 0.58337 val_roc= 0.90019 val_ap= 0.90183 time= 0.11053\n",
      "训练次数: 8 Epoch: 0090 log_lik= 0.43651277 train_kl= 0.00867 train_loss= 0.44518 train_acc= 0.58374 val_roc= 0.90040 val_ap= 0.90199 time= 0.11152\n",
      "训练次数: 8 Epoch: 0091 log_lik= 0.436112 train_kl= 0.00867 train_loss= 0.44478 train_acc= 0.58408 val_roc= 0.90017 val_ap= 0.90168 time= 0.11352\n",
      "训练次数: 8 Epoch: 0092 log_lik= 0.4357238 train_kl= 0.00867 train_loss= 0.44439 train_acc= 0.58409 val_roc= 0.89990 val_ap= 0.90147 time= 0.11152\n",
      "训练次数: 8 Epoch: 0093 log_lik= 0.435369 train_kl= 0.00867 train_loss= 0.44404 train_acc= 0.58439 val_roc= 0.89969 val_ap= 0.90118 time= 0.11153\n",
      "训练次数: 8 Epoch: 0094 log_lik= 0.43490964 train_kl= 0.00867 train_loss= 0.44358 train_acc= 0.58478 val_roc= 0.89939 val_ap= 0.90087 time= 0.11750\n",
      "训练次数: 8 Epoch: 0095 log_lik= 0.43456307 train_kl= 0.00867 train_loss= 0.44323 train_acc= 0.58526 val_roc= 0.89954 val_ap= 0.90115 time= 0.11252\n",
      "训练次数: 8 Epoch: 0096 log_lik= 0.43416053 train_kl= 0.00867 train_loss= 0.44283 train_acc= 0.58567 val_roc= 0.89945 val_ap= 0.90109 time= 0.10954\n",
      "训练次数: 8 Epoch: 0097 log_lik= 0.43382576 train_kl= 0.00867 train_loss= 0.44250 train_acc= 0.58628 val_roc= 0.89941 val_ap= 0.90117 time= 0.10954\n",
      "训练次数: 8 Epoch: 0098 log_lik= 0.43342623 train_kl= 0.00868 train_loss= 0.44210 train_acc= 0.58716 val_roc= 0.89956 val_ap= 0.90145 time= 0.11153\n",
      "训练次数: 8 Epoch: 0099 log_lik= 0.43302667 train_kl= 0.00868 train_loss= 0.44171 train_acc= 0.58802 val_roc= 0.89968 val_ap= 0.90144 time= 0.11153\n",
      "训练次数: 8 Epoch: 0100 log_lik= 0.43261755 train_kl= 0.00868 train_loss= 0.44130 train_acc= 0.58851 val_roc= 0.89964 val_ap= 0.90132 time= 0.11451\n",
      "Optimization Finished!\n",
      "训练次数: 8 ROC score: 0.9234793629761386\n",
      "训练次数: 8 AP score: 0.9268655890706898\n",
      "训练次数: 9 Epoch: 0001 log_lik= 0.7815728 train_kl= 0.00806 train_loss= 0.78963 train_acc= 0.01729 val_roc= 0.65323 val_ap= 0.69206 time= 9.22974\n",
      "训练次数: 9 Epoch: 0002 log_lik= 0.9907125 train_kl= 0.00846 train_loss= 0.99917 train_acc= 0.00159 val_roc= 0.69598 val_ap= 0.72894 time= 0.12547\n",
      "训练次数: 9 Epoch: 0003 log_lik= 0.75134176 train_kl= 0.00811 train_loss= 0.75945 train_acc= 0.00198 val_roc= 0.67843 val_ap= 0.72049 time= 0.11750\n",
      "训练次数: 9 Epoch: 0004 log_lik= 0.7590639 train_kl= 0.00809 train_loss= 0.76715 train_acc= 0.00953 val_roc= 0.65703 val_ap= 0.70354 time= 0.11551\n",
      "训练次数: 9 Epoch: 0005 log_lik= 0.74863064 train_kl= 0.00814 train_loss= 0.75677 train_acc= 0.00567 val_roc= 0.65622 val_ap= 0.70258 time= 0.12248\n",
      "训练次数: 9 Epoch: 0006 log_lik= 0.75597996 train_kl= 0.00820 train_loss= 0.76418 train_acc= 0.00293 val_roc= 0.66916 val_ap= 0.71440 time= 0.11252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0007 log_lik= 0.7469995 train_kl= 0.00819 train_loss= 0.75519 train_acc= 0.00267 val_roc= 0.69187 val_ap= 0.73571 time= 0.11551\n",
      "训练次数: 9 Epoch: 0008 log_lik= 0.7365296 train_kl= 0.00816 train_loss= 0.74469 train_acc= 0.00617 val_roc= 0.72585 val_ap= 0.76299 time= 0.11651\n",
      "训练次数: 9 Epoch: 0009 log_lik= 0.73045963 train_kl= 0.00814 train_loss= 0.73860 train_acc= 0.01078 val_roc= 0.76333 val_ap= 0.79123 time= 0.12049\n",
      "训练次数: 9 Epoch: 0010 log_lik= 0.72379607 train_kl= 0.00814 train_loss= 0.73193 train_acc= 0.02558 val_roc= 0.78859 val_ap= 0.80960 time= 0.11252\n",
      "训练次数: 9 Epoch: 0011 log_lik= 0.7130917 train_kl= 0.00814 train_loss= 0.72124 train_acc= 0.04663 val_roc= 0.80365 val_ap= 0.82096 time= 0.11949\n",
      "训练次数: 9 Epoch: 0012 log_lik= 0.69990784 train_kl= 0.00816 train_loss= 0.70807 train_acc= 0.07924 val_roc= 0.80977 val_ap= 0.82449 time= 0.11850\n",
      "训练次数: 9 Epoch: 0013 log_lik= 0.684048 train_kl= 0.00818 train_loss= 0.69223 train_acc= 0.11384 val_roc= 0.81070 val_ap= 0.82303 time= 0.12049\n",
      "训练次数: 9 Epoch: 0014 log_lik= 0.6679803 train_kl= 0.00822 train_loss= 0.67620 train_acc= 0.15249 val_roc= 0.81213 val_ap= 0.82169 time= 0.11352\n",
      "训练次数: 9 Epoch: 0015 log_lik= 0.65366673 train_kl= 0.00825 train_loss= 0.66192 train_acc= 0.19400 val_roc= 0.81784 val_ap= 0.82447 time= 0.11849\n",
      "训练次数: 9 Epoch: 0016 log_lik= 0.6395907 train_kl= 0.00829 train_loss= 0.64788 train_acc= 0.23828 val_roc= 0.82696 val_ap= 0.83021 time= 0.11551\n",
      "训练次数: 9 Epoch: 0017 log_lik= 0.62500894 train_kl= 0.00832 train_loss= 0.63333 train_acc= 0.29009 val_roc= 0.83491 val_ap= 0.83523 time= 0.11252\n",
      "训练次数: 9 Epoch: 0018 log_lik= 0.61115706 train_kl= 0.00835 train_loss= 0.61951 train_acc= 0.34095 val_roc= 0.83999 val_ap= 0.83908 time= 0.11551\n",
      "训练次数: 9 Epoch: 0019 log_lik= 0.5991045 train_kl= 0.00838 train_loss= 0.60749 train_acc= 0.38413 val_roc= 0.84299 val_ap= 0.84227 time= 0.11153\n",
      "训练次数: 9 Epoch: 0020 log_lik= 0.5872953 train_kl= 0.00840 train_loss= 0.59570 train_acc= 0.41724 val_roc= 0.84632 val_ap= 0.84604 time= 0.12248\n",
      "训练次数: 9 Epoch: 0021 log_lik= 0.5753184 train_kl= 0.00842 train_loss= 0.58374 train_acc= 0.44406 val_roc= 0.85048 val_ap= 0.84974 time= 0.11252\n",
      "训练次数: 9 Epoch: 0022 log_lik= 0.56551665 train_kl= 0.00844 train_loss= 0.57396 train_acc= 0.46359 val_roc= 0.85585 val_ap= 0.85432 time= 0.11252\n",
      "训练次数: 9 Epoch: 0023 log_lik= 0.55588317 train_kl= 0.00846 train_loss= 0.56434 train_acc= 0.47880 val_roc= 0.86166 val_ap= 0.85910 time= 0.11053\n",
      "训练次数: 9 Epoch: 0024 log_lik= 0.54716796 train_kl= 0.00847 train_loss= 0.55564 train_acc= 0.49131 val_roc= 0.86706 val_ap= 0.86360 time= 0.11850\n",
      "训练次数: 9 Epoch: 0025 log_lik= 0.54027987 train_kl= 0.00849 train_loss= 0.54877 train_acc= 0.50120 val_roc= 0.87145 val_ap= 0.86692 time= 0.11252\n",
      "训练次数: 9 Epoch: 0026 log_lik= 0.5357781 train_kl= 0.00850 train_loss= 0.54428 train_acc= 0.50837 val_roc= 0.87454 val_ap= 0.86880 time= 0.11352\n",
      "训练次数: 9 Epoch: 0027 log_lik= 0.5322551 train_kl= 0.00851 train_loss= 0.54076 train_acc= 0.51447 val_roc= 0.87636 val_ap= 0.86945 time= 0.11451\n",
      "训练次数: 9 Epoch: 0028 log_lik= 0.5291405 train_kl= 0.00851 train_loss= 0.53765 train_acc= 0.51869 val_roc= 0.87785 val_ap= 0.86972 time= 0.11750\n",
      "训练次数: 9 Epoch: 0029 log_lik= 0.5272885 train_kl= 0.00851 train_loss= 0.53580 train_acc= 0.52052 val_roc= 0.87844 val_ap= 0.86965 time= 0.11352\n",
      "训练次数: 9 Epoch: 0030 log_lik= 0.52553487 train_kl= 0.00851 train_loss= 0.53404 train_acc= 0.52227 val_roc= 0.87927 val_ap= 0.86980 time= 0.11451\n",
      "训练次数: 9 Epoch: 0031 log_lik= 0.5242229 train_kl= 0.00850 train_loss= 0.53273 train_acc= 0.52459 val_roc= 0.87948 val_ap= 0.87021 time= 0.11850\n",
      "训练次数: 9 Epoch: 0032 log_lik= 0.5227679 train_kl= 0.00850 train_loss= 0.53127 train_acc= 0.52674 val_roc= 0.87954 val_ap= 0.87017 time= 0.10953\n",
      "训练次数: 9 Epoch: 0033 log_lik= 0.521082 train_kl= 0.00849 train_loss= 0.52958 train_acc= 0.52870 val_roc= 0.87922 val_ap= 0.87018 time= 0.11551\n",
      "训练次数: 9 Epoch: 0034 log_lik= 0.5195664 train_kl= 0.00849 train_loss= 0.52806 train_acc= 0.53071 val_roc= 0.87867 val_ap= 0.86961 time= 0.11551\n",
      "训练次数: 9 Epoch: 0035 log_lik= 0.51812905 train_kl= 0.00849 train_loss= 0.52662 train_acc= 0.53181 val_roc= 0.87879 val_ap= 0.87161 time= 0.11053\n",
      "训练次数: 9 Epoch: 0036 log_lik= 0.51673204 train_kl= 0.00849 train_loss= 0.52522 train_acc= 0.53276 val_roc= 0.87901 val_ap= 0.87277 time= 0.11114\n",
      "训练次数: 9 Epoch: 0037 log_lik= 0.51503766 train_kl= 0.00849 train_loss= 0.52353 train_acc= 0.53293 val_roc= 0.87899 val_ap= 0.87340 time= 0.11252\n",
      "训练次数: 9 Epoch: 0038 log_lik= 0.5136933 train_kl= 0.00849 train_loss= 0.52219 train_acc= 0.53246 val_roc= 0.87934 val_ap= 0.87463 time= 0.11451\n",
      "训练次数: 9 Epoch: 0039 log_lik= 0.5118587 train_kl= 0.00850 train_loss= 0.52036 train_acc= 0.53385 val_roc= 0.87912 val_ap= 0.87443 time= 0.11352\n",
      "训练次数: 9 Epoch: 0040 log_lik= 0.5104326 train_kl= 0.00850 train_loss= 0.51893 train_acc= 0.53439 val_roc= 0.87960 val_ap= 0.87629 time= 0.11949\n",
      "训练次数: 9 Epoch: 0041 log_lik= 0.50917643 train_kl= 0.00850 train_loss= 0.51768 train_acc= 0.53550 val_roc= 0.87935 val_ap= 0.87654 time= 0.11252\n",
      "训练次数: 9 Epoch: 0042 log_lik= 0.507643 train_kl= 0.00850 train_loss= 0.51615 train_acc= 0.53602 val_roc= 0.87930 val_ap= 0.87680 time= 0.11802\n",
      "训练次数: 9 Epoch: 0043 log_lik= 0.5063424 train_kl= 0.00850 train_loss= 0.51484 train_acc= 0.53625 val_roc= 0.87967 val_ap= 0.87792 time= 0.11850\n",
      "训练次数: 9 Epoch: 0044 log_lik= 0.5049103 train_kl= 0.00850 train_loss= 0.51341 train_acc= 0.53647 val_roc= 0.87989 val_ap= 0.87883 time= 0.12049\n",
      "训练次数: 9 Epoch: 0045 log_lik= 0.503742 train_kl= 0.00850 train_loss= 0.51224 train_acc= 0.53594 val_roc= 0.88003 val_ap= 0.87946 time= 0.11451\n",
      "训练次数: 9 Epoch: 0046 log_lik= 0.50245076 train_kl= 0.00850 train_loss= 0.51095 train_acc= 0.53639 val_roc= 0.87989 val_ap= 0.88036 time= 0.11352\n",
      "训练次数: 9 Epoch: 0047 log_lik= 0.5014056 train_kl= 0.00850 train_loss= 0.50990 train_acc= 0.53621 val_roc= 0.87967 val_ap= 0.88133 time= 0.11949\n",
      "训练次数: 9 Epoch: 0048 log_lik= 0.50031114 train_kl= 0.00850 train_loss= 0.50881 train_acc= 0.53674 val_roc= 0.87928 val_ap= 0.88169 time= 0.11451\n",
      "训练次数: 9 Epoch: 0049 log_lik= 0.49936613 train_kl= 0.00850 train_loss= 0.50786 train_acc= 0.53693 val_roc= 0.87918 val_ap= 0.88232 time= 0.11252\n",
      "训练次数: 9 Epoch: 0050 log_lik= 0.49856257 train_kl= 0.00850 train_loss= 0.50706 train_acc= 0.53709 val_roc= 0.87915 val_ap= 0.88247 time= 0.11355\n",
      "训练次数: 9 Epoch: 0051 log_lik= 0.49774882 train_kl= 0.00850 train_loss= 0.50624 train_acc= 0.53689 val_roc= 0.87931 val_ap= 0.88339 time= 0.11352\n",
      "训练次数: 9 Epoch: 0052 log_lik= 0.49700925 train_kl= 0.00850 train_loss= 0.50551 train_acc= 0.53678 val_roc= 0.87967 val_ap= 0.88461 time= 0.11352\n",
      "训练次数: 9 Epoch: 0053 log_lik= 0.49623314 train_kl= 0.00850 train_loss= 0.50473 train_acc= 0.53609 val_roc= 0.87944 val_ap= 0.88500 time= 0.12149\n",
      "训练次数: 9 Epoch: 0054 log_lik= 0.49539986 train_kl= 0.00850 train_loss= 0.50390 train_acc= 0.53619 val_roc= 0.87934 val_ap= 0.88544 time= 0.11352\n",
      "训练次数: 9 Epoch: 0055 log_lik= 0.49459806 train_kl= 0.00850 train_loss= 0.50310 train_acc= 0.53564 val_roc= 0.87980 val_ap= 0.88668 time= 0.11451\n",
      "训练次数: 9 Epoch: 0056 log_lik= 0.49386188 train_kl= 0.00850 train_loss= 0.50237 train_acc= 0.53537 val_roc= 0.87961 val_ap= 0.88696 time= 0.12049\n",
      "训练次数: 9 Epoch: 0057 log_lik= 0.4931869 train_kl= 0.00850 train_loss= 0.50169 train_acc= 0.53548 val_roc= 0.87985 val_ap= 0.88723 time= 0.11452\n",
      "训练次数: 9 Epoch: 0058 log_lik= 0.49244767 train_kl= 0.00850 train_loss= 0.50095 train_acc= 0.53530 val_roc= 0.87982 val_ap= 0.88712 time= 0.11352\n",
      "训练次数: 9 Epoch: 0059 log_lik= 0.49183896 train_kl= 0.00850 train_loss= 0.50034 train_acc= 0.53517 val_roc= 0.87956 val_ap= 0.88647 time= 0.11451\n",
      "训练次数: 9 Epoch: 0060 log_lik= 0.4911816 train_kl= 0.00850 train_loss= 0.49968 train_acc= 0.53557 val_roc= 0.87931 val_ap= 0.88637 time= 0.11352\n",
      "训练次数: 9 Epoch: 0061 log_lik= 0.4904525 train_kl= 0.00850 train_loss= 0.49896 train_acc= 0.53590 val_roc= 0.87931 val_ap= 0.88660 time= 0.11252\n",
      "训练次数: 9 Epoch: 0062 log_lik= 0.48971283 train_kl= 0.00851 train_loss= 0.49822 train_acc= 0.53546 val_roc= 0.87925 val_ap= 0.88682 time= 0.11452\n",
      "训练次数: 9 Epoch: 0063 log_lik= 0.48912156 train_kl= 0.00851 train_loss= 0.49763 train_acc= 0.53580 val_roc= 0.87924 val_ap= 0.88693 time= 0.11153\n",
      "训练次数: 9 Epoch: 0064 log_lik= 0.4883591 train_kl= 0.00851 train_loss= 0.49687 train_acc= 0.53564 val_roc= 0.87958 val_ap= 0.88712 time= 0.12148\n",
      "训练次数: 9 Epoch: 0065 log_lik= 0.4876715 train_kl= 0.00851 train_loss= 0.49618 train_acc= 0.53583 val_roc= 0.87957 val_ap= 0.88678 time= 0.11152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0066 log_lik= 0.48686415 train_kl= 0.00851 train_loss= 0.49538 train_acc= 0.53591 val_roc= 0.87985 val_ap= 0.88740 time= 0.11551\n",
      "训练次数: 9 Epoch: 0067 log_lik= 0.48596963 train_kl= 0.00851 train_loss= 0.49448 train_acc= 0.53578 val_roc= 0.88015 val_ap= 0.88779 time= 0.11650\n",
      "训练次数: 9 Epoch: 0068 log_lik= 0.4852119 train_kl= 0.00852 train_loss= 0.49373 train_acc= 0.53605 val_roc= 0.88089 val_ap= 0.88876 time= 0.11951\n",
      "训练次数: 9 Epoch: 0069 log_lik= 0.4844726 train_kl= 0.00852 train_loss= 0.49299 train_acc= 0.53578 val_roc= 0.88162 val_ap= 0.88947 time= 0.11451\n",
      "训练次数: 9 Epoch: 0070 log_lik= 0.4834563 train_kl= 0.00852 train_loss= 0.49198 train_acc= 0.53608 val_roc= 0.88236 val_ap= 0.89040 time= 0.11901\n",
      "训练次数: 9 Epoch: 0071 log_lik= 0.48270586 train_kl= 0.00852 train_loss= 0.49123 train_acc= 0.53553 val_roc= 0.88284 val_ap= 0.89115 time= 0.11352\n",
      "训练次数: 9 Epoch: 0072 log_lik= 0.4816903 train_kl= 0.00852 train_loss= 0.49021 train_acc= 0.53568 val_roc= 0.88381 val_ap= 0.89232 time= 0.11607\n",
      "训练次数: 9 Epoch: 0073 log_lik= 0.4807267 train_kl= 0.00853 train_loss= 0.48925 train_acc= 0.53609 val_roc= 0.88446 val_ap= 0.89313 time= 0.11153\n",
      "训练次数: 9 Epoch: 0074 log_lik= 0.47968152 train_kl= 0.00853 train_loss= 0.48821 train_acc= 0.53599 val_roc= 0.88522 val_ap= 0.89381 time= 0.11516\n",
      "训练次数: 9 Epoch: 0075 log_lik= 0.478655 train_kl= 0.00853 train_loss= 0.48719 train_acc= 0.53712 val_roc= 0.88593 val_ap= 0.89427 time= 0.11650\n",
      "训练次数: 9 Epoch: 0076 log_lik= 0.47756913 train_kl= 0.00854 train_loss= 0.48611 train_acc= 0.53698 val_roc= 0.88645 val_ap= 0.89503 time= 0.11303\n",
      "训练次数: 9 Epoch: 0077 log_lik= 0.4765466 train_kl= 0.00854 train_loss= 0.48509 train_acc= 0.53763 val_roc= 0.88676 val_ap= 0.89539 time= 0.11651\n",
      "训练次数: 9 Epoch: 0078 log_lik= 0.47552526 train_kl= 0.00854 train_loss= 0.48407 train_acc= 0.53861 val_roc= 0.88687 val_ap= 0.89563 time= 0.11650\n",
      "训练次数: 9 Epoch: 0079 log_lik= 0.47453544 train_kl= 0.00855 train_loss= 0.48308 train_acc= 0.53851 val_roc= 0.88710 val_ap= 0.89581 time= 0.11153\n",
      "训练次数: 9 Epoch: 0080 log_lik= 0.47354868 train_kl= 0.00855 train_loss= 0.48210 train_acc= 0.53970 val_roc= 0.88772 val_ap= 0.89641 time= 0.11451\n",
      "训练次数: 9 Epoch: 0081 log_lik= 0.47249117 train_kl= 0.00856 train_loss= 0.48105 train_acc= 0.53986 val_roc= 0.88778 val_ap= 0.89636 time= 0.11352\n",
      "训练次数: 9 Epoch: 0082 log_lik= 0.47162965 train_kl= 0.00856 train_loss= 0.48019 train_acc= 0.54075 val_roc= 0.88732 val_ap= 0.89549 time= 0.11352\n",
      "训练次数: 9 Epoch: 0083 log_lik= 0.4707804 train_kl= 0.00857 train_loss= 0.47935 train_acc= 0.54107 val_roc= 0.88706 val_ap= 0.89476 time= 0.11352\n",
      "训练次数: 9 Epoch: 0084 log_lik= 0.46988362 train_kl= 0.00857 train_loss= 0.47846 train_acc= 0.54154 val_roc= 0.88722 val_ap= 0.89465 time= 0.11153\n",
      "训练次数: 9 Epoch: 0085 log_lik= 0.46905118 train_kl= 0.00858 train_loss= 0.47763 train_acc= 0.54239 val_roc= 0.88691 val_ap= 0.89412 time= 0.11571\n",
      "训练次数: 9 Epoch: 0086 log_lik= 0.4682073 train_kl= 0.00858 train_loss= 0.47679 train_acc= 0.54304 val_roc= 0.88677 val_ap= 0.89318 time= 0.11750\n",
      "训练次数: 9 Epoch: 0087 log_lik= 0.46724817 train_kl= 0.00858 train_loss= 0.47583 train_acc= 0.54408 val_roc= 0.88631 val_ap= 0.89244 time= 0.11053\n",
      "训练次数: 9 Epoch: 0088 log_lik= 0.46649706 train_kl= 0.00858 train_loss= 0.47508 train_acc= 0.54462 val_roc= 0.88600 val_ap= 0.89163 time= 0.11273\n",
      "训练次数: 9 Epoch: 0089 log_lik= 0.46567747 train_kl= 0.00859 train_loss= 0.47426 train_acc= 0.54530 val_roc= 0.88634 val_ap= 0.89132 time= 0.11352\n",
      "训练次数: 9 Epoch: 0090 log_lik= 0.46481752 train_kl= 0.00859 train_loss= 0.47341 train_acc= 0.54616 val_roc= 0.88670 val_ap= 0.89138 time= 0.12257\n",
      "训练次数: 9 Epoch: 0091 log_lik= 0.46399835 train_kl= 0.00859 train_loss= 0.47259 train_acc= 0.54688 val_roc= 0.88697 val_ap= 0.89132 time= 0.11441\n",
      "训练次数: 9 Epoch: 0092 log_lik= 0.46313545 train_kl= 0.00860 train_loss= 0.47173 train_acc= 0.54761 val_roc= 0.88772 val_ap= 0.89164 time= 0.11551\n",
      "训练次数: 9 Epoch: 0093 log_lik= 0.46243373 train_kl= 0.00860 train_loss= 0.47103 train_acc= 0.54835 val_roc= 0.88803 val_ap= 0.89167 time= 0.11152\n",
      "训练次数: 9 Epoch: 0094 log_lik= 0.46168762 train_kl= 0.00860 train_loss= 0.47029 train_acc= 0.54901 val_roc= 0.88908 val_ap= 0.89271 time= 0.15561\n",
      "训练次数: 9 Epoch: 0095 log_lik= 0.4610431 train_kl= 0.00860 train_loss= 0.46965 train_acc= 0.54986 val_roc= 0.88929 val_ap= 0.89303 time= 0.14040\n",
      "训练次数: 9 Epoch: 0096 log_lik= 0.4603816 train_kl= 0.00860 train_loss= 0.46899 train_acc= 0.55037 val_roc= 0.88979 val_ap= 0.89364 time= 0.12148\n",
      "训练次数: 9 Epoch: 0097 log_lik= 0.45962763 train_kl= 0.00860 train_loss= 0.46823 train_acc= 0.55119 val_roc= 0.89024 val_ap= 0.89435 time= 0.12945\n",
      "训练次数: 9 Epoch: 0098 log_lik= 0.4589905 train_kl= 0.00860 train_loss= 0.46759 train_acc= 0.55138 val_roc= 0.89070 val_ap= 0.89541 time= 0.12276\n",
      "训练次数: 9 Epoch: 0099 log_lik= 0.45839015 train_kl= 0.00860 train_loss= 0.46699 train_acc= 0.55194 val_roc= 0.89131 val_ap= 0.89640 time= 0.11153\n",
      "训练次数: 9 Epoch: 0100 log_lik= 0.4577431 train_kl= 0.00860 train_loss= 0.46635 train_acc= 0.55205 val_roc= 0.89215 val_ap= 0.89767 time= 0.11551\n",
      "Optimization Finished!\n",
      "训练次数: 9 ROC score: 0.9113776379132177\n",
      "训练次数: 9 AP score: 0.9125213314168091\n",
      "训练次数: 10 Epoch: 0001 log_lik= 0.77832675 train_kl= 0.00806 train_loss= 0.78638 train_acc= 0.04201 val_roc= 0.68356 val_ap= 0.71362 time= 10.53856\n",
      "训练次数: 10 Epoch: 0002 log_lik= 0.9079682 train_kl= 0.00840 train_loss= 0.91637 train_acc= 0.00159 val_roc= 0.76041 val_ap= 0.76925 time= 0.12746\n",
      "训练次数: 10 Epoch: 0003 log_lik= 0.744195 train_kl= 0.00810 train_loss= 0.75230 train_acc= 0.01593 val_roc= 0.77123 val_ap= 0.76880 time= 0.11750\n",
      "训练次数: 10 Epoch: 0004 log_lik= 0.7549796 train_kl= 0.00808 train_loss= 0.76306 train_acc= 0.07531 val_roc= 0.70627 val_ap= 0.71850 time= 0.11451\n",
      "训练次数: 10 Epoch: 0005 log_lik= 0.7352644 train_kl= 0.00814 train_loss= 0.74340 train_acc= 0.00533 val_roc= 0.69670 val_ap= 0.71432 time= 0.11753\n",
      "训练次数: 10 Epoch: 0006 log_lik= 0.7508965 train_kl= 0.00825 train_loss= 0.75915 train_acc= 0.00288 val_roc= 0.74168 val_ap= 0.75037 time= 0.11750\n",
      "训练次数: 10 Epoch: 0007 log_lik= 0.7189173 train_kl= 0.00821 train_loss= 0.72712 train_acc= 0.00513 val_roc= 0.78219 val_ap= 0.78366 time= 0.11153\n",
      "训练次数: 10 Epoch: 0008 log_lik= 0.7024343 train_kl= 0.00817 train_loss= 0.71061 train_acc= 0.03395 val_roc= 0.79904 val_ap= 0.80005 time= 0.11451\n",
      "训练次数: 10 Epoch: 0009 log_lik= 0.68907946 train_kl= 0.00817 train_loss= 0.69725 train_acc= 0.10156 val_roc= 0.80452 val_ap= 0.80613 time= 0.11949\n",
      "训练次数: 10 Epoch: 0010 log_lik= 0.6708335 train_kl= 0.00819 train_loss= 0.67902 train_acc= 0.16025 val_roc= 0.81143 val_ap= 0.81297 time= 0.11949\n",
      "训练次数: 10 Epoch: 0011 log_lik= 0.6514955 train_kl= 0.00823 train_loss= 0.65973 train_acc= 0.19882 val_roc= 0.82311 val_ap= 0.82470 time= 0.11551\n",
      "训练次数: 10 Epoch: 0012 log_lik= 0.63118905 train_kl= 0.00828 train_loss= 0.63947 train_acc= 0.23431 val_roc= 0.83575 val_ap= 0.83489 time= 0.12348\n",
      "训练次数: 10 Epoch: 0013 log_lik= 0.60952854 train_kl= 0.00832 train_loss= 0.61785 train_acc= 0.28029 val_roc= 0.83986 val_ap= 0.83695 time= 0.11654\n",
      "训练次数: 10 Epoch: 0014 log_lik= 0.5950739 train_kl= 0.00837 train_loss= 0.60344 train_acc= 0.32427 val_roc= 0.83895 val_ap= 0.83421 time= 0.11650\n",
      "训练次数: 10 Epoch: 0015 log_lik= 0.58713186 train_kl= 0.00841 train_loss= 0.59555 train_acc= 0.36452 val_roc= 0.84268 val_ap= 0.83912 time= 0.11757\n",
      "训练次数: 10 Epoch: 0016 log_lik= 0.57528204 train_kl= 0.00845 train_loss= 0.58374 train_acc= 0.40769 val_roc= 0.85001 val_ap= 0.84864 time= 0.11554\n",
      "训练次数: 10 Epoch: 0017 log_lik= 0.56046236 train_kl= 0.00848 train_loss= 0.56895 train_acc= 0.44374 val_roc= 0.85738 val_ap= 0.85805 time= 0.11551\n",
      "训练次数: 10 Epoch: 0018 log_lik= 0.5482123 train_kl= 0.00851 train_loss= 0.55672 train_acc= 0.46317 val_roc= 0.86485 val_ap= 0.86645 time= 0.11850\n",
      "训练次数: 10 Epoch: 0019 log_lik= 0.5370999 train_kl= 0.00852 train_loss= 0.54562 train_acc= 0.47531 val_roc= 0.87234 val_ap= 0.87449 time= 0.12248\n",
      "训练次数: 10 Epoch: 0020 log_lik= 0.5258326 train_kl= 0.00853 train_loss= 0.53436 train_acc= 0.48473 val_roc= 0.87950 val_ap= 0.88233 time= 0.13443\n",
      "训练次数: 10 Epoch: 0021 log_lik= 0.51489383 train_kl= 0.00854 train_loss= 0.52343 train_acc= 0.49606 val_roc= 0.88521 val_ap= 0.88964 time= 0.14936\n",
      "训练次数: 10 Epoch: 0022 log_lik= 0.50612766 train_kl= 0.00854 train_loss= 0.51467 train_acc= 0.50855 val_roc= 0.88894 val_ap= 0.89389 time= 0.13741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0023 log_lik= 0.50095654 train_kl= 0.00855 train_loss= 0.50951 train_acc= 0.51832 val_roc= 0.89307 val_ap= 0.89792 time= 0.12348\n",
      "训练次数: 10 Epoch: 0024 log_lik= 0.49899793 train_kl= 0.00857 train_loss= 0.50756 train_acc= 0.52412 val_roc= 0.89628 val_ap= 0.90105 time= 0.11318\n",
      "训练次数: 10 Epoch: 0025 log_lik= 0.4973944 train_kl= 0.00858 train_loss= 0.50597 train_acc= 0.52706 val_roc= 0.89883 val_ap= 0.90314 time= 0.11651\n",
      "训练次数: 10 Epoch: 0026 log_lik= 0.49593475 train_kl= 0.00859 train_loss= 0.50452 train_acc= 0.52825 val_roc= 0.90022 val_ap= 0.90414 time= 0.11949\n",
      "训练次数: 10 Epoch: 0027 log_lik= 0.49501893 train_kl= 0.00859 train_loss= 0.50361 train_acc= 0.52840 val_roc= 0.90150 val_ap= 0.90570 time= 0.12348\n",
      "训练次数: 10 Epoch: 0028 log_lik= 0.49355695 train_kl= 0.00859 train_loss= 0.50214 train_acc= 0.52825 val_roc= 0.90243 val_ap= 0.90663 time= 0.12347\n",
      "训练次数: 10 Epoch: 0029 log_lik= 0.49079826 train_kl= 0.00857 train_loss= 0.49937 train_acc= 0.53050 val_roc= 0.90350 val_ap= 0.90798 time= 0.14239\n",
      "训练次数: 10 Epoch: 0030 log_lik= 0.48730117 train_kl= 0.00856 train_loss= 0.49586 train_acc= 0.53408 val_roc= 0.90412 val_ap= 0.90812 time= 0.12746\n",
      "训练次数: 10 Epoch: 0031 log_lik= 0.48430154 train_kl= 0.00855 train_loss= 0.49285 train_acc= 0.53767 val_roc= 0.90428 val_ap= 0.90839 time= 0.12547\n",
      "训练次数: 10 Epoch: 0032 log_lik= 0.48210976 train_kl= 0.00854 train_loss= 0.49065 train_acc= 0.53986 val_roc= 0.90522 val_ap= 0.90934 time= 0.13244\n",
      "训练次数: 10 Epoch: 0033 log_lik= 0.48009187 train_kl= 0.00854 train_loss= 0.48863 train_acc= 0.54073 val_roc= 0.90539 val_ap= 0.90981 time= 0.13443\n",
      "训练次数: 10 Epoch: 0034 log_lik= 0.47751004 train_kl= 0.00855 train_loss= 0.48606 train_acc= 0.54131 val_roc= 0.90594 val_ap= 0.91068 time= 0.12248\n",
      "训练次数: 10 Epoch: 0035 log_lik= 0.47467113 train_kl= 0.00856 train_loss= 0.48323 train_acc= 0.54121 val_roc= 0.90707 val_ap= 0.91127 time= 0.11750\n",
      "训练次数: 10 Epoch: 0036 log_lik= 0.4722624 train_kl= 0.00857 train_loss= 0.48083 train_acc= 0.54044 val_roc= 0.90783 val_ap= 0.91175 time= 0.14140\n",
      "训练次数: 10 Epoch: 0037 log_lik= 0.47060356 train_kl= 0.00859 train_loss= 0.47919 train_acc= 0.53965 val_roc= 0.90929 val_ap= 0.91297 time= 0.12945\n",
      "训练次数: 10 Epoch: 0038 log_lik= 0.46911663 train_kl= 0.00860 train_loss= 0.47772 train_acc= 0.53969 val_roc= 0.91068 val_ap= 0.91365 time= 0.14102\n",
      "训练次数: 10 Epoch: 0039 log_lik= 0.4677035 train_kl= 0.00861 train_loss= 0.47632 train_acc= 0.54054 val_roc= 0.91213 val_ap= 0.91503 time= 0.12248\n",
      "训练次数: 10 Epoch: 0040 log_lik= 0.46617085 train_kl= 0.00862 train_loss= 0.47479 train_acc= 0.54081 val_roc= 0.91242 val_ap= 0.91477 time= 0.11850\n",
      "训练次数: 10 Epoch: 0041 log_lik= 0.46486744 train_kl= 0.00862 train_loss= 0.47349 train_acc= 0.54086 val_roc= 0.91321 val_ap= 0.91531 time= 0.11949\n",
      "训练次数: 10 Epoch: 0042 log_lik= 0.46360084 train_kl= 0.00862 train_loss= 0.47222 train_acc= 0.54170 val_roc= 0.91360 val_ap= 0.91532 time= 0.14737\n",
      "训练次数: 10 Epoch: 0043 log_lik= 0.46250188 train_kl= 0.00861 train_loss= 0.47111 train_acc= 0.54242 val_roc= 0.91421 val_ap= 0.91568 time= 0.11850\n",
      "训练次数: 10 Epoch: 0044 log_lik= 0.461112 train_kl= 0.00860 train_loss= 0.46972 train_acc= 0.54373 val_roc= 0.91514 val_ap= 0.91659 time= 0.11153\n",
      "训练次数: 10 Epoch: 0045 log_lik= 0.45975953 train_kl= 0.00860 train_loss= 0.46836 train_acc= 0.54500 val_roc= 0.91609 val_ap= 0.91749 time= 0.11551\n",
      "训练次数: 10 Epoch: 0046 log_lik= 0.45862123 train_kl= 0.00860 train_loss= 0.46722 train_acc= 0.54722 val_roc= 0.91704 val_ap= 0.91842 time= 0.11053\n",
      "训练次数: 10 Epoch: 0047 log_lik= 0.45754465 train_kl= 0.00860 train_loss= 0.46614 train_acc= 0.54906 val_roc= 0.91717 val_ap= 0.91870 time= 0.11650\n",
      "训练次数: 10 Epoch: 0048 log_lik= 0.4565574 train_kl= 0.00860 train_loss= 0.46516 train_acc= 0.55083 val_roc= 0.91788 val_ap= 0.91942 time= 0.11551\n",
      "训练次数: 10 Epoch: 0049 log_lik= 0.45532873 train_kl= 0.00860 train_loss= 0.46393 train_acc= 0.55283 val_roc= 0.91842 val_ap= 0.92034 time= 0.13458\n",
      "训练次数: 10 Epoch: 0050 log_lik= 0.4541132 train_kl= 0.00861 train_loss= 0.46272 train_acc= 0.55399 val_roc= 0.91885 val_ap= 0.92102 time= 0.13343\n",
      "训练次数: 10 Epoch: 0051 log_lik= 0.45302632 train_kl= 0.00862 train_loss= 0.46165 train_acc= 0.55505 val_roc= 0.91967 val_ap= 0.92192 time= 0.13045\n",
      "训练次数: 10 Epoch: 0052 log_lik= 0.45207745 train_kl= 0.00863 train_loss= 0.46071 train_acc= 0.55657 val_roc= 0.92034 val_ap= 0.92258 time= 0.11651\n",
      "训练次数: 10 Epoch: 0053 log_lik= 0.45110637 train_kl= 0.00864 train_loss= 0.45974 train_acc= 0.55779 val_roc= 0.92098 val_ap= 0.92315 time= 0.11650\n",
      "训练次数: 10 Epoch: 0054 log_lik= 0.4502026 train_kl= 0.00864 train_loss= 0.45885 train_acc= 0.55965 val_roc= 0.92138 val_ap= 0.92334 time= 0.11651\n",
      "训练次数: 10 Epoch: 0055 log_lik= 0.44934526 train_kl= 0.00865 train_loss= 0.45799 train_acc= 0.55981 val_roc= 0.92222 val_ap= 0.92427 time= 0.13044\n",
      "训练次数: 10 Epoch: 0056 log_lik= 0.44862264 train_kl= 0.00865 train_loss= 0.45727 train_acc= 0.56110 val_roc= 0.92290 val_ap= 0.92488 time= 0.11651\n",
      "训练次数: 10 Epoch: 0057 log_lik= 0.44787532 train_kl= 0.00865 train_loss= 0.45653 train_acc= 0.56188 val_roc= 0.92343 val_ap= 0.92538 time= 0.11451\n",
      "训练次数: 10 Epoch: 0058 log_lik= 0.447223 train_kl= 0.00865 train_loss= 0.45587 train_acc= 0.56290 val_roc= 0.92429 val_ap= 0.92621 time= 0.12049\n",
      "训练次数: 10 Epoch: 0059 log_lik= 0.4464828 train_kl= 0.00865 train_loss= 0.45513 train_acc= 0.56357 val_roc= 0.92482 val_ap= 0.92662 time= 0.12447\n",
      "训练次数: 10 Epoch: 0060 log_lik= 0.4459011 train_kl= 0.00865 train_loss= 0.45455 train_acc= 0.56452 val_roc= 0.92510 val_ap= 0.92671 time= 0.11453\n",
      "训练次数: 10 Epoch: 0061 log_lik= 0.4452869 train_kl= 0.00865 train_loss= 0.45393 train_acc= 0.56502 val_roc= 0.92518 val_ap= 0.92677 time= 0.11551\n",
      "训练次数: 10 Epoch: 0062 log_lik= 0.44475666 train_kl= 0.00865 train_loss= 0.45340 train_acc= 0.56568 val_roc= 0.92523 val_ap= 0.92671 time= 0.11252\n",
      "训练次数: 10 Epoch: 0063 log_lik= 0.4440915 train_kl= 0.00865 train_loss= 0.45274 train_acc= 0.56616 val_roc= 0.92534 val_ap= 0.92675 time= 0.12547\n",
      "训练次数: 10 Epoch: 0064 log_lik= 0.4435391 train_kl= 0.00865 train_loss= 0.45219 train_acc= 0.56730 val_roc= 0.92531 val_ap= 0.92678 time= 0.13896\n",
      "训练次数: 10 Epoch: 0065 log_lik= 0.4429004 train_kl= 0.00865 train_loss= 0.45155 train_acc= 0.56753 val_roc= 0.92547 val_ap= 0.92650 time= 0.13244\n",
      "训练次数: 10 Epoch: 0066 log_lik= 0.4422754 train_kl= 0.00865 train_loss= 0.45093 train_acc= 0.56831 val_roc= 0.92511 val_ap= 0.92633 time= 0.13244\n",
      "训练次数: 10 Epoch: 0067 log_lik= 0.44176915 train_kl= 0.00866 train_loss= 0.45043 train_acc= 0.56862 val_roc= 0.92498 val_ap= 0.92610 time= 0.11949\n",
      "训练次数: 10 Epoch: 0068 log_lik= 0.44118494 train_kl= 0.00866 train_loss= 0.44985 train_acc= 0.56944 val_roc= 0.92497 val_ap= 0.92611 time= 0.11352\n",
      "训练次数: 10 Epoch: 0069 log_lik= 0.44069502 train_kl= 0.00866 train_loss= 0.44936 train_acc= 0.57007 val_roc= 0.92458 val_ap= 0.92575 time= 0.11451\n",
      "训练次数: 10 Epoch: 0070 log_lik= 0.44019535 train_kl= 0.00866 train_loss= 0.44886 train_acc= 0.57013 val_roc= 0.92453 val_ap= 0.92562 time= 0.11451\n",
      "训练次数: 10 Epoch: 0071 log_lik= 0.43958423 train_kl= 0.00867 train_loss= 0.44825 train_acc= 0.57089 val_roc= 0.92446 val_ap= 0.92560 time= 0.11506\n",
      "训练次数: 10 Epoch: 0072 log_lik= 0.4391211 train_kl= 0.00867 train_loss= 0.44779 train_acc= 0.57164 val_roc= 0.92452 val_ap= 0.92551 time= 0.12248\n",
      "训练次数: 10 Epoch: 0073 log_lik= 0.4386862 train_kl= 0.00867 train_loss= 0.44735 train_acc= 0.57239 val_roc= 0.92433 val_ap= 0.92542 time= 0.11949\n",
      "训练次数: 10 Epoch: 0074 log_lik= 0.43821952 train_kl= 0.00867 train_loss= 0.44689 train_acc= 0.57301 val_roc= 0.92440 val_ap= 0.92549 time= 0.12646\n",
      "训练次数: 10 Epoch: 0075 log_lik= 0.43772456 train_kl= 0.00867 train_loss= 0.44639 train_acc= 0.57389 val_roc= 0.92426 val_ap= 0.92514 time= 0.12347\n",
      "训练次数: 10 Epoch: 0076 log_lik= 0.4373242 train_kl= 0.00867 train_loss= 0.44599 train_acc= 0.57417 val_roc= 0.92411 val_ap= 0.92497 time= 0.11451\n",
      "训练次数: 10 Epoch: 0077 log_lik= 0.43687427 train_kl= 0.00867 train_loss= 0.44554 train_acc= 0.57546 val_roc= 0.92375 val_ap= 0.92510 time= 0.11451\n",
      "训练次数: 10 Epoch: 0078 log_lik= 0.43646193 train_kl= 0.00867 train_loss= 0.44513 train_acc= 0.57583 val_roc= 0.92348 val_ap= 0.92492 time= 0.11949\n",
      "训练次数: 10 Epoch: 0079 log_lik= 0.43608478 train_kl= 0.00867 train_loss= 0.44475 train_acc= 0.57676 val_roc= 0.92319 val_ap= 0.92469 time= 0.11352\n",
      "训练次数: 10 Epoch: 0080 log_lik= 0.43560994 train_kl= 0.00867 train_loss= 0.44428 train_acc= 0.57732 val_roc= 0.92310 val_ap= 0.92465 time= 0.11551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0081 log_lik= 0.43522808 train_kl= 0.00867 train_loss= 0.44390 train_acc= 0.57823 val_roc= 0.92291 val_ap= 0.92467 time= 0.11451\n",
      "训练次数: 10 Epoch: 0082 log_lik= 0.43482926 train_kl= 0.00868 train_loss= 0.44350 train_acc= 0.57911 val_roc= 0.92270 val_ap= 0.92452 time= 0.11602\n",
      "训练次数: 10 Epoch: 0083 log_lik= 0.43445158 train_kl= 0.00868 train_loss= 0.44313 train_acc= 0.57899 val_roc= 0.92254 val_ap= 0.92447 time= 0.12547\n",
      "训练次数: 10 Epoch: 0084 log_lik= 0.43409356 train_kl= 0.00868 train_loss= 0.44277 train_acc= 0.58022 val_roc= 0.92236 val_ap= 0.92438 time= 0.11253\n",
      "训练次数: 10 Epoch: 0085 log_lik= 0.43364474 train_kl= 0.00868 train_loss= 0.44233 train_acc= 0.58079 val_roc= 0.92232 val_ap= 0.92445 time= 0.11849\n",
      "训练次数: 10 Epoch: 0086 log_lik= 0.43330204 train_kl= 0.00868 train_loss= 0.44199 train_acc= 0.58124 val_roc= 0.92196 val_ap= 0.92413 time= 0.11850\n",
      "训练次数: 10 Epoch: 0087 log_lik= 0.43292806 train_kl= 0.00868 train_loss= 0.44161 train_acc= 0.58189 val_roc= 0.92209 val_ap= 0.92421 time= 0.11152\n",
      "训练次数: 10 Epoch: 0088 log_lik= 0.43259543 train_kl= 0.00869 train_loss= 0.44128 train_acc= 0.58246 val_roc= 0.92186 val_ap= 0.92414 time= 0.11053\n",
      "训练次数: 10 Epoch: 0089 log_lik= 0.43229347 train_kl= 0.00869 train_loss= 0.44098 train_acc= 0.58316 val_roc= 0.92153 val_ap= 0.92396 time= 0.12348\n",
      "训练次数: 10 Epoch: 0090 log_lik= 0.43193963 train_kl= 0.00869 train_loss= 0.44063 train_acc= 0.58368 val_roc= 0.92160 val_ap= 0.92401 time= 0.11949\n",
      "训练次数: 10 Epoch: 0091 log_lik= 0.4315336 train_kl= 0.00869 train_loss= 0.44022 train_acc= 0.58364 val_roc= 0.92145 val_ap= 0.92397 time= 0.12148\n",
      "训练次数: 10 Epoch: 0092 log_lik= 0.4313267 train_kl= 0.00869 train_loss= 0.44002 train_acc= 0.58493 val_roc= 0.92142 val_ap= 0.92398 time= 0.11949\n",
      "训练次数: 10 Epoch: 0093 log_lik= 0.43098137 train_kl= 0.00869 train_loss= 0.43967 train_acc= 0.58518 val_roc= 0.92138 val_ap= 0.92401 time= 0.11252\n",
      "训练次数: 10 Epoch: 0094 log_lik= 0.4307066 train_kl= 0.00869 train_loss= 0.43940 train_acc= 0.58518 val_roc= 0.92132 val_ap= 0.92406 time= 0.11419\n",
      "训练次数: 10 Epoch: 0095 log_lik= 0.43036643 train_kl= 0.00869 train_loss= 0.43906 train_acc= 0.58583 val_roc= 0.92118 val_ap= 0.92420 time= 0.11650\n",
      "训练次数: 10 Epoch: 0096 log_lik= 0.43004063 train_kl= 0.00869 train_loss= 0.43873 train_acc= 0.58568 val_roc= 0.92093 val_ap= 0.92413 time= 0.11750\n",
      "训练次数: 10 Epoch: 0097 log_lik= 0.42984873 train_kl= 0.00870 train_loss= 0.43854 train_acc= 0.58561 val_roc= 0.92085 val_ap= 0.92415 time= 0.12746\n",
      "训练次数: 10 Epoch: 0098 log_lik= 0.42959812 train_kl= 0.00870 train_loss= 0.43829 train_acc= 0.58578 val_roc= 0.92089 val_ap= 0.92472 time= 0.14140\n",
      "训练次数: 10 Epoch: 0099 log_lik= 0.42933387 train_kl= 0.00870 train_loss= 0.43803 train_acc= 0.58625 val_roc= 0.92086 val_ap= 0.92475 time= 0.13244\n",
      "训练次数: 10 Epoch: 0100 log_lik= 0.42907223 train_kl= 0.00870 train_loss= 0.43777 train_acc= 0.58629 val_roc= 0.92090 val_ap= 0.92487 time= 0.12646\n",
      "Optimization Finished!\n",
      "训练次数: 10 ROC score: 0.9117845093598437\n",
      "训练次数: 10 AP score: 0.9167447580593524\n",
      "Average Test ROC score: 0.9171908587147903\n",
      "Average Test AP score: 0.9239914588089926\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n实验结果\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动训练10次 CMVAE结果\n",
    "# 1.计算KL散度，预设 Ex=0, En=1.05, He=0.1\n",
    "# 2.取消采样1000次\n",
    "%run train_debug.py\n",
    "\n",
    "'''\n",
    "实验结果\n",
    "1. 取消模型内部多次采样，实验结果波动更大.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8076ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\gae\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "训练次数: 1 Epoch: 0001 log_lik= 0.78198916 train_kl= 0.00805 train_loss= 0.79004 train_acc= 0.15515 val_roc= 0.71973 val_ap= 0.74285 time= 7.73772\n",
      "训练次数: 1 Epoch: 0002 log_lik= 0.78041047 train_kl= 0.00828 train_loss= 0.78869 train_acc= 0.00160 val_roc= 0.81777 val_ap= 0.83050 time= 0.11845\n",
      "训练次数: 1 Epoch: 0003 log_lik= 0.7289245 train_kl= 0.00811 train_loss= 0.73703 train_acc= 0.07471 val_roc= 0.72679 val_ap= 0.74178 time= 0.10854\n",
      "训练次数: 1 Epoch: 0004 log_lik= 0.7267244 train_kl= 0.00821 train_loss= 0.73493 train_acc= 0.01141 val_roc= 0.75947 val_ap= 0.77238 time= 0.10854\n",
      "训练次数: 1 Epoch: 0005 log_lik= 0.71161497 train_kl= 0.00821 train_loss= 0.71982 train_acc= 0.02441 val_roc= 0.79773 val_ap= 0.80962 time= 0.11551\n",
      "训练次数: 1 Epoch: 0006 log_lik= 0.6881741 train_kl= 0.00818 train_loss= 0.69636 train_acc= 0.08602 val_roc= 0.81807 val_ap= 0.82162 time= 0.10854\n",
      "训练次数: 1 Epoch: 0007 log_lik= 0.6637098 train_kl= 0.00819 train_loss= 0.67190 train_acc= 0.19025 val_roc= 0.83118 val_ap= 0.82501 time= 0.11352\n",
      "训练次数: 1 Epoch: 0008 log_lik= 0.6309491 train_kl= 0.00824 train_loss= 0.63918 train_acc= 0.29982 val_roc= 0.84045 val_ap= 0.82755 time= 0.10754\n",
      "训练次数: 1 Epoch: 0009 log_lik= 0.5976768 train_kl= 0.00831 train_loss= 0.60599 train_acc= 0.37111 val_roc= 0.84513 val_ap= 0.82843 time= 0.11352\n",
      "训练次数: 1 Epoch: 0010 log_lik= 0.5780074 train_kl= 0.00842 train_loss= 0.58643 train_acc= 0.41125 val_roc= 0.85287 val_ap= 0.83682 time= 0.10754\n",
      "训练次数: 1 Epoch: 0011 log_lik= 0.5647898 train_kl= 0.00851 train_loss= 0.57330 train_acc= 0.44704 val_roc= 0.85726 val_ap= 0.84234 time= 0.12148\n",
      "训练次数: 1 Epoch: 0012 log_lik= 0.55601084 train_kl= 0.00858 train_loss= 0.56459 train_acc= 0.47585 val_roc= 0.86523 val_ap= 0.85152 time= 0.10754\n",
      "训练次数: 1 Epoch: 0013 log_lik= 0.55130047 train_kl= 0.00862 train_loss= 0.55992 train_acc= 0.49125 val_roc= 0.87444 val_ap= 0.86175 time= 0.10655\n",
      "训练次数: 1 Epoch: 0014 log_lik= 0.5428878 train_kl= 0.00865 train_loss= 0.55153 train_acc= 0.49823 val_roc= 0.88188 val_ap= 0.87038 time= 0.11252\n",
      "训练次数: 1 Epoch: 0015 log_lik= 0.5341338 train_kl= 0.00865 train_loss= 0.54278 train_acc= 0.50050 val_roc= 0.88865 val_ap= 0.88087 time= 0.10854\n",
      "训练次数: 1 Epoch: 0016 log_lik= 0.52635115 train_kl= 0.00863 train_loss= 0.53498 train_acc= 0.50136 val_roc= 0.89377 val_ap= 0.89014 time= 0.10655\n",
      "训练次数: 1 Epoch: 0017 log_lik= 0.5173441 train_kl= 0.00859 train_loss= 0.52594 train_acc= 0.50381 val_roc= 0.89526 val_ap= 0.89606 time= 0.10355\n",
      "训练次数: 1 Epoch: 0018 log_lik= 0.5114406 train_kl= 0.00855 train_loss= 0.51999 train_acc= 0.50367 val_roc= 0.89592 val_ap= 0.89851 time= 0.11452\n",
      "训练次数: 1 Epoch: 0019 log_lik= 0.5079389 train_kl= 0.00852 train_loss= 0.51646 train_acc= 0.50302 val_roc= 0.89831 val_ap= 0.90291 time= 0.10459\n",
      "训练次数: 1 Epoch: 0020 log_lik= 0.5036326 train_kl= 0.00850 train_loss= 0.51213 train_acc= 0.50792 val_roc= 0.90283 val_ap= 0.90850 time= 0.11949\n",
      "训练次数: 1 Epoch: 0021 log_lik= 0.49969468 train_kl= 0.00849 train_loss= 0.50819 train_acc= 0.51367 val_roc= 0.90639 val_ap= 0.91312 time= 0.13097\n",
      "训练次数: 1 Epoch: 0022 log_lik= 0.49641278 train_kl= 0.00850 train_loss= 0.50491 train_acc= 0.51587 val_roc= 0.91034 val_ap= 0.91732 time= 0.11352\n",
      "训练次数: 1 Epoch: 0023 log_lik= 0.49189845 train_kl= 0.00851 train_loss= 0.50041 train_acc= 0.51689 val_roc= 0.91317 val_ap= 0.92002 time= 0.11152\n",
      "训练次数: 1 Epoch: 0024 log_lik= 0.48740664 train_kl= 0.00853 train_loss= 0.49593 train_acc= 0.51654 val_roc= 0.91570 val_ap= 0.92257 time= 0.10157\n",
      "训练次数: 1 Epoch: 0025 log_lik= 0.4833816 train_kl= 0.00855 train_loss= 0.49193 train_acc= 0.51677 val_roc= 0.91882 val_ap= 0.92554 time= 0.10555\n",
      "训练次数: 1 Epoch: 0026 log_lik= 0.479105 train_kl= 0.00857 train_loss= 0.48768 train_acc= 0.51988 val_roc= 0.92141 val_ap= 0.92791 time= 0.10157\n",
      "训练次数: 1 Epoch: 0027 log_lik= 0.4744261 train_kl= 0.00859 train_loss= 0.48302 train_acc= 0.52519 val_roc= 0.92312 val_ap= 0.92929 time= 0.10854\n",
      "训练次数: 1 Epoch: 0028 log_lik= 0.4702617 train_kl= 0.00861 train_loss= 0.47887 train_acc= 0.53113 val_roc= 0.92351 val_ap= 0.92956 time= 0.10953\n",
      "训练次数: 1 Epoch: 0029 log_lik= 0.46728063 train_kl= 0.00863 train_loss= 0.47591 train_acc= 0.53635 val_roc= 0.92377 val_ap= 0.93002 time= 0.10157\n",
      "训练次数: 1 Epoch: 0030 log_lik= 0.46469274 train_kl= 0.00864 train_loss= 0.47333 train_acc= 0.53884 val_roc= 0.92342 val_ap= 0.92987 time= 0.10754\n",
      "训练次数: 1 Epoch: 0031 log_lik= 0.4622628 train_kl= 0.00864 train_loss= 0.47091 train_acc= 0.54079 val_roc= 0.92476 val_ap= 0.93124 time= 0.10356\n",
      "训练次数: 1 Epoch: 0032 log_lik= 0.46039823 train_kl= 0.00865 train_loss= 0.46905 train_acc= 0.54119 val_roc= 0.92648 val_ap= 0.93272 time= 0.11252\n",
      "训练次数: 1 Epoch: 0033 log_lik= 0.4589625 train_kl= 0.00865 train_loss= 0.46761 train_acc= 0.54149 val_roc= 0.92833 val_ap= 0.93447 time= 0.10356\n",
      "训练次数: 1 Epoch: 0034 log_lik= 0.45720512 train_kl= 0.00865 train_loss= 0.46585 train_acc= 0.54337 val_roc= 0.92938 val_ap= 0.93483 time= 0.10359\n",
      "训练次数: 1 Epoch: 0035 log_lik= 0.45500067 train_kl= 0.00865 train_loss= 0.46365 train_acc= 0.54553 val_roc= 0.93020 val_ap= 0.93508 time= 0.10555\n",
      "训练次数: 1 Epoch: 0036 log_lik= 0.4529685 train_kl= 0.00865 train_loss= 0.46161 train_acc= 0.54843 val_roc= 0.92990 val_ap= 0.93456 time= 0.11352\n",
      "训练次数: 1 Epoch: 0037 log_lik= 0.45126775 train_kl= 0.00865 train_loss= 0.45992 train_acc= 0.55022 val_roc= 0.92942 val_ap= 0.93439 time= 0.10807\n",
      "训练次数: 1 Epoch: 0038 log_lik= 0.44995877 train_kl= 0.00865 train_loss= 0.45861 train_acc= 0.55168 val_roc= 0.92913 val_ap= 0.93447 time= 0.10057\n",
      "训练次数: 1 Epoch: 0039 log_lik= 0.4490258 train_kl= 0.00866 train_loss= 0.45768 train_acc= 0.55187 val_roc= 0.92972 val_ap= 0.93529 time= 0.10207\n",
      "训练次数: 1 Epoch: 0040 log_lik= 0.448111 train_kl= 0.00867 train_loss= 0.45678 train_acc= 0.55188 val_roc= 0.93097 val_ap= 0.93675 time= 0.10456\n",
      "训练次数: 1 Epoch: 0041 log_lik= 0.4469264 train_kl= 0.00867 train_loss= 0.45560 train_acc= 0.55290 val_roc= 0.93251 val_ap= 0.93839 time= 0.10177\n",
      "训练次数: 1 Epoch: 0042 log_lik= 0.4456806 train_kl= 0.00868 train_loss= 0.45436 train_acc= 0.55443 val_roc= 0.93383 val_ap= 0.93972 time= 0.10854\n",
      "训练次数: 1 Epoch: 0043 log_lik= 0.4443164 train_kl= 0.00868 train_loss= 0.45300 train_acc= 0.55539 val_roc= 0.93477 val_ap= 0.94077 time= 0.10057\n",
      "训练次数: 1 Epoch: 0044 log_lik= 0.44315717 train_kl= 0.00869 train_loss= 0.45184 train_acc= 0.55685 val_roc= 0.93517 val_ap= 0.94119 time= 0.09958\n",
      "训练次数: 1 Epoch: 0045 log_lik= 0.44211072 train_kl= 0.00869 train_loss= 0.45080 train_acc= 0.55668 val_roc= 0.93478 val_ap= 0.94114 time= 0.10456\n",
      "训练次数: 1 Epoch: 0046 log_lik= 0.44125298 train_kl= 0.00869 train_loss= 0.44994 train_acc= 0.55704 val_roc= 0.93455 val_ap= 0.94099 time= 0.09958\n",
      "训练次数: 1 Epoch: 0047 log_lik= 0.44058505 train_kl= 0.00869 train_loss= 0.44928 train_acc= 0.55745 val_roc= 0.93496 val_ap= 0.94194 time= 0.10282\n",
      "训练次数: 1 Epoch: 0048 log_lik= 0.43993127 train_kl= 0.00869 train_loss= 0.44862 train_acc= 0.55751 val_roc= 0.93517 val_ap= 0.94252 time= 0.10356\n",
      "训练次数: 1 Epoch: 0049 log_lik= 0.4391756 train_kl= 0.00869 train_loss= 0.44787 train_acc= 0.55798 val_roc= 0.93551 val_ap= 0.94303 time= 0.09959\n",
      "训练次数: 1 Epoch: 0050 log_lik= 0.43850866 train_kl= 0.00869 train_loss= 0.44720 train_acc= 0.55846 val_roc= 0.93496 val_ap= 0.94258 time= 0.10057\n",
      "训练次数: 1 Epoch: 0051 log_lik= 0.43769634 train_kl= 0.00869 train_loss= 0.44639 train_acc= 0.55918 val_roc= 0.93425 val_ap= 0.94208 time= 0.10057\n",
      "训练次数: 1 Epoch: 0052 log_lik= 0.43706536 train_kl= 0.00869 train_loss= 0.44575 train_acc= 0.55893 val_roc= 0.93308 val_ap= 0.94153 time= 0.10157\n",
      "训练次数: 1 Epoch: 0053 log_lik= 0.43645674 train_kl= 0.00869 train_loss= 0.44514 train_acc= 0.55891 val_roc= 0.93233 val_ap= 0.94124 time= 0.09958\n",
      "训练次数: 1 Epoch: 0054 log_lik= 0.43593124 train_kl= 0.00869 train_loss= 0.44462 train_acc= 0.55923 val_roc= 0.93180 val_ap= 0.94118 time= 0.10057\n",
      "训练次数: 1 Epoch: 0055 log_lik= 0.43530604 train_kl= 0.00869 train_loss= 0.44399 train_acc= 0.55925 val_roc= 0.93159 val_ap= 0.94133 time= 0.10456\n",
      "训练次数: 1 Epoch: 0056 log_lik= 0.43469527 train_kl= 0.00869 train_loss= 0.44338 train_acc= 0.55958 val_roc= 0.93180 val_ap= 0.94169 time= 0.10257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 1 Epoch: 0057 log_lik= 0.43404877 train_kl= 0.00869 train_loss= 0.44274 train_acc= 0.56025 val_roc= 0.93153 val_ap= 0.94146 time= 0.10256\n",
      "训练次数: 1 Epoch: 0058 log_lik= 0.4334327 train_kl= 0.00869 train_loss= 0.44212 train_acc= 0.56092 val_roc= 0.93126 val_ap= 0.94115 time= 0.10456\n",
      "训练次数: 1 Epoch: 0059 log_lik= 0.4328933 train_kl= 0.00869 train_loss= 0.44158 train_acc= 0.56068 val_roc= 0.93082 val_ap= 0.94082 time= 0.10555\n",
      "训练次数: 1 Epoch: 0060 log_lik= 0.43249375 train_kl= 0.00869 train_loss= 0.44119 train_acc= 0.56095 val_roc= 0.93030 val_ap= 0.94042 time= 0.11356\n",
      "训练次数: 1 Epoch: 0061 log_lik= 0.4319874 train_kl= 0.00870 train_loss= 0.44068 train_acc= 0.56054 val_roc= 0.93006 val_ap= 0.94012 time= 0.11053\n",
      "训练次数: 1 Epoch: 0062 log_lik= 0.4315402 train_kl= 0.00870 train_loss= 0.44024 train_acc= 0.56086 val_roc= 0.93027 val_ap= 0.94029 time= 0.10456\n",
      "训练次数: 1 Epoch: 0063 log_lik= 0.43097904 train_kl= 0.00870 train_loss= 0.43968 train_acc= 0.56116 val_roc= 0.93085 val_ap= 0.94056 time= 0.10555\n",
      "训练次数: 1 Epoch: 0064 log_lik= 0.4305389 train_kl= 0.00870 train_loss= 0.43924 train_acc= 0.56157 val_roc= 0.93123 val_ap= 0.94061 time= 0.10455\n",
      "训练次数: 1 Epoch: 0065 log_lik= 0.43012652 train_kl= 0.00871 train_loss= 0.43883 train_acc= 0.56205 val_roc= 0.93114 val_ap= 0.94049 time= 0.11651\n",
      "训练次数: 1 Epoch: 0066 log_lik= 0.42969885 train_kl= 0.00871 train_loss= 0.43841 train_acc= 0.56241 val_roc= 0.93102 val_ap= 0.94013 time= 0.10655\n",
      "训练次数: 1 Epoch: 0067 log_lik= 0.42922226 train_kl= 0.00871 train_loss= 0.43793 train_acc= 0.56272 val_roc= 0.93108 val_ap= 0.94004 time= 0.10655\n",
      "训练次数: 1 Epoch: 0068 log_lik= 0.42878628 train_kl= 0.00871 train_loss= 0.43749 train_acc= 0.56274 val_roc= 0.93126 val_ap= 0.93992 time= 0.10754\n",
      "训练次数: 1 Epoch: 0069 log_lik= 0.42830533 train_kl= 0.00871 train_loss= 0.43701 train_acc= 0.56319 val_roc= 0.93141 val_ap= 0.93975 time= 0.11451\n",
      "训练次数: 1 Epoch: 0070 log_lik= 0.4280098 train_kl= 0.00871 train_loss= 0.43672 train_acc= 0.56370 val_roc= 0.93169 val_ap= 0.93996 time= 0.11153\n",
      "训练次数: 1 Epoch: 0071 log_lik= 0.4275903 train_kl= 0.00871 train_loss= 0.43630 train_acc= 0.56388 val_roc= 0.93202 val_ap= 0.93996 time= 0.10455\n",
      "训练次数: 1 Epoch: 0072 log_lik= 0.42724296 train_kl= 0.00871 train_loss= 0.43595 train_acc= 0.56396 val_roc= 0.93204 val_ap= 0.93967 time= 0.11312\n",
      "训练次数: 1 Epoch: 0073 log_lik= 0.4268656 train_kl= 0.00871 train_loss= 0.43558 train_acc= 0.56420 val_roc= 0.93198 val_ap= 0.93942 time= 0.10854\n",
      "训练次数: 1 Epoch: 0074 log_lik= 0.42646015 train_kl= 0.00871 train_loss= 0.43517 train_acc= 0.56464 val_roc= 0.93209 val_ap= 0.93935 time= 0.10953\n",
      "训练次数: 1 Epoch: 0075 log_lik= 0.42611587 train_kl= 0.00872 train_loss= 0.43483 train_acc= 0.56536 val_roc= 0.93214 val_ap= 0.93922 time= 0.10854\n",
      "训练次数: 1 Epoch: 0076 log_lik= 0.42578772 train_kl= 0.00872 train_loss= 0.43451 train_acc= 0.56495 val_roc= 0.93173 val_ap= 0.93899 time= 0.10953\n",
      "训练次数: 1 Epoch: 0077 log_lik= 0.42550877 train_kl= 0.00872 train_loss= 0.43423 train_acc= 0.56546 val_roc= 0.93137 val_ap= 0.93876 time= 0.10954\n",
      "训练次数: 1 Epoch: 0078 log_lik= 0.42519292 train_kl= 0.00872 train_loss= 0.43392 train_acc= 0.56565 val_roc= 0.93113 val_ap= 0.93874 time= 0.10654\n",
      "训练次数: 1 Epoch: 0079 log_lik= 0.42476332 train_kl= 0.00872 train_loss= 0.43349 train_acc= 0.56611 val_roc= 0.93098 val_ap= 0.93879 time= 0.10757\n",
      "训练次数: 1 Epoch: 0080 log_lik= 0.42451814 train_kl= 0.00872 train_loss= 0.43324 train_acc= 0.56627 val_roc= 0.93072 val_ap= 0.93860 time= 0.10854\n",
      "训练次数: 1 Epoch: 0081 log_lik= 0.42419186 train_kl= 0.00872 train_loss= 0.43291 train_acc= 0.56650 val_roc= 0.93024 val_ap= 0.93845 time= 0.10555\n",
      "训练次数: 1 Epoch: 0082 log_lik= 0.42383936 train_kl= 0.00872 train_loss= 0.43256 train_acc= 0.56635 val_roc= 0.92940 val_ap= 0.93798 time= 0.10555\n",
      "训练次数: 1 Epoch: 0083 log_lik= 0.42358986 train_kl= 0.00872 train_loss= 0.43231 train_acc= 0.56618 val_roc= 0.92886 val_ap= 0.93769 time= 0.10655\n",
      "训练次数: 1 Epoch: 0084 log_lik= 0.4232795 train_kl= 0.00872 train_loss= 0.43200 train_acc= 0.56645 val_roc= 0.92836 val_ap= 0.93746 time= 0.10356\n",
      "训练次数: 1 Epoch: 0085 log_lik= 0.42300305 train_kl= 0.00872 train_loss= 0.43173 train_acc= 0.56659 val_roc= 0.92842 val_ap= 0.93770 time= 0.11572\n",
      "训练次数: 1 Epoch: 0086 log_lik= 0.42276263 train_kl= 0.00873 train_loss= 0.43149 train_acc= 0.56688 val_roc= 0.92812 val_ap= 0.93760 time= 0.11551\n",
      "训练次数: 1 Epoch: 0087 log_lik= 0.42250898 train_kl= 0.00873 train_loss= 0.43124 train_acc= 0.56705 val_roc= 0.92773 val_ap= 0.93761 time= 0.10755\n",
      "训练次数: 1 Epoch: 0088 log_lik= 0.42222998 train_kl= 0.00873 train_loss= 0.43096 train_acc= 0.56709 val_roc= 0.92718 val_ap= 0.93736 time= 0.10655\n",
      "训练次数: 1 Epoch: 0089 log_lik= 0.42196572 train_kl= 0.00873 train_loss= 0.43070 train_acc= 0.56765 val_roc= 0.92672 val_ap= 0.93719 time= 0.11352\n",
      "训练次数: 1 Epoch: 0090 log_lik= 0.42169583 train_kl= 0.00873 train_loss= 0.43043 train_acc= 0.56742 val_roc= 0.92654 val_ap= 0.93719 time= 0.11011\n",
      "训练次数: 1 Epoch: 0091 log_lik= 0.42145592 train_kl= 0.00873 train_loss= 0.43019 train_acc= 0.56717 val_roc= 0.92638 val_ap= 0.93746 time= 0.10754\n",
      "训练次数: 1 Epoch: 0092 log_lik= 0.42126733 train_kl= 0.00874 train_loss= 0.43000 train_acc= 0.56763 val_roc= 0.92638 val_ap= 0.93772 time= 0.10655\n",
      "训练次数: 1 Epoch: 0093 log_lik= 0.4210693 train_kl= 0.00874 train_loss= 0.42981 train_acc= 0.56794 val_roc= 0.92630 val_ap= 0.93795 time= 0.10555\n",
      "训练次数: 1 Epoch: 0094 log_lik= 0.42088693 train_kl= 0.00874 train_loss= 0.42962 train_acc= 0.56811 val_roc= 0.92621 val_ap= 0.93809 time= 0.11153\n",
      "训练次数: 1 Epoch: 0095 log_lik= 0.42059222 train_kl= 0.00874 train_loss= 0.42933 train_acc= 0.56825 val_roc= 0.92582 val_ap= 0.93795 time= 0.10555\n",
      "训练次数: 1 Epoch: 0096 log_lik= 0.42044976 train_kl= 0.00874 train_loss= 0.42919 train_acc= 0.56776 val_roc= 0.92580 val_ap= 0.93811 time= 0.10555\n",
      "训练次数: 1 Epoch: 0097 log_lik= 0.42029077 train_kl= 0.00874 train_loss= 0.42903 train_acc= 0.56775 val_roc= 0.92572 val_ap= 0.93826 time= 0.10754\n",
      "训练次数: 1 Epoch: 0098 log_lik= 0.4200837 train_kl= 0.00874 train_loss= 0.42882 train_acc= 0.56842 val_roc= 0.92567 val_ap= 0.93840 time= 0.10879\n",
      "训练次数: 1 Epoch: 0099 log_lik= 0.41995007 train_kl= 0.00874 train_loss= 0.42869 train_acc= 0.56799 val_roc= 0.92530 val_ap= 0.93832 time= 0.11650\n",
      "训练次数: 1 Epoch: 0100 log_lik= 0.4197399 train_kl= 0.00874 train_loss= 0.42848 train_acc= 0.56798 val_roc= 0.92523 val_ap= 0.93823 time= 0.10754\n",
      "Optimization Finished!\n",
      "训练次数: 1 ROC score: 0.9212073640131206\n",
      "训练次数: 1 AP score: 0.9270236064505805\n",
      "训练次数: 2 Epoch: 0001 log_lik= 0.7840271 train_kl= 0.00805 train_loss= 0.79208 train_acc= 0.12086 val_roc= 0.70185 val_ap= 0.72442 time= 7.30650\n",
      "训练次数: 2 Epoch: 0002 log_lik= 0.8276057 train_kl= 0.00833 train_loss= 0.83593 train_acc= 0.00159 val_roc= 0.77517 val_ap= 0.78022 time= 0.11646\n",
      "训练次数: 2 Epoch: 0003 log_lik= 0.7451875 train_kl= 0.00809 train_loss= 0.75328 train_acc= 0.05421 val_roc= 0.71278 val_ap= 0.71735 time= 0.11452\n",
      "训练次数: 2 Epoch: 0004 log_lik= 0.7323756 train_kl= 0.00814 train_loss= 0.74052 train_acc= 0.01486 val_roc= 0.71231 val_ap= 0.72715 time= 0.11637\n",
      "训练次数: 2 Epoch: 0005 log_lik= 0.7585128 train_kl= 0.00826 train_loss= 0.76678 train_acc= 0.00247 val_roc= 0.75301 val_ap= 0.75617 time= 0.11459\n",
      "训练次数: 2 Epoch: 0006 log_lik= 0.72157514 train_kl= 0.00821 train_loss= 0.72979 train_acc= 0.01071 val_roc= 0.79112 val_ap= 0.79079 time= 0.10647\n",
      "训练次数: 2 Epoch: 0007 log_lik= 0.70296526 train_kl= 0.00817 train_loss= 0.71113 train_acc= 0.06815 val_roc= 0.80326 val_ap= 0.80436 time= 0.11750\n",
      "训练次数: 2 Epoch: 0008 log_lik= 0.69133985 train_kl= 0.00816 train_loss= 0.69950 train_acc= 0.16998 val_roc= 0.80436 val_ap= 0.80236 time= 0.10954\n",
      "训练次数: 2 Epoch: 0009 log_lik= 0.67461884 train_kl= 0.00817 train_loss= 0.68279 train_acc= 0.25896 val_roc= 0.80433 val_ap= 0.80046 time= 0.11551\n",
      "训练次数: 2 Epoch: 0010 log_lik= 0.6526727 train_kl= 0.00821 train_loss= 0.66088 train_acc= 0.31717 val_roc= 0.80720 val_ap= 0.80066 time= 0.10954\n",
      "训练次数: 2 Epoch: 0011 log_lik= 0.6313713 train_kl= 0.00826 train_loss= 0.63963 train_acc= 0.35021 val_roc= 0.81133 val_ap= 0.80311 time= 0.10953\n",
      "训练次数: 2 Epoch: 0012 log_lik= 0.6183608 train_kl= 0.00833 train_loss= 0.62669 train_acc= 0.37170 val_roc= 0.81724 val_ap= 0.81092 time= 0.10953\n",
      "训练次数: 2 Epoch: 0013 log_lik= 0.61147624 train_kl= 0.00839 train_loss= 0.61987 train_acc= 0.39080 val_roc= 0.82505 val_ap= 0.82181 time= 0.10556\n",
      "训练次数: 2 Epoch: 0014 log_lik= 0.604528 train_kl= 0.00844 train_loss= 0.61296 train_acc= 0.40936 val_roc= 0.83072 val_ap= 0.82827 time= 0.10854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0015 log_lik= 0.5960421 train_kl= 0.00846 train_loss= 0.60450 train_acc= 0.42210 val_roc= 0.83702 val_ap= 0.83585 time= 0.10953\n",
      "训练次数: 2 Epoch: 0016 log_lik= 0.5828382 train_kl= 0.00846 train_loss= 0.59130 train_acc= 0.43723 val_roc= 0.84382 val_ap= 0.84388 time= 0.11154\n",
      "训练次数: 2 Epoch: 0017 log_lik= 0.56738615 train_kl= 0.00844 train_loss= 0.57583 train_acc= 0.45308 val_roc= 0.84810 val_ap= 0.84716 time= 0.10754\n",
      "训练次数: 2 Epoch: 0018 log_lik= 0.5545941 train_kl= 0.00842 train_loss= 0.56302 train_acc= 0.46804 val_roc= 0.85126 val_ap= 0.84722 time= 0.10754\n",
      "训练次数: 2 Epoch: 0019 log_lik= 0.5462507 train_kl= 0.00842 train_loss= 0.55467 train_acc= 0.48064 val_roc= 0.85611 val_ap= 0.84977 time= 0.11053\n",
      "训练次数: 2 Epoch: 0020 log_lik= 0.5398001 train_kl= 0.00842 train_loss= 0.54822 train_acc= 0.49096 val_roc= 0.86213 val_ap= 0.85377 time= 0.10655\n",
      "训练次数: 2 Epoch: 0021 log_lik= 0.53251016 train_kl= 0.00843 train_loss= 0.54094 train_acc= 0.49971 val_roc= 0.86777 val_ap= 0.85694 time= 0.10931\n",
      "训练次数: 2 Epoch: 0022 log_lik= 0.525473 train_kl= 0.00845 train_loss= 0.53392 train_acc= 0.50552 val_roc= 0.87301 val_ap= 0.86177 time= 0.11254\n",
      "训练次数: 2 Epoch: 0023 log_lik= 0.5201929 train_kl= 0.00848 train_loss= 0.52867 train_acc= 0.50703 val_roc= 0.87782 val_ap= 0.86625 time= 0.11203\n",
      "训练次数: 2 Epoch: 0024 log_lik= 0.51681006 train_kl= 0.00851 train_loss= 0.52532 train_acc= 0.50791 val_roc= 0.88300 val_ap= 0.87049 time= 0.10555\n",
      "训练次数: 2 Epoch: 0025 log_lik= 0.5137828 train_kl= 0.00855 train_loss= 0.52233 train_acc= 0.51085 val_roc= 0.88839 val_ap= 0.87538 time= 0.10953\n",
      "训练次数: 2 Epoch: 0026 log_lik= 0.51029086 train_kl= 0.00857 train_loss= 0.51886 train_acc= 0.51519 val_roc= 0.89257 val_ap= 0.88032 time= 0.10555\n",
      "训练次数: 2 Epoch: 0027 log_lik= 0.5070586 train_kl= 0.00859 train_loss= 0.51565 train_acc= 0.52127 val_roc= 0.89615 val_ap= 0.88598 time= 0.10157\n",
      "训练次数: 2 Epoch: 0028 log_lik= 0.50338435 train_kl= 0.00860 train_loss= 0.51198 train_acc= 0.52658 val_roc= 0.89835 val_ap= 0.89011 time= 0.10655\n",
      "训练次数: 2 Epoch: 0029 log_lik= 0.49916288 train_kl= 0.00860 train_loss= 0.50776 train_acc= 0.53162 val_roc= 0.90166 val_ap= 0.89517 time= 0.10555\n",
      "训练次数: 2 Epoch: 0030 log_lik= 0.4942324 train_kl= 0.00859 train_loss= 0.50282 train_acc= 0.53733 val_roc= 0.90428 val_ap= 0.89915 time= 0.10356\n",
      "训练次数: 2 Epoch: 0031 log_lik= 0.4901329 train_kl= 0.00858 train_loss= 0.49872 train_acc= 0.54249 val_roc= 0.90935 val_ap= 0.90590 time= 0.10157\n",
      "训练次数: 2 Epoch: 0032 log_lik= 0.4862314 train_kl= 0.00858 train_loss= 0.49481 train_acc= 0.54732 val_roc= 0.91359 val_ap= 0.91145 time= 0.10257\n",
      "训练次数: 2 Epoch: 0033 log_lik= 0.48268414 train_kl= 0.00857 train_loss= 0.49126 train_acc= 0.55138 val_roc= 0.91720 val_ap= 0.91702 time= 0.11053\n",
      "训练次数: 2 Epoch: 0034 log_lik= 0.47950298 train_kl= 0.00858 train_loss= 0.48808 train_acc= 0.55466 val_roc= 0.92005 val_ap= 0.92238 time= 0.10455\n",
      "训练次数: 2 Epoch: 0035 log_lik= 0.4766694 train_kl= 0.00858 train_loss= 0.48525 train_acc= 0.55764 val_roc= 0.92248 val_ap= 0.92647 time= 0.10356\n",
      "训练次数: 2 Epoch: 0036 log_lik= 0.47427088 train_kl= 0.00859 train_loss= 0.48286 train_acc= 0.56025 val_roc= 0.92336 val_ap= 0.92952 time= 0.10256\n",
      "训练次数: 2 Epoch: 0037 log_lik= 0.4726172 train_kl= 0.00860 train_loss= 0.48121 train_acc= 0.56203 val_roc= 0.92421 val_ap= 0.93149 time= 0.11053\n",
      "训练次数: 2 Epoch: 0038 log_lik= 0.4711848 train_kl= 0.00860 train_loss= 0.47979 train_acc= 0.56278 val_roc= 0.92559 val_ap= 0.93358 time= 0.10654\n",
      "训练次数: 2 Epoch: 0039 log_lik= 0.4699068 train_kl= 0.00861 train_loss= 0.47852 train_acc= 0.56355 val_roc= 0.92685 val_ap= 0.93487 time= 0.10057\n",
      "训练次数: 2 Epoch: 0040 log_lik= 0.46879178 train_kl= 0.00861 train_loss= 0.47740 train_acc= 0.56297 val_roc= 0.92686 val_ap= 0.93480 time= 0.11152\n",
      "训练次数: 2 Epoch: 0041 log_lik= 0.46800122 train_kl= 0.00861 train_loss= 0.47662 train_acc= 0.56240 val_roc= 0.92659 val_ap= 0.93435 time= 0.10655\n",
      "训练次数: 2 Epoch: 0042 log_lik= 0.46700877 train_kl= 0.00861 train_loss= 0.47562 train_acc= 0.56284 val_roc= 0.92559 val_ap= 0.93344 time= 0.11750\n",
      "训练次数: 2 Epoch: 0043 log_lik= 0.46565267 train_kl= 0.00861 train_loss= 0.47427 train_acc= 0.56306 val_roc= 0.92460 val_ap= 0.93270 time= 0.11650\n",
      "训练次数: 2 Epoch: 0044 log_lik= 0.4640389 train_kl= 0.00861 train_loss= 0.47265 train_acc= 0.56506 val_roc= 0.92368 val_ap= 0.93174 time= 0.11750\n",
      "训练次数: 2 Epoch: 0045 log_lik= 0.46250963 train_kl= 0.00861 train_loss= 0.47112 train_acc= 0.56699 val_roc= 0.92362 val_ap= 0.93143 time= 0.10854\n",
      "训练次数: 2 Epoch: 0046 log_lik= 0.46120286 train_kl= 0.00861 train_loss= 0.46981 train_acc= 0.56894 val_roc= 0.92326 val_ap= 0.93074 time= 0.11750\n",
      "训练次数: 2 Epoch: 0047 log_lik= 0.45997745 train_kl= 0.00860 train_loss= 0.46858 train_acc= 0.57087 val_roc= 0.92262 val_ap= 0.93000 time= 0.11452\n",
      "训练次数: 2 Epoch: 0048 log_lik= 0.45860538 train_kl= 0.00861 train_loss= 0.46721 train_acc= 0.57182 val_roc= 0.92255 val_ap= 0.92993 time= 0.11551\n",
      "训练次数: 2 Epoch: 0049 log_lik= 0.45720005 train_kl= 0.00861 train_loss= 0.46581 train_acc= 0.57310 val_roc= 0.92145 val_ap= 0.92871 time= 0.10756\n",
      "训练次数: 2 Epoch: 0050 log_lik= 0.4559791 train_kl= 0.00861 train_loss= 0.46459 train_acc= 0.57359 val_roc= 0.92087 val_ap= 0.92794 time= 0.10555\n",
      "训练次数: 2 Epoch: 0051 log_lik= 0.4549085 train_kl= 0.00862 train_loss= 0.46352 train_acc= 0.57431 val_roc= 0.92095 val_ap= 0.92774 time= 0.11949\n",
      "训练次数: 2 Epoch: 0052 log_lik= 0.45386615 train_kl= 0.00862 train_loss= 0.46249 train_acc= 0.57434 val_roc= 0.92083 val_ap= 0.92722 time= 0.10754\n",
      "训练次数: 2 Epoch: 0053 log_lik= 0.45294458 train_kl= 0.00863 train_loss= 0.46157 train_acc= 0.57489 val_roc= 0.92054 val_ap= 0.92643 time= 0.10953\n",
      "训练次数: 2 Epoch: 0054 log_lik= 0.45200267 train_kl= 0.00864 train_loss= 0.46064 train_acc= 0.57492 val_roc= 0.92021 val_ap= 0.92566 time= 0.10655\n",
      "训练次数: 2 Epoch: 0055 log_lik= 0.45111892 train_kl= 0.00864 train_loss= 0.45976 train_acc= 0.57587 val_roc= 0.91975 val_ap= 0.92520 time= 0.10755\n",
      "训练次数: 2 Epoch: 0056 log_lik= 0.4502528 train_kl= 0.00865 train_loss= 0.45890 train_acc= 0.57646 val_roc= 0.91889 val_ap= 0.92388 time= 0.10654\n",
      "训练次数: 2 Epoch: 0057 log_lik= 0.44942954 train_kl= 0.00865 train_loss= 0.45808 train_acc= 0.57607 val_roc= 0.91794 val_ap= 0.92242 time= 0.10953\n",
      "训练次数: 2 Epoch: 0058 log_lik= 0.44851923 train_kl= 0.00865 train_loss= 0.45717 train_acc= 0.57707 val_roc= 0.91730 val_ap= 0.92186 time= 0.10754\n",
      "训练次数: 2 Epoch: 0059 log_lik= 0.44754398 train_kl= 0.00865 train_loss= 0.45619 train_acc= 0.57765 val_roc= 0.91654 val_ap= 0.92126 time= 0.11252\n",
      "训练次数: 2 Epoch: 0060 log_lik= 0.44670564 train_kl= 0.00864 train_loss= 0.45535 train_acc= 0.57863 val_roc= 0.91615 val_ap= 0.92126 time= 0.10854\n",
      "训练次数: 2 Epoch: 0061 log_lik= 0.44576907 train_kl= 0.00864 train_loss= 0.45441 train_acc= 0.57930 val_roc= 0.91606 val_ap= 0.92122 time= 0.10655\n",
      "训练次数: 2 Epoch: 0062 log_lik= 0.4450339 train_kl= 0.00864 train_loss= 0.45368 train_acc= 0.57985 val_roc= 0.91616 val_ap= 0.92136 time= 0.11153\n",
      "训练次数: 2 Epoch: 0063 log_lik= 0.44418603 train_kl= 0.00865 train_loss= 0.45283 train_acc= 0.58082 val_roc= 0.91629 val_ap= 0.92124 time= 0.11551\n",
      "训练次数: 2 Epoch: 0064 log_lik= 0.44353914 train_kl= 0.00865 train_loss= 0.45219 train_acc= 0.58236 val_roc= 0.91662 val_ap= 0.92135 time= 0.11152\n",
      "训练次数: 2 Epoch: 0065 log_lik= 0.44273508 train_kl= 0.00865 train_loss= 0.45139 train_acc= 0.58365 val_roc= 0.91642 val_ap= 0.92136 time= 0.10854\n",
      "训练次数: 2 Epoch: 0066 log_lik= 0.44212008 train_kl= 0.00866 train_loss= 0.45078 train_acc= 0.58394 val_roc= 0.91629 val_ap= 0.92160 time= 0.10953\n",
      "训练次数: 2 Epoch: 0067 log_lik= 0.4413088 train_kl= 0.00866 train_loss= 0.44997 train_acc= 0.58500 val_roc= 0.91625 val_ap= 0.92173 time= 0.11252\n",
      "训练次数: 2 Epoch: 0068 log_lik= 0.44078255 train_kl= 0.00867 train_loss= 0.44945 train_acc= 0.58546 val_roc= 0.91622 val_ap= 0.92169 time= 0.10853\n",
      "训练次数: 2 Epoch: 0069 log_lik= 0.44007632 train_kl= 0.00867 train_loss= 0.44875 train_acc= 0.58651 val_roc= 0.91644 val_ap= 0.92171 time= 0.10953\n",
      "训练次数: 2 Epoch: 0070 log_lik= 0.4394874 train_kl= 0.00868 train_loss= 0.44816 train_acc= 0.58690 val_roc= 0.91677 val_ap= 0.92190 time= 0.10655\n",
      "训练次数: 2 Epoch: 0071 log_lik= 0.43893617 train_kl= 0.00868 train_loss= 0.44761 train_acc= 0.58745 val_roc= 0.91707 val_ap= 0.92212 time= 0.11252\n",
      "训练次数: 2 Epoch: 0072 log_lik= 0.43836662 train_kl= 0.00868 train_loss= 0.44705 train_acc= 0.58813 val_roc= 0.91730 val_ap= 0.92243 time= 0.11756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 2 Epoch: 0073 log_lik= 0.43786013 train_kl= 0.00868 train_loss= 0.44654 train_acc= 0.58872 val_roc= 0.91736 val_ap= 0.92243 time= 0.11303\n",
      "训练次数: 2 Epoch: 0074 log_lik= 0.4373554 train_kl= 0.00868 train_loss= 0.44604 train_acc= 0.58959 val_roc= 0.91735 val_ap= 0.92222 time= 0.10755\n",
      "训练次数: 2 Epoch: 0075 log_lik= 0.43695098 train_kl= 0.00868 train_loss= 0.44563 train_acc= 0.58915 val_roc= 0.91722 val_ap= 0.92186 time= 0.10854\n",
      "训练次数: 2 Epoch: 0076 log_lik= 0.43642673 train_kl= 0.00868 train_loss= 0.44511 train_acc= 0.58979 val_roc= 0.91725 val_ap= 0.92182 time= 0.11053\n",
      "训练次数: 2 Epoch: 0077 log_lik= 0.43594876 train_kl= 0.00868 train_loss= 0.44463 train_acc= 0.58994 val_roc= 0.91716 val_ap= 0.92177 time= 0.10854\n",
      "训练次数: 2 Epoch: 0078 log_lik= 0.43547362 train_kl= 0.00868 train_loss= 0.44416 train_acc= 0.59022 val_roc= 0.91719 val_ap= 0.92207 time= 0.11053\n",
      "训练次数: 2 Epoch: 0079 log_lik= 0.4350462 train_kl= 0.00868 train_loss= 0.44373 train_acc= 0.58986 val_roc= 0.91754 val_ap= 0.92236 time= 0.10456\n",
      "训练次数: 2 Epoch: 0080 log_lik= 0.4345926 train_kl= 0.00869 train_loss= 0.44328 train_acc= 0.58948 val_roc= 0.91739 val_ap= 0.92225 time= 0.10754\n",
      "训练次数: 2 Epoch: 0081 log_lik= 0.4341706 train_kl= 0.00869 train_loss= 0.44286 train_acc= 0.59000 val_roc= 0.91751 val_ap= 0.92238 time= 0.10754\n",
      "训练次数: 2 Epoch: 0082 log_lik= 0.43375984 train_kl= 0.00869 train_loss= 0.44245 train_acc= 0.59025 val_roc= 0.91767 val_ap= 0.92265 time= 0.10655\n",
      "训练次数: 2 Epoch: 0083 log_lik= 0.433346 train_kl= 0.00869 train_loss= 0.44204 train_acc= 0.58966 val_roc= 0.91756 val_ap= 0.92274 time= 0.10854\n",
      "训练次数: 2 Epoch: 0084 log_lik= 0.43296257 train_kl= 0.00870 train_loss= 0.44166 train_acc= 0.58984 val_roc= 0.91709 val_ap= 0.92227 time= 0.10953\n",
      "训练次数: 2 Epoch: 0085 log_lik= 0.4325461 train_kl= 0.00870 train_loss= 0.44124 train_acc= 0.58955 val_roc= 0.91681 val_ap= 0.92229 time= 0.11451\n",
      "训练次数: 2 Epoch: 0086 log_lik= 0.43206984 train_kl= 0.00870 train_loss= 0.44077 train_acc= 0.58991 val_roc= 0.91639 val_ap= 0.92231 time= 0.10654\n",
      "训练次数: 2 Epoch: 0087 log_lik= 0.43175572 train_kl= 0.00870 train_loss= 0.44045 train_acc= 0.58919 val_roc= 0.91599 val_ap= 0.92213 time= 0.10754\n",
      "训练次数: 2 Epoch: 0088 log_lik= 0.4313521 train_kl= 0.00870 train_loss= 0.44005 train_acc= 0.58962 val_roc= 0.91593 val_ap= 0.92224 time= 0.10854\n",
      "训练次数: 2 Epoch: 0089 log_lik= 0.43097055 train_kl= 0.00870 train_loss= 0.43967 train_acc= 0.58885 val_roc= 0.91577 val_ap= 0.92193 time= 0.11551\n",
      "训练次数: 2 Epoch: 0090 log_lik= 0.43060446 train_kl= 0.00870 train_loss= 0.43930 train_acc= 0.58887 val_roc= 0.91561 val_ap= 0.92201 time= 0.10655\n",
      "训练次数: 2 Epoch: 0091 log_lik= 0.4302483 train_kl= 0.00870 train_loss= 0.43895 train_acc= 0.58943 val_roc= 0.91532 val_ap= 0.92196 time= 0.11452\n",
      "训练次数: 2 Epoch: 0092 log_lik= 0.42986816 train_kl= 0.00870 train_loss= 0.43857 train_acc= 0.58914 val_roc= 0.91492 val_ap= 0.92162 time= 0.10654\n",
      "训练次数: 2 Epoch: 0093 log_lik= 0.42953515 train_kl= 0.00870 train_loss= 0.43823 train_acc= 0.58856 val_roc= 0.91483 val_ap= 0.92147 time= 0.10854\n",
      "训练次数: 2 Epoch: 0094 log_lik= 0.42915004 train_kl= 0.00870 train_loss= 0.43785 train_acc= 0.58865 val_roc= 0.91489 val_ap= 0.92148 time= 0.10655\n",
      "训练次数: 2 Epoch: 0095 log_lik= 0.42885643 train_kl= 0.00870 train_loss= 0.43756 train_acc= 0.58828 val_roc= 0.91464 val_ap= 0.92152 time= 0.11255\n",
      "训练次数: 2 Epoch: 0096 log_lik= 0.42848557 train_kl= 0.00870 train_loss= 0.43719 train_acc= 0.58849 val_roc= 0.91443 val_ap= 0.92116 time= 0.10555\n",
      "训练次数: 2 Epoch: 0097 log_lik= 0.42815095 train_kl= 0.00871 train_loss= 0.43686 train_acc= 0.58840 val_roc= 0.91431 val_ap= 0.92099 time= 0.10854\n",
      "训练次数: 2 Epoch: 0098 log_lik= 0.4278059 train_kl= 0.00871 train_loss= 0.43651 train_acc= 0.58835 val_roc= 0.91405 val_ap= 0.92084 time= 0.11451\n",
      "训练次数: 2 Epoch: 0099 log_lik= 0.42748037 train_kl= 0.00871 train_loss= 0.43619 train_acc= 0.58909 val_roc= 0.91407 val_ap= 0.92107 time= 0.11053\n",
      "训练次数: 2 Epoch: 0100 log_lik= 0.42718178 train_kl= 0.00871 train_loss= 0.43589 train_acc= 0.58877 val_roc= 0.91408 val_ap= 0.92113 time= 0.10854\n",
      "Optimization Finished!\n",
      "训练次数: 2 ROC score: 0.9193962459807943\n",
      "训练次数: 2 AP score: 0.9269273476924795\n",
      "训练次数: 3 Epoch: 0001 log_lik= 0.7834847 train_kl= 0.00805 train_loss= 0.79154 train_acc= 0.07434 val_roc= 0.68944 val_ap= 0.72366 time= 7.59472\n",
      "训练次数: 3 Epoch: 0002 log_lik= 0.84937304 train_kl= 0.00835 train_loss= 0.85772 train_acc= 0.00159 val_roc= 0.77487 val_ap= 0.79427 time= 0.11547\n",
      "训练次数: 3 Epoch: 0003 log_lik= 0.7423017 train_kl= 0.00810 train_loss= 0.75041 train_acc= 0.00647 val_roc= 0.72404 val_ap= 0.74397 time= 0.11252\n",
      "训练次数: 3 Epoch: 0004 log_lik= 0.7350754 train_kl= 0.00813 train_loss= 0.74321 train_acc= 0.01101 val_roc= 0.71269 val_ap= 0.73640 time= 0.12646\n",
      "训练次数: 3 Epoch: 0005 log_lik= 0.7434402 train_kl= 0.00822 train_loss= 0.75166 train_acc= 0.00724 val_roc= 0.75321 val_ap= 0.76874 time= 0.11551\n",
      "训练次数: 3 Epoch: 0006 log_lik= 0.7207527 train_kl= 0.00819 train_loss= 0.72895 train_acc= 0.01391 val_roc= 0.81421 val_ap= 0.81472 time= 0.11313\n",
      "训练次数: 3 Epoch: 0007 log_lik= 0.6993922 train_kl= 0.00817 train_loss= 0.70756 train_acc= 0.05348 val_roc= 0.84973 val_ap= 0.83283 time= 0.11352\n",
      "训练次数: 3 Epoch: 0008 log_lik= 0.68041754 train_kl= 0.00817 train_loss= 0.68859 train_acc= 0.15249 val_roc= 0.85475 val_ap= 0.83176 time= 0.10984\n",
      "训练次数: 3 Epoch: 0009 log_lik= 0.65526974 train_kl= 0.00819 train_loss= 0.66346 train_acc= 0.26201 val_roc= 0.85394 val_ap= 0.83142 time= 0.10655\n",
      "训练次数: 3 Epoch: 0010 log_lik= 0.62613755 train_kl= 0.00824 train_loss= 0.63438 train_acc= 0.33494 val_roc= 0.84980 val_ap= 0.82957 time= 0.11053\n",
      "训练次数: 3 Epoch: 0011 log_lik= 0.6025082 train_kl= 0.00831 train_loss= 0.61082 train_acc= 0.37249 val_roc= 0.84622 val_ap= 0.82664 time= 0.11352\n",
      "训练次数: 3 Epoch: 0012 log_lik= 0.5882771 train_kl= 0.00838 train_loss= 0.59666 train_acc= 0.40504 val_roc= 0.84684 val_ap= 0.82573 time= 0.11451\n",
      "训练次数: 3 Epoch: 0013 log_lik= 0.576338 train_kl= 0.00845 train_loss= 0.58479 train_acc= 0.44263 val_roc= 0.84654 val_ap= 0.82556 time= 0.11053\n",
      "训练次数: 3 Epoch: 0014 log_lik= 0.57100904 train_kl= 0.00851 train_loss= 0.57952 train_acc= 0.47136 val_roc= 0.84596 val_ap= 0.82696 time= 0.10754\n",
      "训练次数: 3 Epoch: 0015 log_lik= 0.5657306 train_kl= 0.00855 train_loss= 0.57428 train_acc= 0.49069 val_roc= 0.84746 val_ap= 0.83251 time= 0.10854\n",
      "训练次数: 3 Epoch: 0016 log_lik= 0.55660987 train_kl= 0.00856 train_loss= 0.56517 train_acc= 0.50593 val_roc= 0.85092 val_ap= 0.83947 time= 0.10954\n",
      "训练次数: 3 Epoch: 0017 log_lik= 0.5463063 train_kl= 0.00855 train_loss= 0.55485 train_acc= 0.51574 val_roc= 0.85496 val_ap= 0.84600 time= 0.11352\n",
      "训练次数: 3 Epoch: 0018 log_lik= 0.53598106 train_kl= 0.00853 train_loss= 0.54451 train_acc= 0.52205 val_roc= 0.85994 val_ap= 0.85246 time= 0.10555\n",
      "训练次数: 3 Epoch: 0019 log_lik= 0.5273481 train_kl= 0.00850 train_loss= 0.53585 train_acc= 0.52514 val_roc= 0.86396 val_ap= 0.85752 time= 0.10555\n",
      "训练次数: 3 Epoch: 0020 log_lik= 0.5214359 train_kl= 0.00847 train_loss= 0.52991 train_acc= 0.52484 val_roc= 0.86848 val_ap= 0.86336 time= 0.11850\n",
      "训练次数: 3 Epoch: 0021 log_lik= 0.5173983 train_kl= 0.00846 train_loss= 0.52585 train_acc= 0.52315 val_roc= 0.87173 val_ap= 0.86711 time= 0.11053\n",
      "训练次数: 3 Epoch: 0022 log_lik= 0.5141808 train_kl= 0.00845 train_loss= 0.52263 train_acc= 0.51926 val_roc= 0.87567 val_ap= 0.87243 time= 0.11551\n",
      "训练次数: 3 Epoch: 0023 log_lik= 0.51056707 train_kl= 0.00845 train_loss= 0.51902 train_acc= 0.51463 val_roc= 0.88022 val_ap= 0.87581 time= 0.11252\n",
      "训练次数: 3 Epoch: 0024 log_lik= 0.50712913 train_kl= 0.00847 train_loss= 0.51560 train_acc= 0.51111 val_roc= 0.88384 val_ap= 0.87839 time= 0.11650\n",
      "训练次数: 3 Epoch: 0025 log_lik= 0.5038568 train_kl= 0.00849 train_loss= 0.51234 train_acc= 0.51013 val_roc= 0.88625 val_ap= 0.88042 time= 0.10455\n",
      "训练次数: 3 Epoch: 0026 log_lik= 0.50125265 train_kl= 0.00851 train_loss= 0.50976 train_acc= 0.51026 val_roc= 0.88731 val_ap= 0.87990 time= 0.10456\n",
      "训练次数: 3 Epoch: 0027 log_lik= 0.4992098 train_kl= 0.00853 train_loss= 0.50774 train_acc= 0.51132 val_roc= 0.88749 val_ap= 0.87899 time= 0.10854\n",
      "训练次数: 3 Epoch: 0028 log_lik= 0.49708837 train_kl= 0.00854 train_loss= 0.50563 train_acc= 0.51310 val_roc= 0.88775 val_ap= 0.87768 time= 0.12248\n",
      "训练次数: 3 Epoch: 0029 log_lik= 0.4953951 train_kl= 0.00855 train_loss= 0.50395 train_acc= 0.51453 val_roc= 0.88801 val_ap= 0.87695 time= 0.10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0030 log_lik= 0.493152 train_kl= 0.00856 train_loss= 0.50171 train_acc= 0.51700 val_roc= 0.88856 val_ap= 0.87713 time= 0.10954\n",
      "训练次数: 3 Epoch: 0031 log_lik= 0.4901003 train_kl= 0.00856 train_loss= 0.49866 train_acc= 0.52001 val_roc= 0.88981 val_ap= 0.87679 time= 0.10456\n",
      "训练次数: 3 Epoch: 0032 log_lik= 0.48694566 train_kl= 0.00855 train_loss= 0.49550 train_acc= 0.52203 val_roc= 0.89104 val_ap= 0.87649 time= 0.11153\n",
      "训练次数: 3 Epoch: 0033 log_lik= 0.48462504 train_kl= 0.00854 train_loss= 0.49317 train_acc= 0.52492 val_roc= 0.89203 val_ap= 0.87793 time= 0.11451\n",
      "训练次数: 3 Epoch: 0034 log_lik= 0.48216787 train_kl= 0.00854 train_loss= 0.49071 train_acc= 0.52644 val_roc= 0.89252 val_ap= 0.87782 time= 0.10807\n",
      "训练次数: 3 Epoch: 0035 log_lik= 0.48028412 train_kl= 0.00854 train_loss= 0.48882 train_acc= 0.52906 val_roc= 0.89218 val_ap= 0.87703 time= 0.10755\n",
      "训练次数: 3 Epoch: 0036 log_lik= 0.47868517 train_kl= 0.00854 train_loss= 0.48723 train_acc= 0.53024 val_roc= 0.89271 val_ap= 0.87753 time= 0.10655\n",
      "训练次数: 3 Epoch: 0037 log_lik= 0.47767296 train_kl= 0.00855 train_loss= 0.48622 train_acc= 0.53087 val_roc= 0.89312 val_ap= 0.87771 time= 0.10706\n",
      "训练次数: 3 Epoch: 0038 log_lik= 0.47709957 train_kl= 0.00856 train_loss= 0.48566 train_acc= 0.53078 val_roc= 0.89393 val_ap= 0.87916 time= 0.10356\n",
      "训练次数: 3 Epoch: 0039 log_lik= 0.4764211 train_kl= 0.00857 train_loss= 0.48499 train_acc= 0.53115 val_roc= 0.89518 val_ap= 0.88059 time= 0.11054\n",
      "训练次数: 3 Epoch: 0040 log_lik= 0.47538623 train_kl= 0.00858 train_loss= 0.48396 train_acc= 0.53155 val_roc= 0.89647 val_ap= 0.88147 time= 0.10562\n",
      "训练次数: 3 Epoch: 0041 log_lik= 0.4742944 train_kl= 0.00858 train_loss= 0.48287 train_acc= 0.53253 val_roc= 0.89761 val_ap= 0.88216 time= 0.10356\n",
      "训练次数: 3 Epoch: 0042 log_lik= 0.47320837 train_kl= 0.00858 train_loss= 0.48179 train_acc= 0.53319 val_roc= 0.89864 val_ap= 0.88381 time= 0.10655\n",
      "训练次数: 3 Epoch: 0043 log_lik= 0.472063 train_kl= 0.00858 train_loss= 0.48064 train_acc= 0.53383 val_roc= 0.89902 val_ap= 0.88403 time= 0.12248\n",
      "训练次数: 3 Epoch: 0044 log_lik= 0.47094214 train_kl= 0.00858 train_loss= 0.47952 train_acc= 0.53415 val_roc= 0.89945 val_ap= 0.88472 time= 0.10755\n",
      "训练次数: 3 Epoch: 0045 log_lik= 0.46976066 train_kl= 0.00857 train_loss= 0.47834 train_acc= 0.53549 val_roc= 0.90000 val_ap= 0.88616 time= 0.11418\n",
      "训练次数: 3 Epoch: 0046 log_lik= 0.468501 train_kl= 0.00857 train_loss= 0.47707 train_acc= 0.53625 val_roc= 0.90098 val_ap= 0.88743 time= 0.10954\n",
      "训练次数: 3 Epoch: 0047 log_lik= 0.46755427 train_kl= 0.00857 train_loss= 0.47613 train_acc= 0.53725 val_roc= 0.90217 val_ap= 0.88850 time= 0.10654\n",
      "训练次数: 3 Epoch: 0048 log_lik= 0.46633056 train_kl= 0.00857 train_loss= 0.47490 train_acc= 0.53746 val_roc= 0.90318 val_ap= 0.89048 time= 0.11551\n",
      "训练次数: 3 Epoch: 0049 log_lik= 0.4654003 train_kl= 0.00858 train_loss= 0.47398 train_acc= 0.53786 val_roc= 0.90389 val_ap= 0.89215 time= 0.10456\n",
      "训练次数: 3 Epoch: 0050 log_lik= 0.46463135 train_kl= 0.00858 train_loss= 0.47321 train_acc= 0.53781 val_roc= 0.90473 val_ap= 0.89344 time= 0.10854\n",
      "训练次数: 3 Epoch: 0051 log_lik= 0.4637642 train_kl= 0.00858 train_loss= 0.47235 train_acc= 0.53777 val_roc= 0.90580 val_ap= 0.89510 time= 0.10854\n",
      "训练次数: 3 Epoch: 0052 log_lik= 0.46293253 train_kl= 0.00859 train_loss= 0.47152 train_acc= 0.53834 val_roc= 0.90603 val_ap= 0.89527 time= 0.11504\n",
      "训练次数: 3 Epoch: 0053 log_lik= 0.46201313 train_kl= 0.00859 train_loss= 0.47060 train_acc= 0.53858 val_roc= 0.90669 val_ap= 0.89660 time= 0.10954\n",
      "训练次数: 3 Epoch: 0054 log_lik= 0.4611248 train_kl= 0.00859 train_loss= 0.46972 train_acc= 0.53887 val_roc= 0.90685 val_ap= 0.89692 time= 0.11650\n",
      "训练次数: 3 Epoch: 0055 log_lik= 0.46034226 train_kl= 0.00859 train_loss= 0.46894 train_acc= 0.53917 val_roc= 0.90737 val_ap= 0.89741 time= 0.10754\n",
      "训练次数: 3 Epoch: 0056 log_lik= 0.45939097 train_kl= 0.00859 train_loss= 0.46799 train_acc= 0.53993 val_roc= 0.90726 val_ap= 0.89759 time= 0.10655\n",
      "训练次数: 3 Epoch: 0057 log_lik= 0.4585632 train_kl= 0.00860 train_loss= 0.46716 train_acc= 0.54052 val_roc= 0.90759 val_ap= 0.89727 time= 0.10953\n",
      "训练次数: 3 Epoch: 0058 log_lik= 0.45757955 train_kl= 0.00860 train_loss= 0.46618 train_acc= 0.54225 val_roc= 0.90814 val_ap= 0.89741 time= 0.11452\n",
      "训练次数: 3 Epoch: 0059 log_lik= 0.45679033 train_kl= 0.00860 train_loss= 0.46539 train_acc= 0.54283 val_roc= 0.90902 val_ap= 0.89771 time= 0.11351\n",
      "训练次数: 3 Epoch: 0060 log_lik= 0.45596454 train_kl= 0.00860 train_loss= 0.46457 train_acc= 0.54402 val_roc= 0.90948 val_ap= 0.89773 time= 0.11053\n",
      "训练次数: 3 Epoch: 0061 log_lik= 0.45520675 train_kl= 0.00861 train_loss= 0.46382 train_acc= 0.54514 val_roc= 0.90974 val_ap= 0.89776 time= 0.11053\n",
      "训练次数: 3 Epoch: 0062 log_lik= 0.4543522 train_kl= 0.00861 train_loss= 0.46297 train_acc= 0.54666 val_roc= 0.90970 val_ap= 0.89745 time= 0.11352\n",
      "训练次数: 3 Epoch: 0063 log_lik= 0.4536131 train_kl= 0.00862 train_loss= 0.46223 train_acc= 0.54694 val_roc= 0.90990 val_ap= 0.89759 time= 0.11252\n",
      "训练次数: 3 Epoch: 0064 log_lik= 0.45299038 train_kl= 0.00862 train_loss= 0.46161 train_acc= 0.54797 val_roc= 0.91039 val_ap= 0.89796 time= 0.10854\n",
      "训练次数: 3 Epoch: 0065 log_lik= 0.45240143 train_kl= 0.00862 train_loss= 0.46102 train_acc= 0.54884 val_roc= 0.91062 val_ap= 0.89791 time= 0.10854\n",
      "训练次数: 3 Epoch: 0066 log_lik= 0.45170727 train_kl= 0.00862 train_loss= 0.46033 train_acc= 0.54965 val_roc= 0.91094 val_ap= 0.89839 time= 0.10712\n",
      "训练次数: 3 Epoch: 0067 log_lik= 0.45110178 train_kl= 0.00863 train_loss= 0.45973 train_acc= 0.55059 val_roc= 0.91154 val_ap= 0.89917 time= 0.10754\n",
      "训练次数: 3 Epoch: 0068 log_lik= 0.45039558 train_kl= 0.00863 train_loss= 0.45902 train_acc= 0.55144 val_roc= 0.91240 val_ap= 0.90057 time= 0.11650\n",
      "训练次数: 3 Epoch: 0069 log_lik= 0.4497538 train_kl= 0.00863 train_loss= 0.45838 train_acc= 0.55235 val_roc= 0.91304 val_ap= 0.90145 time= 0.11451\n",
      "训练次数: 3 Epoch: 0070 log_lik= 0.44911855 train_kl= 0.00863 train_loss= 0.45775 train_acc= 0.55330 val_roc= 0.91327 val_ap= 0.90219 time= 0.10854\n",
      "训练次数: 3 Epoch: 0071 log_lik= 0.4485313 train_kl= 0.00863 train_loss= 0.45716 train_acc= 0.55431 val_roc= 0.91318 val_ap= 0.90203 time= 0.10954\n",
      "训练次数: 3 Epoch: 0072 log_lik= 0.4479437 train_kl= 0.00863 train_loss= 0.45658 train_acc= 0.55459 val_roc= 0.91298 val_ap= 0.90183 time= 0.12540\n",
      "训练次数: 3 Epoch: 0073 log_lik= 0.44741762 train_kl= 0.00863 train_loss= 0.45605 train_acc= 0.55541 val_roc= 0.91271 val_ap= 0.90190 time= 0.10854\n",
      "训练次数: 3 Epoch: 0074 log_lik= 0.44685575 train_kl= 0.00864 train_loss= 0.45549 train_acc= 0.55634 val_roc= 0.91249 val_ap= 0.90254 time= 0.10755\n",
      "训练次数: 3 Epoch: 0075 log_lik= 0.44631854 train_kl= 0.00864 train_loss= 0.45496 train_acc= 0.55739 val_roc= 0.91259 val_ap= 0.90287 time= 0.10953\n",
      "训练次数: 3 Epoch: 0076 log_lik= 0.44577125 train_kl= 0.00864 train_loss= 0.45441 train_acc= 0.55836 val_roc= 0.91242 val_ap= 0.90332 time= 0.11659\n",
      "训练次数: 3 Epoch: 0077 log_lik= 0.44526 train_kl= 0.00864 train_loss= 0.45390 train_acc= 0.55900 val_roc= 0.91219 val_ap= 0.90316 time= 0.11353\n",
      "训练次数: 3 Epoch: 0078 log_lik= 0.44469503 train_kl= 0.00864 train_loss= 0.45334 train_acc= 0.55975 val_roc= 0.91162 val_ap= 0.90287 time= 0.11651\n",
      "训练次数: 3 Epoch: 0079 log_lik= 0.44425604 train_kl= 0.00864 train_loss= 0.45290 train_acc= 0.56046 val_roc= 0.91110 val_ap= 0.90220 time= 0.11252\n",
      "训练次数: 3 Epoch: 0080 log_lik= 0.44380257 train_kl= 0.00864 train_loss= 0.45245 train_acc= 0.56126 val_roc= 0.91064 val_ap= 0.90189 time= 0.11551\n",
      "训练次数: 3 Epoch: 0081 log_lik= 0.44332415 train_kl= 0.00865 train_loss= 0.45197 train_acc= 0.56174 val_roc= 0.91018 val_ap= 0.90170 time= 0.11352\n",
      "训练次数: 3 Epoch: 0082 log_lik= 0.4429104 train_kl= 0.00865 train_loss= 0.45156 train_acc= 0.56261 val_roc= 0.90996 val_ap= 0.90158 time= 0.11152\n",
      "训练次数: 3 Epoch: 0083 log_lik= 0.4424358 train_kl= 0.00865 train_loss= 0.45108 train_acc= 0.56381 val_roc= 0.90989 val_ap= 0.90178 time= 0.11750\n",
      "训练次数: 3 Epoch: 0084 log_lik= 0.4419775 train_kl= 0.00865 train_loss= 0.45063 train_acc= 0.56409 val_roc= 0.90971 val_ap= 0.90167 time= 0.11619\n",
      "训练次数: 3 Epoch: 0085 log_lik= 0.44164985 train_kl= 0.00865 train_loss= 0.45030 train_acc= 0.56495 val_roc= 0.90968 val_ap= 0.90173 time= 0.10655\n",
      "训练次数: 3 Epoch: 0086 log_lik= 0.44134474 train_kl= 0.00865 train_loss= 0.45000 train_acc= 0.56545 val_roc= 0.90945 val_ap= 0.90100 time= 0.10953\n",
      "训练次数: 3 Epoch: 0087 log_lik= 0.44092324 train_kl= 0.00866 train_loss= 0.44958 train_acc= 0.56564 val_roc= 0.90950 val_ap= 0.90094 time= 0.11153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 3 Epoch: 0088 log_lik= 0.44058734 train_kl= 0.00866 train_loss= 0.44924 train_acc= 0.56625 val_roc= 0.90940 val_ap= 0.90080 time= 0.11651\n",
      "训练次数: 3 Epoch: 0089 log_lik= 0.44020584 train_kl= 0.00866 train_loss= 0.44886 train_acc= 0.56657 val_roc= 0.90950 val_ap= 0.90090 time= 0.11370\n",
      "训练次数: 3 Epoch: 0090 log_lik= 0.43995005 train_kl= 0.00866 train_loss= 0.44861 train_acc= 0.56690 val_roc= 0.90945 val_ap= 0.90098 time= 0.10754\n",
      "训练次数: 3 Epoch: 0091 log_lik= 0.43962303 train_kl= 0.00866 train_loss= 0.44828 train_acc= 0.56784 val_roc= 0.90960 val_ap= 0.90123 time= 0.10954\n",
      "训练次数: 3 Epoch: 0092 log_lik= 0.43933687 train_kl= 0.00866 train_loss= 0.44800 train_acc= 0.56800 val_roc= 0.91005 val_ap= 0.90191 time= 0.10555\n",
      "训练次数: 3 Epoch: 0093 log_lik= 0.43898362 train_kl= 0.00866 train_loss= 0.44765 train_acc= 0.56816 val_roc= 0.91041 val_ap= 0.90202 time= 0.11551\n",
      "训练次数: 3 Epoch: 0094 log_lik= 0.4387166 train_kl= 0.00866 train_loss= 0.44738 train_acc= 0.56885 val_roc= 0.91055 val_ap= 0.90244 time= 0.11053\n",
      "训练次数: 3 Epoch: 0095 log_lik= 0.4383252 train_kl= 0.00866 train_loss= 0.44699 train_acc= 0.56966 val_roc= 0.91088 val_ap= 0.90258 time= 0.11153\n",
      "训练次数: 3 Epoch: 0096 log_lik= 0.4380497 train_kl= 0.00866 train_loss= 0.44671 train_acc= 0.57019 val_roc= 0.91109 val_ap= 0.90273 time= 0.11750\n",
      "训练次数: 3 Epoch: 0097 log_lik= 0.43779764 train_kl= 0.00866 train_loss= 0.44646 train_acc= 0.57090 val_roc= 0.91122 val_ap= 0.90319 time= 0.10655\n",
      "训练次数: 3 Epoch: 0098 log_lik= 0.4374972 train_kl= 0.00867 train_loss= 0.44616 train_acc= 0.57109 val_roc= 0.91161 val_ap= 0.90350 time= 0.10607\n",
      "训练次数: 3 Epoch: 0099 log_lik= 0.43723974 train_kl= 0.00867 train_loss= 0.44591 train_acc= 0.57147 val_roc= 0.91180 val_ap= 0.90377 time= 0.10807\n",
      "训练次数: 3 Epoch: 0100 log_lik= 0.4369567 train_kl= 0.00867 train_loss= 0.44562 train_acc= 0.57180 val_roc= 0.91200 val_ap= 0.90430 time= 0.11551\n",
      "Optimization Finished!\n",
      "训练次数: 3 ROC score: 0.9045148328046406\n",
      "训练次数: 3 AP score: 0.914904265733392\n",
      "训练次数: 4 Epoch: 0001 log_lik= 0.7807729 train_kl= 0.00806 train_loss= 0.78883 train_acc= 0.00964 val_roc= 0.64551 val_ap= 0.66365 time= 7.82003\n",
      "训练次数: 4 Epoch: 0002 log_lik= 0.99985754 train_kl= 0.00847 train_loss= 1.00833 train_acc= 0.00159 val_roc= 0.69935 val_ap= 0.69516 time= 0.12840\n",
      "训练次数: 4 Epoch: 0003 log_lik= 0.75076 train_kl= 0.00810 train_loss= 0.75886 train_acc= 0.00180 val_roc= 0.65164 val_ap= 0.65880 time= 0.12148\n",
      "训练次数: 4 Epoch: 0004 log_lik= 0.77061146 train_kl= 0.00807 train_loss= 0.77868 train_acc= 0.01544 val_roc= 0.62699 val_ap= 0.64101 time= 0.11255\n",
      "训练次数: 4 Epoch: 0005 log_lik= 0.7648523 train_kl= 0.00808 train_loss= 0.77293 train_acc= 0.00788 val_roc= 0.62622 val_ap= 0.64030 time= 0.11977\n",
      "训练次数: 4 Epoch: 0006 log_lik= 0.75562394 train_kl= 0.00811 train_loss= 0.76374 train_acc= 0.00331 val_roc= 0.62817 val_ap= 0.64244 time= 0.10953\n",
      "训练次数: 4 Epoch: 0007 log_lik= 0.75524473 train_kl= 0.00816 train_loss= 0.76341 train_acc= 0.00235 val_roc= 0.63122 val_ap= 0.64717 time= 0.10954\n",
      "训练次数: 4 Epoch: 0008 log_lik= 0.7602744 train_kl= 0.00819 train_loss= 0.76846 train_acc= 0.00239 val_roc= 0.63365 val_ap= 0.65033 time= 0.12049\n",
      "训练次数: 4 Epoch: 0009 log_lik= 0.7566476 train_kl= 0.00818 train_loss= 0.76483 train_acc= 0.00224 val_roc= 0.63614 val_ap= 0.65306 time= 0.10953\n",
      "训练次数: 4 Epoch: 0010 log_lik= 0.7504082 train_kl= 0.00816 train_loss= 0.75857 train_acc= 0.00248 val_roc= 0.63854 val_ap= 0.65489 time= 0.10953\n",
      "训练次数: 4 Epoch: 0011 log_lik= 0.7471294 train_kl= 0.00814 train_loss= 0.75527 train_acc= 0.00295 val_roc= 0.64348 val_ap= 0.66169 time= 0.10953\n",
      "训练次数: 4 Epoch: 0012 log_lik= 0.74626833 train_kl= 0.00813 train_loss= 0.75439 train_acc= 0.00506 val_roc= 0.64760 val_ap= 0.66853 time= 0.10754\n",
      "训练次数: 4 Epoch: 0013 log_lik= 0.74539906 train_kl= 0.00812 train_loss= 0.75352 train_acc= 0.00827 val_roc= 0.65236 val_ap= 0.67385 time= 0.11153\n",
      "训练次数: 4 Epoch: 0014 log_lik= 0.74373007 train_kl= 0.00811 train_loss= 0.75184 train_acc= 0.01401 val_roc= 0.65713 val_ap= 0.68026 time= 0.11252\n",
      "训练次数: 4 Epoch: 0015 log_lik= 0.7403227 train_kl= 0.00812 train_loss= 0.74844 train_acc= 0.01668 val_roc= 0.66171 val_ap= 0.68563 time= 0.10953\n",
      "训练次数: 4 Epoch: 0016 log_lik= 0.73582023 train_kl= 0.00812 train_loss= 0.74394 train_acc= 0.02730 val_roc= 0.66664 val_ap= 0.69231 time= 0.11750\n",
      "训练次数: 4 Epoch: 0017 log_lik= 0.73027086 train_kl= 0.00813 train_loss= 0.73841 train_acc= 0.03262 val_roc= 0.67373 val_ap= 0.70033 time= 0.12148\n",
      "训练次数: 4 Epoch: 0018 log_lik= 0.7240147 train_kl= 0.00815 train_loss= 0.73217 train_acc= 0.03855 val_roc= 0.67767 val_ap= 0.70370 time= 0.11853\n",
      "训练次数: 4 Epoch: 0019 log_lik= 0.71896195 train_kl= 0.00817 train_loss= 0.72713 train_acc= 0.04334 val_roc= 0.68392 val_ap= 0.70658 time= 0.11801\n",
      "训练次数: 4 Epoch: 0020 log_lik= 0.71430075 train_kl= 0.00819 train_loss= 0.72249 train_acc= 0.04952 val_roc= 0.69421 val_ap= 0.71004 time= 0.10754\n",
      "训练次数: 4 Epoch: 0021 log_lik= 0.7073152 train_kl= 0.00820 train_loss= 0.71551 train_acc= 0.06409 val_roc= 0.71072 val_ap= 0.71958 time= 0.11452\n",
      "训练次数: 4 Epoch: 0022 log_lik= 0.69811076 train_kl= 0.00820 train_loss= 0.70631 train_acc= 0.08576 val_roc= 0.72874 val_ap= 0.72833 time= 0.11252\n",
      "训练次数: 4 Epoch: 0023 log_lik= 0.68817997 train_kl= 0.00820 train_loss= 0.69638 train_acc= 0.12957 val_roc= 0.74376 val_ap= 0.73671 time= 0.11850\n",
      "训练次数: 4 Epoch: 0024 log_lik= 0.679087 train_kl= 0.00820 train_loss= 0.68728 train_acc= 0.18678 val_roc= 0.75253 val_ap= 0.73954 time= 0.11252\n",
      "训练次数: 4 Epoch: 0025 log_lik= 0.6701802 train_kl= 0.00820 train_loss= 0.67838 train_acc= 0.25560 val_roc= 0.75847 val_ap= 0.74233 time= 0.11551\n",
      "训练次数: 4 Epoch: 0026 log_lik= 0.6612249 train_kl= 0.00821 train_loss= 0.66944 train_acc= 0.30912 val_roc= 0.75985 val_ap= 0.74114 time= 0.11551\n",
      "训练次数: 4 Epoch: 0027 log_lik= 0.6520886 train_kl= 0.00823 train_loss= 0.66032 train_acc= 0.35310 val_roc= 0.76199 val_ap= 0.74193 time= 0.11651\n",
      "训练次数: 4 Epoch: 0028 log_lik= 0.643967 train_kl= 0.00825 train_loss= 0.65222 train_acc= 0.38824 val_roc= 0.76367 val_ap= 0.74252 time= 0.11651\n",
      "训练次数: 4 Epoch: 0029 log_lik= 0.636318 train_kl= 0.00827 train_loss= 0.64459 train_acc= 0.41494 val_roc= 0.76578 val_ap= 0.74337 time= 0.10665\n",
      "训练次数: 4 Epoch: 0030 log_lik= 0.62959594 train_kl= 0.00830 train_loss= 0.63789 train_acc= 0.43612 val_roc= 0.76774 val_ap= 0.75025 time= 0.11053\n",
      "训练次数: 4 Epoch: 0031 log_lik= 0.62393266 train_kl= 0.00832 train_loss= 0.63225 train_acc= 0.44905 val_roc= 0.76978 val_ap= 0.75392 time= 0.11054\n",
      "训练次数: 4 Epoch: 0032 log_lik= 0.61918426 train_kl= 0.00834 train_loss= 0.62752 train_acc= 0.46247 val_roc= 0.77275 val_ap= 0.75705 time= 0.11949\n",
      "训练次数: 4 Epoch: 0033 log_lik= 0.61505246 train_kl= 0.00835 train_loss= 0.62341 train_acc= 0.47197 val_roc= 0.77619 val_ap= 0.76147 time= 0.11750\n",
      "训练次数: 4 Epoch: 0034 log_lik= 0.60986525 train_kl= 0.00837 train_loss= 0.61823 train_acc= 0.48156 val_roc= 0.78082 val_ap= 0.76606 time= 0.10555\n",
      "训练次数: 4 Epoch: 0035 log_lik= 0.6042845 train_kl= 0.00838 train_loss= 0.61266 train_acc= 0.48533 val_roc= 0.78493 val_ap= 0.77126 time= 0.11451\n",
      "训练次数: 4 Epoch: 0036 log_lik= 0.59903073 train_kl= 0.00839 train_loss= 0.60742 train_acc= 0.48764 val_roc= 0.78875 val_ap= 0.77481 time= 0.10555\n",
      "训练次数: 4 Epoch: 0037 log_lik= 0.594134 train_kl= 0.00839 train_loss= 0.60252 train_acc= 0.48765 val_roc= 0.79363 val_ap= 0.78025 time= 0.10954\n",
      "训练次数: 4 Epoch: 0038 log_lik= 0.58929515 train_kl= 0.00839 train_loss= 0.59768 train_acc= 0.49054 val_roc= 0.79932 val_ap= 0.78596 time= 0.10953\n",
      "训练次数: 4 Epoch: 0039 log_lik= 0.5841484 train_kl= 0.00838 train_loss= 0.59253 train_acc= 0.49785 val_roc= 0.80543 val_ap= 0.79246 time= 0.11252\n",
      "训练次数: 4 Epoch: 0040 log_lik= 0.5786756 train_kl= 0.00838 train_loss= 0.58706 train_acc= 0.50957 val_roc= 0.81049 val_ap= 0.79681 time= 0.11750\n",
      "训练次数: 4 Epoch: 0041 log_lik= 0.57381314 train_kl= 0.00838 train_loss= 0.58220 train_acc= 0.51890 val_roc= 0.81493 val_ap= 0.80089 time= 0.11236\n",
      "训练次数: 4 Epoch: 0042 log_lik= 0.5693253 train_kl= 0.00839 train_loss= 0.57772 train_acc= 0.52727 val_roc= 0.81912 val_ap= 0.80424 time= 0.10853\n",
      "训练次数: 4 Epoch: 0043 log_lik= 0.56506395 train_kl= 0.00840 train_loss= 0.57347 train_acc= 0.53201 val_roc= 0.82339 val_ap= 0.80809 time= 0.11053\n",
      "训练次数: 4 Epoch: 0044 log_lik= 0.561188 train_kl= 0.00842 train_loss= 0.56961 train_acc= 0.53456 val_roc= 0.82809 val_ap= 0.81282 time= 0.11152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 4 Epoch: 0045 log_lik= 0.557435 train_kl= 0.00843 train_loss= 0.56587 train_acc= 0.53700 val_roc= 0.83186 val_ap= 0.81633 time= 0.11950\n",
      "训练次数: 4 Epoch: 0046 log_lik= 0.5538593 train_kl= 0.00845 train_loss= 0.56231 train_acc= 0.54164 val_roc= 0.83646 val_ap= 0.82299 time= 0.12248\n",
      "训练次数: 4 Epoch: 0047 log_lik= 0.5503837 train_kl= 0.00846 train_loss= 0.55884 train_acc= 0.54229 val_roc= 0.83919 val_ap= 0.82726 time= 0.12348\n",
      "训练次数: 4 Epoch: 0048 log_lik= 0.54713535 train_kl= 0.00847 train_loss= 0.55561 train_acc= 0.54683 val_roc= 0.84174 val_ap= 0.83037 time= 0.12671\n",
      "训练次数: 4 Epoch: 0049 log_lik= 0.5441116 train_kl= 0.00848 train_loss= 0.55259 train_acc= 0.55043 val_roc= 0.84207 val_ap= 0.83128 time= 0.12447\n",
      "训练次数: 4 Epoch: 0050 log_lik= 0.54123604 train_kl= 0.00849 train_loss= 0.54972 train_acc= 0.55454 val_roc= 0.84213 val_ap= 0.83232 time= 0.11451\n",
      "训练次数: 4 Epoch: 0051 log_lik= 0.53853154 train_kl= 0.00849 train_loss= 0.54702 train_acc= 0.55846 val_roc= 0.84252 val_ap= 0.83373 time= 0.11750\n",
      "训练次数: 4 Epoch: 0052 log_lik= 0.5364304 train_kl= 0.00849 train_loss= 0.54492 train_acc= 0.56195 val_roc= 0.84301 val_ap= 0.83581 time= 0.12061\n",
      "训练次数: 4 Epoch: 0053 log_lik= 0.53396344 train_kl= 0.00849 train_loss= 0.54245 train_acc= 0.56638 val_roc= 0.84390 val_ap= 0.83771 time= 0.10706\n",
      "训练次数: 4 Epoch: 0054 log_lik= 0.53165674 train_kl= 0.00849 train_loss= 0.54014 train_acc= 0.56807 val_roc= 0.84392 val_ap= 0.83780 time= 0.10655\n",
      "训练次数: 4 Epoch: 0055 log_lik= 0.52918863 train_kl= 0.00849 train_loss= 0.53767 train_acc= 0.57234 val_roc= 0.84367 val_ap= 0.83807 time= 0.11055\n",
      "训练次数: 4 Epoch: 0056 log_lik= 0.5267587 train_kl= 0.00848 train_loss= 0.53524 train_acc= 0.57387 val_roc= 0.84359 val_ap= 0.83807 time= 0.10555\n",
      "训练次数: 4 Epoch: 0057 log_lik= 0.52415967 train_kl= 0.00848 train_loss= 0.53264 train_acc= 0.57664 val_roc= 0.84299 val_ap= 0.83648 time= 0.11352\n",
      "训练次数: 4 Epoch: 0058 log_lik= 0.5221372 train_kl= 0.00848 train_loss= 0.53062 train_acc= 0.57879 val_roc= 0.84253 val_ap= 0.83521 time= 0.10754\n",
      "训练次数: 4 Epoch: 0059 log_lik= 0.52000237 train_kl= 0.00848 train_loss= 0.52848 train_acc= 0.58177 val_roc= 0.84216 val_ap= 0.83370 time= 0.10854\n",
      "训练次数: 4 Epoch: 0060 log_lik= 0.5179823 train_kl= 0.00848 train_loss= 0.52646 train_acc= 0.58329 val_roc= 0.84201 val_ap= 0.83263 time= 0.10954\n",
      "训练次数: 4 Epoch: 0061 log_lik= 0.5162294 train_kl= 0.00848 train_loss= 0.52471 train_acc= 0.58509 val_roc= 0.84228 val_ap= 0.83280 time= 0.11252\n",
      "训练次数: 4 Epoch: 0062 log_lik= 0.51447386 train_kl= 0.00848 train_loss= 0.52295 train_acc= 0.58752 val_roc= 0.84301 val_ap= 0.83340 time= 0.10655\n",
      "训练次数: 4 Epoch: 0063 log_lik= 0.512939 train_kl= 0.00848 train_loss= 0.52142 train_acc= 0.58970 val_roc= 0.84390 val_ap= 0.83385 time= 0.10953\n",
      "训练次数: 4 Epoch: 0064 log_lik= 0.5119827 train_kl= 0.00848 train_loss= 0.52047 train_acc= 0.59065 val_roc= 0.84467 val_ap= 0.83405 time= 0.11669\n",
      "训练次数: 4 Epoch: 0065 log_lik= 0.5110228 train_kl= 0.00848 train_loss= 0.51951 train_acc= 0.58996 val_roc= 0.84503 val_ap= 0.83414 time= 0.10660\n",
      "训练次数: 4 Epoch: 0066 log_lik= 0.51005995 train_kl= 0.00849 train_loss= 0.51855 train_acc= 0.59024 val_roc= 0.84554 val_ap= 0.83365 time= 0.11551\n",
      "训练次数: 4 Epoch: 0067 log_lik= 0.5091049 train_kl= 0.00849 train_loss= 0.51759 train_acc= 0.58868 val_roc= 0.84555 val_ap= 0.83189 time= 0.11352\n",
      "训练次数: 4 Epoch: 0068 log_lik= 0.5082667 train_kl= 0.00849 train_loss= 0.51676 train_acc= 0.58759 val_roc= 0.84599 val_ap= 0.83215 time= 0.11451\n",
      "训练次数: 4 Epoch: 0069 log_lik= 0.507611 train_kl= 0.00849 train_loss= 0.51610 train_acc= 0.58729 val_roc= 0.84682 val_ap= 0.83309 time= 0.11053\n",
      "训练次数: 4 Epoch: 0070 log_lik= 0.50647545 train_kl= 0.00849 train_loss= 0.51496 train_acc= 0.58646 val_roc= 0.84759 val_ap= 0.83359 time= 0.11053\n",
      "训练次数: 4 Epoch: 0071 log_lik= 0.50554097 train_kl= 0.00849 train_loss= 0.51403 train_acc= 0.58601 val_roc= 0.84863 val_ap= 0.83443 time= 0.10755\n",
      "训练次数: 4 Epoch: 0072 log_lik= 0.5045915 train_kl= 0.00849 train_loss= 0.51308 train_acc= 0.58565 val_roc= 0.85003 val_ap= 0.83615 time= 0.11451\n",
      "训练次数: 4 Epoch: 0073 log_lik= 0.5038784 train_kl= 0.00848 train_loss= 0.51236 train_acc= 0.58537 val_roc= 0.85081 val_ap= 0.83647 time= 0.10655\n",
      "训练次数: 4 Epoch: 0074 log_lik= 0.5029279 train_kl= 0.00848 train_loss= 0.51141 train_acc= 0.58366 val_roc= 0.85220 val_ap= 0.83714 time= 0.10256\n",
      "训练次数: 4 Epoch: 0075 log_lik= 0.5021268 train_kl= 0.00848 train_loss= 0.51061 train_acc= 0.58374 val_roc= 0.85381 val_ap= 0.83828 time= 0.10655\n",
      "训练次数: 4 Epoch: 0076 log_lik= 0.50128984 train_kl= 0.00849 train_loss= 0.50977 train_acc= 0.58289 val_roc= 0.85563 val_ap= 0.83964 time= 0.11451\n",
      "训练次数: 4 Epoch: 0077 log_lik= 0.5005158 train_kl= 0.00849 train_loss= 0.50900 train_acc= 0.58155 val_roc= 0.85791 val_ap= 0.84191 time= 0.10953\n",
      "训练次数: 4 Epoch: 0078 log_lik= 0.49962246 train_kl= 0.00849 train_loss= 0.50811 train_acc= 0.58027 val_roc= 0.85985 val_ap= 0.84376 time= 0.10409\n",
      "训练次数: 4 Epoch: 0079 log_lik= 0.49899986 train_kl= 0.00850 train_loss= 0.50750 train_acc= 0.57930 val_roc= 0.86200 val_ap= 0.84621 time= 0.10555\n",
      "训练次数: 4 Epoch: 0080 log_lik= 0.49812904 train_kl= 0.00850 train_loss= 0.50663 train_acc= 0.57843 val_roc= 0.86435 val_ap= 0.84881 time= 0.11252\n",
      "训练次数: 4 Epoch: 0081 log_lik= 0.4973655 train_kl= 0.00850 train_loss= 0.50587 train_acc= 0.57805 val_roc= 0.86617 val_ap= 0.85049 time= 0.10456\n",
      "训练次数: 4 Epoch: 0082 log_lik= 0.4965598 train_kl= 0.00850 train_loss= 0.50506 train_acc= 0.57682 val_roc= 0.86818 val_ap= 0.85322 time= 0.10157\n",
      "训练次数: 4 Epoch: 0083 log_lik= 0.49563652 train_kl= 0.00850 train_loss= 0.50414 train_acc= 0.57519 val_roc= 0.86959 val_ap= 0.85585 time= 0.10654\n",
      "训练次数: 4 Epoch: 0084 log_lik= 0.49484384 train_kl= 0.00850 train_loss= 0.50335 train_acc= 0.57526 val_roc= 0.87056 val_ap= 0.85669 time= 0.10157\n",
      "训练次数: 4 Epoch: 0085 log_lik= 0.4938565 train_kl= 0.00850 train_loss= 0.50236 train_acc= 0.57441 val_roc= 0.87212 val_ap= 0.85979 time= 0.10854\n",
      "训练次数: 4 Epoch: 0086 log_lik= 0.49270165 train_kl= 0.00850 train_loss= 0.50120 train_acc= 0.57363 val_roc= 0.87372 val_ap= 0.86207 time= 0.11410\n",
      "训练次数: 4 Epoch: 0087 log_lik= 0.49200648 train_kl= 0.00850 train_loss= 0.50051 train_acc= 0.57346 val_roc= 0.87515 val_ap= 0.86416 time= 0.11153\n",
      "训练次数: 4 Epoch: 0088 log_lik= 0.49092433 train_kl= 0.00850 train_loss= 0.49943 train_acc= 0.57217 val_roc= 0.87646 val_ap= 0.86662 time= 0.10215\n",
      "训练次数: 4 Epoch: 0089 log_lik= 0.4900588 train_kl= 0.00850 train_loss= 0.49856 train_acc= 0.57184 val_roc= 0.87771 val_ap= 0.86822 time= 0.10359\n",
      "训练次数: 4 Epoch: 0090 log_lik= 0.4891092 train_kl= 0.00850 train_loss= 0.49761 train_acc= 0.57183 val_roc= 0.87908 val_ap= 0.87046 time= 0.10555\n",
      "训练次数: 4 Epoch: 0091 log_lik= 0.48816228 train_kl= 0.00851 train_loss= 0.49667 train_acc= 0.57130 val_roc= 0.88044 val_ap= 0.87309 time= 0.11949\n",
      "训练次数: 4 Epoch: 0092 log_lik= 0.48720476 train_kl= 0.00851 train_loss= 0.49571 train_acc= 0.57178 val_roc= 0.88183 val_ap= 0.87466 time= 0.10555\n",
      "训练次数: 4 Epoch: 0093 log_lik= 0.48621768 train_kl= 0.00851 train_loss= 0.49473 train_acc= 0.57188 val_roc= 0.88238 val_ap= 0.87551 time= 0.11352\n",
      "训练次数: 4 Epoch: 0094 log_lik= 0.48520672 train_kl= 0.00852 train_loss= 0.49372 train_acc= 0.57196 val_roc= 0.88360 val_ap= 0.87713 time= 0.10976\n",
      "训练次数: 4 Epoch: 0095 log_lik= 0.48435515 train_kl= 0.00852 train_loss= 0.49287 train_acc= 0.57257 val_roc= 0.88418 val_ap= 0.87817 time= 0.10954\n",
      "训练次数: 4 Epoch: 0096 log_lik= 0.48348218 train_kl= 0.00852 train_loss= 0.49200 train_acc= 0.57420 val_roc= 0.88473 val_ap= 0.87899 time= 0.10654\n",
      "训练次数: 4 Epoch: 0097 log_lik= 0.48244143 train_kl= 0.00853 train_loss= 0.49097 train_acc= 0.57591 val_roc= 0.88514 val_ap= 0.87894 time= 0.10655\n",
      "训练次数: 4 Epoch: 0098 log_lik= 0.48146167 train_kl= 0.00853 train_loss= 0.48999 train_acc= 0.57776 val_roc= 0.88560 val_ap= 0.87934 time= 0.10954\n",
      "训练次数: 4 Epoch: 0099 log_lik= 0.48061022 train_kl= 0.00853 train_loss= 0.48914 train_acc= 0.57873 val_roc= 0.88576 val_ap= 0.87992 time= 0.10655\n",
      "训练次数: 4 Epoch: 0100 log_lik= 0.47972283 train_kl= 0.00854 train_loss= 0.48826 train_acc= 0.58009 val_roc= 0.88661 val_ap= 0.88062 time= 0.10754\n",
      "Optimization Finished!\n",
      "训练次数: 4 ROC score: 0.8846537451976567\n",
      "训练次数: 4 AP score: 0.8841205496328489\n",
      "训练次数: 5 Epoch: 0001 log_lik= 0.7814862 train_kl= 0.00805 train_loss= 0.78954 train_acc= 0.10783 val_roc= 0.68321 val_ap= 0.71214 time= 7.83714\n",
      "训练次数: 5 Epoch: 0002 log_lik= 0.89151883 train_kl= 0.00839 train_loss= 0.89991 train_acc= 0.00159 val_roc= 0.78142 val_ap= 0.78190 time= 0.11650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0003 log_lik= 0.74084425 train_kl= 0.00810 train_loss= 0.74894 train_acc= 0.05959 val_roc= 0.73008 val_ap= 0.74556 time= 0.11153\n",
      "训练次数: 5 Epoch: 0004 log_lik= 0.72689986 train_kl= 0.00815 train_loss= 0.73505 train_acc= 0.01274 val_roc= 0.72245 val_ap= 0.74260 time= 0.11153\n",
      "训练次数: 5 Epoch: 0005 log_lik= 0.75345176 train_kl= 0.00827 train_loss= 0.76173 train_acc= 0.00271 val_roc= 0.75412 val_ap= 0.77047 time= 0.11551\n",
      "训练次数: 5 Epoch: 0006 log_lik= 0.7181968 train_kl= 0.00824 train_loss= 0.72644 train_acc= 0.00828 val_roc= 0.79429 val_ap= 0.80174 time= 0.10854\n",
      "训练次数: 5 Epoch: 0007 log_lik= 0.6913396 train_kl= 0.00820 train_loss= 0.69954 train_acc= 0.05063 val_roc= 0.80798 val_ap= 0.81441 time= 0.12348\n",
      "训练次数: 5 Epoch: 0008 log_lik= 0.67700636 train_kl= 0.00819 train_loss= 0.68519 train_acc= 0.14866 val_roc= 0.80675 val_ap= 0.81564 time= 0.11053\n",
      "训练次数: 5 Epoch: 0009 log_lik= 0.6604038 train_kl= 0.00820 train_loss= 0.66861 train_acc= 0.24629 val_roc= 0.80931 val_ap= 0.81992 time= 0.11252\n",
      "训练次数: 5 Epoch: 0010 log_lik= 0.6398203 train_kl= 0.00824 train_loss= 0.64806 train_acc= 0.30298 val_roc= 0.81382 val_ap= 0.82436 time= 0.11551\n",
      "训练次数: 5 Epoch: 0011 log_lik= 0.61877036 train_kl= 0.00829 train_loss= 0.62706 train_acc= 0.33350 val_roc= 0.82061 val_ap= 0.83041 time= 0.11153\n",
      "训练次数: 5 Epoch: 0012 log_lik= 0.6014832 train_kl= 0.00834 train_loss= 0.60983 train_acc= 0.35983 val_roc= 0.82544 val_ap= 0.83538 time= 0.11551\n",
      "训练次数: 5 Epoch: 0013 log_lik= 0.5893417 train_kl= 0.00840 train_loss= 0.59774 train_acc= 0.38575 val_roc= 0.83057 val_ap= 0.84267 time= 0.10820\n",
      "训练次数: 5 Epoch: 0014 log_lik= 0.578505 train_kl= 0.00845 train_loss= 0.58695 train_acc= 0.41069 val_roc= 0.83565 val_ap= 0.85011 time= 0.11451\n",
      "训练次数: 5 Epoch: 0015 log_lik= 0.5690343 train_kl= 0.00848 train_loss= 0.57752 train_acc= 0.43011 val_roc= 0.84161 val_ap= 0.85754 time= 0.10655\n",
      "训练次数: 5 Epoch: 0016 log_lik= 0.55865264 train_kl= 0.00850 train_loss= 0.56715 train_acc= 0.44702 val_roc= 0.84614 val_ap= 0.86230 time= 0.11053\n",
      "训练次数: 5 Epoch: 0017 log_lik= 0.5502114 train_kl= 0.00850 train_loss= 0.55871 train_acc= 0.46162 val_roc= 0.84905 val_ap= 0.86464 time= 0.10953\n",
      "训练次数: 5 Epoch: 0018 log_lik= 0.5433974 train_kl= 0.00850 train_loss= 0.55190 train_acc= 0.47150 val_roc= 0.85222 val_ap= 0.86726 time= 0.10854\n",
      "训练次数: 5 Epoch: 0019 log_lik= 0.53729224 train_kl= 0.00849 train_loss= 0.54578 train_acc= 0.47947 val_roc= 0.85728 val_ap= 0.87332 time= 0.11849\n",
      "训练次数: 5 Epoch: 0020 log_lik= 0.5313473 train_kl= 0.00848 train_loss= 0.53982 train_acc= 0.48452 val_roc= 0.86160 val_ap= 0.87871 time= 0.10954\n",
      "训练次数: 5 Epoch: 0021 log_lik= 0.5252389 train_kl= 0.00846 train_loss= 0.53370 train_acc= 0.48778 val_roc= 0.86617 val_ap= 0.88403 time= 0.11551\n",
      "训练次数: 5 Epoch: 0022 log_lik= 0.5198202 train_kl= 0.00846 train_loss= 0.52828 train_acc= 0.49135 val_roc= 0.87040 val_ap= 0.88892 time= 0.11053\n",
      "训练次数: 5 Epoch: 0023 log_lik= 0.5147769 train_kl= 0.00846 train_loss= 0.52324 train_acc= 0.49543 val_roc= 0.87396 val_ap= 0.89298 time= 0.11153\n",
      "训练次数: 5 Epoch: 0024 log_lik= 0.50975573 train_kl= 0.00847 train_loss= 0.51823 train_acc= 0.50061 val_roc= 0.87727 val_ap= 0.89589 time= 0.10660\n",
      "训练次数: 5 Epoch: 0025 log_lik= 0.5056627 train_kl= 0.00849 train_loss= 0.51415 train_acc= 0.50710 val_roc= 0.88077 val_ap= 0.89875 time= 0.11252\n",
      "训练次数: 5 Epoch: 0026 log_lik= 0.50300366 train_kl= 0.00851 train_loss= 0.51152 train_acc= 0.51234 val_roc= 0.88323 val_ap= 0.89970 time= 0.10754\n",
      "训练次数: 5 Epoch: 0027 log_lik= 0.50067997 train_kl= 0.00853 train_loss= 0.50921 train_acc= 0.51863 val_roc= 0.88603 val_ap= 0.90050 time= 0.10854\n",
      "训练次数: 5 Epoch: 0028 log_lik= 0.49739632 train_kl= 0.00855 train_loss= 0.50595 train_acc= 0.52478 val_roc= 0.88855 val_ap= 0.90118 time= 0.11551\n",
      "训练次数: 5 Epoch: 0029 log_lik= 0.49378535 train_kl= 0.00856 train_loss= 0.50235 train_acc= 0.53177 val_roc= 0.89043 val_ap= 0.90019 time= 0.11153\n",
      "训练次数: 5 Epoch: 0030 log_lik= 0.49076423 train_kl= 0.00857 train_loss= 0.49933 train_acc= 0.53690 val_roc= 0.89235 val_ap= 0.89985 time= 0.10854\n",
      "训练次数: 5 Epoch: 0031 log_lik= 0.48834813 train_kl= 0.00857 train_loss= 0.49692 train_acc= 0.54042 val_roc= 0.89435 val_ap= 0.90086 time= 0.10655\n",
      "训练次数: 5 Epoch: 0032 log_lik= 0.4858801 train_kl= 0.00858 train_loss= 0.49446 train_acc= 0.54346 val_roc= 0.89592 val_ap= 0.90272 time= 0.10754\n",
      "训练次数: 5 Epoch: 0033 log_lik= 0.48325405 train_kl= 0.00858 train_loss= 0.49183 train_acc= 0.54644 val_roc= 0.89771 val_ap= 0.90511 time= 0.10854\n",
      "训练次数: 5 Epoch: 0034 log_lik= 0.48053595 train_kl= 0.00858 train_loss= 0.48911 train_acc= 0.54941 val_roc= 0.89945 val_ap= 0.90725 time= 0.11551\n",
      "训练次数: 5 Epoch: 0035 log_lik= 0.47792286 train_kl= 0.00858 train_loss= 0.48650 train_acc= 0.55224 val_roc= 0.90194 val_ap= 0.91027 time= 0.11452\n",
      "训练次数: 5 Epoch: 0036 log_lik= 0.47561723 train_kl= 0.00858 train_loss= 0.48420 train_acc= 0.55510 val_roc= 0.90383 val_ap= 0.91193 time= 0.10655\n",
      "训练次数: 5 Epoch: 0037 log_lik= 0.4734139 train_kl= 0.00859 train_loss= 0.48200 train_acc= 0.55757 val_roc= 0.90493 val_ap= 0.91309 time= 0.10854\n",
      "训练次数: 5 Epoch: 0038 log_lik= 0.47199798 train_kl= 0.00859 train_loss= 0.48059 train_acc= 0.55958 val_roc= 0.90629 val_ap= 0.91416 time= 0.10854\n",
      "训练次数: 5 Epoch: 0039 log_lik= 0.4704964 train_kl= 0.00860 train_loss= 0.47909 train_acc= 0.56150 val_roc= 0.90750 val_ap= 0.91503 time= 0.10854\n",
      "训练次数: 5 Epoch: 0040 log_lik= 0.46912467 train_kl= 0.00860 train_loss= 0.47773 train_acc= 0.56320 val_roc= 0.90843 val_ap= 0.91549 time= 0.10861\n",
      "训练次数: 5 Epoch: 0041 log_lik= 0.4674617 train_kl= 0.00860 train_loss= 0.47607 train_acc= 0.56521 val_roc= 0.90873 val_ap= 0.91563 time= 0.10954\n",
      "训练次数: 5 Epoch: 0042 log_lik= 0.46589857 train_kl= 0.00861 train_loss= 0.47451 train_acc= 0.56629 val_roc= 0.90942 val_ap= 0.91611 time= 0.11053\n",
      "训练次数: 5 Epoch: 0043 log_lik= 0.46474802 train_kl= 0.00861 train_loss= 0.47336 train_acc= 0.56683 val_roc= 0.90942 val_ap= 0.91612 time= 0.11053\n",
      "训练次数: 5 Epoch: 0044 log_lik= 0.46368873 train_kl= 0.00861 train_loss= 0.47230 train_acc= 0.56702 val_roc= 0.90898 val_ap= 0.91591 time= 0.10857\n",
      "训练次数: 5 Epoch: 0045 log_lik= 0.46259177 train_kl= 0.00861 train_loss= 0.47121 train_acc= 0.56762 val_roc= 0.90898 val_ap= 0.91629 time= 0.10655\n",
      "训练次数: 5 Epoch: 0046 log_lik= 0.4615333 train_kl= 0.00861 train_loss= 0.47015 train_acc= 0.56952 val_roc= 0.90876 val_ap= 0.91664 time= 0.11152\n",
      "训练次数: 5 Epoch: 0047 log_lik= 0.46044576 train_kl= 0.00862 train_loss= 0.46906 train_acc= 0.57072 val_roc= 0.90898 val_ap= 0.91705 time= 0.10755\n",
      "训练次数: 5 Epoch: 0048 log_lik= 0.45949546 train_kl= 0.00862 train_loss= 0.46811 train_acc= 0.57322 val_roc= 0.90944 val_ap= 0.91751 time= 0.10754\n",
      "训练次数: 5 Epoch: 0049 log_lik= 0.45877448 train_kl= 0.00862 train_loss= 0.46739 train_acc= 0.57437 val_roc= 0.90974 val_ap= 0.91764 time= 0.10953\n",
      "训练次数: 5 Epoch: 0050 log_lik= 0.45793563 train_kl= 0.00862 train_loss= 0.46655 train_acc= 0.57592 val_roc= 0.90996 val_ap= 0.91796 time= 0.11650\n",
      "训练次数: 5 Epoch: 0051 log_lik= 0.45736304 train_kl= 0.00862 train_loss= 0.46598 train_acc= 0.57651 val_roc= 0.90993 val_ap= 0.91778 time= 0.10953\n",
      "训练次数: 5 Epoch: 0052 log_lik= 0.4566023 train_kl= 0.00862 train_loss= 0.46522 train_acc= 0.57727 val_roc= 0.91010 val_ap= 0.91796 time= 0.11850\n",
      "训练次数: 5 Epoch: 0053 log_lik= 0.45599708 train_kl= 0.00862 train_loss= 0.46461 train_acc= 0.57875 val_roc= 0.91012 val_ap= 0.91831 time= 0.11053\n",
      "训练次数: 5 Epoch: 0054 log_lik= 0.455485 train_kl= 0.00862 train_loss= 0.46410 train_acc= 0.57983 val_roc= 0.90986 val_ap= 0.91783 time= 0.10754\n",
      "训练次数: 5 Epoch: 0055 log_lik= 0.454978 train_kl= 0.00862 train_loss= 0.46360 train_acc= 0.58039 val_roc= 0.90997 val_ap= 0.91780 time= 0.11053\n",
      "训练次数: 5 Epoch: 0056 log_lik= 0.45434442 train_kl= 0.00862 train_loss= 0.46296 train_acc= 0.58151 val_roc= 0.91026 val_ap= 0.91783 time= 0.12156\n",
      "训练次数: 5 Epoch: 0057 log_lik= 0.4539251 train_kl= 0.00862 train_loss= 0.46255 train_acc= 0.58252 val_roc= 0.91035 val_ap= 0.91773 time= 0.11145\n",
      "训练次数: 5 Epoch: 0058 log_lik= 0.45341328 train_kl= 0.00863 train_loss= 0.46204 train_acc= 0.58291 val_roc= 0.91083 val_ap= 0.91787 time= 0.11750\n",
      "训练次数: 5 Epoch: 0059 log_lik= 0.45282573 train_kl= 0.00863 train_loss= 0.46145 train_acc= 0.58409 val_roc= 0.91130 val_ap= 0.91802 time= 0.11751\n",
      "训练次数: 5 Epoch: 0060 log_lik= 0.4524564 train_kl= 0.00863 train_loss= 0.46109 train_acc= 0.58457 val_roc= 0.91132 val_ap= 0.91790 time= 0.10854\n",
      "训练次数: 5 Epoch: 0061 log_lik= 0.45194152 train_kl= 0.00863 train_loss= 0.46057 train_acc= 0.58510 val_roc= 0.91145 val_ap= 0.91796 time= 0.11352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 5 Epoch: 0062 log_lik= 0.45152602 train_kl= 0.00863 train_loss= 0.46016 train_acc= 0.58522 val_roc= 0.91161 val_ap= 0.91780 time= 0.11153\n",
      "训练次数: 5 Epoch: 0063 log_lik= 0.4512084 train_kl= 0.00863 train_loss= 0.45984 train_acc= 0.58537 val_roc= 0.91164 val_ap= 0.91757 time= 0.11750\n",
      "训练次数: 5 Epoch: 0064 log_lik= 0.45088363 train_kl= 0.00863 train_loss= 0.45951 train_acc= 0.58584 val_roc= 0.91201 val_ap= 0.91790 time= 0.11650\n",
      "训练次数: 5 Epoch: 0065 log_lik= 0.450508 train_kl= 0.00863 train_loss= 0.45913 train_acc= 0.58581 val_roc= 0.91249 val_ap= 0.91820 time= 0.10954\n",
      "训练次数: 5 Epoch: 0066 log_lik= 0.45017785 train_kl= 0.00862 train_loss= 0.45880 train_acc= 0.58614 val_roc= 0.91285 val_ap= 0.91854 time= 0.12049\n",
      "训练次数: 5 Epoch: 0067 log_lik= 0.44990703 train_kl= 0.00862 train_loss= 0.45853 train_acc= 0.58630 val_roc= 0.91314 val_ap= 0.91874 time= 0.12347\n",
      "训练次数: 5 Epoch: 0068 log_lik= 0.44951436 train_kl= 0.00862 train_loss= 0.45814 train_acc= 0.58674 val_roc= 0.91349 val_ap= 0.91919 time= 0.11750\n",
      "训练次数: 5 Epoch: 0069 log_lik= 0.4491674 train_kl= 0.00863 train_loss= 0.45779 train_acc= 0.58660 val_roc= 0.91396 val_ap= 0.91960 time= 0.10854\n",
      "训练次数: 5 Epoch: 0070 log_lik= 0.4488021 train_kl= 0.00863 train_loss= 0.45743 train_acc= 0.58638 val_roc= 0.91440 val_ap= 0.92018 time= 0.11053\n",
      "训练次数: 5 Epoch: 0071 log_lik= 0.44846803 train_kl= 0.00863 train_loss= 0.45710 train_acc= 0.58657 val_roc= 0.91502 val_ap= 0.92104 time= 0.10954\n",
      "训练次数: 5 Epoch: 0072 log_lik= 0.44814762 train_kl= 0.00863 train_loss= 0.45678 train_acc= 0.58706 val_roc= 0.91570 val_ap= 0.92178 time= 0.11053\n",
      "训练次数: 5 Epoch: 0073 log_lik= 0.44793448 train_kl= 0.00863 train_loss= 0.45657 train_acc= 0.58701 val_roc= 0.91628 val_ap= 0.92249 time= 0.11252\n",
      "训练次数: 5 Epoch: 0074 log_lik= 0.44763955 train_kl= 0.00863 train_loss= 0.45627 train_acc= 0.58667 val_roc= 0.91668 val_ap= 0.92302 time= 0.12647\n",
      "训练次数: 5 Epoch: 0075 log_lik= 0.4473411 train_kl= 0.00863 train_loss= 0.45597 train_acc= 0.58667 val_roc= 0.91671 val_ap= 0.92291 time= 0.10854\n",
      "训练次数: 5 Epoch: 0076 log_lik= 0.4469735 train_kl= 0.00863 train_loss= 0.45561 train_acc= 0.58717 val_roc= 0.91709 val_ap= 0.92331 time= 0.11053\n",
      "训练次数: 5 Epoch: 0077 log_lik= 0.44679728 train_kl= 0.00863 train_loss= 0.45543 train_acc= 0.58705 val_roc= 0.91748 val_ap= 0.92376 time= 0.11551\n",
      "训练次数: 5 Epoch: 0078 log_lik= 0.4464361 train_kl= 0.00863 train_loss= 0.45507 train_acc= 0.58746 val_roc= 0.91795 val_ap= 0.92433 time= 0.10854\n",
      "训练次数: 5 Epoch: 0079 log_lik= 0.4460989 train_kl= 0.00863 train_loss= 0.45473 train_acc= 0.58759 val_roc= 0.91856 val_ap= 0.92495 time= 0.11683\n",
      "训练次数: 5 Epoch: 0080 log_lik= 0.44579685 train_kl= 0.00863 train_loss= 0.45443 train_acc= 0.58799 val_roc= 0.91891 val_ap= 0.92530 time= 0.10953\n",
      "训练次数: 5 Epoch: 0081 log_lik= 0.4454928 train_kl= 0.00864 train_loss= 0.45413 train_acc= 0.58805 val_roc= 0.91930 val_ap= 0.92574 time= 0.11750\n",
      "训练次数: 5 Epoch: 0082 log_lik= 0.44519717 train_kl= 0.00864 train_loss= 0.45383 train_acc= 0.58848 val_roc= 0.91988 val_ap= 0.92632 time= 0.10854\n",
      "训练次数: 5 Epoch: 0083 log_lik= 0.4448613 train_kl= 0.00864 train_loss= 0.45350 train_acc= 0.58890 val_roc= 0.92057 val_ap= 0.92703 time= 0.11451\n",
      "训练次数: 5 Epoch: 0084 log_lik= 0.44462195 train_kl= 0.00864 train_loss= 0.45326 train_acc= 0.58886 val_roc= 0.92114 val_ap= 0.92764 time= 0.11650\n",
      "训练次数: 5 Epoch: 0085 log_lik= 0.4442956 train_kl= 0.00864 train_loss= 0.45294 train_acc= 0.58938 val_roc= 0.92164 val_ap= 0.92819 time= 0.12945\n",
      "训练次数: 5 Epoch: 0086 log_lik= 0.4440065 train_kl= 0.00864 train_loss= 0.45265 train_acc= 0.58991 val_roc= 0.92218 val_ap= 0.92889 time= 0.10854\n",
      "训练次数: 5 Epoch: 0087 log_lik= 0.44369859 train_kl= 0.00864 train_loss= 0.45234 train_acc= 0.58982 val_roc= 0.92244 val_ap= 0.92926 time= 0.11750\n",
      "训练次数: 5 Epoch: 0088 log_lik= 0.44343576 train_kl= 0.00864 train_loss= 0.45208 train_acc= 0.59085 val_roc= 0.92277 val_ap= 0.92963 time= 0.12578\n",
      "训练次数: 5 Epoch: 0089 log_lik= 0.44314045 train_kl= 0.00865 train_loss= 0.45179 train_acc= 0.59082 val_roc= 0.92304 val_ap= 0.93003 time= 0.10654\n",
      "训练次数: 5 Epoch: 0090 log_lik= 0.44284627 train_kl= 0.00865 train_loss= 0.45149 train_acc= 0.59144 val_roc= 0.92339 val_ap= 0.93044 time= 0.10754\n",
      "训练次数: 5 Epoch: 0091 log_lik= 0.44262743 train_kl= 0.00865 train_loss= 0.45127 train_acc= 0.59162 val_roc= 0.92377 val_ap= 0.93087 time= 0.10854\n",
      "训练次数: 5 Epoch: 0092 log_lik= 0.4423086 train_kl= 0.00865 train_loss= 0.45096 train_acc= 0.59173 val_roc= 0.92406 val_ap= 0.93124 time= 0.11053\n",
      "训练次数: 5 Epoch: 0093 log_lik= 0.4421235 train_kl= 0.00865 train_loss= 0.45077 train_acc= 0.59229 val_roc= 0.92439 val_ap= 0.93167 time= 0.10954\n",
      "训练次数: 5 Epoch: 0094 log_lik= 0.44183514 train_kl= 0.00865 train_loss= 0.45048 train_acc= 0.59202 val_roc= 0.92458 val_ap= 0.93179 time= 0.10854\n",
      "训练次数: 5 Epoch: 0095 log_lik= 0.44163486 train_kl= 0.00865 train_loss= 0.45028 train_acc= 0.59196 val_roc= 0.92479 val_ap= 0.93211 time= 0.10755\n",
      "训练次数: 5 Epoch: 0096 log_lik= 0.44144064 train_kl= 0.00865 train_loss= 0.45009 train_acc= 0.59223 val_roc= 0.92479 val_ap= 0.93236 time= 0.11750\n",
      "训练次数: 5 Epoch: 0097 log_lik= 0.44110018 train_kl= 0.00865 train_loss= 0.44975 train_acc= 0.59201 val_roc= 0.92504 val_ap= 0.93285 time= 0.10654\n",
      "训练次数: 5 Epoch: 0098 log_lik= 0.44094548 train_kl= 0.00865 train_loss= 0.44960 train_acc= 0.59261 val_roc= 0.92531 val_ap= 0.93323 time= 0.10456\n",
      "训练次数: 5 Epoch: 0099 log_lik= 0.4406814 train_kl= 0.00865 train_loss= 0.44933 train_acc= 0.59253 val_roc= 0.92570 val_ap= 0.93364 time= 0.10854\n",
      "训练次数: 5 Epoch: 0100 log_lik= 0.44048008 train_kl= 0.00865 train_loss= 0.44913 train_acc= 0.59288 val_roc= 0.92588 val_ap= 0.93388 time= 0.11610\n",
      "Optimization Finished!\n",
      "训练次数: 5 ROC score: 0.925268877214839\n",
      "训练次数: 5 AP score: 0.9295822785364523\n",
      "训练次数: 6 Epoch: 0001 log_lik= 0.78471553 train_kl= 0.00805 train_loss= 0.79277 train_acc= 0.11084 val_roc= 0.67951 val_ap= 0.69164 time= 8.05103\n",
      "训练次数: 6 Epoch: 0002 log_lik= 0.78345907 train_kl= 0.00827 train_loss= 0.79173 train_acc= 0.00162 val_roc= 0.72321 val_ap= 0.73300 time= 0.12049\n",
      "训练次数: 6 Epoch: 0003 log_lik= 0.74174035 train_kl= 0.00811 train_loss= 0.74985 train_acc= 0.02137 val_roc= 0.65993 val_ap= 0.67939 time= 0.11750\n",
      "训练次数: 6 Epoch: 0004 log_lik= 0.73249304 train_kl= 0.00818 train_loss= 0.74067 train_acc= 0.00933 val_roc= 0.69634 val_ap= 0.71601 time= 0.12846\n",
      "训练次数: 6 Epoch: 0005 log_lik= 0.7217337 train_kl= 0.00821 train_loss= 0.72994 train_acc= 0.01289 val_roc= 0.76819 val_ap= 0.77300 time= 0.12646\n",
      "训练次数: 6 Epoch: 0006 log_lik= 0.6963725 train_kl= 0.00819 train_loss= 0.70456 train_acc= 0.05753 val_roc= 0.80652 val_ap= 0.80446 time= 0.11650\n",
      "训练次数: 6 Epoch: 0007 log_lik= 0.67005897 train_kl= 0.00820 train_loss= 0.67826 train_acc= 0.16488 val_roc= 0.81813 val_ap= 0.81494 time= 0.11750\n",
      "训练次数: 6 Epoch: 0008 log_lik= 0.63903916 train_kl= 0.00824 train_loss= 0.64728 train_acc= 0.26194 val_roc= 0.81937 val_ap= 0.81446 time= 0.11451\n",
      "训练次数: 6 Epoch: 0009 log_lik= 0.6120864 train_kl= 0.00830 train_loss= 0.62039 train_acc= 0.33199 val_roc= 0.82213 val_ap= 0.81872 time= 0.12347\n",
      "训练次数: 6 Epoch: 0010 log_lik= 0.59189206 train_kl= 0.00837 train_loss= 0.60027 train_acc= 0.39035 val_roc= 0.82952 val_ap= 0.83031 time= 0.11252\n",
      "训练次数: 6 Epoch: 0011 log_lik= 0.5761707 train_kl= 0.00844 train_loss= 0.58461 train_acc= 0.43728 val_roc= 0.83589 val_ap= 0.83956 time= 0.11252\n",
      "训练次数: 6 Epoch: 0012 log_lik= 0.56001025 train_kl= 0.00848 train_loss= 0.56849 train_acc= 0.46739 val_roc= 0.84656 val_ap= 0.85102 time= 0.11252\n",
      "训练次数: 6 Epoch: 0013 log_lik= 0.5433761 train_kl= 0.00850 train_loss= 0.55188 train_acc= 0.48534 val_roc= 0.85904 val_ap= 0.86459 time= 0.11252\n",
      "训练次数: 6 Epoch: 0014 log_lik= 0.5286496 train_kl= 0.00852 train_loss= 0.53717 train_acc= 0.49540 val_roc= 0.87133 val_ap= 0.87755 time= 0.10854\n",
      "训练次数: 6 Epoch: 0015 log_lik= 0.5188613 train_kl= 0.00853 train_loss= 0.52739 train_acc= 0.49875 val_roc= 0.87944 val_ap= 0.88584 time= 0.10655\n",
      "训练次数: 6 Epoch: 0016 log_lik= 0.5144716 train_kl= 0.00855 train_loss= 0.52302 train_acc= 0.49873 val_roc= 0.88331 val_ap= 0.88945 time= 0.10854\n",
      "训练次数: 6 Epoch: 0017 log_lik= 0.51147354 train_kl= 0.00857 train_loss= 0.52004 train_acc= 0.50228 val_roc= 0.88613 val_ap= 0.89211 time= 0.12447\n",
      "训练次数: 6 Epoch: 0018 log_lik= 0.50727475 train_kl= 0.00858 train_loss= 0.51585 train_acc= 0.50916 val_roc= 0.88861 val_ap= 0.89458 time= 0.11551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0019 log_lik= 0.50226814 train_kl= 0.00859 train_loss= 0.51086 train_acc= 0.51546 val_roc= 0.89080 val_ap= 0.89733 time= 0.10955\n",
      "训练次数: 6 Epoch: 0020 log_lik= 0.49684373 train_kl= 0.00860 train_loss= 0.50544 train_acc= 0.52060 val_roc= 0.89332 val_ap= 0.90009 time= 0.10555\n",
      "训练次数: 6 Epoch: 0021 log_lik= 0.49177286 train_kl= 0.00860 train_loss= 0.50038 train_acc= 0.52425 val_roc= 0.89513 val_ap= 0.90338 time= 0.11152\n",
      "训练次数: 6 Epoch: 0022 log_lik= 0.48737532 train_kl= 0.00861 train_loss= 0.49598 train_acc= 0.52820 val_roc= 0.89575 val_ap= 0.90492 time= 0.11451\n",
      "训练次数: 6 Epoch: 0023 log_lik= 0.48408157 train_kl= 0.00862 train_loss= 0.49270 train_acc= 0.53247 val_roc= 0.89459 val_ap= 0.90421 time= 0.10655\n",
      "训练次数: 6 Epoch: 0024 log_lik= 0.48092198 train_kl= 0.00862 train_loss= 0.48954 train_acc= 0.53680 val_roc= 0.89345 val_ap= 0.90307 time= 0.11551\n",
      "训练次数: 6 Epoch: 0025 log_lik= 0.47835073 train_kl= 0.00863 train_loss= 0.48698 train_acc= 0.53979 val_roc= 0.89284 val_ap= 0.90300 time= 0.10655\n",
      "训练次数: 6 Epoch: 0026 log_lik= 0.4765268 train_kl= 0.00863 train_loss= 0.48515 train_acc= 0.54004 val_roc= 0.89268 val_ap= 0.90336 time= 0.11551\n",
      "训练次数: 6 Epoch: 0027 log_lik= 0.47433388 train_kl= 0.00862 train_loss= 0.48295 train_acc= 0.54048 val_roc= 0.89276 val_ap= 0.90364 time= 0.11452\n",
      "训练次数: 6 Epoch: 0028 log_lik= 0.4725071 train_kl= 0.00861 train_loss= 0.48112 train_acc= 0.54022 val_roc= 0.89320 val_ap= 0.90418 time= 0.10754\n",
      "训练次数: 6 Epoch: 0029 log_lik= 0.47020411 train_kl= 0.00860 train_loss= 0.47881 train_acc= 0.54087 val_roc= 0.89448 val_ap= 0.90553 time= 0.11105\n",
      "训练次数: 6 Epoch: 0030 log_lik= 0.4678906 train_kl= 0.00860 train_loss= 0.47649 train_acc= 0.54192 val_roc= 0.89523 val_ap= 0.90545 time= 0.11152\n",
      "训练次数: 6 Epoch: 0031 log_lik= 0.46589544 train_kl= 0.00860 train_loss= 0.47449 train_acc= 0.54267 val_roc= 0.89585 val_ap= 0.90511 time= 0.11252\n",
      "训练次数: 6 Epoch: 0032 log_lik= 0.46399122 train_kl= 0.00860 train_loss= 0.47259 train_acc= 0.54325 val_roc= 0.89607 val_ap= 0.90415 time= 0.11252\n",
      "训练次数: 6 Epoch: 0033 log_lik= 0.46188483 train_kl= 0.00861 train_loss= 0.47050 train_acc= 0.54319 val_roc= 0.89716 val_ap= 0.90441 time= 0.11152\n",
      "训练次数: 6 Epoch: 0034 log_lik= 0.45987418 train_kl= 0.00863 train_loss= 0.46850 train_acc= 0.54425 val_roc= 0.89782 val_ap= 0.90469 time= 0.11451\n",
      "训练次数: 6 Epoch: 0035 log_lik= 0.45793834 train_kl= 0.00864 train_loss= 0.46658 train_acc= 0.54466 val_roc= 0.89952 val_ap= 0.90614 time= 0.11651\n",
      "训练次数: 6 Epoch: 0036 log_lik= 0.45642737 train_kl= 0.00865 train_loss= 0.46508 train_acc= 0.54534 val_roc= 0.90046 val_ap= 0.90633 time= 0.10953\n",
      "训练次数: 6 Epoch: 0037 log_lik= 0.45484325 train_kl= 0.00866 train_loss= 0.46351 train_acc= 0.54659 val_roc= 0.90113 val_ap= 0.90690 time= 0.11053\n",
      "训练次数: 6 Epoch: 0038 log_lik= 0.45311603 train_kl= 0.00866 train_loss= 0.46178 train_acc= 0.54836 val_roc= 0.90136 val_ap= 0.90746 time= 0.11850\n",
      "训练次数: 6 Epoch: 0039 log_lik= 0.4513334 train_kl= 0.00866 train_loss= 0.46000 train_acc= 0.55065 val_roc= 0.90234 val_ap= 0.90849 time= 0.11750\n",
      "训练次数: 6 Epoch: 0040 log_lik= 0.45004645 train_kl= 0.00866 train_loss= 0.45871 train_acc= 0.55201 val_roc= 0.90260 val_ap= 0.90855 time= 0.11153\n",
      "训练次数: 6 Epoch: 0041 log_lik= 0.44895372 train_kl= 0.00865 train_loss= 0.45761 train_acc= 0.55306 val_roc= 0.90228 val_ap= 0.90882 time= 0.11053\n",
      "训练次数: 6 Epoch: 0042 log_lik= 0.44802263 train_kl= 0.00865 train_loss= 0.45667 train_acc= 0.55428 val_roc= 0.90188 val_ap= 0.90772 time= 0.11347\n",
      "训练次数: 6 Epoch: 0043 log_lik= 0.44725144 train_kl= 0.00865 train_loss= 0.45590 train_acc= 0.55490 val_roc= 0.90120 val_ap= 0.90638 time= 0.13741\n",
      "训练次数: 6 Epoch: 0044 log_lik= 0.44634628 train_kl= 0.00865 train_loss= 0.45500 train_acc= 0.55536 val_roc= 0.90094 val_ap= 0.90619 time= 0.13562\n",
      "训练次数: 6 Epoch: 0045 log_lik= 0.44539872 train_kl= 0.00866 train_loss= 0.45405 train_acc= 0.55583 val_roc= 0.90116 val_ap= 0.90633 time= 0.11451\n",
      "训练次数: 6 Epoch: 0046 log_lik= 0.44431734 train_kl= 0.00866 train_loss= 0.45298 train_acc= 0.55591 val_roc= 0.90181 val_ap= 0.90711 time= 0.13941\n",
      "训练次数: 6 Epoch: 0047 log_lik= 0.44332588 train_kl= 0.00867 train_loss= 0.45200 train_acc= 0.55597 val_roc= 0.90233 val_ap= 0.90720 time= 0.10655\n",
      "训练次数: 6 Epoch: 0048 log_lik= 0.44233218 train_kl= 0.00868 train_loss= 0.45101 train_acc= 0.55610 val_roc= 0.90254 val_ap= 0.90722 time= 0.11451\n",
      "训练次数: 6 Epoch: 0049 log_lik= 0.44150117 train_kl= 0.00868 train_loss= 0.45018 train_acc= 0.55688 val_roc= 0.90256 val_ap= 0.90697 time= 0.10954\n",
      "训练次数: 6 Epoch: 0050 log_lik= 0.4406857 train_kl= 0.00869 train_loss= 0.44937 train_acc= 0.55745 val_roc= 0.90208 val_ap= 0.90688 time= 0.10854\n",
      "训练次数: 6 Epoch: 0051 log_lik= 0.43986595 train_kl= 0.00869 train_loss= 0.44855 train_acc= 0.55776 val_roc= 0.90162 val_ap= 0.90673 time= 0.10655\n",
      "训练次数: 6 Epoch: 0052 log_lik= 0.43913752 train_kl= 0.00868 train_loss= 0.44782 train_acc= 0.55746 val_roc= 0.90076 val_ap= 0.90624 time= 0.10655\n",
      "训练次数: 6 Epoch: 0053 log_lik= 0.43837577 train_kl= 0.00868 train_loss= 0.44706 train_acc= 0.55781 val_roc= 0.90045 val_ap= 0.90617 time= 0.11551\n",
      "训练次数: 6 Epoch: 0054 log_lik= 0.43757883 train_kl= 0.00868 train_loss= 0.44626 train_acc= 0.55804 val_roc= 0.89985 val_ap= 0.90568 time= 0.12450\n",
      "训练次数: 6 Epoch: 0055 log_lik= 0.4369044 train_kl= 0.00868 train_loss= 0.44558 train_acc= 0.55847 val_roc= 0.89997 val_ap= 0.90632 time= 0.11253\n",
      "训练次数: 6 Epoch: 0056 log_lik= 0.43626758 train_kl= 0.00868 train_loss= 0.44494 train_acc= 0.55892 val_roc= 0.89949 val_ap= 0.90604 time= 0.15635\n",
      "训练次数: 6 Epoch: 0057 log_lik= 0.43570694 train_kl= 0.00868 train_loss= 0.44438 train_acc= 0.55957 val_roc= 0.89903 val_ap= 0.90581 time= 0.14637\n",
      "训练次数: 6 Epoch: 0058 log_lik= 0.43505892 train_kl= 0.00868 train_loss= 0.44374 train_acc= 0.56013 val_roc= 0.89849 val_ap= 0.90520 time= 0.10555\n",
      "训练次数: 6 Epoch: 0059 log_lik= 0.4345661 train_kl= 0.00869 train_loss= 0.44325 train_acc= 0.56029 val_roc= 0.89809 val_ap= 0.90526 time= 0.11750\n",
      "训练次数: 6 Epoch: 0060 log_lik= 0.43397442 train_kl= 0.00869 train_loss= 0.44266 train_acc= 0.56037 val_roc= 0.89799 val_ap= 0.90529 time= 0.11551\n",
      "训练次数: 6 Epoch: 0061 log_lik= 0.43342212 train_kl= 0.00869 train_loss= 0.44212 train_acc= 0.56106 val_roc= 0.89769 val_ap= 0.90570 time= 0.11651\n",
      "训练次数: 6 Epoch: 0062 log_lik= 0.43285668 train_kl= 0.00870 train_loss= 0.44155 train_acc= 0.56116 val_roc= 0.89754 val_ap= 0.90576 time= 0.11252\n",
      "训练次数: 6 Epoch: 0063 log_lik= 0.43227962 train_kl= 0.00870 train_loss= 0.44098 train_acc= 0.56167 val_roc= 0.89715 val_ap= 0.90546 time= 0.15535\n",
      "训练次数: 6 Epoch: 0064 log_lik= 0.4318659 train_kl= 0.00870 train_loss= 0.44057 train_acc= 0.56159 val_roc= 0.89705 val_ap= 0.90565 time= 0.14438\n",
      "训练次数: 6 Epoch: 0065 log_lik= 0.43132856 train_kl= 0.00870 train_loss= 0.44003 train_acc= 0.56211 val_roc= 0.89667 val_ap= 0.90557 time= 0.10854\n",
      "训练次数: 6 Epoch: 0066 log_lik= 0.43085086 train_kl= 0.00870 train_loss= 0.43955 train_acc= 0.56220 val_roc= 0.89662 val_ap= 0.90563 time= 0.10806\n",
      "训练次数: 6 Epoch: 0067 log_lik= 0.43035468 train_kl= 0.00870 train_loss= 0.43906 train_acc= 0.56303 val_roc= 0.89630 val_ap= 0.90611 time= 0.11153\n",
      "训练次数: 6 Epoch: 0068 log_lik= 0.42988905 train_kl= 0.00870 train_loss= 0.43859 train_acc= 0.56286 val_roc= 0.89610 val_ap= 0.90582 time= 0.12049\n",
      "训练次数: 6 Epoch: 0069 log_lik= 0.42946562 train_kl= 0.00871 train_loss= 0.43817 train_acc= 0.56318 val_roc= 0.89610 val_ap= 0.90616 time= 0.11252\n",
      "训练次数: 6 Epoch: 0070 log_lik= 0.42899325 train_kl= 0.00871 train_loss= 0.43770 train_acc= 0.56297 val_roc= 0.89601 val_ap= 0.90605 time= 0.11451\n",
      "训练次数: 6 Epoch: 0071 log_lik= 0.42854363 train_kl= 0.00871 train_loss= 0.43725 train_acc= 0.56456 val_roc= 0.89598 val_ap= 0.90603 time= 0.12447\n",
      "训练次数: 6 Epoch: 0072 log_lik= 0.42807114 train_kl= 0.00871 train_loss= 0.43678 train_acc= 0.56528 val_roc= 0.89557 val_ap= 0.90547 time= 0.12001\n",
      "训练次数: 6 Epoch: 0073 log_lik= 0.42770836 train_kl= 0.00871 train_loss= 0.43642 train_acc= 0.56511 val_roc= 0.89557 val_ap= 0.90542 time= 0.11750\n",
      "训练次数: 6 Epoch: 0074 log_lik= 0.4272565 train_kl= 0.00872 train_loss= 0.43597 train_acc= 0.56562 val_roc= 0.89565 val_ap= 0.90560 time= 0.10855\n",
      "训练次数: 6 Epoch: 0075 log_lik= 0.42686394 train_kl= 0.00872 train_loss= 0.43558 train_acc= 0.56649 val_roc= 0.89586 val_ap= 0.90558 time= 0.11152\n",
      "训练次数: 6 Epoch: 0076 log_lik= 0.4264903 train_kl= 0.00872 train_loss= 0.43521 train_acc= 0.56679 val_roc= 0.89627 val_ap= 0.90591 time= 0.11876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 6 Epoch: 0077 log_lik= 0.4260971 train_kl= 0.00872 train_loss= 0.43482 train_acc= 0.56763 val_roc= 0.89643 val_ap= 0.90617 time= 0.12547\n",
      "训练次数: 6 Epoch: 0078 log_lik= 0.425685 train_kl= 0.00872 train_loss= 0.43441 train_acc= 0.56848 val_roc= 0.89627 val_ap= 0.90623 time= 0.11153\n",
      "训练次数: 6 Epoch: 0079 log_lik= 0.4253715 train_kl= 0.00872 train_loss= 0.43409 train_acc= 0.56889 val_roc= 0.89608 val_ap= 0.90568 time= 0.11352\n",
      "训练次数: 6 Epoch: 0080 log_lik= 0.42500335 train_kl= 0.00872 train_loss= 0.43372 train_acc= 0.56964 val_roc= 0.89582 val_ap= 0.90550 time= 0.11750\n",
      "训练次数: 6 Epoch: 0081 log_lik= 0.42468587 train_kl= 0.00872 train_loss= 0.43341 train_acc= 0.57007 val_roc= 0.89562 val_ap= 0.90533 time= 0.11252\n",
      "训练次数: 6 Epoch: 0082 log_lik= 0.4243054 train_kl= 0.00872 train_loss= 0.43303 train_acc= 0.57049 val_roc= 0.89523 val_ap= 0.90462 time= 0.10987\n",
      "训练次数: 6 Epoch: 0083 log_lik= 0.42395353 train_kl= 0.00872 train_loss= 0.43268 train_acc= 0.57092 val_roc= 0.89516 val_ap= 0.90459 time= 0.10854\n",
      "训练次数: 6 Epoch: 0084 log_lik= 0.42366353 train_kl= 0.00872 train_loss= 0.43239 train_acc= 0.57170 val_roc= 0.89520 val_ap= 0.90476 time= 0.10754\n",
      "训练次数: 6 Epoch: 0085 log_lik= 0.42330885 train_kl= 0.00873 train_loss= 0.43203 train_acc= 0.57145 val_roc= 0.89510 val_ap= 0.90443 time= 0.11451\n",
      "训练次数: 6 Epoch: 0086 log_lik= 0.42301917 train_kl= 0.00873 train_loss= 0.43175 train_acc= 0.57181 val_roc= 0.89462 val_ap= 0.90403 time= 0.12049\n",
      "训练次数: 6 Epoch: 0087 log_lik= 0.42274415 train_kl= 0.00873 train_loss= 0.43147 train_acc= 0.57215 val_roc= 0.89397 val_ap= 0.90351 time= 0.11451\n",
      "训练次数: 6 Epoch: 0088 log_lik= 0.42253888 train_kl= 0.00873 train_loss= 0.43127 train_acc= 0.57228 val_roc= 0.89374 val_ap= 0.90368 time= 0.11451\n",
      "训练次数: 6 Epoch: 0089 log_lik= 0.42223322 train_kl= 0.00873 train_loss= 0.43097 train_acc= 0.57256 val_roc= 0.89349 val_ap= 0.90337 time= 0.11252\n",
      "训练次数: 6 Epoch: 0090 log_lik= 0.421979 train_kl= 0.00873 train_loss= 0.43071 train_acc= 0.57295 val_roc= 0.89323 val_ap= 0.90333 time= 0.11651\n",
      "训练次数: 6 Epoch: 0091 log_lik= 0.42179406 train_kl= 0.00873 train_loss= 0.43053 train_acc= 0.57321 val_roc= 0.89315 val_ap= 0.90340 time= 0.10754\n",
      "训练次数: 6 Epoch: 0092 log_lik= 0.42154932 train_kl= 0.00874 train_loss= 0.43028 train_acc= 0.57326 val_roc= 0.89287 val_ap= 0.90326 time= 0.10754\n",
      "训练次数: 6 Epoch: 0093 log_lik= 0.4213228 train_kl= 0.00873 train_loss= 0.43006 train_acc= 0.57329 val_roc= 0.89244 val_ap= 0.90290 time= 0.11651\n",
      "训练次数: 6 Epoch: 0094 log_lik= 0.42118686 train_kl= 0.00873 train_loss= 0.42992 train_acc= 0.57335 val_roc= 0.89186 val_ap= 0.90226 time= 0.10854\n",
      "训练次数: 6 Epoch: 0095 log_lik= 0.42094585 train_kl= 0.00873 train_loss= 0.42968 train_acc= 0.57381 val_roc= 0.89148 val_ap= 0.90225 time= 0.11053\n",
      "训练次数: 6 Epoch: 0096 log_lik= 0.42072007 train_kl= 0.00874 train_loss= 0.42946 train_acc= 0.57320 val_roc= 0.89170 val_ap= 0.90251 time= 0.11850\n",
      "训练次数: 6 Epoch: 0097 log_lik= 0.42057103 train_kl= 0.00874 train_loss= 0.42931 train_acc= 0.57370 val_roc= 0.89203 val_ap= 0.90314 time= 0.11352\n",
      "训练次数: 6 Epoch: 0098 log_lik= 0.4204426 train_kl= 0.00874 train_loss= 0.42918 train_acc= 0.57419 val_roc= 0.89221 val_ap= 0.90332 time= 0.11252\n",
      "训练次数: 6 Epoch: 0099 log_lik= 0.42024714 train_kl= 0.00874 train_loss= 0.42899 train_acc= 0.57388 val_roc= 0.89218 val_ap= 0.90348 time= 0.11153\n",
      "训练次数: 6 Epoch: 0100 log_lik= 0.42003295 train_kl= 0.00874 train_loss= 0.42877 train_acc= 0.57428 val_roc= 0.89213 val_ap= 0.90354 time= 0.11975\n",
      "Optimization Finished!\n",
      "训练次数: 6 ROC score: 0.9103298539223488\n",
      "训练次数: 6 AP score: 0.9219501393084609\n",
      "训练次数: 7 Epoch: 0001 log_lik= 0.77914363 train_kl= 0.00806 train_loss= 0.78720 train_acc= 0.02012 val_roc= 0.64104 val_ap= 0.67753 time= 8.45879\n",
      "训练次数: 7 Epoch: 0002 log_lik= 0.95019686 train_kl= 0.00844 train_loss= 0.95864 train_acc= 0.00161 val_roc= 0.74306 val_ap= 0.74646 time= 0.13040\n",
      "训练次数: 7 Epoch: 0003 log_lik= 0.73515797 train_kl= 0.00813 train_loss= 0.74329 train_acc= 0.00455 val_roc= 0.74327 val_ap= 0.74837 time= 0.11053\n",
      "训练次数: 7 Epoch: 0004 log_lik= 0.72835356 train_kl= 0.00816 train_loss= 0.73651 train_acc= 0.01062 val_roc= 0.75275 val_ap= 0.76021 time= 0.11053\n",
      "训练次数: 7 Epoch: 0005 log_lik= 0.72866696 train_kl= 0.00824 train_loss= 0.73690 train_acc= 0.00929 val_roc= 0.79439 val_ap= 0.79984 time= 0.12148\n",
      "训练次数: 7 Epoch: 0006 log_lik= 0.7025856 train_kl= 0.00821 train_loss= 0.71080 train_acc= 0.04031 val_roc= 0.81558 val_ap= 0.81132 time= 0.11352\n",
      "训练次数: 7 Epoch: 0007 log_lik= 0.68276036 train_kl= 0.00820 train_loss= 0.69096 train_acc= 0.14414 val_roc= 0.82131 val_ap= 0.80876 time= 0.11352\n",
      "训练次数: 7 Epoch: 0008 log_lik= 0.6637207 train_kl= 0.00821 train_loss= 0.67193 train_acc= 0.25241 val_roc= 0.82093 val_ap= 0.80564 time= 0.11573\n",
      "训练次数: 7 Epoch: 0009 log_lik= 0.6450658 train_kl= 0.00824 train_loss= 0.65331 train_acc= 0.31968 val_roc= 0.81837 val_ap= 0.80313 time= 0.11041\n",
      "训练次数: 7 Epoch: 0010 log_lik= 0.63298714 train_kl= 0.00830 train_loss= 0.64128 train_acc= 0.36262 val_roc= 0.81737 val_ap= 0.80183 time= 0.11152\n",
      "训练次数: 7 Epoch: 0011 log_lik= 0.62741244 train_kl= 0.00835 train_loss= 0.63576 train_acc= 0.39160 val_roc= 0.81962 val_ap= 0.80492 time= 0.11153\n",
      "训练次数: 7 Epoch: 0012 log_lik= 0.61934644 train_kl= 0.00837 train_loss= 0.62772 train_acc= 0.41343 val_roc= 0.82457 val_ap= 0.81053 time= 0.11153\n",
      "训练次数: 7 Epoch: 0013 log_lik= 0.6060654 train_kl= 0.00837 train_loss= 0.61443 train_acc= 0.42666 val_roc= 0.83079 val_ap= 0.81830 time= 0.12846\n",
      "训练次数: 7 Epoch: 0014 log_lik= 0.59372395 train_kl= 0.00835 train_loss= 0.60207 train_acc= 0.42808 val_roc= 0.83762 val_ap= 0.82869 time= 0.11949\n",
      "训练次数: 7 Epoch: 0015 log_lik= 0.5853291 train_kl= 0.00833 train_loss= 0.59366 train_acc= 0.42404 val_roc= 0.84301 val_ap= 0.83883 time= 0.11352\n",
      "训练次数: 7 Epoch: 0016 log_lik= 0.57886654 train_kl= 0.00833 train_loss= 0.58719 train_acc= 0.42504 val_roc= 0.84711 val_ap= 0.84564 time= 0.11351\n",
      "训练次数: 7 Epoch: 0017 log_lik= 0.5709448 train_kl= 0.00833 train_loss= 0.57928 train_acc= 0.43369 val_roc= 0.85131 val_ap= 0.85130 time= 0.11252\n",
      "训练次数: 7 Epoch: 0018 log_lik= 0.561032 train_kl= 0.00835 train_loss= 0.56939 train_acc= 0.45172 val_roc= 0.85570 val_ap= 0.85515 time= 0.11351\n",
      "训练次数: 7 Epoch: 0019 log_lik= 0.5503946 train_kl= 0.00838 train_loss= 0.55878 train_acc= 0.47168 val_roc= 0.85978 val_ap= 0.85902 time= 0.11153\n",
      "训练次数: 7 Epoch: 0020 log_lik= 0.5409743 train_kl= 0.00842 train_loss= 0.54940 train_acc= 0.49055 val_roc= 0.86384 val_ap= 0.86218 time= 0.11451\n",
      "训练次数: 7 Epoch: 0021 log_lik= 0.5344611 train_kl= 0.00846 train_loss= 0.54292 train_acc= 0.50240 val_roc= 0.86925 val_ap= 0.86782 time= 0.11053\n",
      "训练次数: 7 Epoch: 0022 log_lik= 0.52723914 train_kl= 0.00848 train_loss= 0.53572 train_acc= 0.50956 val_roc= 0.87441 val_ap= 0.87367 time= 0.11153\n",
      "训练次数: 7 Epoch: 0023 log_lik= 0.5196836 train_kl= 0.00850 train_loss= 0.52818 train_acc= 0.51297 val_roc= 0.87763 val_ap= 0.87636 time= 0.11850\n",
      "训练次数: 7 Epoch: 0024 log_lik= 0.51334 train_kl= 0.00850 train_loss= 0.52184 train_acc= 0.51496 val_roc= 0.87804 val_ap= 0.87595 time= 0.10854\n",
      "训练次数: 7 Epoch: 0025 log_lik= 0.5090993 train_kl= 0.00851 train_loss= 0.51761 train_acc= 0.51667 val_roc= 0.87854 val_ap= 0.87603 time= 0.11204\n",
      "训练次数: 7 Epoch: 0026 log_lik= 0.5062485 train_kl= 0.00852 train_loss= 0.51477 train_acc= 0.51970 val_roc= 0.87992 val_ap= 0.87776 time= 0.12547\n",
      "训练次数: 7 Epoch: 0027 log_lik= 0.5030878 train_kl= 0.00854 train_loss= 0.51163 train_acc= 0.52407 val_roc= 0.88172 val_ap= 0.87974 time= 0.11650\n",
      "训练次数: 7 Epoch: 0028 log_lik= 0.4996894 train_kl= 0.00855 train_loss= 0.50824 train_acc= 0.52794 val_roc= 0.88412 val_ap= 0.88154 time= 0.11252\n",
      "训练次数: 7 Epoch: 0029 log_lik= 0.49736166 train_kl= 0.00857 train_loss= 0.50593 train_acc= 0.52910 val_roc= 0.88645 val_ap= 0.88368 time= 0.10854\n",
      "训练次数: 7 Epoch: 0030 log_lik= 0.49488774 train_kl= 0.00858 train_loss= 0.50347 train_acc= 0.52928 val_roc= 0.88804 val_ap= 0.88508 time= 0.11153\n",
      "训练次数: 7 Epoch: 0031 log_lik= 0.49204504 train_kl= 0.00859 train_loss= 0.50063 train_acc= 0.52990 val_roc= 0.88973 val_ap= 0.88636 time= 0.10854\n",
      "训练次数: 7 Epoch: 0032 log_lik= 0.4885284 train_kl= 0.00859 train_loss= 0.49712 train_acc= 0.53076 val_roc= 0.89134 val_ap= 0.88775 time= 0.11650\n",
      "训练次数: 7 Epoch: 0033 log_lik= 0.48512858 train_kl= 0.00859 train_loss= 0.49372 train_acc= 0.53232 val_roc= 0.89291 val_ap= 0.88885 time= 0.10953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0034 log_lik= 0.48250303 train_kl= 0.00859 train_loss= 0.49109 train_acc= 0.53396 val_roc= 0.89450 val_ap= 0.89080 time= 0.11059\n",
      "训练次数: 7 Epoch: 0035 log_lik= 0.47970894 train_kl= 0.00859 train_loss= 0.48830 train_acc= 0.53590 val_roc= 0.89607 val_ap= 0.89276 time= 0.10854\n",
      "训练次数: 7 Epoch: 0036 log_lik= 0.4771739 train_kl= 0.00860 train_loss= 0.48577 train_acc= 0.53783 val_roc= 0.89764 val_ap= 0.89470 time= 0.11054\n",
      "训练次数: 7 Epoch: 0037 log_lik= 0.47480795 train_kl= 0.00860 train_loss= 0.48341 train_acc= 0.54049 val_roc= 0.89932 val_ap= 0.89615 time= 0.11352\n",
      "训练次数: 7 Epoch: 0038 log_lik= 0.47289675 train_kl= 0.00861 train_loss= 0.48150 train_acc= 0.54209 val_roc= 0.90110 val_ap= 0.89797 time= 0.12646\n",
      "训练次数: 7 Epoch: 0039 log_lik= 0.4710224 train_kl= 0.00861 train_loss= 0.47963 train_acc= 0.54491 val_roc= 0.90246 val_ap= 0.89973 time= 0.11252\n",
      "训练次数: 7 Epoch: 0040 log_lik= 0.46925145 train_kl= 0.00861 train_loss= 0.47786 train_acc= 0.54657 val_roc= 0.90470 val_ap= 0.90347 time= 0.11451\n",
      "训练次数: 7 Epoch: 0041 log_lik= 0.46781975 train_kl= 0.00861 train_loss= 0.47643 train_acc= 0.54809 val_roc= 0.90581 val_ap= 0.90493 time= 0.11651\n",
      "训练次数: 7 Epoch: 0042 log_lik= 0.46634716 train_kl= 0.00861 train_loss= 0.47496 train_acc= 0.54925 val_roc= 0.90705 val_ap= 0.90650 time= 0.11450\n",
      "训练次数: 7 Epoch: 0043 log_lik= 0.46527585 train_kl= 0.00862 train_loss= 0.47389 train_acc= 0.55023 val_roc= 0.90763 val_ap= 0.90737 time= 0.10953\n",
      "训练次数: 7 Epoch: 0044 log_lik= 0.46419308 train_kl= 0.00862 train_loss= 0.47281 train_acc= 0.55088 val_roc= 0.90864 val_ap= 0.90967 time= 0.11651\n",
      "训练次数: 7 Epoch: 0045 log_lik= 0.46312356 train_kl= 0.00862 train_loss= 0.47174 train_acc= 0.55107 val_roc= 0.91010 val_ap= 0.91192 time= 0.10853\n",
      "训练次数: 7 Epoch: 0046 log_lik= 0.4620916 train_kl= 0.00861 train_loss= 0.47071 train_acc= 0.55135 val_roc= 0.91188 val_ap= 0.91468 time= 0.11291\n",
      "训练次数: 7 Epoch: 0047 log_lik= 0.4609761 train_kl= 0.00861 train_loss= 0.46959 train_acc= 0.55174 val_roc= 0.91269 val_ap= 0.91641 time= 0.11053\n",
      "训练次数: 7 Epoch: 0048 log_lik= 0.45989245 train_kl= 0.00861 train_loss= 0.46850 train_acc= 0.55236 val_roc= 0.91399 val_ap= 0.91815 time= 0.10954\n",
      "训练次数: 7 Epoch: 0049 log_lik= 0.45889655 train_kl= 0.00861 train_loss= 0.46751 train_acc= 0.55272 val_roc= 0.91434 val_ap= 0.91907 time= 0.11153\n",
      "训练次数: 7 Epoch: 0050 log_lik= 0.45801017 train_kl= 0.00861 train_loss= 0.46662 train_acc= 0.55315 val_roc= 0.91463 val_ap= 0.91929 time= 0.10754\n",
      "训练次数: 7 Epoch: 0051 log_lik= 0.45688763 train_kl= 0.00861 train_loss= 0.46550 train_acc= 0.55385 val_roc= 0.91503 val_ap= 0.91942 time= 0.11053\n",
      "训练次数: 7 Epoch: 0052 log_lik= 0.45583242 train_kl= 0.00861 train_loss= 0.46444 train_acc= 0.55422 val_roc= 0.91525 val_ap= 0.91889 time= 0.12051\n",
      "训练次数: 7 Epoch: 0053 log_lik= 0.45486358 train_kl= 0.00861 train_loss= 0.46348 train_acc= 0.55454 val_roc= 0.91629 val_ap= 0.91957 time= 0.11152\n",
      "训练次数: 7 Epoch: 0054 log_lik= 0.4539592 train_kl= 0.00862 train_loss= 0.46258 train_acc= 0.55527 val_roc= 0.91736 val_ap= 0.92068 time= 0.11053\n",
      "训练次数: 7 Epoch: 0055 log_lik= 0.45318758 train_kl= 0.00862 train_loss= 0.46181 train_acc= 0.55552 val_roc= 0.91821 val_ap= 0.92160 time= 0.11850\n",
      "训练次数: 7 Epoch: 0056 log_lik= 0.4522914 train_kl= 0.00862 train_loss= 0.46092 train_acc= 0.55579 val_roc= 0.91888 val_ap= 0.92261 time= 0.11053\n",
      "训练次数: 7 Epoch: 0057 log_lik= 0.4516258 train_kl= 0.00863 train_loss= 0.46025 train_acc= 0.55597 val_roc= 0.91892 val_ap= 0.92265 time= 0.11053\n",
      "训练次数: 7 Epoch: 0058 log_lik= 0.45091528 train_kl= 0.00863 train_loss= 0.45955 train_acc= 0.55654 val_roc= 0.91823 val_ap= 0.92155 time= 0.11473\n",
      "训练次数: 7 Epoch: 0059 log_lik= 0.45022556 train_kl= 0.00863 train_loss= 0.45886 train_acc= 0.55691 val_roc= 0.91769 val_ap= 0.92082 time= 0.11651\n",
      "训练次数: 7 Epoch: 0060 log_lik= 0.44947642 train_kl= 0.00864 train_loss= 0.45811 train_acc= 0.55760 val_roc= 0.91717 val_ap= 0.91989 time= 0.11253\n",
      "训练次数: 7 Epoch: 0061 log_lik= 0.4487979 train_kl= 0.00864 train_loss= 0.45743 train_acc= 0.55871 val_roc= 0.91714 val_ap= 0.91964 time= 0.11949\n",
      "训练次数: 7 Epoch: 0062 log_lik= 0.44803065 train_kl= 0.00864 train_loss= 0.45667 train_acc= 0.55974 val_roc= 0.91714 val_ap= 0.91978 time= 0.11163\n",
      "训练次数: 7 Epoch: 0063 log_lik= 0.4471968 train_kl= 0.00864 train_loss= 0.45584 train_acc= 0.56011 val_roc= 0.91758 val_ap= 0.92048 time= 0.11850\n",
      "训练次数: 7 Epoch: 0064 log_lik= 0.4465171 train_kl= 0.00864 train_loss= 0.45516 train_acc= 0.56120 val_roc= 0.91752 val_ap= 0.92090 time= 0.11252\n",
      "训练次数: 7 Epoch: 0065 log_lik= 0.44594336 train_kl= 0.00864 train_loss= 0.45459 train_acc= 0.56207 val_roc= 0.91793 val_ap= 0.92106 time= 0.11551\n",
      "训练次数: 7 Epoch: 0066 log_lik= 0.44540673 train_kl= 0.00865 train_loss= 0.45406 train_acc= 0.56251 val_roc= 0.91767 val_ap= 0.92073 time= 0.11949\n",
      "训练次数: 7 Epoch: 0067 log_lik= 0.444734 train_kl= 0.00865 train_loss= 0.45339 train_acc= 0.56280 val_roc= 0.91780 val_ap= 0.92084 time= 0.11152\n",
      "训练次数: 7 Epoch: 0068 log_lik= 0.44431627 train_kl= 0.00866 train_loss= 0.45297 train_acc= 0.56350 val_roc= 0.91751 val_ap= 0.92065 time= 0.10954\n",
      "训练次数: 7 Epoch: 0069 log_lik= 0.44385055 train_kl= 0.00866 train_loss= 0.45251 train_acc= 0.56411 val_roc= 0.91748 val_ap= 0.92064 time= 0.11949\n",
      "训练次数: 7 Epoch: 0070 log_lik= 0.44331428 train_kl= 0.00866 train_loss= 0.45197 train_acc= 0.56517 val_roc= 0.91746 val_ap= 0.92076 time= 0.11273\n",
      "训练次数: 7 Epoch: 0071 log_lik= 0.44285193 train_kl= 0.00866 train_loss= 0.45151 train_acc= 0.56581 val_roc= 0.91764 val_ap= 0.92093 time= 0.11153\n",
      "训练次数: 7 Epoch: 0072 log_lik= 0.44237328 train_kl= 0.00866 train_loss= 0.45103 train_acc= 0.56624 val_roc= 0.91780 val_ap= 0.92108 time= 0.11352\n",
      "训练次数: 7 Epoch: 0073 log_lik= 0.4418645 train_kl= 0.00866 train_loss= 0.45053 train_acc= 0.56678 val_roc= 0.91782 val_ap= 0.92115 time= 0.11252\n",
      "训练次数: 7 Epoch: 0074 log_lik= 0.44137004 train_kl= 0.00866 train_loss= 0.45003 train_acc= 0.56755 val_roc= 0.91803 val_ap= 0.92145 time= 0.11053\n",
      "训练次数: 7 Epoch: 0075 log_lik= 0.4410442 train_kl= 0.00866 train_loss= 0.44970 train_acc= 0.56822 val_roc= 0.91801 val_ap= 0.92148 time= 0.11053\n",
      "训练次数: 7 Epoch: 0076 log_lik= 0.4406532 train_kl= 0.00866 train_loss= 0.44931 train_acc= 0.56849 val_roc= 0.91768 val_ap= 0.92105 time= 0.11153\n",
      "训练次数: 7 Epoch: 0077 log_lik= 0.44027355 train_kl= 0.00866 train_loss= 0.44893 train_acc= 0.56886 val_roc= 0.91743 val_ap= 0.92090 time= 0.12049\n",
      "训练次数: 7 Epoch: 0078 log_lik= 0.4398105 train_kl= 0.00866 train_loss= 0.44847 train_acc= 0.56899 val_roc= 0.91759 val_ap= 0.92111 time= 0.12746\n",
      "训练次数: 7 Epoch: 0079 log_lik= 0.43950507 train_kl= 0.00866 train_loss= 0.44817 train_acc= 0.56927 val_roc= 0.91735 val_ap= 0.92108 time= 0.12646\n",
      "训练次数: 7 Epoch: 0080 log_lik= 0.4391118 train_kl= 0.00866 train_loss= 0.44777 train_acc= 0.56931 val_roc= 0.91748 val_ap= 0.92128 time= 0.12646\n",
      "训练次数: 7 Epoch: 0081 log_lik= 0.43869472 train_kl= 0.00866 train_loss= 0.44736 train_acc= 0.56985 val_roc= 0.91745 val_ap= 0.92147 time= 0.11650\n",
      "训练次数: 7 Epoch: 0082 log_lik= 0.43836448 train_kl= 0.00866 train_loss= 0.44703 train_acc= 0.56987 val_roc= 0.91767 val_ap= 0.92171 time= 0.11152\n",
      "训练次数: 7 Epoch: 0083 log_lik= 0.43796402 train_kl= 0.00866 train_loss= 0.44663 train_acc= 0.56990 val_roc= 0.91790 val_ap= 0.92203 time= 0.11252\n",
      "训练次数: 7 Epoch: 0084 log_lik= 0.4375581 train_kl= 0.00866 train_loss= 0.44622 train_acc= 0.57004 val_roc= 0.91801 val_ap= 0.92230 time= 0.11551\n",
      "训练次数: 7 Epoch: 0085 log_lik= 0.4372376 train_kl= 0.00867 train_loss= 0.44590 train_acc= 0.57053 val_roc= 0.91814 val_ap= 0.92267 time= 0.11750\n",
      "训练次数: 7 Epoch: 0086 log_lik= 0.43689358 train_kl= 0.00867 train_loss= 0.44556 train_acc= 0.57107 val_roc= 0.91803 val_ap= 0.92254 time= 0.10854\n",
      "训练次数: 7 Epoch: 0087 log_lik= 0.43655372 train_kl= 0.00867 train_loss= 0.44522 train_acc= 0.57137 val_roc= 0.91785 val_ap= 0.92242 time= 0.11252\n",
      "训练次数: 7 Epoch: 0088 log_lik= 0.4361875 train_kl= 0.00867 train_loss= 0.44486 train_acc= 0.57192 val_roc= 0.91781 val_ap= 0.92241 time= 0.11252\n",
      "训练次数: 7 Epoch: 0089 log_lik= 0.43582103 train_kl= 0.00867 train_loss= 0.44449 train_acc= 0.57248 val_roc= 0.91784 val_ap= 0.92263 time= 0.11970\n",
      "训练次数: 7 Epoch: 0090 log_lik= 0.4355062 train_kl= 0.00867 train_loss= 0.44418 train_acc= 0.57246 val_roc= 0.91780 val_ap= 0.92289 time= 0.11352\n",
      "训练次数: 7 Epoch: 0091 log_lik= 0.43515018 train_kl= 0.00867 train_loss= 0.44382 train_acc= 0.57319 val_roc= 0.91840 val_ap= 0.92357 time= 0.11352\n",
      "训练次数: 7 Epoch: 0092 log_lik= 0.4348027 train_kl= 0.00867 train_loss= 0.44347 train_acc= 0.57355 val_roc= 0.91848 val_ap= 0.92373 time= 0.11153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 7 Epoch: 0093 log_lik= 0.43451503 train_kl= 0.00867 train_loss= 0.44319 train_acc= 0.57372 val_roc= 0.91840 val_ap= 0.92390 time= 0.11849\n",
      "训练次数: 7 Epoch: 0094 log_lik= 0.43411502 train_kl= 0.00867 train_loss= 0.44279 train_acc= 0.57429 val_roc= 0.91843 val_ap= 0.92423 time= 0.11153\n",
      "训练次数: 7 Epoch: 0095 log_lik= 0.433782 train_kl= 0.00868 train_loss= 0.44246 train_acc= 0.57541 val_roc= 0.91871 val_ap= 0.92444 time= 0.11252\n",
      "训练次数: 7 Epoch: 0096 log_lik= 0.4334235 train_kl= 0.00868 train_loss= 0.44210 train_acc= 0.57594 val_roc= 0.91882 val_ap= 0.92466 time= 0.11156\n",
      "训练次数: 7 Epoch: 0097 log_lik= 0.43311155 train_kl= 0.00868 train_loss= 0.44179 train_acc= 0.57633 val_roc= 0.91888 val_ap= 0.92506 time= 0.11750\n",
      "训练次数: 7 Epoch: 0098 log_lik= 0.43279603 train_kl= 0.00868 train_loss= 0.44147 train_acc= 0.57662 val_roc= 0.91907 val_ap= 0.92544 time= 0.12049\n",
      "训练次数: 7 Epoch: 0099 log_lik= 0.4324435 train_kl= 0.00868 train_loss= 0.44112 train_acc= 0.57721 val_roc= 0.91959 val_ap= 0.92609 time= 0.11053\n",
      "训练次数: 7 Epoch: 0100 log_lik= 0.43216196 train_kl= 0.00868 train_loss= 0.44084 train_acc= 0.57791 val_roc= 0.91959 val_ap= 0.92622 time= 0.11750\n",
      "Optimization Finished!\n",
      "训练次数: 7 ROC score: 0.9109527633052363\n",
      "训练次数: 7 AP score: 0.9257805839620542\n",
      "训练次数: 8 Epoch: 0001 log_lik= 0.784246 train_kl= 0.00805 train_loss= 0.79230 train_acc= 0.07260 val_roc= 0.64150 val_ap= 0.69807 time= 8.95653\n",
      "训练次数: 8 Epoch: 0002 log_lik= 0.90940464 train_kl= 0.00840 train_loss= 0.91780 train_acc= 0.00160 val_roc= 0.72751 val_ap= 0.76208 time= 0.13044\n",
      "训练次数: 8 Epoch: 0003 log_lik= 0.7494096 train_kl= 0.00810 train_loss= 0.75751 train_acc= 0.00748 val_roc= 0.72159 val_ap= 0.75856 time= 0.12805\n",
      "训练次数: 8 Epoch: 0004 log_lik= 0.7409023 train_kl= 0.00813 train_loss= 0.74903 train_acc= 0.00805 val_roc= 0.70433 val_ap= 0.73988 time= 0.12347\n",
      "训练次数: 8 Epoch: 0005 log_lik= 0.75526184 train_kl= 0.00822 train_loss= 0.76348 train_acc= 0.00248 val_roc= 0.73542 val_ap= 0.76732 time= 0.12846\n",
      "训练次数: 8 Epoch: 0006 log_lik= 0.73741287 train_kl= 0.00820 train_loss= 0.74562 train_acc= 0.00330 val_roc= 0.78304 val_ap= 0.80589 time= 0.11152\n",
      "训练次数: 8 Epoch: 0007 log_lik= 0.7172148 train_kl= 0.00817 train_loss= 0.72539 train_acc= 0.01668 val_roc= 0.81145 val_ap= 0.82198 time= 0.11451\n",
      "训练次数: 8 Epoch: 0008 log_lik= 0.7007153 train_kl= 0.00816 train_loss= 0.70888 train_acc= 0.08852 val_roc= 0.81561 val_ap= 0.81904 time= 0.11551\n",
      "训练次数: 8 Epoch: 0009 log_lik= 0.68074894 train_kl= 0.00818 train_loss= 0.68893 train_acc= 0.19472 val_roc= 0.81075 val_ap= 0.81080 time= 0.11252\n",
      "训练次数: 8 Epoch: 0010 log_lik= 0.6597818 train_kl= 0.00823 train_loss= 0.66801 train_acc= 0.26835 val_roc= 0.80610 val_ap= 0.80338 time= 0.11750\n",
      "训练次数: 8 Epoch: 0011 log_lik= 0.64769727 train_kl= 0.00829 train_loss= 0.65598 train_acc= 0.31065 val_roc= 0.80433 val_ap= 0.80201 time= 0.11651\n",
      "训练次数: 8 Epoch: 0012 log_lik= 0.6423894 train_kl= 0.00834 train_loss= 0.65073 train_acc= 0.35046 val_roc= 0.80111 val_ap= 0.79692 time= 0.11651\n",
      "训练次数: 8 Epoch: 0013 log_lik= 0.6388592 train_kl= 0.00837 train_loss= 0.64723 train_acc= 0.38835 val_roc= 0.80063 val_ap= 0.79492 time= 0.11849\n",
      "训练次数: 8 Epoch: 0014 log_lik= 0.6321721 train_kl= 0.00838 train_loss= 0.64055 train_acc= 0.41547 val_roc= 0.80341 val_ap= 0.79860 time= 0.12348\n",
      "训练次数: 8 Epoch: 0015 log_lik= 0.62217045 train_kl= 0.00836 train_loss= 0.63053 train_acc= 0.43568 val_roc= 0.80887 val_ap= 0.80536 time= 0.11252\n",
      "训练次数: 8 Epoch: 0016 log_lik= 0.6129979 train_kl= 0.00833 train_loss= 0.62133 train_acc= 0.44722 val_roc= 0.81505 val_ap= 0.81266 time= 0.11451\n",
      "训练次数: 8 Epoch: 0017 log_lik= 0.6060563 train_kl= 0.00831 train_loss= 0.61436 train_acc= 0.45232 val_roc= 0.82187 val_ap= 0.82032 time= 0.11053\n",
      "训练次数: 8 Epoch: 0018 log_lik= 0.60142845 train_kl= 0.00829 train_loss= 0.60972 train_acc= 0.45675 val_roc= 0.82830 val_ap= 0.82756 time= 0.11053\n",
      "训练次数: 8 Epoch: 0019 log_lik= 0.5967962 train_kl= 0.00828 train_loss= 0.60508 train_acc= 0.45829 val_roc= 0.83506 val_ap= 0.83454 time= 0.11750\n",
      "训练次数: 8 Epoch: 0020 log_lik= 0.5901324 train_kl= 0.00829 train_loss= 0.59842 train_acc= 0.46133 val_roc= 0.84188 val_ap= 0.84238 time= 0.11053\n",
      "训练次数: 8 Epoch: 0021 log_lik= 0.5823273 train_kl= 0.00831 train_loss= 0.59064 train_acc= 0.46453 val_roc= 0.84875 val_ap= 0.84940 time= 0.11652\n",
      "训练次数: 8 Epoch: 0022 log_lik= 0.57413155 train_kl= 0.00834 train_loss= 0.58247 train_acc= 0.46855 val_roc= 0.85613 val_ap= 0.85636 time= 0.12248\n",
      "训练次数: 8 Epoch: 0023 log_lik= 0.5658506 train_kl= 0.00837 train_loss= 0.57422 train_acc= 0.47317 val_roc= 0.86423 val_ap= 0.86430 time= 0.11949\n",
      "训练次数: 8 Epoch: 0024 log_lik= 0.55812955 train_kl= 0.00840 train_loss= 0.56653 train_acc= 0.47800 val_roc= 0.87350 val_ap= 0.87388 time= 0.11153\n",
      "训练次数: 8 Epoch: 0025 log_lik= 0.5497835 train_kl= 0.00843 train_loss= 0.55822 train_acc= 0.48403 val_roc= 0.88058 val_ap= 0.88165 time= 0.12347\n",
      "训练次数: 8 Epoch: 0026 log_lik= 0.5411527 train_kl= 0.00845 train_loss= 0.54961 train_acc= 0.49048 val_roc= 0.88864 val_ap= 0.88849 time= 0.11352\n",
      "训练次数: 8 Epoch: 0027 log_lik= 0.5325364 train_kl= 0.00847 train_loss= 0.54101 train_acc= 0.49883 val_roc= 0.89385 val_ap= 0.89352 time= 0.12547\n",
      "训练次数: 8 Epoch: 0028 log_lik= 0.52494943 train_kl= 0.00848 train_loss= 0.53343 train_acc= 0.50636 val_roc= 0.89942 val_ap= 0.89938 time= 0.11750\n",
      "训练次数: 8 Epoch: 0029 log_lik= 0.5195432 train_kl= 0.00850 train_loss= 0.52804 train_acc= 0.51031 val_roc= 0.90315 val_ap= 0.90297 time= 0.11366\n",
      "训练次数: 8 Epoch: 0030 log_lik= 0.5158389 train_kl= 0.00851 train_loss= 0.52435 train_acc= 0.51237 val_roc= 0.90464 val_ap= 0.90432 time= 0.12109\n",
      "训练次数: 8 Epoch: 0031 log_lik= 0.51270175 train_kl= 0.00852 train_loss= 0.52122 train_acc= 0.51546 val_roc= 0.90588 val_ap= 0.90566 time= 0.12347\n",
      "训练次数: 8 Epoch: 0032 log_lik= 0.50999624 train_kl= 0.00853 train_loss= 0.51853 train_acc= 0.51952 val_roc= 0.90655 val_ap= 0.90657 time= 0.11850\n",
      "训练次数: 8 Epoch: 0033 log_lik= 0.50737816 train_kl= 0.00854 train_loss= 0.51592 train_acc= 0.52236 val_roc= 0.90750 val_ap= 0.90808 time= 0.11750\n",
      "训练次数: 8 Epoch: 0034 log_lik= 0.5043277 train_kl= 0.00855 train_loss= 0.51288 train_acc= 0.52371 val_roc= 0.90870 val_ap= 0.90965 time= 0.12148\n",
      "训练次数: 8 Epoch: 0035 log_lik= 0.5017048 train_kl= 0.00855 train_loss= 0.51026 train_acc= 0.52339 val_roc= 0.91077 val_ap= 0.91216 time= 0.11053\n",
      "训练次数: 8 Epoch: 0036 log_lik= 0.49944532 train_kl= 0.00856 train_loss= 0.50800 train_acc= 0.52341 val_roc= 0.91301 val_ap= 0.91517 time= 0.12152\n",
      "训练次数: 8 Epoch: 0037 log_lik= 0.4968815 train_kl= 0.00855 train_loss= 0.50544 train_acc= 0.52414 val_roc= 0.91437 val_ap= 0.91650 time= 0.11352\n",
      "训练次数: 8 Epoch: 0038 log_lik= 0.49424985 train_kl= 0.00855 train_loss= 0.50280 train_acc= 0.52584 val_roc= 0.91540 val_ap= 0.91705 time= 0.12248\n",
      "训练次数: 8 Epoch: 0039 log_lik= 0.49153623 train_kl= 0.00855 train_loss= 0.50009 train_acc= 0.52828 val_roc= 0.91616 val_ap= 0.91751 time= 0.11252\n",
      "训练次数: 8 Epoch: 0040 log_lik= 0.48933777 train_kl= 0.00855 train_loss= 0.49789 train_acc= 0.53029 val_roc= 0.91622 val_ap= 0.91743 time= 0.11153\n",
      "训练次数: 8 Epoch: 0041 log_lik= 0.48720807 train_kl= 0.00855 train_loss= 0.49576 train_acc= 0.53236 val_roc= 0.91593 val_ap= 0.91722 time= 0.10954\n",
      "训练次数: 8 Epoch: 0042 log_lik= 0.48453897 train_kl= 0.00855 train_loss= 0.49309 train_acc= 0.53495 val_roc= 0.91587 val_ap= 0.91753 time= 0.11551\n",
      "训练次数: 8 Epoch: 0043 log_lik= 0.48205754 train_kl= 0.00856 train_loss= 0.49062 train_acc= 0.53719 val_roc= 0.91563 val_ap= 0.91764 time= 0.11252\n",
      "训练次数: 8 Epoch: 0044 log_lik= 0.48006818 train_kl= 0.00856 train_loss= 0.48863 train_acc= 0.53865 val_roc= 0.91554 val_ap= 0.91767 time= 0.11153\n",
      "训练次数: 8 Epoch: 0045 log_lik= 0.4787035 train_kl= 0.00857 train_loss= 0.48727 train_acc= 0.53945 val_roc= 0.91514 val_ap= 0.91759 time= 0.11551\n",
      "训练次数: 8 Epoch: 0046 log_lik= 0.47758675 train_kl= 0.00858 train_loss= 0.48616 train_acc= 0.53971 val_roc= 0.91606 val_ap= 0.91867 time= 0.11551\n",
      "训练次数: 8 Epoch: 0047 log_lik= 0.47624573 train_kl= 0.00858 train_loss= 0.48482 train_acc= 0.54050 val_roc= 0.91589 val_ap= 0.91817 time= 0.11551\n",
      "训练次数: 8 Epoch: 0048 log_lik= 0.47483432 train_kl= 0.00858 train_loss= 0.48341 train_acc= 0.54177 val_roc= 0.91609 val_ap= 0.91838 time= 0.11007\n",
      "训练次数: 8 Epoch: 0049 log_lik= 0.47376332 train_kl= 0.00858 train_loss= 0.48234 train_acc= 0.54239 val_roc= 0.91668 val_ap= 0.91918 time= 0.10953\n",
      "训练次数: 8 Epoch: 0050 log_lik= 0.47256407 train_kl= 0.00858 train_loss= 0.48114 train_acc= 0.54339 val_roc= 0.91704 val_ap= 0.91967 time= 0.11153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 8 Epoch: 0051 log_lik= 0.47119644 train_kl= 0.00857 train_loss= 0.47977 train_acc= 0.54476 val_roc= 0.91811 val_ap= 0.92121 time= 0.11849\n",
      "训练次数: 8 Epoch: 0052 log_lik= 0.46994674 train_kl= 0.00857 train_loss= 0.47852 train_acc= 0.54628 val_roc= 0.91908 val_ap= 0.92244 time= 0.11153\n",
      "训练次数: 8 Epoch: 0053 log_lik= 0.46875942 train_kl= 0.00857 train_loss= 0.47733 train_acc= 0.54698 val_roc= 0.91978 val_ap= 0.92351 time= 0.11053\n",
      "训练次数: 8 Epoch: 0054 log_lik= 0.467615 train_kl= 0.00857 train_loss= 0.47619 train_acc= 0.54766 val_roc= 0.92099 val_ap= 0.92507 time= 0.11352\n",
      "训练次数: 8 Epoch: 0055 log_lik= 0.4663131 train_kl= 0.00858 train_loss= 0.47489 train_acc= 0.54861 val_roc= 0.92228 val_ap= 0.92644 time= 0.11750\n",
      "训练次数: 8 Epoch: 0056 log_lik= 0.46491963 train_kl= 0.00858 train_loss= 0.47350 train_acc= 0.54912 val_roc= 0.92372 val_ap= 0.92799 time= 0.11557\n",
      "训练次数: 8 Epoch: 0057 log_lik= 0.46357796 train_kl= 0.00859 train_loss= 0.47217 train_acc= 0.54966 val_roc= 0.92500 val_ap= 0.92951 time= 0.11551\n",
      "训练次数: 8 Epoch: 0058 log_lik= 0.46250692 train_kl= 0.00860 train_loss= 0.47110 train_acc= 0.54978 val_roc= 0.92591 val_ap= 0.93045 time= 0.11047\n",
      "训练次数: 8 Epoch: 0059 log_lik= 0.4613898 train_kl= 0.00860 train_loss= 0.46999 train_acc= 0.55088 val_roc= 0.92617 val_ap= 0.93097 time= 0.11053\n",
      "训练次数: 8 Epoch: 0060 log_lik= 0.46036094 train_kl= 0.00861 train_loss= 0.46897 train_acc= 0.55124 val_roc= 0.92640 val_ap= 0.93144 time= 0.10870\n",
      "训练次数: 8 Epoch: 0061 log_lik= 0.45940983 train_kl= 0.00861 train_loss= 0.46802 train_acc= 0.55232 val_roc= 0.92683 val_ap= 0.93193 time= 0.11053\n",
      "训练次数: 8 Epoch: 0062 log_lik= 0.45846847 train_kl= 0.00861 train_loss= 0.46708 train_acc= 0.55302 val_roc= 0.92729 val_ap= 0.93234 time= 0.11253\n",
      "训练次数: 8 Epoch: 0063 log_lik= 0.45746878 train_kl= 0.00861 train_loss= 0.46608 train_acc= 0.55469 val_roc= 0.92783 val_ap= 0.93302 time= 0.11850\n",
      "训练次数: 8 Epoch: 0064 log_lik= 0.45657146 train_kl= 0.00861 train_loss= 0.46518 train_acc= 0.55629 val_roc= 0.92823 val_ap= 0.93350 time= 0.11152\n",
      "训练次数: 8 Epoch: 0065 log_lik= 0.45562458 train_kl= 0.00861 train_loss= 0.46424 train_acc= 0.55784 val_roc= 0.92871 val_ap= 0.93415 time= 0.12909\n",
      "训练次数: 8 Epoch: 0066 log_lik= 0.45471525 train_kl= 0.00861 train_loss= 0.46333 train_acc= 0.55927 val_roc= 0.92901 val_ap= 0.93460 time= 0.12049\n",
      "训练次数: 8 Epoch: 0067 log_lik= 0.45374447 train_kl= 0.00861 train_loss= 0.46236 train_acc= 0.56062 val_roc= 0.92933 val_ap= 0.93496 time= 0.11153\n",
      "训练次数: 8 Epoch: 0068 log_lik= 0.45294583 train_kl= 0.00862 train_loss= 0.46156 train_acc= 0.56232 val_roc= 0.92942 val_ap= 0.93513 time= 0.11650\n",
      "训练次数: 8 Epoch: 0069 log_lik= 0.45200706 train_kl= 0.00862 train_loss= 0.46063 train_acc= 0.56391 val_roc= 0.92961 val_ap= 0.93539 time= 0.11750\n",
      "训练次数: 8 Epoch: 0070 log_lik= 0.45123282 train_kl= 0.00862 train_loss= 0.45986 train_acc= 0.56485 val_roc= 0.92991 val_ap= 0.93567 time= 0.11850\n",
      "训练次数: 8 Epoch: 0071 log_lik= 0.45044544 train_kl= 0.00863 train_loss= 0.45907 train_acc= 0.56597 val_roc= 0.93010 val_ap= 0.93585 time= 0.11650\n",
      "训练次数: 8 Epoch: 0072 log_lik= 0.4497384 train_kl= 0.00863 train_loss= 0.45837 train_acc= 0.56702 val_roc= 0.92997 val_ap= 0.93582 time= 0.12148\n",
      "训练次数: 8 Epoch: 0073 log_lik= 0.4490128 train_kl= 0.00864 train_loss= 0.45765 train_acc= 0.56818 val_roc= 0.92998 val_ap= 0.93589 time= 0.11451\n",
      "训练次数: 8 Epoch: 0074 log_lik= 0.44821236 train_kl= 0.00864 train_loss= 0.45685 train_acc= 0.56982 val_roc= 0.92955 val_ap= 0.93582 time= 0.11252\n",
      "训练次数: 8 Epoch: 0075 log_lik= 0.44753003 train_kl= 0.00864 train_loss= 0.45617 train_acc= 0.57038 val_roc= 0.92967 val_ap= 0.93591 time= 0.11252\n",
      "训练次数: 8 Epoch: 0076 log_lik= 0.4469079 train_kl= 0.00864 train_loss= 0.45555 train_acc= 0.57215 val_roc= 0.92980 val_ap= 0.93646 time= 0.11551\n",
      "训练次数: 8 Epoch: 0077 log_lik= 0.44625133 train_kl= 0.00865 train_loss= 0.45490 train_acc= 0.57277 val_roc= 0.92958 val_ap= 0.93652 time= 0.11850\n",
      "训练次数: 8 Epoch: 0078 log_lik= 0.44554603 train_kl= 0.00865 train_loss= 0.45419 train_acc= 0.57418 val_roc= 0.92958 val_ap= 0.93657 time= 0.12049\n",
      "训练次数: 8 Epoch: 0079 log_lik= 0.44502825 train_kl= 0.00865 train_loss= 0.45368 train_acc= 0.57529 val_roc= 0.92942 val_ap= 0.93669 time= 0.12148\n",
      "训练次数: 8 Epoch: 0080 log_lik= 0.4443829 train_kl= 0.00865 train_loss= 0.45303 train_acc= 0.57652 val_roc= 0.92927 val_ap= 0.93685 time= 0.12049\n",
      "训练次数: 8 Epoch: 0081 log_lik= 0.4438272 train_kl= 0.00865 train_loss= 0.45248 train_acc= 0.57725 val_roc= 0.92910 val_ap= 0.93678 time= 0.11252\n",
      "训练次数: 8 Epoch: 0082 log_lik= 0.44341743 train_kl= 0.00865 train_loss= 0.45207 train_acc= 0.57818 val_roc= 0.92901 val_ap= 0.93691 time= 0.11153\n",
      "训练次数: 8 Epoch: 0083 log_lik= 0.442848 train_kl= 0.00865 train_loss= 0.45150 train_acc= 0.57865 val_roc= 0.92916 val_ap= 0.93719 time= 0.11949\n",
      "训练次数: 8 Epoch: 0084 log_lik= 0.44237605 train_kl= 0.00866 train_loss= 0.45103 train_acc= 0.57961 val_roc= 0.92903 val_ap= 0.93715 time= 0.11252\n",
      "训练次数: 8 Epoch: 0085 log_lik= 0.4418934 train_kl= 0.00866 train_loss= 0.45055 train_acc= 0.58000 val_roc= 0.92913 val_ap= 0.93743 time= 0.11849\n",
      "训练次数: 8 Epoch: 0086 log_lik= 0.44147593 train_kl= 0.00866 train_loss= 0.45014 train_acc= 0.58070 val_roc= 0.92906 val_ap= 0.93755 time= 0.11750\n",
      "训练次数: 8 Epoch: 0087 log_lik= 0.44100687 train_kl= 0.00866 train_loss= 0.44967 train_acc= 0.58121 val_roc= 0.92888 val_ap= 0.93757 time= 0.11352\n",
      "训练次数: 8 Epoch: 0088 log_lik= 0.4404802 train_kl= 0.00866 train_loss= 0.44914 train_acc= 0.58245 val_roc= 0.92877 val_ap= 0.93769 time= 0.12148\n",
      "训练次数: 8 Epoch: 0089 log_lik= 0.4402124 train_kl= 0.00867 train_loss= 0.44888 train_acc= 0.58232 val_roc= 0.92890 val_ap= 0.93797 time= 0.11451\n",
      "训练次数: 8 Epoch: 0090 log_lik= 0.43974286 train_kl= 0.00867 train_loss= 0.44841 train_acc= 0.58332 val_roc= 0.92899 val_ap= 0.93833 time= 0.11153\n",
      "训练次数: 8 Epoch: 0091 log_lik= 0.43924016 train_kl= 0.00867 train_loss= 0.44791 train_acc= 0.58417 val_roc= 0.92903 val_ap= 0.93856 time= 0.11053\n",
      "训练次数: 8 Epoch: 0092 log_lik= 0.43896273 train_kl= 0.00867 train_loss= 0.44763 train_acc= 0.58466 val_roc= 0.92935 val_ap= 0.93902 time= 0.11053\n",
      "训练次数: 8 Epoch: 0093 log_lik= 0.4385829 train_kl= 0.00867 train_loss= 0.44725 train_acc= 0.58504 val_roc= 0.92945 val_ap= 0.93935 time= 0.11949\n",
      "训练次数: 8 Epoch: 0094 log_lik= 0.43817067 train_kl= 0.00867 train_loss= 0.44684 train_acc= 0.58582 val_roc= 0.92953 val_ap= 0.93955 time= 0.13044\n",
      "训练次数: 8 Epoch: 0095 log_lik= 0.4378167 train_kl= 0.00867 train_loss= 0.44648 train_acc= 0.58646 val_roc= 0.92967 val_ap= 0.93976 time= 0.11850\n",
      "训练次数: 8 Epoch: 0096 log_lik= 0.43749177 train_kl= 0.00867 train_loss= 0.44616 train_acc= 0.58697 val_roc= 0.92968 val_ap= 0.93984 time= 0.11153\n",
      "训练次数: 8 Epoch: 0097 log_lik= 0.43719247 train_kl= 0.00867 train_loss= 0.44586 train_acc= 0.58761 val_roc= 0.92987 val_ap= 0.93995 time= 0.11949\n",
      "训练次数: 8 Epoch: 0098 log_lik= 0.4368622 train_kl= 0.00867 train_loss= 0.44553 train_acc= 0.58785 val_roc= 0.92982 val_ap= 0.94000 time= 0.11949\n",
      "训练次数: 8 Epoch: 0099 log_lik= 0.4364755 train_kl= 0.00867 train_loss= 0.44515 train_acc= 0.58822 val_roc= 0.92984 val_ap= 0.94010 time= 0.11949\n",
      "训练次数: 8 Epoch: 0100 log_lik= 0.4362149 train_kl= 0.00867 train_loss= 0.44489 train_acc= 0.58825 val_roc= 0.93008 val_ap= 0.94043 time= 0.12547\n",
      "Optimization Finished!\n",
      "训练次数: 8 ROC score: 0.9187157264815702\n",
      "训练次数: 8 AP score: 0.9274449289037038\n",
      "训练次数: 9 Epoch: 0001 log_lik= 0.78429085 train_kl= 0.00805 train_loss= 0.79234 train_acc= 0.12729 val_roc= 0.65950 val_ap= 0.66541 time= 9.22742\n",
      "训练次数: 9 Epoch: 0002 log_lik= 0.8040458 train_kl= 0.00831 train_loss= 0.81236 train_acc= 0.00159 val_roc= 0.75376 val_ap= 0.73024 time= 0.12044\n",
      "训练次数: 9 Epoch: 0003 log_lik= 0.7366755 train_kl= 0.00811 train_loss= 0.74478 train_acc= 0.04424 val_roc= 0.65168 val_ap= 0.66157 time= 0.12149\n",
      "训练次数: 9 Epoch: 0004 log_lik= 0.72908545 train_kl= 0.00815 train_loss= 0.73724 train_acc= 0.05104 val_roc= 0.65541 val_ap= 0.66662 time= 0.12248\n",
      "训练次数: 9 Epoch: 0005 log_lik= 0.7276333 train_kl= 0.00821 train_loss= 0.73584 train_acc= 0.05915 val_roc= 0.72664 val_ap= 0.71635 time= 0.11684\n",
      "训练次数: 9 Epoch: 0006 log_lik= 0.6988302 train_kl= 0.00818 train_loss= 0.70701 train_acc= 0.13096 val_roc= 0.80397 val_ap= 0.78497 time= 0.11252\n",
      "训练次数: 9 Epoch: 0007 log_lik= 0.67046493 train_kl= 0.00817 train_loss= 0.67864 train_acc= 0.25445 val_roc= 0.82867 val_ap= 0.81767 time= 0.11352\n",
      "训练次数: 9 Epoch: 0008 log_lik= 0.637848 train_kl= 0.00820 train_loss= 0.64605 train_acc= 0.34006 val_roc= 0.82777 val_ap= 0.82008 time= 0.12347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0009 log_lik= 0.60646224 train_kl= 0.00826 train_loss= 0.61473 train_acc= 0.37553 val_roc= 0.82618 val_ap= 0.81826 time= 0.11357\n",
      "训练次数: 9 Epoch: 0010 log_lik= 0.5856824 train_kl= 0.00835 train_loss= 0.59404 train_acc= 0.40523 val_roc= 0.82989 val_ap= 0.82193 time= 0.12945\n",
      "训练次数: 9 Epoch: 0011 log_lik= 0.5744449 train_kl= 0.00845 train_loss= 0.58289 train_acc= 0.44109 val_roc= 0.83720 val_ap= 0.83146 time= 0.12248\n",
      "训练次数: 9 Epoch: 0012 log_lik= 0.5687688 train_kl= 0.00852 train_loss= 0.57729 train_acc= 0.47004 val_roc= 0.84468 val_ap= 0.84159 time= 0.11252\n",
      "训练次数: 9 Epoch: 0013 log_lik= 0.56306285 train_kl= 0.00857 train_loss= 0.57163 train_acc= 0.49122 val_roc= 0.85431 val_ap= 0.85166 time= 0.11254\n",
      "训练次数: 9 Epoch: 0014 log_lik= 0.5511629 train_kl= 0.00858 train_loss= 0.55974 train_acc= 0.50760 val_roc= 0.86345 val_ap= 0.86081 time= 0.11252\n",
      "训练次数: 9 Epoch: 0015 log_lik= 0.5381906 train_kl= 0.00857 train_loss= 0.54676 train_acc= 0.51664 val_roc= 0.87105 val_ap= 0.86798 time= 0.12248\n",
      "训练次数: 9 Epoch: 0016 log_lik= 0.5284747 train_kl= 0.00854 train_loss= 0.53701 train_acc= 0.51908 val_roc= 0.87733 val_ap= 0.87359 time= 0.11949\n",
      "训练次数: 9 Epoch: 0017 log_lik= 0.5218661 train_kl= 0.00851 train_loss= 0.53037 train_acc= 0.51909 val_roc= 0.88265 val_ap= 0.87870 time= 0.11551\n",
      "训练次数: 9 Epoch: 0018 log_lik= 0.5172054 train_kl= 0.00848 train_loss= 0.52568 train_acc= 0.52084 val_roc= 0.88583 val_ap= 0.88180 time= 0.11210\n",
      "训练次数: 9 Epoch: 0019 log_lik= 0.5143804 train_kl= 0.00846 train_loss= 0.52284 train_acc= 0.52437 val_roc= 0.88787 val_ap= 0.88376 time= 0.12248\n",
      "训练次数: 9 Epoch: 0020 log_lik= 0.51227355 train_kl= 0.00845 train_loss= 0.52072 train_acc= 0.52831 val_roc= 0.89030 val_ap= 0.88523 time= 0.11353\n",
      "训练次数: 9 Epoch: 0021 log_lik= 0.5096072 train_kl= 0.00845 train_loss= 0.51806 train_acc= 0.53010 val_roc= 0.89339 val_ap= 0.88722 time= 0.12447\n",
      "训练次数: 9 Epoch: 0022 log_lik= 0.50646436 train_kl= 0.00847 train_loss= 0.51493 train_acc= 0.53013 val_roc= 0.89721 val_ap= 0.89050 time= 0.11949\n",
      "训练次数: 9 Epoch: 0023 log_lik= 0.5036462 train_kl= 0.00849 train_loss= 0.51214 train_acc= 0.52774 val_roc= 0.90004 val_ap= 0.89319 time= 0.11750\n",
      "训练次数: 9 Epoch: 0024 log_lik= 0.5019136 train_kl= 0.00852 train_loss= 0.51043 train_acc= 0.52548 val_roc= 0.90264 val_ap= 0.89570 time= 0.11750\n",
      "训练次数: 9 Epoch: 0025 log_lik= 0.4997594 train_kl= 0.00855 train_loss= 0.50830 train_acc= 0.52651 val_roc= 0.90376 val_ap= 0.89682 time= 0.11903\n",
      "训练次数: 9 Epoch: 0026 log_lik= 0.49724048 train_kl= 0.00857 train_loss= 0.50581 train_acc= 0.53076 val_roc= 0.90493 val_ap= 0.89870 time= 0.11351\n",
      "训练次数: 9 Epoch: 0027 log_lik= 0.49418816 train_kl= 0.00858 train_loss= 0.50277 train_acc= 0.53487 val_roc= 0.90613 val_ap= 0.90048 time= 0.11557\n",
      "训练次数: 9 Epoch: 0028 log_lik= 0.49123442 train_kl= 0.00858 train_loss= 0.49982 train_acc= 0.53807 val_roc= 0.90885 val_ap= 0.90345 time= 0.11252\n",
      "训练次数: 9 Epoch: 0029 log_lik= 0.48763242 train_kl= 0.00858 train_loss= 0.49622 train_acc= 0.54139 val_roc= 0.91116 val_ap= 0.90611 time= 0.12049\n",
      "训练次数: 9 Epoch: 0030 log_lik= 0.48385754 train_kl= 0.00858 train_loss= 0.49244 train_acc= 0.54427 val_roc= 0.91297 val_ap= 0.90828 time= 0.11252\n",
      "训练次数: 9 Epoch: 0031 log_lik= 0.48100913 train_kl= 0.00857 train_loss= 0.48958 train_acc= 0.54782 val_roc= 0.91389 val_ap= 0.90843 time= 0.12049\n",
      "训练次数: 9 Epoch: 0032 log_lik= 0.47872713 train_kl= 0.00857 train_loss= 0.48729 train_acc= 0.55086 val_roc= 0.91402 val_ap= 0.90846 time= 0.12945\n",
      "训练次数: 9 Epoch: 0033 log_lik= 0.47711083 train_kl= 0.00856 train_loss= 0.48567 train_acc= 0.55270 val_roc= 0.91405 val_ap= 0.90895 time= 0.11551\n",
      "训练次数: 9 Epoch: 0034 log_lik= 0.4760221 train_kl= 0.00856 train_loss= 0.48458 train_acc= 0.55319 val_roc= 0.91428 val_ap= 0.90990 time= 0.12049\n",
      "训练次数: 9 Epoch: 0035 log_lik= 0.47483432 train_kl= 0.00856 train_loss= 0.48339 train_acc= 0.55318 val_roc= 0.91444 val_ap= 0.91049 time= 0.12746\n",
      "训练次数: 9 Epoch: 0036 log_lik= 0.47359893 train_kl= 0.00856 train_loss= 0.48216 train_acc= 0.55282 val_roc= 0.91453 val_ap= 0.91096 time= 0.11705\n",
      "训练次数: 9 Epoch: 0037 log_lik= 0.4723318 train_kl= 0.00857 train_loss= 0.48090 train_acc= 0.55237 val_roc= 0.91425 val_ap= 0.91074 time= 0.10854\n",
      "训练次数: 9 Epoch: 0038 log_lik= 0.4712069 train_kl= 0.00857 train_loss= 0.47978 train_acc= 0.55127 val_roc= 0.91346 val_ap= 0.90994 time= 0.11452\n",
      "训练次数: 9 Epoch: 0039 log_lik= 0.47008792 train_kl= 0.00858 train_loss= 0.47867 train_acc= 0.55119 val_roc= 0.91248 val_ap= 0.90912 time= 0.11352\n",
      "训练次数: 9 Epoch: 0040 log_lik= 0.46857446 train_kl= 0.00858 train_loss= 0.47716 train_acc= 0.55178 val_roc= 0.91200 val_ap= 0.90971 time= 0.11750\n",
      "训练次数: 9 Epoch: 0041 log_lik= 0.466847 train_kl= 0.00859 train_loss= 0.47544 train_acc= 0.55288 val_roc= 0.91175 val_ap= 0.91046 time= 0.11252\n",
      "训练次数: 9 Epoch: 0042 log_lik= 0.46533382 train_kl= 0.00859 train_loss= 0.47393 train_acc= 0.55348 val_roc= 0.91172 val_ap= 0.91142 time= 0.11849\n",
      "训练次数: 9 Epoch: 0043 log_lik= 0.4638638 train_kl= 0.00860 train_loss= 0.47246 train_acc= 0.55420 val_roc= 0.91184 val_ap= 0.91201 time= 0.11053\n",
      "训练次数: 9 Epoch: 0044 log_lik= 0.4624385 train_kl= 0.00860 train_loss= 0.47104 train_acc= 0.55442 val_roc= 0.91195 val_ap= 0.91265 time= 0.11152\n",
      "训练次数: 9 Epoch: 0045 log_lik= 0.46143457 train_kl= 0.00860 train_loss= 0.47004 train_acc= 0.55408 val_roc= 0.91187 val_ap= 0.91287 time= 0.12248\n",
      "训练次数: 9 Epoch: 0046 log_lik= 0.4606808 train_kl= 0.00860 train_loss= 0.46929 train_acc= 0.55374 val_roc= 0.91203 val_ap= 0.91348 time= 0.11949\n",
      "训练次数: 9 Epoch: 0047 log_lik= 0.45993447 train_kl= 0.00861 train_loss= 0.46854 train_acc= 0.55313 val_roc= 0.91237 val_ap= 0.91417 time= 0.11352\n",
      "训练次数: 9 Epoch: 0048 log_lik= 0.45902586 train_kl= 0.00861 train_loss= 0.46764 train_acc= 0.55293 val_roc= 0.91246 val_ap= 0.91492 time= 0.11352\n",
      "训练次数: 9 Epoch: 0049 log_lik= 0.45805296 train_kl= 0.00861 train_loss= 0.46666 train_acc= 0.55286 val_roc= 0.91302 val_ap= 0.91577 time= 0.11551\n",
      "训练次数: 9 Epoch: 0050 log_lik= 0.4571602 train_kl= 0.00861 train_loss= 0.46577 train_acc= 0.55238 val_roc= 0.91324 val_ap= 0.91628 time= 0.11352\n",
      "训练次数: 9 Epoch: 0051 log_lik= 0.45630994 train_kl= 0.00861 train_loss= 0.46492 train_acc= 0.55258 val_roc= 0.91395 val_ap= 0.91685 time= 0.11152\n",
      "训练次数: 9 Epoch: 0052 log_lik= 0.45535293 train_kl= 0.00861 train_loss= 0.46396 train_acc= 0.55305 val_roc= 0.91528 val_ap= 0.91810 time= 0.11053\n",
      "训练次数: 9 Epoch: 0053 log_lik= 0.45416299 train_kl= 0.00861 train_loss= 0.46278 train_acc= 0.55319 val_roc= 0.91662 val_ap= 0.91938 time= 0.11153\n",
      "训练次数: 9 Epoch: 0054 log_lik= 0.45319754 train_kl= 0.00862 train_loss= 0.46181 train_acc= 0.55369 val_roc= 0.91742 val_ap= 0.92019 time= 0.10854\n",
      "训练次数: 9 Epoch: 0055 log_lik= 0.45238656 train_kl= 0.00862 train_loss= 0.46100 train_acc= 0.55463 val_roc= 0.91810 val_ap= 0.92078 time= 0.11352\n",
      "训练次数: 9 Epoch: 0056 log_lik= 0.45155296 train_kl= 0.00862 train_loss= 0.46017 train_acc= 0.55463 val_roc= 0.91892 val_ap= 0.92137 time= 0.11153\n",
      "训练次数: 9 Epoch: 0057 log_lik= 0.4507665 train_kl= 0.00862 train_loss= 0.45939 train_acc= 0.55456 val_roc= 0.91944 val_ap= 0.92154 time= 0.11551\n",
      "训练次数: 9 Epoch: 0058 log_lik= 0.4497362 train_kl= 0.00862 train_loss= 0.45836 train_acc= 0.55495 val_roc= 0.91986 val_ap= 0.92175 time= 0.10754\n",
      "训练次数: 9 Epoch: 0059 log_lik= 0.44903493 train_kl= 0.00862 train_loss= 0.45766 train_acc= 0.55522 val_roc= 0.92014 val_ap= 0.92202 time= 0.10854\n",
      "训练次数: 9 Epoch: 0060 log_lik= 0.4482319 train_kl= 0.00863 train_loss= 0.45686 train_acc= 0.55601 val_roc= 0.92047 val_ap= 0.92201 time= 0.10953\n",
      "训练次数: 9 Epoch: 0061 log_lik= 0.4474953 train_kl= 0.00863 train_loss= 0.45612 train_acc= 0.55657 val_roc= 0.92015 val_ap= 0.92188 time= 0.11053\n",
      "训练次数: 9 Epoch: 0062 log_lik= 0.44669557 train_kl= 0.00863 train_loss= 0.45533 train_acc= 0.55662 val_roc= 0.91983 val_ap= 0.92169 time= 0.11583\n",
      "训练次数: 9 Epoch: 0063 log_lik= 0.44606 train_kl= 0.00864 train_loss= 0.45470 train_acc= 0.55762 val_roc= 0.91950 val_ap= 0.92132 time= 0.11352\n",
      "训练次数: 9 Epoch: 0064 log_lik= 0.44539857 train_kl= 0.00864 train_loss= 0.45404 train_acc= 0.55793 val_roc= 0.91941 val_ap= 0.92104 time= 0.10953\n",
      "训练次数: 9 Epoch: 0065 log_lik= 0.4449154 train_kl= 0.00864 train_loss= 0.45356 train_acc= 0.55794 val_roc= 0.91946 val_ap= 0.92085 time= 0.11352\n",
      "训练次数: 9 Epoch: 0066 log_lik= 0.44433376 train_kl= 0.00865 train_loss= 0.45298 train_acc= 0.55850 val_roc= 0.91947 val_ap= 0.92066 time= 0.11352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 9 Epoch: 0067 log_lik= 0.4438663 train_kl= 0.00865 train_loss= 0.45252 train_acc= 0.55906 val_roc= 0.91911 val_ap= 0.92021 time= 0.11053\n",
      "训练次数: 9 Epoch: 0068 log_lik= 0.44335818 train_kl= 0.00865 train_loss= 0.45201 train_acc= 0.55913 val_roc= 0.91897 val_ap= 0.92017 time= 0.11451\n",
      "训练次数: 9 Epoch: 0069 log_lik= 0.4429143 train_kl= 0.00865 train_loss= 0.45157 train_acc= 0.55939 val_roc= 0.91915 val_ap= 0.92027 time= 0.11352\n",
      "训练次数: 9 Epoch: 0070 log_lik= 0.44221866 train_kl= 0.00865 train_loss= 0.45087 train_acc= 0.55987 val_roc= 0.91913 val_ap= 0.92067 time= 0.12944\n",
      "训练次数: 9 Epoch: 0071 log_lik= 0.44180965 train_kl= 0.00865 train_loss= 0.45046 train_acc= 0.56016 val_roc= 0.91949 val_ap= 0.92114 time= 0.11352\n",
      "训练次数: 9 Epoch: 0072 log_lik= 0.44123176 train_kl= 0.00865 train_loss= 0.44989 train_acc= 0.56048 val_roc= 0.92004 val_ap= 0.92145 time= 0.12149\n",
      "训练次数: 9 Epoch: 0073 log_lik= 0.44076318 train_kl= 0.00865 train_loss= 0.44942 train_acc= 0.56083 val_roc= 0.92046 val_ap= 0.92154 time= 0.11750\n",
      "训练次数: 9 Epoch: 0074 log_lik= 0.44026658 train_kl= 0.00865 train_loss= 0.44892 train_acc= 0.56108 val_roc= 0.92061 val_ap= 0.92137 time= 0.11352\n",
      "训练次数: 9 Epoch: 0075 log_lik= 0.4398025 train_kl= 0.00865 train_loss= 0.44846 train_acc= 0.56133 val_roc= 0.92155 val_ap= 0.92226 time= 0.11650\n",
      "训练次数: 9 Epoch: 0076 log_lik= 0.43937215 train_kl= 0.00865 train_loss= 0.44803 train_acc= 0.56152 val_roc= 0.92184 val_ap= 0.92255 time= 0.11252\n",
      "训练次数: 9 Epoch: 0077 log_lik= 0.43893406 train_kl= 0.00866 train_loss= 0.44759 train_acc= 0.56210 val_roc= 0.92200 val_ap= 0.92309 time= 0.11651\n",
      "训练次数: 9 Epoch: 0078 log_lik= 0.4384884 train_kl= 0.00866 train_loss= 0.44715 train_acc= 0.56197 val_roc= 0.92221 val_ap= 0.92327 time= 0.11252\n",
      "训练次数: 9 Epoch: 0079 log_lik= 0.43819195 train_kl= 0.00866 train_loss= 0.44685 train_acc= 0.56219 val_roc= 0.92203 val_ap= 0.92268 time= 0.11850\n",
      "训练次数: 9 Epoch: 0080 log_lik= 0.43777823 train_kl= 0.00866 train_loss= 0.44644 train_acc= 0.56232 val_roc= 0.92174 val_ap= 0.92172 time= 0.11153\n",
      "训练次数: 9 Epoch: 0081 log_lik= 0.4373304 train_kl= 0.00866 train_loss= 0.44599 train_acc= 0.56222 val_roc= 0.92168 val_ap= 0.92113 time= 0.12049\n",
      "训练次数: 9 Epoch: 0082 log_lik= 0.4369723 train_kl= 0.00866 train_loss= 0.44564 train_acc= 0.56242 val_roc= 0.92167 val_ap= 0.92107 time= 0.11451\n",
      "训练次数: 9 Epoch: 0083 log_lik= 0.43665394 train_kl= 0.00867 train_loss= 0.44532 train_acc= 0.56249 val_roc= 0.92102 val_ap= 0.92041 time= 0.11451\n",
      "训练次数: 9 Epoch: 0084 log_lik= 0.4363236 train_kl= 0.00867 train_loss= 0.44499 train_acc= 0.56226 val_roc= 0.92037 val_ap= 0.91998 time= 0.11451\n",
      "训练次数: 9 Epoch: 0085 log_lik= 0.43599257 train_kl= 0.00867 train_loss= 0.44466 train_acc= 0.56221 val_roc= 0.91992 val_ap= 0.91938 time= 0.11053\n",
      "训练次数: 9 Epoch: 0086 log_lik= 0.43569636 train_kl= 0.00867 train_loss= 0.44437 train_acc= 0.56275 val_roc= 0.91989 val_ap= 0.91919 time= 0.11750\n",
      "训练次数: 9 Epoch: 0087 log_lik= 0.43535298 train_kl= 0.00867 train_loss= 0.44403 train_acc= 0.56238 val_roc= 0.91989 val_ap= 0.91901 time= 0.11650\n",
      "训练次数: 9 Epoch: 0088 log_lik= 0.43507978 train_kl= 0.00867 train_loss= 0.44375 train_acc= 0.56266 val_roc= 0.91986 val_ap= 0.91877 time= 0.11352\n",
      "训练次数: 9 Epoch: 0089 log_lik= 0.43483293 train_kl= 0.00867 train_loss= 0.44351 train_acc= 0.56260 val_roc= 0.91979 val_ap= 0.91864 time= 0.11750\n",
      "训练次数: 9 Epoch: 0090 log_lik= 0.4346467 train_kl= 0.00867 train_loss= 0.44332 train_acc= 0.56259 val_roc= 0.91989 val_ap= 0.91856 time= 0.12049\n",
      "训练次数: 9 Epoch: 0091 log_lik= 0.43431023 train_kl= 0.00868 train_loss= 0.44299 train_acc= 0.56257 val_roc= 0.91978 val_ap= 0.91814 time= 0.11850\n",
      "训练次数: 9 Epoch: 0092 log_lik= 0.4340847 train_kl= 0.00868 train_loss= 0.44276 train_acc= 0.56283 val_roc= 0.91981 val_ap= 0.91786 time= 0.11651\n",
      "训练次数: 9 Epoch: 0093 log_lik= 0.4338247 train_kl= 0.00868 train_loss= 0.44250 train_acc= 0.56292 val_roc= 0.91998 val_ap= 0.91772 time= 0.12148\n",
      "训练次数: 9 Epoch: 0094 log_lik= 0.43357792 train_kl= 0.00868 train_loss= 0.44225 train_acc= 0.56285 val_roc= 0.92044 val_ap= 0.91789 time= 0.12646\n",
      "训练次数: 9 Epoch: 0095 log_lik= 0.43337318 train_kl= 0.00868 train_loss= 0.44205 train_acc= 0.56325 val_roc= 0.92064 val_ap= 0.91795 time= 0.11950\n",
      "训练次数: 9 Epoch: 0096 log_lik= 0.43309334 train_kl= 0.00868 train_loss= 0.44177 train_acc= 0.56343 val_roc= 0.92086 val_ap= 0.91792 time= 0.11152\n",
      "训练次数: 9 Epoch: 0097 log_lik= 0.4329071 train_kl= 0.00868 train_loss= 0.44158 train_acc= 0.56311 val_roc= 0.92122 val_ap= 0.91821 time= 0.11650\n",
      "训练次数: 9 Epoch: 0098 log_lik= 0.43270737 train_kl= 0.00868 train_loss= 0.44138 train_acc= 0.56350 val_roc= 0.92154 val_ap= 0.91845 time= 0.11352\n",
      "训练次数: 9 Epoch: 0099 log_lik= 0.4324531 train_kl= 0.00868 train_loss= 0.44113 train_acc= 0.56360 val_roc= 0.92213 val_ap= 0.91889 time= 0.11551\n",
      "训练次数: 9 Epoch: 0100 log_lik= 0.4321882 train_kl= 0.00868 train_loss= 0.44087 train_acc= 0.56364 val_roc= 0.92258 val_ap= 0.91923 time= 0.11253\n",
      "Optimization Finished!\n",
      "训练次数: 9 ROC score: 0.9140060994710671\n",
      "训练次数: 9 AP score: 0.9173813184433202\n",
      "训练次数: 10 Epoch: 0001 log_lik= 0.778632 train_kl= 0.00806 train_loss= 0.78669 train_acc= 0.02788 val_roc= 0.71263 val_ap= 0.73725 time= 9.27386\n",
      "训练次数: 10 Epoch: 0002 log_lik= 0.87188256 train_kl= 0.00837 train_loss= 0.88025 train_acc= 0.00159 val_roc= 0.73461 val_ap= 0.76672 time= 0.13140\n",
      "训练次数: 10 Epoch: 0003 log_lik= 0.73296165 train_kl= 0.00814 train_loss= 0.74110 train_acc= 0.00378 val_roc= 0.68220 val_ap= 0.72026 time= 0.12148\n",
      "训练次数: 10 Epoch: 0004 log_lik= 0.74587107 train_kl= 0.00821 train_loss= 0.75408 train_acc= 0.00405 val_roc= 0.71016 val_ap= 0.74430 time= 0.12049\n",
      "训练次数: 10 Epoch: 0005 log_lik= 0.7310551 train_kl= 0.00818 train_loss= 0.73924 train_acc= 0.00633 val_roc= 0.76938 val_ap= 0.79687 time= 0.11451\n",
      "训练次数: 10 Epoch: 0006 log_lik= 0.7186073 train_kl= 0.00815 train_loss= 0.72676 train_acc= 0.01974 val_roc= 0.80819 val_ap= 0.82585 time= 0.11750\n",
      "训练次数: 10 Epoch: 0007 log_lik= 0.7049162 train_kl= 0.00816 train_loss= 0.71308 train_acc= 0.05141 val_roc= 0.82712 val_ap= 0.83913 time= 0.11750\n",
      "训练次数: 10 Epoch: 0008 log_lik= 0.68585825 train_kl= 0.00820 train_loss= 0.69406 train_acc= 0.10046 val_roc= 0.84792 val_ap= 0.85699 time= 0.12148\n",
      "训练次数: 10 Epoch: 0009 log_lik= 0.6599989 train_kl= 0.00824 train_loss= 0.66823 train_acc= 0.18277 val_roc= 0.85605 val_ap= 0.86217 time= 0.11453\n",
      "训练次数: 10 Epoch: 0010 log_lik= 0.6342063 train_kl= 0.00828 train_loss= 0.64249 train_acc= 0.27050 val_roc= 0.85092 val_ap= 0.85158 time= 0.11252\n",
      "训练次数: 10 Epoch: 0011 log_lik= 0.62020874 train_kl= 0.00833 train_loss= 0.62854 train_acc= 0.33036 val_roc= 0.85619 val_ap= 0.85724 time= 0.11750\n",
      "训练次数: 10 Epoch: 0012 log_lik= 0.6044989 train_kl= 0.00837 train_loss= 0.61287 train_acc= 0.37951 val_roc= 0.86626 val_ap= 0.87089 time= 0.11650\n",
      "训练次数: 10 Epoch: 0013 log_lik= 0.58881885 train_kl= 0.00840 train_loss= 0.59722 train_acc= 0.41280 val_roc= 0.87065 val_ap= 0.87599 time= 0.11949\n",
      "训练次数: 10 Epoch: 0014 log_lik= 0.5797516 train_kl= 0.00842 train_loss= 0.58817 train_acc= 0.43466 val_roc= 0.87396 val_ap= 0.87946 time= 0.11650\n",
      "训练次数: 10 Epoch: 0015 log_lik= 0.56978893 train_kl= 0.00843 train_loss= 0.57822 train_acc= 0.45353 val_roc= 0.87568 val_ap= 0.88090 time= 0.11551\n",
      "训练次数: 10 Epoch: 0016 log_lik= 0.5588117 train_kl= 0.00843 train_loss= 0.56724 train_acc= 0.47086 val_roc= 0.87619 val_ap= 0.88062 time= 0.11352\n",
      "训练次数: 10 Epoch: 0017 log_lik= 0.5503505 train_kl= 0.00843 train_loss= 0.55878 train_acc= 0.48354 val_roc= 0.87536 val_ap= 0.87949 time= 0.11352\n",
      "训练次数: 10 Epoch: 0018 log_lik= 0.54490095 train_kl= 0.00844 train_loss= 0.55334 train_acc= 0.49243 val_roc= 0.87690 val_ap= 0.88074 time= 0.11550\n",
      "训练次数: 10 Epoch: 0019 log_lik= 0.5398461 train_kl= 0.00844 train_loss= 0.54829 train_acc= 0.49771 val_roc= 0.87970 val_ap= 0.88348 time= 0.11252\n",
      "训练次数: 10 Epoch: 0020 log_lik= 0.5354552 train_kl= 0.00845 train_loss= 0.54391 train_acc= 0.50266 val_roc= 0.88214 val_ap= 0.88631 time= 0.11391\n",
      "训练次数: 10 Epoch: 0021 log_lik= 0.5321898 train_kl= 0.00846 train_loss= 0.54065 train_acc= 0.50527 val_roc= 0.88314 val_ap= 0.88786 time= 0.11352\n",
      "训练次数: 10 Epoch: 0022 log_lik= 0.5293423 train_kl= 0.00846 train_loss= 0.53781 train_acc= 0.50800 val_roc= 0.88433 val_ap= 0.88990 time= 0.11614\n",
      "训练次数: 10 Epoch: 0023 log_lik= 0.5263698 train_kl= 0.00847 train_loss= 0.53484 train_acc= 0.51013 val_roc= 0.88477 val_ap= 0.89068 time= 0.11866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0024 log_lik= 0.52319413 train_kl= 0.00847 train_loss= 0.53167 train_acc= 0.51176 val_roc= 0.88550 val_ap= 0.89136 time= 0.12248\n",
      "训练次数: 10 Epoch: 0025 log_lik= 0.5201341 train_kl= 0.00848 train_loss= 0.52861 train_acc= 0.51356 val_roc= 0.88592 val_ap= 0.89116 time= 0.11558\n",
      "训练次数: 10 Epoch: 0026 log_lik= 0.51765716 train_kl= 0.00848 train_loss= 0.52614 train_acc= 0.51493 val_roc= 0.88726 val_ap= 0.89193 time= 0.11451\n",
      "训练次数: 10 Epoch: 0027 log_lik= 0.5146273 train_kl= 0.00849 train_loss= 0.52311 train_acc= 0.51866 val_roc= 0.88997 val_ap= 0.89449 time= 0.12002\n",
      "训练次数: 10 Epoch: 0028 log_lik= 0.51045173 train_kl= 0.00849 train_loss= 0.51895 train_acc= 0.52248 val_roc= 0.89357 val_ap= 0.89926 time= 0.11849\n",
      "训练次数: 10 Epoch: 0029 log_lik= 0.5068826 train_kl= 0.00851 train_loss= 0.51539 train_acc= 0.52692 val_roc= 0.89630 val_ap= 0.90294 time= 0.11551\n",
      "训练次数: 10 Epoch: 0030 log_lik= 0.5041049 train_kl= 0.00852 train_loss= 0.51262 train_acc= 0.53078 val_roc= 0.89612 val_ap= 0.90311 time= 0.12248\n",
      "训练次数: 10 Epoch: 0031 log_lik= 0.50100636 train_kl= 0.00853 train_loss= 0.50953 train_acc= 0.53604 val_roc= 0.89636 val_ap= 0.90371 time= 0.11949\n",
      "训练次数: 10 Epoch: 0032 log_lik= 0.49701774 train_kl= 0.00853 train_loss= 0.50555 train_acc= 0.54208 val_roc= 0.89630 val_ap= 0.90374 time= 0.11902\n",
      "训练次数: 10 Epoch: 0033 log_lik= 0.4933207 train_kl= 0.00853 train_loss= 0.50186 train_acc= 0.54656 val_roc= 0.89702 val_ap= 0.90494 time= 0.11451\n",
      "训练次数: 10 Epoch: 0034 log_lik= 0.49015614 train_kl= 0.00854 train_loss= 0.49870 train_acc= 0.55003 val_roc= 0.89858 val_ap= 0.90627 time= 0.11751\n",
      "训练次数: 10 Epoch: 0035 log_lik= 0.4874928 train_kl= 0.00855 train_loss= 0.49604 train_acc= 0.55285 val_roc= 0.90007 val_ap= 0.90753 time= 0.11352\n",
      "训练次数: 10 Epoch: 0036 log_lik= 0.4848493 train_kl= 0.00856 train_loss= 0.49341 train_acc= 0.55556 val_roc= 0.90227 val_ap= 0.90977 time= 0.12148\n",
      "训练次数: 10 Epoch: 0037 log_lik= 0.48251164 train_kl= 0.00858 train_loss= 0.49109 train_acc= 0.55913 val_roc= 0.90364 val_ap= 0.91136 time= 0.11451\n",
      "训练次数: 10 Epoch: 0038 log_lik= 0.48041075 train_kl= 0.00859 train_loss= 0.48900 train_acc= 0.56160 val_roc= 0.90423 val_ap= 0.91257 time= 0.11650\n",
      "训练次数: 10 Epoch: 0039 log_lik= 0.4787906 train_kl= 0.00860 train_loss= 0.48739 train_acc= 0.56429 val_roc= 0.90429 val_ap= 0.91302 time= 0.11551\n",
      "训练次数: 10 Epoch: 0040 log_lik= 0.47701475 train_kl= 0.00861 train_loss= 0.48562 train_acc= 0.56738 val_roc= 0.90447 val_ap= 0.91349 time= 0.11850\n",
      "训练次数: 10 Epoch: 0041 log_lik= 0.47534722 train_kl= 0.00861 train_loss= 0.48395 train_acc= 0.56895 val_roc= 0.90471 val_ap= 0.91360 time= 0.11750\n",
      "训练次数: 10 Epoch: 0042 log_lik= 0.4735756 train_kl= 0.00860 train_loss= 0.48218 train_acc= 0.57083 val_roc= 0.90477 val_ap= 0.91301 time= 0.11657\n",
      "训练次数: 10 Epoch: 0043 log_lik= 0.4720104 train_kl= 0.00859 train_loss= 0.48060 train_acc= 0.57276 val_roc= 0.90532 val_ap= 0.91292 time= 0.13096\n",
      "训练次数: 10 Epoch: 0044 log_lik= 0.47059196 train_kl= 0.00859 train_loss= 0.47918 train_acc= 0.57514 val_roc= 0.90532 val_ap= 0.91268 time= 0.12646\n",
      "训练次数: 10 Epoch: 0045 log_lik= 0.46916077 train_kl= 0.00858 train_loss= 0.47774 train_acc= 0.57656 val_roc= 0.90510 val_ap= 0.91226 time= 0.11551\n",
      "训练次数: 10 Epoch: 0046 log_lik= 0.46796426 train_kl= 0.00858 train_loss= 0.47655 train_acc= 0.57804 val_roc= 0.90439 val_ap= 0.91171 time= 0.11850\n",
      "训练次数: 10 Epoch: 0047 log_lik= 0.4668378 train_kl= 0.00859 train_loss= 0.47543 train_acc= 0.57932 val_roc= 0.90419 val_ap= 0.91153 time= 0.12447\n",
      "训练次数: 10 Epoch: 0048 log_lik= 0.46550155 train_kl= 0.00859 train_loss= 0.47410 train_acc= 0.58122 val_roc= 0.90418 val_ap= 0.91115 time= 0.12148\n",
      "训练次数: 10 Epoch: 0049 log_lik= 0.46430716 train_kl= 0.00860 train_loss= 0.47291 train_acc= 0.58204 val_roc= 0.90454 val_ap= 0.91149 time= 0.12447\n",
      "训练次数: 10 Epoch: 0050 log_lik= 0.4632936 train_kl= 0.00861 train_loss= 0.47190 train_acc= 0.58374 val_roc= 0.90481 val_ap= 0.91126 time= 0.11352\n",
      "训练次数: 10 Epoch: 0051 log_lik= 0.4624185 train_kl= 0.00861 train_loss= 0.47103 train_acc= 0.58446 val_roc= 0.90519 val_ap= 0.91148 time= 0.12253\n",
      "训练次数: 10 Epoch: 0052 log_lik= 0.46151173 train_kl= 0.00862 train_loss= 0.47013 train_acc= 0.58534 val_roc= 0.90512 val_ap= 0.91118 time= 0.13343\n",
      "训练次数: 10 Epoch: 0053 log_lik= 0.46061254 train_kl= 0.00862 train_loss= 0.46923 train_acc= 0.58623 val_roc= 0.90509 val_ap= 0.91139 time= 0.12846\n",
      "训练次数: 10 Epoch: 0054 log_lik= 0.4594313 train_kl= 0.00862 train_loss= 0.46805 train_acc= 0.58640 val_roc= 0.90458 val_ap= 0.91110 time= 0.11949\n",
      "训练次数: 10 Epoch: 0055 log_lik= 0.4588764 train_kl= 0.00862 train_loss= 0.46750 train_acc= 0.58688 val_roc= 0.90406 val_ap= 0.91093 time= 0.11352\n",
      "训练次数: 10 Epoch: 0056 log_lik= 0.45800674 train_kl= 0.00862 train_loss= 0.46663 train_acc= 0.58651 val_roc= 0.90467 val_ap= 0.91176 time= 0.12054\n",
      "训练次数: 10 Epoch: 0057 log_lik= 0.4570609 train_kl= 0.00862 train_loss= 0.46568 train_acc= 0.58708 val_roc= 0.90500 val_ap= 0.91194 time= 0.11850\n",
      "训练次数: 10 Epoch: 0058 log_lik= 0.45620394 train_kl= 0.00862 train_loss= 0.46482 train_acc= 0.58752 val_roc= 0.90575 val_ap= 0.91279 time= 0.12148\n",
      "训练次数: 10 Epoch: 0059 log_lik= 0.45550564 train_kl= 0.00862 train_loss= 0.46412 train_acc= 0.58801 val_roc= 0.90617 val_ap= 0.91301 time= 0.11850\n",
      "训练次数: 10 Epoch: 0060 log_lik= 0.45466366 train_kl= 0.00862 train_loss= 0.46328 train_acc= 0.58823 val_roc= 0.90653 val_ap= 0.91359 time= 0.12048\n",
      "训练次数: 10 Epoch: 0061 log_lik= 0.45385683 train_kl= 0.00862 train_loss= 0.46248 train_acc= 0.58797 val_roc= 0.90658 val_ap= 0.91365 time= 0.12248\n",
      "训练次数: 10 Epoch: 0062 log_lik= 0.45305467 train_kl= 0.00863 train_loss= 0.46168 train_acc= 0.58778 val_roc= 0.90681 val_ap= 0.91381 time= 0.11750\n",
      "训练次数: 10 Epoch: 0063 log_lik= 0.45244834 train_kl= 0.00863 train_loss= 0.46108 train_acc= 0.58829 val_roc= 0.90687 val_ap= 0.91377 time= 0.11551\n",
      "训练次数: 10 Epoch: 0064 log_lik= 0.45173132 train_kl= 0.00863 train_loss= 0.46036 train_acc= 0.58830 val_roc= 0.90713 val_ap= 0.91391 time= 0.12547\n",
      "训练次数: 10 Epoch: 0065 log_lik= 0.45105678 train_kl= 0.00863 train_loss= 0.45969 train_acc= 0.58817 val_roc= 0.90697 val_ap= 0.91339 time= 0.12500\n",
      "训练次数: 10 Epoch: 0066 log_lik= 0.4504134 train_kl= 0.00864 train_loss= 0.45905 train_acc= 0.58816 val_roc= 0.90726 val_ap= 0.91376 time= 0.11452\n",
      "训练次数: 10 Epoch: 0067 log_lik= 0.44988164 train_kl= 0.00864 train_loss= 0.45852 train_acc= 0.58834 val_roc= 0.90682 val_ap= 0.91319 time= 0.11371\n",
      "训练次数: 10 Epoch: 0068 log_lik= 0.44925457 train_kl= 0.00864 train_loss= 0.45789 train_acc= 0.58855 val_roc= 0.90650 val_ap= 0.91274 time= 0.11554\n",
      "训练次数: 10 Epoch: 0069 log_lik= 0.4486608 train_kl= 0.00864 train_loss= 0.45730 train_acc= 0.58877 val_roc= 0.90652 val_ap= 0.91267 time= 0.12388\n",
      "训练次数: 10 Epoch: 0070 log_lik= 0.44808587 train_kl= 0.00864 train_loss= 0.45672 train_acc= 0.58923 val_roc= 0.90642 val_ap= 0.91267 time= 0.11352\n",
      "训练次数: 10 Epoch: 0071 log_lik= 0.44750124 train_kl= 0.00864 train_loss= 0.45614 train_acc= 0.58898 val_roc= 0.90671 val_ap= 0.91303 time= 0.12148\n",
      "训练次数: 10 Epoch: 0072 log_lik= 0.4468735 train_kl= 0.00864 train_loss= 0.45551 train_acc= 0.58863 val_roc= 0.90688 val_ap= 0.91327 time= 0.12248\n",
      "训练次数: 10 Epoch: 0073 log_lik= 0.44645372 train_kl= 0.00864 train_loss= 0.45509 train_acc= 0.58901 val_roc= 0.90695 val_ap= 0.91351 time= 0.11750\n",
      "训练次数: 10 Epoch: 0074 log_lik= 0.44586548 train_kl= 0.00864 train_loss= 0.45451 train_acc= 0.58862 val_roc= 0.90653 val_ap= 0.91318 time= 0.12646\n",
      "训练次数: 10 Epoch: 0075 log_lik= 0.44530484 train_kl= 0.00864 train_loss= 0.45395 train_acc= 0.58833 val_roc= 0.90672 val_ap= 0.91370 time= 0.11252\n",
      "训练次数: 10 Epoch: 0076 log_lik= 0.4447505 train_kl= 0.00865 train_loss= 0.45340 train_acc= 0.58835 val_roc= 0.90691 val_ap= 0.91391 time= 0.12248\n",
      "训练次数: 10 Epoch: 0077 log_lik= 0.4442143 train_kl= 0.00865 train_loss= 0.45286 train_acc= 0.58823 val_roc= 0.90692 val_ap= 0.91393 time= 0.12447\n",
      "训练次数: 10 Epoch: 0078 log_lik= 0.4437911 train_kl= 0.00865 train_loss= 0.45244 train_acc= 0.58828 val_roc= 0.90731 val_ap= 0.91442 time= 0.11451\n",
      "训练次数: 10 Epoch: 0079 log_lik= 0.44326603 train_kl= 0.00865 train_loss= 0.45192 train_acc= 0.58832 val_roc= 0.90742 val_ap= 0.91434 time= 0.12248\n",
      "训练次数: 10 Epoch: 0080 log_lik= 0.44279087 train_kl= 0.00865 train_loss= 0.45144 train_acc= 0.58824 val_roc= 0.90765 val_ap= 0.91451 time= 0.11551\n",
      "训练次数: 10 Epoch: 0081 log_lik= 0.44225627 train_kl= 0.00865 train_loss= 0.45091 train_acc= 0.58832 val_roc= 0.90807 val_ap= 0.91501 time= 0.12746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数: 10 Epoch: 0082 log_lik= 0.44170982 train_kl= 0.00865 train_loss= 0.45036 train_acc= 0.58804 val_roc= 0.90863 val_ap= 0.91538 time= 0.12348\n",
      "训练次数: 10 Epoch: 0083 log_lik= 0.44111916 train_kl= 0.00865 train_loss= 0.44977 train_acc= 0.58839 val_roc= 0.90882 val_ap= 0.91549 time= 0.11850\n",
      "训练次数: 10 Epoch: 0084 log_lik= 0.44070727 train_kl= 0.00865 train_loss= 0.44936 train_acc= 0.58820 val_roc= 0.90919 val_ap= 0.91608 time= 0.11053\n",
      "训练次数: 10 Epoch: 0085 log_lik= 0.44010544 train_kl= 0.00866 train_loss= 0.44876 train_acc= 0.58824 val_roc= 0.90964 val_ap= 0.91649 time= 0.11451\n",
      "训练次数: 10 Epoch: 0086 log_lik= 0.43967724 train_kl= 0.00866 train_loss= 0.44834 train_acc= 0.58831 val_roc= 0.91003 val_ap= 0.91669 time= 0.11551\n",
      "训练次数: 10 Epoch: 0087 log_lik= 0.43916297 train_kl= 0.00866 train_loss= 0.44782 train_acc= 0.58796 val_roc= 0.90984 val_ap= 0.91640 time= 0.11451\n",
      "训练次数: 10 Epoch: 0088 log_lik= 0.4387665 train_kl= 0.00866 train_loss= 0.44743 train_acc= 0.58817 val_roc= 0.90984 val_ap= 0.91641 time= 0.12347\n",
      "训练次数: 10 Epoch: 0089 log_lik= 0.43824923 train_kl= 0.00866 train_loss= 0.44691 train_acc= 0.58819 val_roc= 0.91021 val_ap= 0.91677 time= 0.12348\n",
      "训练次数: 10 Epoch: 0090 log_lik= 0.43782488 train_kl= 0.00867 train_loss= 0.44649 train_acc= 0.58831 val_roc= 0.91037 val_ap= 0.91697 time= 0.11650\n",
      "训练次数: 10 Epoch: 0091 log_lik= 0.43745908 train_kl= 0.00867 train_loss= 0.44613 train_acc= 0.58856 val_roc= 0.91068 val_ap= 0.91740 time= 0.11750\n",
      "训练次数: 10 Epoch: 0092 log_lik= 0.43695673 train_kl= 0.00867 train_loss= 0.44563 train_acc= 0.58816 val_roc= 0.91070 val_ap= 0.91738 time= 0.12049\n",
      "训练次数: 10 Epoch: 0093 log_lik= 0.4365651 train_kl= 0.00867 train_loss= 0.44524 train_acc= 0.58816 val_roc= 0.91087 val_ap= 0.91752 time= 0.13144\n",
      "训练次数: 10 Epoch: 0094 log_lik= 0.43614826 train_kl= 0.00867 train_loss= 0.44482 train_acc= 0.58811 val_roc= 0.91081 val_ap= 0.91742 time= 0.11252\n",
      "训练次数: 10 Epoch: 0095 log_lik= 0.43580174 train_kl= 0.00867 train_loss= 0.44448 train_acc= 0.58838 val_roc= 0.91109 val_ap= 0.91773 time= 0.11153\n",
      "训练次数: 10 Epoch: 0096 log_lik= 0.43538535 train_kl= 0.00868 train_loss= 0.44406 train_acc= 0.58794 val_roc= 0.91113 val_ap= 0.91788 time= 0.11451\n",
      "训练次数: 10 Epoch: 0097 log_lik= 0.4350318 train_kl= 0.00868 train_loss= 0.44371 train_acc= 0.58823 val_roc= 0.91106 val_ap= 0.91783 time= 0.12547\n",
      "训练次数: 10 Epoch: 0098 log_lik= 0.4347182 train_kl= 0.00868 train_loss= 0.44340 train_acc= 0.58833 val_roc= 0.91122 val_ap= 0.91807 time= 0.12347\n",
      "训练次数: 10 Epoch: 0099 log_lik= 0.4343403 train_kl= 0.00868 train_loss= 0.44302 train_acc= 0.58858 val_roc= 0.91154 val_ap= 0.91821 time= 0.11750\n",
      "训练次数: 10 Epoch: 0100 log_lik= 0.43391162 train_kl= 0.00868 train_loss= 0.44259 train_acc= 0.58858 val_roc= 0.91168 val_ap= 0.91840 time= 0.11352\n",
      "Optimization Finished!\n",
      "训练次数: 10 ROC score: 0.9303745737751549\n",
      "训练次数: 10 AP score: 0.937273027610084\n",
      "Average Test ROC score: 0.9139420082166427\n",
      "Average Test AP score: 0.9212388046273375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 自动训练10次 CMVAE结果\n",
    "# 1. KL散度，预设 Ex = 0, En=1.06, He = 0.105\n",
    "# 2. 取消1000次采样.\n",
    "%run train_debug.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gae] *",
   "language": "python",
   "name": "conda-env-gae-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
