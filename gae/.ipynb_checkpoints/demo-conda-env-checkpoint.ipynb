{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c81566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCM MODEL VAE\n",
    "# 测试修改 lost函数 与 隐变量 z\n",
    "# 将z_log_std -> z_std，去掉log\n",
    "self.z_std = GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                    output_dim=FLAGS.hidden2,\n",
    "                                    adj=self.adj,\n",
    "                                    act=lambda x: x,\n",
    "                                    dropout=self.dropout,\n",
    "                                    logging=self.logging)(self.hidden1)\n",
    "\n",
    "self.z = self.z_mean + tf.random_normal([self.n_samples, FLAGS.hidden2]) * self.z_std\n",
    "\n",
    "# Latent loss\n",
    "self.kl = (0.5 / num_nodes) * tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                1 + 2 * tf.log(model.z_std) - tf.square(model.z_mean) -\n",
    "                tf.square(model.z_std), 1\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcefc342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:103: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:106: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\initializations.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:42: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:73: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 1.68414 train_acc= 0.49525 val_roc= 0.69668 val_ap= 0.72832 time= 1.65992\n",
      "Epoch: 0002 train_loss= 1.31838 train_acc= 0.44386 val_roc= 0.68930 val_ap= 0.72269 time= 0.08956\n",
      "Epoch: 0003 train_loss= 1.14245 train_acc= 0.35351 val_roc= 0.69011 val_ap= 0.72599 time= 0.08065\n",
      "Epoch: 0004 train_loss= 0.99360 train_acc= 0.31506 val_roc= 0.69744 val_ap= 0.73087 time= 0.08164\n",
      "Epoch: 0005 train_loss= 0.87664 train_acc= 0.29703 val_roc= 0.71403 val_ap= 0.74224 time= 0.08861\n",
      "Epoch: 0006 train_loss= 0.80157 train_acc= 0.30300 val_roc= 0.74295 val_ap= 0.76665 time= 0.07965\n",
      "Epoch: 0007 train_loss= 0.74114 train_acc= 0.32439 val_roc= 0.76864 val_ap= 0.78745 time= 0.08064\n",
      "Epoch: 0008 train_loss= 0.71858 train_acc= 0.35629 val_roc= 0.76005 val_ap= 0.77585 time= 0.08264\n",
      "Epoch: 0009 train_loss= 0.70947 train_acc= 0.33176 val_roc= 0.74528 val_ap= 0.76373 time= 0.08072\n",
      "Epoch: 0010 train_loss= 0.70267 train_acc= 0.21520 val_roc= 0.76106 val_ap= 0.77623 time= 0.08264\n",
      "Epoch: 0011 train_loss= 0.70076 train_acc= 0.14903 val_roc= 0.80989 val_ap= 0.82124 time= 0.09757\n",
      "Epoch: 0012 train_loss= 0.68632 train_acc= 0.14641 val_roc= 0.83834 val_ap= 0.84581 time= 0.07865\n",
      "Epoch: 0013 train_loss= 0.67146 train_acc= 0.16918 val_roc= 0.83988 val_ap= 0.84207 time= 0.08064\n",
      "Epoch: 0014 train_loss= 0.66270 train_acc= 0.19866 val_roc= 0.84301 val_ap= 0.84485 time= 0.08363\n",
      "Epoch: 0015 train_loss= 0.64677 train_acc= 0.24333 val_roc= 0.84622 val_ap= 0.84954 time= 0.08164\n",
      "Epoch: 0016 train_loss= 0.62620 train_acc= 0.29851 val_roc= 0.84463 val_ap= 0.84789 time= 0.08164\n",
      "Epoch: 0017 train_loss= 0.60741 train_acc= 0.35183 val_roc= 0.84664 val_ap= 0.85142 time= 0.07965\n",
      "Epoch: 0018 train_loss= 0.58795 train_acc= 0.40225 val_roc= 0.85115 val_ap= 0.85681 time= 0.07965\n",
      "Epoch: 0019 train_loss= 0.57065 train_acc= 0.44320 val_roc= 0.85333 val_ap= 0.85851 time= 0.08164\n",
      "Epoch: 0020 train_loss= 0.56377 train_acc= 0.46544 val_roc= 0.85593 val_ap= 0.86155 time= 0.08065\n",
      "Epoch: 0021 train_loss= 0.56049 train_acc= 0.46978 val_roc= 0.85692 val_ap= 0.86187 time= 0.07964\n",
      "Epoch: 0022 train_loss= 0.56147 train_acc= 0.47219 val_roc= 0.85816 val_ap= 0.86124 time= 0.08761\n",
      "Epoch: 0023 train_loss= 0.56263 train_acc= 0.47451 val_roc= 0.86487 val_ap= 0.86799 time= 0.08093\n",
      "Epoch: 0024 train_loss= 0.55319 train_acc= 0.48487 val_roc= 0.87082 val_ap= 0.87408 time= 0.08064\n",
      "Epoch: 0025 train_loss= 0.54200 train_acc= 0.49366 val_roc= 0.87656 val_ap= 0.87914 time= 0.07965\n",
      "Epoch: 0026 train_loss= 0.53464 train_acc= 0.49614 val_roc= 0.87982 val_ap= 0.88211 time= 0.08064\n",
      "Epoch: 0027 train_loss= 0.52629 train_acc= 0.49941 val_roc= 0.88310 val_ap= 0.88580 time= 0.08065\n",
      "Epoch: 0028 train_loss= 0.52013 train_acc= 0.50010 val_roc= 0.88519 val_ap= 0.88753 time= 0.07965\n",
      "Epoch: 0029 train_loss= 0.51719 train_acc= 0.49898 val_roc= 0.88787 val_ap= 0.89018 time= 0.08064\n",
      "Epoch: 0030 train_loss= 0.51412 train_acc= 0.49774 val_roc= 0.89053 val_ap= 0.89235 time= 0.08761\n",
      "Epoch: 0031 train_loss= 0.51135 train_acc= 0.49982 val_roc= 0.89284 val_ap= 0.89358 time= 0.08164\n",
      "Epoch: 0032 train_loss= 0.50638 train_acc= 0.50700 val_roc= 0.89430 val_ap= 0.89429 time= 0.08463\n",
      "Epoch: 0033 train_loss= 0.50299 train_acc= 0.51203 val_roc= 0.89610 val_ap= 0.89554 time= 0.08363\n",
      "Epoch: 0034 train_loss= 0.50035 train_acc= 0.51319 val_roc= 0.89770 val_ap= 0.89703 time= 0.08064\n",
      "Epoch: 0035 train_loss= 0.49842 train_acc= 0.51096 val_roc= 0.89923 val_ap= 0.89832 time= 0.08264\n",
      "Epoch: 0036 train_loss= 0.49723 train_acc= 0.50912 val_roc= 0.90007 val_ap= 0.89927 time= 0.08064\n",
      "Epoch: 0037 train_loss= 0.49535 train_acc= 0.50885 val_roc= 0.90124 val_ap= 0.90060 time= 0.08264\n",
      "Epoch: 0038 train_loss= 0.49422 train_acc= 0.50968 val_roc= 0.90354 val_ap= 0.90260 time= 0.08363\n",
      "Epoch: 0039 train_loss= 0.49105 train_acc= 0.51548 val_roc= 0.90554 val_ap= 0.90411 time= 0.08264\n",
      "Epoch: 0040 train_loss= 0.48777 train_acc= 0.51698 val_roc= 0.90794 val_ap= 0.90631 time= 0.08264\n",
      "Epoch: 0041 train_loss= 0.48509 train_acc= 0.51922 val_roc= 0.90961 val_ap= 0.90831 time= 0.08762\n",
      "Epoch: 0042 train_loss= 0.48363 train_acc= 0.52120 val_roc= 0.91032 val_ap= 0.90937 time= 0.07965\n",
      "Epoch: 0043 train_loss= 0.48296 train_acc= 0.52236 val_roc= 0.91156 val_ap= 0.91112 time= 0.07965\n",
      "Epoch: 0044 train_loss= 0.48101 train_acc= 0.52324 val_roc= 0.91243 val_ap= 0.91225 time= 0.08064\n",
      "Epoch: 0045 train_loss= 0.47970 train_acc= 0.52373 val_roc= 0.91427 val_ap= 0.91426 time= 0.08064\n",
      "Epoch: 0046 train_loss= 0.47744 train_acc= 0.52493 val_roc= 0.91553 val_ap= 0.91559 time= 0.08164\n",
      "Epoch: 0047 train_loss= 0.47665 train_acc= 0.52470 val_roc= 0.91631 val_ap= 0.91611 time= 0.08115\n",
      "Epoch: 0048 train_loss= 0.47546 train_acc= 0.52719 val_roc= 0.91644 val_ap= 0.91612 time= 0.08164\n",
      "Epoch: 0049 train_loss= 0.47404 train_acc= 0.52741 val_roc= 0.91651 val_ap= 0.91643 time= 0.08164\n",
      "Epoch: 0050 train_loss= 0.47380 train_acc= 0.52833 val_roc= 0.91664 val_ap= 0.91673 time= 0.09409\n",
      "Epoch: 0051 train_loss= 0.47342 train_acc= 0.52853 val_roc= 0.91712 val_ap= 0.91733 time= 0.08662\n",
      "Epoch: 0052 train_loss= 0.47321 train_acc= 0.52696 val_roc= 0.91784 val_ap= 0.91786 time= 0.08761\n",
      "Epoch: 0053 train_loss= 0.47197 train_acc= 0.52889 val_roc= 0.91850 val_ap= 0.91857 time= 0.08064\n",
      "Epoch: 0054 train_loss= 0.47085 train_acc= 0.52997 val_roc= 0.91901 val_ap= 0.91901 time= 0.08064\n",
      "Epoch: 0055 train_loss= 0.47048 train_acc= 0.52988 val_roc= 0.91967 val_ap= 0.91965 time= 0.08264\n",
      "Epoch: 0056 train_loss= 0.47006 train_acc= 0.53121 val_roc= 0.91957 val_ap= 0.91987 time= 0.08363\n",
      "Epoch: 0057 train_loss= 0.46866 train_acc= 0.53205 val_roc= 0.91983 val_ap= 0.92026 time= 0.08164\n",
      "Epoch: 0058 train_loss= 0.46805 train_acc= 0.53077 val_roc= 0.91998 val_ap= 0.92067 time= 0.08188\n",
      "Epoch: 0059 train_loss= 0.46731 train_acc= 0.53147 val_roc= 0.92057 val_ap= 0.92138 time= 0.08064\n",
      "Epoch: 0060 train_loss= 0.46626 train_acc= 0.53245 val_roc= 0.92132 val_ap= 0.92201 time= 0.08164\n",
      "Epoch: 0061 train_loss= 0.46541 train_acc= 0.53322 val_roc= 0.92245 val_ap= 0.92274 time= 0.08264\n",
      "Epoch: 0062 train_loss= 0.46456 train_acc= 0.53314 val_roc= 0.92290 val_ap= 0.92336 time= 0.08463\n",
      "Epoch: 0063 train_loss= 0.46386 train_acc= 0.53354 val_roc= 0.92309 val_ap= 0.92388 time= 0.08761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0064 train_loss= 0.46302 train_acc= 0.53435 val_roc= 0.92255 val_ap= 0.92340 time= 0.08463\n",
      "Epoch: 0065 train_loss= 0.46266 train_acc= 0.53455 val_roc= 0.92313 val_ap= 0.92401 time= 0.08264\n",
      "Epoch: 0066 train_loss= 0.46236 train_acc= 0.53447 val_roc= 0.92368 val_ap= 0.92431 time= 0.08164\n",
      "Epoch: 0067 train_loss= 0.46098 train_acc= 0.53565 val_roc= 0.92372 val_ap= 0.92411 time= 0.08363\n",
      "Epoch: 0068 train_loss= 0.46109 train_acc= 0.53454 val_roc= 0.92387 val_ap= 0.92378 time= 0.08164\n",
      "Epoch: 0069 train_loss= 0.46073 train_acc= 0.53479 val_roc= 0.92442 val_ap= 0.92412 time= 0.08164\n",
      "Epoch: 0070 train_loss= 0.45998 train_acc= 0.53595 val_roc= 0.92505 val_ap= 0.92475 time= 0.08264\n",
      "Epoch: 0071 train_loss= 0.45945 train_acc= 0.53618 val_roc= 0.92578 val_ap= 0.92551 time= 0.08264\n",
      "Epoch: 0072 train_loss= 0.45883 train_acc= 0.53663 val_roc= 0.92602 val_ap= 0.92567 time= 0.08064\n",
      "Epoch: 0073 train_loss= 0.45856 train_acc= 0.53588 val_roc= 0.92576 val_ap= 0.92551 time= 0.08463\n",
      "Epoch: 0074 train_loss= 0.45813 train_acc= 0.53726 val_roc= 0.92537 val_ap= 0.92503 time= 0.08264\n",
      "Epoch: 0075 train_loss= 0.45780 train_acc= 0.53811 val_roc= 0.92569 val_ap= 0.92498 time= 0.08022\n",
      "Epoch: 0076 train_loss= 0.45705 train_acc= 0.53783 val_roc= 0.92685 val_ap= 0.92594 time= 0.08065\n",
      "Epoch: 0077 train_loss= 0.45680 train_acc= 0.53816 val_roc= 0.92771 val_ap= 0.92653 time= 0.08363\n",
      "Epoch: 0078 train_loss= 0.45647 train_acc= 0.53879 val_roc= 0.92802 val_ap= 0.92677 time= 0.08263\n",
      "Epoch: 0079 train_loss= 0.45594 train_acc= 0.53786 val_roc= 0.92779 val_ap= 0.92668 time= 0.08463\n",
      "Epoch: 0080 train_loss= 0.45593 train_acc= 0.53818 val_roc= 0.92766 val_ap= 0.92683 time= 0.08363\n",
      "Epoch: 0081 train_loss= 0.45520 train_acc= 0.53912 val_roc= 0.92763 val_ap= 0.92680 time= 0.07965\n",
      "Epoch: 0082 train_loss= 0.45547 train_acc= 0.53897 val_roc= 0.92904 val_ap= 0.92768 time= 0.08064\n",
      "Epoch: 0083 train_loss= 0.45440 train_acc= 0.53989 val_roc= 0.92958 val_ap= 0.92792 time= 0.08164\n",
      "Epoch: 0084 train_loss= 0.45431 train_acc= 0.53962 val_roc= 0.92994 val_ap= 0.92850 time= 0.08121\n",
      "Epoch: 0085 train_loss= 0.45391 train_acc= 0.53870 val_roc= 0.92930 val_ap= 0.92824 time= 0.08164\n",
      "Epoch: 0086 train_loss= 0.45385 train_acc= 0.54004 val_roc= 0.92910 val_ap= 0.92819 time= 0.08165\n",
      "Epoch: 0087 train_loss= 0.45318 train_acc= 0.53851 val_roc= 0.92930 val_ap= 0.92813 time= 0.08065\n",
      "Epoch: 0088 train_loss= 0.45349 train_acc= 0.53983 val_roc= 0.92977 val_ap= 0.92836 time= 0.08064\n",
      "Epoch: 0089 train_loss= 0.45282 train_acc= 0.53955 val_roc= 0.93003 val_ap= 0.92866 time= 0.08164\n",
      "Epoch: 0090 train_loss= 0.45294 train_acc= 0.53957 val_roc= 0.93029 val_ap= 0.92897 time= 0.09657\n",
      "Epoch: 0091 train_loss= 0.45223 train_acc= 0.53951 val_roc= 0.92990 val_ap= 0.92903 time= 0.09083\n",
      "Epoch: 0092 train_loss= 0.45190 train_acc= 0.53893 val_roc= 0.92943 val_ap= 0.92876 time= 0.07965\n",
      "Epoch: 0093 train_loss= 0.45158 train_acc= 0.53993 val_roc= 0.92964 val_ap= 0.92883 time= 0.07865\n",
      "Epoch: 0094 train_loss= 0.45166 train_acc= 0.53978 val_roc= 0.93003 val_ap= 0.92927 time= 0.08363\n",
      "Epoch: 0095 train_loss= 0.45103 train_acc= 0.53993 val_roc= 0.93023 val_ap= 0.92908 time= 0.08363\n",
      "Epoch: 0096 train_loss= 0.45128 train_acc= 0.53984 val_roc= 0.93030 val_ap= 0.92941 time= 0.08064\n",
      "Epoch: 0097 train_loss= 0.45098 train_acc= 0.54001 val_roc= 0.92987 val_ap= 0.92924 time= 0.08064\n",
      "Epoch: 0098 train_loss= 0.45057 train_acc= 0.54017 val_roc= 0.92923 val_ap= 0.92900 time= 0.08164\n",
      "Epoch: 0099 train_loss= 0.45027 train_acc= 0.53989 val_roc= 0.92890 val_ap= 0.92874 time= 0.08263\n",
      "Epoch: 0100 train_loss= 0.45023 train_acc= 0.53958 val_roc= 0.93006 val_ap= 0.92951 time= 0.08164\n",
      "Epoch: 0101 train_loss= 0.44947 train_acc= 0.53971 val_roc= 0.93052 val_ap= 0.92988 time= 0.08463\n",
      "Epoch: 0102 train_loss= 0.44930 train_acc= 0.54001 val_roc= 0.93092 val_ap= 0.93048 time= 0.08164\n",
      "Epoch: 0103 train_loss= 0.44955 train_acc= 0.53993 val_roc= 0.93036 val_ap= 0.93015 time= 0.08264\n",
      "Epoch: 0104 train_loss= 0.44895 train_acc= 0.53980 val_roc= 0.93017 val_ap= 0.93023 time= 0.07965\n",
      "Epoch: 0105 train_loss= 0.44873 train_acc= 0.54044 val_roc= 0.93007 val_ap= 0.93019 time= 0.08132\n",
      "Epoch: 0106 train_loss= 0.44885 train_acc= 0.53919 val_roc= 0.93073 val_ap= 0.93087 time= 0.08116\n",
      "Epoch: 0107 train_loss= 0.44846 train_acc= 0.54083 val_roc= 0.93123 val_ap= 0.93110 time= 0.08164\n",
      "Epoch: 0108 train_loss= 0.44849 train_acc= 0.54029 val_roc= 0.93117 val_ap= 0.93111 time= 0.08167\n",
      "Epoch: 0109 train_loss= 0.44797 train_acc= 0.54159 val_roc= 0.93068 val_ap= 0.93099 time= 0.08463\n",
      "Epoch: 0110 train_loss= 0.44749 train_acc= 0.54028 val_roc= 0.93049 val_ap= 0.93079 time= 0.08761\n",
      "Epoch: 0111 train_loss= 0.44723 train_acc= 0.54059 val_roc= 0.93091 val_ap= 0.93130 time= 0.08164\n",
      "Epoch: 0112 train_loss= 0.44698 train_acc= 0.54120 val_roc= 0.93144 val_ap= 0.93191 time= 0.08064\n",
      "Epoch: 0113 train_loss= 0.44732 train_acc= 0.54093 val_roc= 0.93167 val_ap= 0.93218 time= 0.08064\n",
      "Epoch: 0114 train_loss= 0.44705 train_acc= 0.54106 val_roc= 0.93126 val_ap= 0.93172 time= 0.08065\n",
      "Epoch: 0115 train_loss= 0.44629 train_acc= 0.54136 val_roc= 0.93053 val_ap= 0.93115 time= 0.08264\n",
      "Epoch: 0116 train_loss= 0.44658 train_acc= 0.54067 val_roc= 0.93060 val_ap= 0.93139 time= 0.08164\n",
      "Epoch: 0117 train_loss= 0.44622 train_acc= 0.54064 val_roc= 0.93102 val_ap= 0.93184 time= 0.08064\n",
      "Epoch: 0118 train_loss= 0.44595 train_acc= 0.54188 val_roc= 0.93136 val_ap= 0.93224 time= 0.08463\n",
      "Epoch: 0119 train_loss= 0.44577 train_acc= 0.54162 val_roc= 0.93110 val_ap= 0.93187 time= 0.08065\n",
      "Epoch: 0120 train_loss= 0.44605 train_acc= 0.54131 val_roc= 0.93075 val_ap= 0.93167 time= 0.08064\n",
      "Epoch: 0121 train_loss= 0.44540 train_acc= 0.54131 val_roc= 0.93084 val_ap= 0.93184 time= 0.08164\n",
      "Epoch: 0122 train_loss= 0.44602 train_acc= 0.54130 val_roc= 0.93069 val_ap= 0.93181 time= 0.08064\n",
      "Epoch: 0123 train_loss= 0.44529 train_acc= 0.54028 val_roc= 0.93032 val_ap= 0.93144 time= 0.07965\n",
      "Epoch: 0124 train_loss= 0.44499 train_acc= 0.54152 val_roc= 0.93006 val_ap= 0.93126 time= 0.08371\n",
      "Epoch: 0125 train_loss= 0.44493 train_acc= 0.54089 val_roc= 0.93004 val_ap= 0.93164 time= 0.08229\n",
      "Epoch: 0126 train_loss= 0.44464 train_acc= 0.54069 val_roc= 0.93053 val_ap= 0.93218 time= 0.08364\n",
      "Epoch: 0127 train_loss= 0.44447 train_acc= 0.54208 val_roc= 0.93058 val_ap= 0.93232 time= 0.08065\n",
      "Epoch: 0128 train_loss= 0.44444 train_acc= 0.54107 val_roc= 0.93042 val_ap= 0.93220 time= 0.08264\n",
      "Epoch: 0129 train_loss= 0.44410 train_acc= 0.54114 val_roc= 0.92967 val_ap= 0.93168 time= 0.08263\n",
      "Epoch: 0130 train_loss= 0.44438 train_acc= 0.54171 val_roc= 0.92964 val_ap= 0.93191 time= 0.08164\n",
      "Epoch: 0131 train_loss= 0.44390 train_acc= 0.54196 val_roc= 0.92971 val_ap= 0.93197 time= 0.08164\n",
      "Epoch: 0132 train_loss= 0.44350 train_acc= 0.54187 val_roc= 0.93000 val_ap= 0.93236 time= 0.08164\n",
      "Epoch: 0133 train_loss= 0.44363 train_acc= 0.54241 val_roc= 0.92987 val_ap= 0.93222 time= 0.08961\n",
      "Epoch: 0134 train_loss= 0.44373 train_acc= 0.54140 val_roc= 0.92929 val_ap= 0.93185 time= 0.08264\n",
      "Epoch: 0135 train_loss= 0.44340 train_acc= 0.54174 val_roc= 0.92899 val_ap= 0.93175 time= 0.08164\n",
      "Epoch: 0136 train_loss= 0.44301 train_acc= 0.54212 val_roc= 0.92940 val_ap= 0.93212 time= 0.08064\n",
      "Epoch: 0137 train_loss= 0.44336 train_acc= 0.54252 val_roc= 0.92968 val_ap= 0.93245 time= 0.08164\n",
      "Epoch: 0138 train_loss= 0.44309 train_acc= 0.54243 val_roc= 0.92927 val_ap= 0.93204 time= 0.08263\n",
      "Epoch: 0139 train_loss= 0.44280 train_acc= 0.54171 val_roc= 0.92932 val_ap= 0.93219 time= 0.08164\n",
      "Epoch: 0140 train_loss= 0.44288 train_acc= 0.54148 val_roc= 0.92913 val_ap= 0.93223 time= 0.08173\n",
      "Epoch: 0141 train_loss= 0.44242 train_acc= 0.54261 val_roc= 0.92901 val_ap= 0.93228 time= 0.08662\n",
      "Epoch: 0142 train_loss= 0.44221 train_acc= 0.54254 val_roc= 0.92912 val_ap= 0.93219 time= 0.08463\n",
      "Epoch: 0143 train_loss= 0.44243 train_acc= 0.54236 val_roc= 0.92938 val_ap= 0.93237 time= 0.08164\n",
      "Epoch: 0144 train_loss= 0.44204 train_acc= 0.54228 val_roc= 0.92926 val_ap= 0.93238 time= 0.08320\n",
      "Epoch: 0145 train_loss= 0.44227 train_acc= 0.54197 val_roc= 0.92873 val_ap= 0.93201 time= 0.08264\n",
      "Epoch: 0146 train_loss= 0.44212 train_acc= 0.54195 val_roc= 0.92851 val_ap= 0.93220 time= 0.08064\n",
      "Epoch: 0147 train_loss= 0.44213 train_acc= 0.54164 val_roc= 0.92884 val_ap= 0.93239 time= 0.08164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0148 train_loss= 0.44210 train_acc= 0.54208 val_roc= 0.92910 val_ap= 0.93262 time= 0.08164\n",
      "Epoch: 0149 train_loss= 0.44205 train_acc= 0.54277 val_roc= 0.92880 val_ap= 0.93253 time= 0.07965\n",
      "Epoch: 0150 train_loss= 0.44168 train_acc= 0.54264 val_roc= 0.92820 val_ap= 0.93220 time= 0.08861\n",
      "Epoch: 0151 train_loss= 0.44145 train_acc= 0.54319 val_roc= 0.92809 val_ap= 0.93221 time= 0.08065\n",
      "Epoch: 0152 train_loss= 0.44121 train_acc= 0.54261 val_roc= 0.92815 val_ap= 0.93199 time= 0.08064\n",
      "Epoch: 0153 train_loss= 0.44148 train_acc= 0.54272 val_roc= 0.92839 val_ap= 0.93236 time= 0.08164\n",
      "Epoch: 0154 train_loss= 0.44153 train_acc= 0.54301 val_roc= 0.92823 val_ap= 0.93248 time= 0.08264\n",
      "Epoch: 0155 train_loss= 0.44118 train_acc= 0.54316 val_roc= 0.92828 val_ap= 0.93245 time= 0.07965\n",
      "Epoch: 0156 train_loss= 0.44107 train_acc= 0.54277 val_roc= 0.92806 val_ap= 0.93224 time= 0.07965\n",
      "Epoch: 0157 train_loss= 0.44091 train_acc= 0.54238 val_roc= 0.92783 val_ap= 0.93192 time= 0.08363\n",
      "Epoch: 0158 train_loss= 0.44093 train_acc= 0.54302 val_roc= 0.92787 val_ap= 0.93207 time= 0.08264\n",
      "Epoch: 0159 train_loss= 0.44100 train_acc= 0.54358 val_roc= 0.92790 val_ap= 0.93218 time= 0.08064\n",
      "Epoch: 0160 train_loss= 0.44062 train_acc= 0.54346 val_roc= 0.92786 val_ap= 0.93223 time= 0.08264\n",
      "Epoch: 0161 train_loss= 0.44067 train_acc= 0.54383 val_roc= 0.92754 val_ap= 0.93159 time= 0.08363\n",
      "Epoch: 0162 train_loss= 0.44070 train_acc= 0.54264 val_roc= 0.92745 val_ap= 0.93177 time= 0.07965\n",
      "Epoch: 0163 train_loss= 0.44086 train_acc= 0.54253 val_roc= 0.92732 val_ap= 0.93182 time= 0.08164\n",
      "Epoch: 0164 train_loss= 0.44048 train_acc= 0.54296 val_roc= 0.92721 val_ap= 0.93177 time= 0.08164\n",
      "Epoch: 0165 train_loss= 0.44052 train_acc= 0.54312 val_roc= 0.92761 val_ap= 0.93197 time= 0.08319\n",
      "Epoch: 0166 train_loss= 0.44001 train_acc= 0.54273 val_roc= 0.92712 val_ap= 0.93136 time= 0.08064\n",
      "Epoch: 0167 train_loss= 0.44002 train_acc= 0.54410 val_roc= 0.92696 val_ap= 0.93102 time= 0.08064\n",
      "Epoch: 0168 train_loss= 0.43984 train_acc= 0.54231 val_roc= 0.92663 val_ap= 0.93123 time= 0.08363\n",
      "Epoch: 0169 train_loss= 0.43995 train_acc= 0.54400 val_roc= 0.92686 val_ap= 0.93164 time= 0.08363\n",
      "Epoch: 0170 train_loss= 0.43987 train_acc= 0.54422 val_roc= 0.92727 val_ap= 0.93177 time= 0.08064\n",
      "Epoch: 0171 train_loss= 0.43982 train_acc= 0.54467 val_roc= 0.92744 val_ap= 0.93168 time= 0.08164\n",
      "Epoch: 0172 train_loss= 0.43988 train_acc= 0.54349 val_roc= 0.92716 val_ap= 0.93171 time= 0.07965\n",
      "Epoch: 0173 train_loss= 0.43941 train_acc= 0.54328 val_roc= 0.92686 val_ap= 0.93162 time= 0.07866\n",
      "Epoch: 0174 train_loss= 0.43943 train_acc= 0.54325 val_roc= 0.92712 val_ap= 0.93213 time= 0.08115\n",
      "Epoch: 0175 train_loss= 0.43906 train_acc= 0.54371 val_roc= 0.92706 val_ap= 0.93169 time= 0.08463\n",
      "Epoch: 0176 train_loss= 0.43923 train_acc= 0.54321 val_roc= 0.92731 val_ap= 0.93200 time= 0.08264\n",
      "Epoch: 0177 train_loss= 0.43908 train_acc= 0.54301 val_roc= 0.92706 val_ap= 0.93215 time= 0.08761\n",
      "Epoch: 0178 train_loss= 0.43907 train_acc= 0.54400 val_roc= 0.92693 val_ap= 0.93209 time= 0.08164\n",
      "Epoch: 0179 train_loss= 0.43843 train_acc= 0.54413 val_roc= 0.92695 val_ap= 0.93215 time= 0.08164\n",
      "Epoch: 0180 train_loss= 0.43843 train_acc= 0.54412 val_roc= 0.92705 val_ap= 0.93214 time= 0.08363\n",
      "Epoch: 0181 train_loss= 0.43834 train_acc= 0.54395 val_roc= 0.92638 val_ap= 0.93197 time= 0.08064\n",
      "Epoch: 0182 train_loss= 0.43835 train_acc= 0.54410 val_roc= 0.92609 val_ap= 0.93177 time= 0.08164\n",
      "Epoch: 0183 train_loss= 0.43814 train_acc= 0.54476 val_roc= 0.92621 val_ap= 0.93174 time= 0.08514\n",
      "Epoch: 0184 train_loss= 0.43809 train_acc= 0.54457 val_roc= 0.92648 val_ap= 0.93189 time= 0.08263\n",
      "Epoch: 0185 train_loss= 0.43777 train_acc= 0.54521 val_roc= 0.92598 val_ap= 0.93119 time= 0.07965\n",
      "Epoch: 0186 train_loss= 0.43786 train_acc= 0.54495 val_roc= 0.92598 val_ap= 0.93132 time= 0.08274\n",
      "Epoch: 0187 train_loss= 0.43770 train_acc= 0.54568 val_roc= 0.92625 val_ap= 0.93205 time= 0.08263\n",
      "Epoch: 0188 train_loss= 0.43737 train_acc= 0.54603 val_roc= 0.92673 val_ap= 0.93266 time= 0.08264\n",
      "Epoch: 0189 train_loss= 0.43730 train_acc= 0.54681 val_roc= 0.92735 val_ap= 0.93299 time= 0.07965\n",
      "Epoch: 0190 train_loss= 0.43705 train_acc= 0.54656 val_roc= 0.92732 val_ap= 0.93285 time= 0.08365\n",
      "Epoch: 0191 train_loss= 0.43716 train_acc= 0.54576 val_roc= 0.92672 val_ap= 0.93236 time= 0.08164\n",
      "Epoch: 0192 train_loss= 0.43682 train_acc= 0.54618 val_roc= 0.92641 val_ap= 0.93227 time= 0.08064\n",
      "Epoch: 0193 train_loss= 0.43693 train_acc= 0.54696 val_roc= 0.92631 val_ap= 0.93237 time= 0.08065\n",
      "Epoch: 0194 train_loss= 0.43658 train_acc= 0.54723 val_roc= 0.92706 val_ap= 0.93304 time= 0.07965\n",
      "Epoch: 0195 train_loss= 0.43641 train_acc= 0.54647 val_roc= 0.92724 val_ap= 0.93332 time= 0.07965\n",
      "Epoch: 0196 train_loss= 0.43647 train_acc= 0.54734 val_roc= 0.92705 val_ap= 0.93343 time= 0.08363\n",
      "Epoch: 0197 train_loss= 0.43591 train_acc= 0.54691 val_roc= 0.92676 val_ap= 0.93308 time= 0.08562\n",
      "Epoch: 0198 train_loss= 0.43588 train_acc= 0.54706 val_roc= 0.92552 val_ap= 0.93205 time= 0.08164\n",
      "Epoch: 0199 train_loss= 0.43588 train_acc= 0.54745 val_roc= 0.92513 val_ap= 0.93149 time= 0.08264\n",
      "Epoch: 0200 train_loss= 0.43586 train_acc= 0.54811 val_roc= 0.92598 val_ap= 0.93251 time= 0.08263\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.9042231815906874\n",
      "Test AP score: 0.9178358731387544\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始模型输出 kl散度、loss\n",
    "# VGAE\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66051b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:103: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\jupyter\\gae\\gae\\train_debug.py:106: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\model.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\initializations.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:29: The name tf.sparse_retain is deprecated. Please use tf.sparse.retain instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1719: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\layers.py:80: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:98: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\gae-0.0.1-py3.7.egg\\gae\\optimizer.py:100: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch: 0001 train_loss= 1.82320 train_acc= 0.49874 val_roc= 0.74409 val_ap= 0.76382 time= 2.02444\n",
      "Epoch: 0002 train_loss= 1.82752 train_acc= 0.48856 val_roc= 0.76761 val_ap= 0.78038 time= 0.08529\n",
      "Epoch: 0003 train_loss= 1.81299 train_acc= 0.48564 val_roc= 0.80225 val_ap= 0.80610 time= 0.08165\n",
      "Epoch: 0004 train_loss= 1.81707 train_acc= 0.48787 val_roc= 0.80668 val_ap= 0.80828 time= 0.08265\n",
      "Epoch: 0005 train_loss= 1.75999 train_acc= 0.48558 val_roc= 0.82028 val_ap= 0.82033 time= 0.08107\n",
      "Epoch: 0006 train_loss= 1.74312 train_acc= 0.48353 val_roc= 0.84733 val_ap= 0.83793 time= 0.08064\n",
      "Epoch: 0007 train_loss= 1.68253 train_acc= 0.48179 val_roc= 0.86918 val_ap= 0.85610 time= 0.08961\n",
      "Epoch: 0008 train_loss= 1.72231 train_acc= 0.49071 val_roc= 0.87225 val_ap= 0.86074 time= 0.08064\n",
      "Epoch: 0009 train_loss= 1.66280 train_acc= 0.49375 val_roc= 0.87319 val_ap= 0.86218 time= 0.08364\n",
      "Epoch: 0010 train_loss= 1.60973 train_acc= 0.49338 val_roc= 0.87442 val_ap= 0.86002 time= 0.08166\n",
      "Epoch: 0011 train_loss= 1.60619 train_acc= 0.49700 val_roc= 0.87598 val_ap= 0.86030 time= 0.08264\n",
      "Epoch: 0012 train_loss= 1.57381 train_acc= 0.49527 val_roc= 0.88174 val_ap= 0.86727 time= 0.08280\n",
      "Epoch: 0013 train_loss= 1.56281 train_acc= 0.49672 val_roc= 0.88788 val_ap= 0.87559 time= 0.08294\n",
      "Epoch: 0014 train_loss= 1.56926 train_acc= 0.49301 val_roc= 0.89650 val_ap= 0.88474 time= 0.08200\n",
      "Epoch: 0015 train_loss= 1.59318 train_acc= 0.49632 val_roc= 0.89938 val_ap= 0.88812 time= 0.08102\n",
      "Epoch: 0016 train_loss= 1.55660 train_acc= 0.49966 val_roc= 0.90023 val_ap= 0.88870 time= 0.08098\n",
      "Epoch: 0017 train_loss= 1.54140 train_acc= 0.50163 val_roc= 0.90107 val_ap= 0.88948 time= 0.08402\n",
      "Epoch: 0018 train_loss= 1.51853 train_acc= 0.50058 val_roc= 0.90234 val_ap= 0.89087 time= 0.08307\n",
      "Epoch: 0019 train_loss= 1.56054 train_acc= 0.50009 val_roc= 0.90340 val_ap= 0.89200 time= 0.08090\n",
      "Epoch: 0020 train_loss= 1.51049 train_acc= 0.50023 val_roc= 0.90507 val_ap= 0.89474 time= 0.08203\n",
      "Epoch: 0021 train_loss= 1.50437 train_acc= 0.49961 val_roc= 0.90546 val_ap= 0.89554 time= 0.08199\n",
      "Epoch: 0022 train_loss= 1.54052 train_acc= 0.50093 val_roc= 0.90504 val_ap= 0.89382 time= 0.08352\n",
      "Epoch: 0023 train_loss= 1.53252 train_acc= 0.50087 val_roc= 0.90182 val_ap= 0.88926 time= 0.07997\n",
      "Epoch: 0024 train_loss= 1.50078 train_acc= 0.49936 val_roc= 0.90085 val_ap= 0.88698 time= 0.08200\n",
      "Epoch: 0025 train_loss= 1.50426 train_acc= 0.50012 val_roc= 0.90155 val_ap= 0.88815 time= 0.08199\n",
      "Epoch: 0026 train_loss= 1.49890 train_acc= 0.50102 val_roc= 0.90095 val_ap= 0.88696 time= 0.08299\n",
      "Epoch: 0027 train_loss= 1.47648 train_acc= 0.50123 val_roc= 0.90001 val_ap= 0.88685 time= 0.08407\n",
      "Epoch: 0028 train_loss= 1.48754 train_acc= 0.50081 val_roc= 0.89993 val_ap= 0.88814 time= 0.08196\n",
      "Epoch: 0029 train_loss= 1.49080 train_acc= 0.50085 val_roc= 0.90103 val_ap= 0.88929 time= 0.08202\n",
      "Epoch: 0030 train_loss= 1.46605 train_acc= 0.50119 val_roc= 0.90270 val_ap= 0.89118 time= 0.08102\n",
      "Epoch: 0031 train_loss= 1.45048 train_acc= 0.50186 val_roc= 0.90405 val_ap= 0.89307 time= 0.08303\n",
      "Epoch: 0032 train_loss= 1.49455 train_acc= 0.50175 val_roc= 0.90356 val_ap= 0.89148 time= 0.08401\n",
      "Epoch: 0033 train_loss= 1.48083 train_acc= 0.50064 val_roc= 0.90345 val_ap= 0.89123 time= 0.08348\n",
      "Epoch: 0034 train_loss= 1.46197 train_acc= 0.50263 val_roc= 0.90201 val_ap= 0.88963 time= 0.08086\n",
      "Epoch: 0035 train_loss= 1.46218 train_acc= 0.50282 val_roc= 0.90029 val_ap= 0.88938 time= 0.09010\n",
      "Epoch: 0036 train_loss= 1.46111 train_acc= 0.50062 val_roc= 0.90030 val_ap= 0.88954 time= 0.08194\n",
      "Epoch: 0037 train_loss= 1.47659 train_acc= 0.50199 val_roc= 0.90205 val_ap= 0.89209 time= 0.08300\n",
      "Epoch: 0038 train_loss= 1.49126 train_acc= 0.50189 val_roc= 0.90386 val_ap= 0.89411 time= 0.07999\n",
      "Epoch: 0039 train_loss= 1.48164 train_acc= 0.50154 val_roc= 0.90363 val_ap= 0.89383 time= 0.08259\n",
      "Epoch: 0040 train_loss= 1.48472 train_acc= 0.50114 val_roc= 0.90263 val_ap= 0.89335 time= 0.08150\n",
      "Epoch: 0041 train_loss= 1.49622 train_acc= 0.50209 val_roc= 0.90100 val_ap= 0.89257 time= 0.08097\n",
      "Epoch: 0042 train_loss= 1.46999 train_acc= 0.50096 val_roc= 0.89994 val_ap= 0.89219 time= 0.08200\n",
      "Epoch: 0043 train_loss= 1.46814 train_acc= 0.50167 val_roc= 0.89958 val_ap= 0.89214 time= 0.08906\n",
      "Epoch: 0044 train_loss= 1.44850 train_acc= 0.50060 val_roc= 0.90055 val_ap= 0.89339 time= 0.08196\n",
      "Epoch: 0045 train_loss= 1.48194 train_acc= 0.50078 val_roc= 0.90169 val_ap= 0.89482 time= 0.08301\n",
      "Epoch: 0046 train_loss= 1.48045 train_acc= 0.50035 val_roc= 0.90181 val_ap= 0.89565 time= 0.08199\n",
      "Epoch: 0047 train_loss= 1.47161 train_acc= 0.50211 val_roc= 0.90181 val_ap= 0.89528 time= 0.08301\n",
      "Epoch: 0048 train_loss= 1.51858 train_acc= 0.50149 val_roc= 0.90199 val_ap= 0.89543 time= 0.08353\n",
      "Epoch: 0049 train_loss= 1.51490 train_acc= 0.50090 val_roc= 0.90292 val_ap= 0.89629 time= 0.08294\n",
      "Epoch: 0050 train_loss= 1.45934 train_acc= 0.50299 val_roc= 0.90341 val_ap= 0.89709 time= 0.08300\n",
      "Epoch: 0051 train_loss= 1.47627 train_acc= 0.50083 val_roc= 0.90237 val_ap= 0.89560 time= 0.08467\n",
      "Epoch: 0052 train_loss= 1.51295 train_acc= 0.50049 val_roc= 0.90111 val_ap= 0.89468 time= 0.08961\n",
      "Epoch: 0053 train_loss= 1.48537 train_acc= 0.50150 val_roc= 0.90186 val_ap= 0.89644 time= 0.08172\n",
      "Epoch: 0054 train_loss= 1.45596 train_acc= 0.50320 val_roc= 0.90170 val_ap= 0.89603 time= 0.08292\n",
      "Epoch: 0055 train_loss= 1.48210 train_acc= 0.50278 val_roc= 0.90238 val_ap= 0.89695 time= 0.08206\n",
      "Epoch: 0056 train_loss= 1.42899 train_acc= 0.50185 val_roc= 0.90296 val_ap= 0.89798 time= 0.08147\n",
      "Epoch: 0057 train_loss= 1.43434 train_acc= 0.50155 val_roc= 0.90269 val_ap= 0.89695 time= 0.08151\n",
      "Epoch: 0058 train_loss= 1.43949 train_acc= 0.50192 val_roc= 0.90168 val_ap= 0.89689 time= 0.08301\n",
      "Epoch: 0059 train_loss= 1.47449 train_acc= 0.50080 val_roc= 0.90065 val_ap= 0.89609 time= 0.08194\n",
      "Epoch: 0060 train_loss= 1.43148 train_acc= 0.50060 val_roc= 0.89903 val_ap= 0.89507 time= 0.08306\n",
      "Epoch: 0061 train_loss= 1.45563 train_acc= 0.50170 val_roc= 0.89917 val_ap= 0.89474 time= 0.08199\n",
      "Epoch: 0062 train_loss= 1.47058 train_acc= 0.50208 val_roc= 0.90020 val_ap= 0.89533 time= 0.08203\n",
      "Epoch: 0063 train_loss= 1.50093 train_acc= 0.50216 val_roc= 0.90170 val_ap= 0.89603 time= 0.08147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0064 train_loss= 1.43527 train_acc= 0.50172 val_roc= 0.90341 val_ap= 0.89735 time= 0.08166\n",
      "Epoch: 0065 train_loss= 1.46163 train_acc= 0.50108 val_roc= 0.90379 val_ap= 0.89694 time= 0.08189\n",
      "Epoch: 0066 train_loss= 1.46644 train_acc= 0.50129 val_roc= 0.90303 val_ap= 0.89685 time= 0.08997\n",
      "Epoch: 0067 train_loss= 1.48590 train_acc= 0.50167 val_roc= 0.90123 val_ap= 0.89517 time= 0.08510\n",
      "Epoch: 0068 train_loss= 1.45754 train_acc= 0.50212 val_roc= 0.90014 val_ap= 0.89310 time= 0.08486\n",
      "Epoch: 0069 train_loss= 1.43957 train_acc= 0.50290 val_roc= 0.90066 val_ap= 0.89349 time= 0.08403\n",
      "Epoch: 0070 train_loss= 1.46769 train_acc= 0.50280 val_roc= 0.90264 val_ap= 0.89508 time= 0.08205\n",
      "Epoch: 0071 train_loss= 1.44754 train_acc= 0.50226 val_roc= 0.90548 val_ap= 0.89758 time= 0.08298\n",
      "Epoch: 0072 train_loss= 1.43914 train_acc= 0.50312 val_roc= 0.90643 val_ap= 0.89815 time= 0.08193\n",
      "Epoch: 0073 train_loss= 1.44366 train_acc= 0.50279 val_roc= 0.90678 val_ap= 0.89825 time= 0.08102\n",
      "Epoch: 0074 train_loss= 1.44697 train_acc= 0.50255 val_roc= 0.90672 val_ap= 0.89831 time= 0.08201\n",
      "Epoch: 0075 train_loss= 1.42927 train_acc= 0.50299 val_roc= 0.90562 val_ap= 0.89731 time= 0.08203\n",
      "Epoch: 0076 train_loss= 1.44197 train_acc= 0.50316 val_roc= 0.90477 val_ap= 0.89670 time= 0.08297\n",
      "Epoch: 0077 train_loss= 1.47977 train_acc= 0.50283 val_roc= 0.90383 val_ap= 0.89556 time= 0.08199\n",
      "Epoch: 0078 train_loss= 1.45091 train_acc= 0.50245 val_roc= 0.90348 val_ap= 0.89629 time= 0.08254\n",
      "Epoch: 0079 train_loss= 1.43689 train_acc= 0.50194 val_roc= 0.90418 val_ap= 0.89783 time= 0.08190\n",
      "Epoch: 0080 train_loss= 1.47194 train_acc= 0.50183 val_roc= 0.90363 val_ap= 0.89765 time= 0.08101\n",
      "Epoch: 0081 train_loss= 1.43985 train_acc= 0.50076 val_roc= 0.90353 val_ap= 0.89775 time= 0.08197\n",
      "Epoch: 0082 train_loss= 1.42342 train_acc= 0.50227 val_roc= 0.90350 val_ap= 0.89746 time= 0.08245\n",
      "Epoch: 0083 train_loss= 1.44933 train_acc= 0.50052 val_roc= 0.90332 val_ap= 0.89668 time= 0.08103\n",
      "Epoch: 0084 train_loss= 1.44683 train_acc= 0.50215 val_roc= 0.90321 val_ap= 0.89609 time= 0.08106\n",
      "Epoch: 0085 train_loss= 1.44051 train_acc= 0.50132 val_roc= 0.90332 val_ap= 0.89561 time= 0.08904\n",
      "Epoch: 0086 train_loss= 1.45159 train_acc= 0.50144 val_roc= 0.90473 val_ap= 0.89644 time= 0.08287\n",
      "Epoch: 0087 train_loss= 1.44481 train_acc= 0.50197 val_roc= 0.90574 val_ap= 0.89838 time= 0.08263\n",
      "Epoch: 0088 train_loss= 1.46750 train_acc= 0.50152 val_roc= 0.90713 val_ap= 0.90076 time= 0.08245\n",
      "Epoch: 0089 train_loss= 1.47580 train_acc= 0.50149 val_roc= 0.90674 val_ap= 0.89972 time= 0.08400\n",
      "Epoch: 0090 train_loss= 1.41813 train_acc= 0.50054 val_roc= 0.90609 val_ap= 0.89930 time= 0.08406\n",
      "Epoch: 0091 train_loss= 1.43817 train_acc= 0.50159 val_roc= 0.90543 val_ap= 0.90024 time= 0.08298\n",
      "Epoch: 0092 train_loss= 1.43549 train_acc= 0.50309 val_roc= 0.90358 val_ap= 0.89776 time= 0.08097\n",
      "Epoch: 0093 train_loss= 1.45447 train_acc= 0.50265 val_roc= 0.90207 val_ap= 0.89655 time= 0.08299\n",
      "Epoch: 0094 train_loss= 1.44462 train_acc= 0.50128 val_roc= 0.90220 val_ap= 0.89642 time= 0.08300\n",
      "Epoch: 0095 train_loss= 1.44806 train_acc= 0.50156 val_roc= 0.90192 val_ap= 0.89641 time= 0.08100\n",
      "Epoch: 0096 train_loss= 1.48862 train_acc= 0.50146 val_roc= 0.90260 val_ap= 0.89608 time= 0.08302\n",
      "Epoch: 0097 train_loss= 1.44379 train_acc= 0.50201 val_roc= 0.90293 val_ap= 0.89628 time= 0.08350\n",
      "Epoch: 0098 train_loss= 1.44780 train_acc= 0.50180 val_roc= 0.90334 val_ap= 0.89719 time= 0.08153\n",
      "Epoch: 0099 train_loss= 1.45679 train_acc= 0.50223 val_roc= 0.90263 val_ap= 0.89712 time= 0.08300\n",
      "Epoch: 0100 train_loss= 1.42314 train_acc= 0.50117 val_roc= 0.90113 val_ap= 0.89811 time= 0.08196\n",
      "Epoch: 0101 train_loss= 1.42386 train_acc= 0.50199 val_roc= 0.89948 val_ap= 0.89664 time= 0.08298\n",
      "Epoch: 0102 train_loss= 1.47584 train_acc= 0.50111 val_roc= 0.89985 val_ap= 0.89506 time= 0.08307\n",
      "Epoch: 0103 train_loss= 1.46156 train_acc= 0.49998 val_roc= 0.90066 val_ap= 0.89504 time= 0.08298\n",
      "Epoch: 0104 train_loss= 1.47229 train_acc= 0.50162 val_roc= 0.90170 val_ap= 0.89564 time= 0.08201\n",
      "Epoch: 0105 train_loss= 1.42819 train_acc= 0.50324 val_roc= 0.90279 val_ap= 0.89546 time= 0.08403\n",
      "Epoch: 0106 train_loss= 1.48442 train_acc= 0.50278 val_roc= 0.90413 val_ap= 0.89815 time= 0.08194\n",
      "Epoch: 0107 train_loss= 1.45534 train_acc= 0.50292 val_roc= 0.90580 val_ap= 0.90049 time= 0.08200\n",
      "Epoch: 0108 train_loss= 1.42362 train_acc= 0.50260 val_roc= 0.90695 val_ap= 0.90182 time= 0.08503\n",
      "Epoch: 0109 train_loss= 1.40222 train_acc= 0.50235 val_roc= 0.90731 val_ap= 0.90240 time= 0.08397\n",
      "Epoch: 0110 train_loss= 1.40870 train_acc= 0.50226 val_roc= 0.90675 val_ap= 0.90195 time= 0.08000\n",
      "Epoch: 0111 train_loss= 1.48479 train_acc= 0.50284 val_roc= 0.90640 val_ap= 0.90078 time= 0.08101\n",
      "Epoch: 0112 train_loss= 1.44205 train_acc= 0.50242 val_roc= 0.90643 val_ap= 0.90048 time= 0.08303\n",
      "Epoch: 0113 train_loss= 1.44378 train_acc= 0.50223 val_roc= 0.90698 val_ap= 0.90179 time= 0.08301\n",
      "Epoch: 0114 train_loss= 1.43782 train_acc= 0.50221 val_roc= 0.90708 val_ap= 0.90193 time= 0.08194\n",
      "Epoch: 0115 train_loss= 1.44971 train_acc= 0.50310 val_roc= 0.90817 val_ap= 0.90228 time= 0.08304\n",
      "Epoch: 0116 train_loss= 1.45566 train_acc= 0.50242 val_roc= 0.90909 val_ap= 0.90328 time= 0.08400\n",
      "Epoch: 0117 train_loss= 1.43447 train_acc= 0.50234 val_roc= 0.90982 val_ap= 0.90382 time= 0.08405\n",
      "Epoch: 0118 train_loss= 1.44159 train_acc= 0.50254 val_roc= 0.90934 val_ap= 0.90429 time= 0.09010\n",
      "Epoch: 0119 train_loss= 1.45732 train_acc= 0.50236 val_roc= 0.90818 val_ap= 0.90386 time= 0.08293\n",
      "Epoch: 0120 train_loss= 1.46570 train_acc= 0.50273 val_roc= 0.90818 val_ap= 0.90408 time= 0.08290\n",
      "Epoch: 0121 train_loss= 1.46365 train_acc= 0.50276 val_roc= 0.90841 val_ap= 0.90398 time= 0.08406\n",
      "Epoch: 0122 train_loss= 1.41996 train_acc= 0.50269 val_roc= 0.90713 val_ap= 0.90226 time= 0.08304\n",
      "Epoch: 0123 train_loss= 1.48402 train_acc= 0.50268 val_roc= 0.90639 val_ap= 0.90120 time= 0.08293\n",
      "Epoch: 0124 train_loss= 1.44996 train_acc= 0.50221 val_roc= 0.90395 val_ap= 0.89872 time= 0.08311\n",
      "Epoch: 0125 train_loss= 1.48107 train_acc= 0.50218 val_roc= 0.90301 val_ap= 0.89746 time= 0.08292\n",
      "Epoch: 0126 train_loss= 1.46457 train_acc= 0.50149 val_roc= 0.90361 val_ap= 0.89754 time= 0.08209\n",
      "Epoch: 0127 train_loss= 1.45141 train_acc= 0.50308 val_roc= 0.90454 val_ap= 0.89956 time= 0.08290\n",
      "Epoch: 0128 train_loss= 1.47493 train_acc= 0.50260 val_roc= 0.90477 val_ap= 0.89979 time= 0.08106\n",
      "Epoch: 0129 train_loss= 1.43421 train_acc= 0.50152 val_roc= 0.90413 val_ap= 0.90026 time= 0.08299\n",
      "Epoch: 0130 train_loss= 1.43311 train_acc= 0.50298 val_roc= 0.90273 val_ap= 0.89971 time= 0.08401\n",
      "Epoch: 0131 train_loss= 1.42946 train_acc= 0.50258 val_roc= 0.90155 val_ap= 0.89889 time= 0.08104\n",
      "Epoch: 0132 train_loss= 1.43498 train_acc= 0.50232 val_roc= 0.90058 val_ap= 0.89713 time= 0.08192\n",
      "Epoch: 0133 train_loss= 1.42772 train_acc= 0.50265 val_roc= 0.90033 val_ap= 0.89692 time= 0.08492\n",
      "Epoch: 0134 train_loss= 1.46938 train_acc= 0.50276 val_roc= 0.90097 val_ap= 0.89693 time= 0.08304\n",
      "Epoch: 0135 train_loss= 1.41722 train_acc= 0.50313 val_roc= 0.90204 val_ap= 0.89707 time= 0.08302\n",
      "Epoch: 0136 train_loss= 1.45754 train_acc= 0.50237 val_roc= 0.90329 val_ap= 0.89810 time= 0.08201\n",
      "Epoch: 0137 train_loss= 1.44227 train_acc= 0.50212 val_roc= 0.90458 val_ap= 0.89990 time= 0.08506\n",
      "Epoch: 0138 train_loss= 1.42878 train_acc= 0.50390 val_roc= 0.90622 val_ap= 0.90207 time= 0.08393\n",
      "Epoch: 0139 train_loss= 1.46874 train_acc= 0.50302 val_roc= 0.90604 val_ap= 0.90183 time= 0.08166\n",
      "Epoch: 0140 train_loss= 1.48231 train_acc= 0.50235 val_roc= 0.90568 val_ap= 0.90157 time= 0.08422\n",
      "Epoch: 0141 train_loss= 1.44723 train_acc= 0.50172 val_roc= 0.90571 val_ap= 0.90193 time= 0.08264\n",
      "Epoch: 0142 train_loss= 1.42050 train_acc= 0.50302 val_roc= 0.90530 val_ap= 0.90172 time= 0.08368\n",
      "Epoch: 0143 train_loss= 1.43348 train_acc= 0.50362 val_roc= 0.90432 val_ap= 0.89930 time= 0.08215\n",
      "Epoch: 0144 train_loss= 1.45924 train_acc= 0.50335 val_roc= 0.90432 val_ap= 0.89906 time= 0.08171\n",
      "Epoch: 0145 train_loss= 1.46113 train_acc= 0.50126 val_roc= 0.90575 val_ap= 0.89981 time= 0.08095\n",
      "Epoch: 0146 train_loss= 1.46270 train_acc= 0.50219 val_roc= 0.90739 val_ap= 0.90092 time= 0.08199\n",
      "Epoch: 0147 train_loss= 1.43479 train_acc= 0.50324 val_roc= 0.90762 val_ap= 0.90076 time= 0.08105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0148 train_loss= 1.44816 train_acc= 0.50214 val_roc= 0.90737 val_ap= 0.90205 time= 0.08199\n",
      "Epoch: 0149 train_loss= 1.44230 train_acc= 0.50087 val_roc= 0.90714 val_ap= 0.90184 time= 0.08302\n",
      "Epoch: 0150 train_loss= 1.40675 train_acc= 0.50149 val_roc= 0.90604 val_ap= 0.90118 time= 0.08252\n",
      "Epoch: 0151 train_loss= 1.43018 train_acc= 0.50240 val_roc= 0.90435 val_ap= 0.89893 time= 0.08192\n",
      "Epoch: 0152 train_loss= 1.44901 train_acc= 0.50291 val_roc= 0.90306 val_ap= 0.89663 time= 0.08297\n",
      "Epoch: 0153 train_loss= 1.42898 train_acc= 0.50203 val_roc= 0.90272 val_ap= 0.89690 time= 0.08298\n",
      "Epoch: 0154 train_loss= 1.40532 train_acc= 0.50163 val_roc= 0.90332 val_ap= 0.89766 time= 0.08250\n",
      "Epoch: 0155 train_loss= 1.46644 train_acc= 0.50166 val_roc= 0.90538 val_ap= 0.89889 time= 0.08094\n",
      "Epoch: 0156 train_loss= 1.44956 train_acc= 0.50197 val_roc= 0.90585 val_ap= 0.90033 time= 0.08205\n",
      "Epoch: 0157 train_loss= 1.41104 train_acc= 0.50246 val_roc= 0.90407 val_ap= 0.89841 time= 0.08803\n",
      "Epoch: 0158 train_loss= 1.43078 train_acc= 0.50152 val_roc= 0.90282 val_ap= 0.89623 time= 0.08392\n",
      "Epoch: 0159 train_loss= 1.36223 train_acc= 0.50229 val_roc= 0.90204 val_ap= 0.89620 time= 0.08209\n",
      "Epoch: 0160 train_loss= 1.44188 train_acc= 0.50291 val_roc= 0.90131 val_ap= 0.89671 time= 0.08197\n",
      "Epoch: 0161 train_loss= 1.42538 train_acc= 0.50156 val_roc= 0.90100 val_ap= 0.89741 time= 0.08201\n",
      "Epoch: 0162 train_loss= 1.44701 train_acc= 0.50221 val_roc= 0.90143 val_ap= 0.89780 time= 0.08510\n",
      "Epoch: 0163 train_loss= 1.46496 train_acc= 0.50260 val_roc= 0.90233 val_ap= 0.89884 time= 0.08096\n",
      "Epoch: 0164 train_loss= 1.45589 train_acc= 0.50260 val_roc= 0.90181 val_ap= 0.89783 time= 0.08193\n",
      "Epoch: 0165 train_loss= 1.43932 train_acc= 0.50292 val_roc= 0.90058 val_ap= 0.89723 time= 0.08210\n",
      "Epoch: 0166 train_loss= 1.44312 train_acc= 0.50265 val_roc= 0.89844 val_ap= 0.89581 time= 0.08201\n",
      "Epoch: 0167 train_loss= 1.40828 train_acc= 0.50158 val_roc= 0.89758 val_ap= 0.89685 time= 0.08296\n",
      "Epoch: 0168 train_loss= 1.40745 train_acc= 0.50244 val_roc= 0.89734 val_ap= 0.89754 time= 0.08298\n",
      "Epoch: 0169 train_loss= 1.44191 train_acc= 0.50242 val_roc= 0.89800 val_ap= 0.89740 time= 0.08200\n",
      "Epoch: 0170 train_loss= 1.45040 train_acc= 0.50245 val_roc= 0.89847 val_ap= 0.89725 time= 0.08100\n",
      "Epoch: 0171 train_loss= 1.43183 train_acc= 0.50301 val_roc= 0.89916 val_ap= 0.89782 time= 0.08200\n",
      "Epoch: 0172 train_loss= 1.44627 train_acc= 0.50239 val_roc= 0.89941 val_ap= 0.89797 time= 0.08300\n",
      "Epoch: 0173 train_loss= 1.43671 train_acc= 0.50224 val_roc= 0.89941 val_ap= 0.89686 time= 0.08251\n",
      "Epoch: 0174 train_loss= 1.40893 train_acc= 0.50285 val_roc= 0.89791 val_ap= 0.89402 time= 0.08193\n",
      "Epoch: 0175 train_loss= 1.44790 train_acc= 0.50313 val_roc= 0.89675 val_ap= 0.89347 time= 0.08300\n",
      "Epoch: 0176 train_loss= 1.45598 train_acc= 0.50307 val_roc= 0.89456 val_ap= 0.89352 time= 0.08399\n",
      "Epoch: 0177 train_loss= 1.42980 train_acc= 0.50238 val_roc= 0.89438 val_ap= 0.89449 time= 0.09052\n",
      "Epoch: 0178 train_loss= 1.46783 train_acc= 0.50209 val_roc= 0.89586 val_ap= 0.89755 time= 0.08397\n",
      "Epoch: 0179 train_loss= 1.45138 train_acc= 0.50316 val_roc= 0.89729 val_ap= 0.89912 time= 0.08294\n",
      "Epoch: 0180 train_loss= 1.40347 train_acc= 0.50313 val_roc= 0.89761 val_ap= 0.89930 time= 0.08213\n",
      "Epoch: 0181 train_loss= 1.40947 train_acc= 0.50243 val_roc= 0.89834 val_ap= 0.89963 time= 0.08597\n",
      "Epoch: 0182 train_loss= 1.48057 train_acc= 0.50239 val_roc= 0.89886 val_ap= 0.89936 time= 0.08217\n",
      "Epoch: 0183 train_loss= 1.42996 train_acc= 0.50323 val_roc= 0.89833 val_ap= 0.89841 time= 0.08284\n",
      "Epoch: 0184 train_loss= 1.41467 train_acc= 0.50300 val_roc= 0.89946 val_ap= 0.89880 time= 0.08202\n",
      "Epoch: 0185 train_loss= 1.46184 train_acc= 0.50272 val_roc= 0.90025 val_ap= 0.89951 time= 0.08295\n",
      "Epoch: 0186 train_loss= 1.42524 train_acc= 0.50315 val_roc= 0.90143 val_ap= 0.90091 time= 0.08301\n",
      "Epoch: 0187 train_loss= 1.47937 train_acc= 0.50258 val_roc= 0.90177 val_ap= 0.90208 time= 0.08402\n",
      "Epoch: 0188 train_loss= 1.42224 train_acc= 0.50295 val_roc= 0.90264 val_ap= 0.90347 time= 0.08199\n",
      "Epoch: 0189 train_loss= 1.42300 train_acc= 0.50157 val_roc= 0.90327 val_ap= 0.90318 time= 0.08193\n",
      "Epoch: 0190 train_loss= 1.46636 train_acc= 0.50279 val_roc= 0.90376 val_ap= 0.90338 time= 0.08307\n",
      "Epoch: 0191 train_loss= 1.44064 train_acc= 0.50359 val_roc= 0.90351 val_ap= 0.90300 time= 0.09795\n",
      "Epoch: 0192 train_loss= 1.43981 train_acc= 0.50377 val_roc= 0.90299 val_ap= 0.90300 time= 0.08309\n",
      "Epoch: 0193 train_loss= 1.44370 train_acc= 0.50323 val_roc= 0.90351 val_ap= 0.90359 time= 0.08197\n",
      "Epoch: 0194 train_loss= 1.44297 train_acc= 0.50194 val_roc= 0.90512 val_ap= 0.90371 time= 0.08106\n",
      "Epoch: 0195 train_loss= 1.41415 train_acc= 0.50238 val_roc= 0.90653 val_ap= 0.90499 time= 0.08292\n",
      "Epoch: 0196 train_loss= 1.44073 train_acc= 0.50387 val_roc= 0.90779 val_ap= 0.90526 time= 0.08302\n",
      "Epoch: 0197 train_loss= 1.43944 train_acc= 0.50404 val_roc= 0.90841 val_ap= 0.90446 time= 0.07999\n",
      "Epoch: 0198 train_loss= 1.40487 train_acc= 0.50255 val_roc= 0.90809 val_ap= 0.90410 time= 0.08161\n",
      "Epoch: 0199 train_loss= 1.42462 train_acc= 0.50146 val_roc= 0.90729 val_ap= 0.90402 time= 0.08864\n",
      "Epoch: 0200 train_loss= 1.41486 train_acc= 0.50287 val_roc= 0.90627 val_ap= 0.90479 time= 0.07976\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.8814581840571204\n",
      "Test AP score: 0.8890479538487895\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (200,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mE:\\jupyter\\gae\\gae\\train_debug.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0my4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_roc_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Train Loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'o-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Train Accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Results'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2840\u001b[0m     return gca().plot(\n\u001b[0;32m   2841\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2842\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\vgae-tf\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (200,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CMGAE\n",
    "%run train_debug.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进损失函数后的 GCMAE\n",
    "%run train_debug.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vgae-tf] *",
   "language": "python",
   "name": "conda-env-vgae-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
